[{"id": 1678497, "name": "Failed to create query error when upgrading external metastore to Unity Catalog", "views": 893, "accessibility": 1, "description": "The \"Create query for upgrade\" command only works when run on a warehouse in Data Explorer.", "codename": "failed-to-create-query-error-when-upgrading-external-metastore-to-unity-catalog", "created_at": "2022-12-21T18:38:39.377Z", "updated_at": "2023-03-15T19:26:53.629Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.8156f4b3d72e9d8f0f376d27d74193c7\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to migrate your default Hive metastore to Unity Catalog by following the steps in the <strong>Upgrade a schema or multiple tables to Unity Catalog</strong> (<a href=\"https://docs.databricks.com/data-governance/unity-catalog/migrate.html#upgrade-a-schema-or-multiple-tables-to-unity-catalog\" rel=\"noopener noreferrer\" target=\"_blank\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/data-governance/unity-catalog/migrate#upgrade-a-schema-or-multiple-tables-to-unity-catalog\" rel=\"noopener noreferrer\" target=\"_blank\">Azure</a>) documentation.</p><p>You have created the necessary <strong>storage credential</strong> (<a href=\"https://docs.databricks.com/data-governance/unity-catalog/manage-external-locations-and-credentials.html#create-a-storage-credential\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/data-governance/unity-catalog/manage-external-locations-and-credentials#--create-a-storage-credential\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a>) as well as the <strong>external location</strong> (<a href=\"https://docs.databricks.com/data-governance/unity-catalog/manage-external-locations-and-credentials.html#create-an-external-location\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/data-governance/unity-catalog/manage-external-locations-and-credentials#--create-an-external-location\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a>). Permissions are correct. You click <strong>Create Query for Upgrade</strong> and try to run the generated query, but it generates a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Failed to create query</span> error.</p><pre>Failed to create query try again {\"message\": \"Internal Server Error\"}</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Create query for upgrade</span> only works when running the command on a warehouse. If you select a cluster instead of a warehouse in <strong>Data Explorer</strong>, the upgrade query fails to run.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Ensure you select a warehouse from the drop down menu in the upper right corner of the <strong>Data Explorer</strong> page before you try to run an upgrade query.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1677798840777-data-explorer-warehouse-cluster-menu.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Cluster and warehouse drop down menu in Data Explorer.\"></p>", "body_txt": "Problem You are trying to migrate your default Hive metastore to Unity Catalog by following the steps in the Upgrade a schema or multiple tables to Unity Catalog (AWS | Azure) documentation. You have created the necessary storage credential (AWS | Azure) as well as the external location (AWS | Azure). Permissions are correct. You click Create Query for Upgrade and try to run the generated query, but it generates a Failed to create query error. Failed to create query try again {\"message\": \"Internal Server Error\"} Cause Create query for upgrade only works when running the command on a warehouse. If you select a cluster instead of a warehouse in Data Explorer, the upgrade query fails to run. Solution Ensure you select a warehouse from the drop down menu in the upper right corner of the Data Explorer page before you try to run an upgrade query.", "format": "html", "updated_at": "2023-03-15T19:26:53.617Z"}, "author": {"id": 818781, "email": "atanu.sarkar@databricks.com", "name": "Atanu.Sarkar ", "first_name": "Atanu.Sarkar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-05T13:43:53.249Z", "updated_at": "2023-03-24T21:50:55.464Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256862, "name": "Metastore", "codename": "metastore", "accessibility": 1, "description": "These articles can help you manage your Apache Hive Metastore for Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3085391, "name": "aws"}, {"id": 3085392, "name": "azure"}, {"id": 3056857, "name": "metastore"}, {"id": 3085394, "name": "sql"}, {"id": 3085393, "name": "sql warehouse"}, {"id": 3085398, "name": "uc"}, {"id": 3085399, "name": "unity catalog"}, {"id": 3056858, "name": "upgrade"}, {"id": 3085395, "name": "warehouse"}], "url": "https://kb.databricks.com/metastore/failed-to-create-query-error-when-upgrading-external-metastore-to-unity-catalog"}, {"id": 1673512, "name": "Delta Live Tables pipelines are not running VACUUM automatically", "views": 2079, "accessibility": 1, "description": "You must have a maintenance cluster defined for VACUUM to run automatically.", "codename": "delta-live-tables-pipelines-are-not-running-vacuum-automatically", "created_at": "2022-12-15T22:40:30.226Z", "updated_at": "2023-02-02T23:13:36.565Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"908ce5175ff16\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.bd9adf46117270e85850788010b28b2b\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Delta Live Tables supports auto-vacuum by default. You setup a Delta Live Tables pipeline, but notice <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> is not running automatically.\u00a0</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><div id=\"isPasted\"><p id=\"isPasted\">A Delta Live Tables pipeline needs a separate maintenance <strong>cluster configuration</strong> (<a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-configuration.html#cluster-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/workflows/delta-live-tables/delta-live-tables-configuration#--cluster-configuration\" rel=\"noopener noreferrer\" target=\"_blank\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/workflows/delta-live-tables/delta-live-tables-configuration.html#cluster-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>) inside the pipeline settings to ensure <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> runs automatically. If the maintenance cluster is not specified within the pipeline JSON file or if the maintenance cluster does not have access to your storage location, then <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> does not run.</p></div><h2 data-toc=\"true\" id=\"example-configuration-2\">Example configuration</h2><p>In this example Delta Live Tables pipeline JSON file, there is a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">default</span> label which identifies the configuration for the default cluster. This should also contain a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">maintenance</span> label that identifies the configuration for the maintenance cluster.</p><p>Since the maintenance cluster configuration is not present, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> does not automatically run.</p><div class=\"f-accordion-panel\" data-controller=\"accordion\">\n<h2>AWS</h2>\n<pre id=\"isPasted\">{\r\n\u00a0 \"clusters\": [\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"default\",\r\n\u00a0 \u00a0 \u00a0 \"node_type_id\": \"c5.4xlarge\",\r\n\u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"c5.4xlarge\",\r\n\u00a0 \u00a0 \u00a0 \"num_workers\": 20,\r\n\u00a0 \u00a0 \u00a0 \"spark_conf\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\"\r\n\u00a0 \u00a0 \u00a0 },\r\n\u00a0 \u00a0 \u00a0 \"aws_attributes\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"instance_profile_arn\": \"arn:aws:...\"\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 }\r\n  ]\r\n}</pre>\n<i class=\"far fa-arrows-alt ui-sortable-handle\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-accordion fa-trash-alt\" contenteditable=\"false\" data-action=\"accordion#delete\" href=\"#\">Delete</a>\n</div><div class=\"f-accordion-panel\" data-controller=\"accordion\">\n<h2>Azure</h2>\n<pre id=\"isPasted\">{\r\n\u00a0 \"clusters\": [\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"default\",\r\n\u00a0 \u00a0 \u00a0 \"node_type_id\": \"Standard_D3_v2\",\r\n\u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"Standard_D3_v2\",\r\n\u00a0 \u00a0 \u00a0 \"num_workers\": 20,\r\n\u00a0 \u00a0 \u00a0 \"spark_conf\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\"\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 }\r\n\u00a0 ]\r\n}</pre>\n<i class=\"far fa-arrows-alt ui-sortable-handle\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-accordion fa-trash-alt\" contenteditable=\"false\" data-action=\"accordion#delete\" href=\"#\">Delete</a>\n</div><div class=\"f-accordion-panel\" data-controller=\"accordion\">\n<h2>GCP</h2>\n<pre id=\"isPasted\">{\r\n\u00a0 \"clusters\": [\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"default\",\r\n\u00a0 \u00a0 \u00a0 \"node_type_id\": \"n1-standard-4\",\r\n\u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"n1-standard-4\",\r\n\u00a0 \u00a0 \u00a0 \"num_workers\": 20,\r\n\u00a0 \u00a0 \u00a0 \"spark_conf\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\"\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 }\r\n\u00a0 ]\r\n}</pre>\n<i class=\"far fa-arrows-alt ui-sortable-handle\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-accordion fa-trash-alt\" contenteditable=\"false\" data-action=\"accordion#delete\" href=\"#\">Delete</a>\n</div><h1 data-toc=\"true\" id=\"solution-3\">Solution</h1><p>Configure a maintenance cluster in the Delta Live Tables pipeline JSON file.</p><p>You have to specify configurations for two different cluster types:</p><ul>\n<li>A default cluster where all processing is performed.</li>\n<li>A maintenance cluster where daily maintenance tasks are run.\u00a0</li>\n</ul><p>Each cluster is identified using the label field.</p><p>The maintenance cluster is responsible for performing <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> and other maintenance tasks.</p><div class=\"f-accordion-panel\" data-controller=\"accordion\">\n<h2>AWS</h2>\n<pre id=\"isPasted\">{\r\n\u00a0 \"clusters\": [\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"default\",\r\n\u00a0 \u00a0 \u00a0 \"node_type_id\": \"&lt;instance-type&gt;\",\r\n\u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"&lt;instance-type&gt;\",\r\n\u00a0 \u00a0 \u00a0 \"num_workers\": 20,\r\n\u00a0 \u00a0 \u00a0 \"spark_conf\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\"\r\n\u00a0 \u00a0 \u00a0 },\r\n\u00a0 \u00a0 \u00a0 \"aws_attributes\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"instance_profile_arn\": \"arn:aws:...\"\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 },\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"maintenance\",\r\n\u00a0 \u00a0 \u00a0 \"aws_attributes\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"instance_profile_arn\": \"arn:aws:...\"\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 }\r\n\u00a0 ]\r\n}</pre>\n<div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">If the maintenance cluster requires access to storage via an instance profile, you need to specify it with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">instance_profile_arn</span>.</p>\n</div>\n</div>\n<i class=\"far fa-arrows-alt ui-sortable-handle\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-accordion fa-trash-alt\" contenteditable=\"false\" data-action=\"accordion#delete\" href=\"#\">Delete</a>\n</div><div class=\"f-accordion-panel\" data-controller=\"accordion\">\n<h2>Azure</h2>\n<pre id=\"isPasted\">{\r\n\u00a0 \"clusters\": [\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"default\",\r\n\u00a0 \u00a0 \u00a0 \"node_type_id\": \"Standard_D3_v2\",\r\n\u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"Standard_D3_v2\",\r\n\u00a0 \u00a0 \u00a0 \"num_workers\": 20,\r\n\u00a0 \u00a0 \u00a0 \"spark_conf\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\"\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 },\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"maintenance\"\r\n\u00a0 \u00a0 }\r\n\u00a0 ]\r\n}</pre>\n<div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">If you need to use Azure Data Lake Storage credential passthrough, or another configuration to access your storage location, specify it for both the default cluster and the maintenance cluster.</p>\n</div>\n</div>\n<i class=\"far fa-arrows-alt ui-sortable-handle\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-accordion fa-trash-alt\" contenteditable=\"false\" data-action=\"accordion#delete\" href=\"#\">Delete</a>\n</div><div class=\"f-accordion-panel\" data-controller=\"accordion\">\n<h2>GCP</h2>\n<pre id=\"isPasted\">{\r\n\u00a0 \"clusters\": [\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"default\",\r\n\u00a0 \u00a0 \u00a0 \"node_type_id\": \"n1-standard-4\",\r\n\u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"n1-standard-4\",\r\n\u00a0 \u00a0 \u00a0 \"num_workers\": 20,\r\n\u00a0 \u00a0 \u00a0 \"spark_conf\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\"\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 },\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"label\": \"maintenance\"\r\n\u00a0 \u00a0 }\r\n\u00a0 ]\r\n}</pre>\n<div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">When using cluster policies to configure Delta Live Tables clusters, you should apply a single policy to both the default and maintenance clusters.</p>\n</div>\n</div>\n<i class=\"far fa-arrows-alt ui-sortable-handle\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-accordion fa-trash-alt\" contenteditable=\"false\" data-action=\"accordion#delete\" href=\"#\">Delete</a>\n</div><p><br></p>", "body_txt": "Problem Delta Live Tables supports auto-vacuum by default. You setup a Delta Live Tables pipeline, but notice VACUUM is not running automatically.\u00a0 Cause A Delta Live Tables pipeline needs a separate maintenance cluster configuration (AWS | Azure | GCP) inside the pipeline settings to ensure VACUUM runs automatically. If the maintenance cluster is not specified within the pipeline JSON file or if the maintenance cluster does not have access to your storage location, then VACUUM does not run. Example configuration In this example Delta Live Tables pipeline JSON file, there is a default label which identifies the configuration for the default cluster. This should also contain a maintenance label that identifies the configuration for the maintenance cluster. Since the maintenance cluster configuration is not present, VACUUM does not automatically run. AWS\n{ \u00a0 \"clusters\": [ \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"default\", \u00a0 \u00a0 \u00a0 \"node_type_id\": \"c5.4xlarge\", \u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"c5.4xlarge\", \u00a0 \u00a0 \u00a0 \"num_workers\": 20, \u00a0 \u00a0 \u00a0 \"spark_conf\": { \u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\" \u00a0 \u00a0 \u00a0 }, \u00a0 \u00a0 \u00a0 \"aws_attributes\": { \u00a0 \u00a0 \u00a0 \u00a0 \"instance_profile_arn\": \"arn:aws:...\" \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 } ] } Azure\n{ \u00a0 \"clusters\": [ \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"default\", \u00a0 \u00a0 \u00a0 \"node_type_id\": \"Standard_D3_v2\", \u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"Standard_D3_v2\", \u00a0 \u00a0 \u00a0 \"num_workers\": 20, \u00a0 \u00a0 \u00a0 \"spark_conf\": { \u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\" \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 } \u00a0 ] } GCP\n{ \u00a0 \"clusters\": [ \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"default\", \u00a0 \u00a0 \u00a0 \"node_type_id\": \"n1-standard-4\", \u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"n1-standard-4\", \u00a0 \u00a0 \u00a0 \"num_workers\": 20, \u00a0 \u00a0 \u00a0 \"spark_conf\": { \u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\" \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 } \u00a0 ] } Solution Configure a maintenance cluster in the Delta Live Tables pipeline JSON file. You have to specify configurations for two different cluster types: A default cluster where all processing is performed.\nA maintenance cluster where daily maintenance tasks are run.\u00a0 Each cluster is identified using the label field. The maintenance cluster is responsible for performing VACUUM and other maintenance tasks. AWS\n{ \u00a0 \"clusters\": [ \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"default\", \u00a0 \u00a0 \u00a0 \"node_type_id\": \"&lt;instance-type&gt;\", \u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"&lt;instance-type&gt;\", \u00a0 \u00a0 \u00a0 \"num_workers\": 20, \u00a0 \u00a0 \u00a0 \"spark_conf\": { \u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\" \u00a0 \u00a0 \u00a0 }, \u00a0 \u00a0 \u00a0 \"aws_attributes\": { \u00a0 \u00a0 \u00a0 \u00a0 \"instance_profile_arn\": \"arn:aws:...\" \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 }, \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"maintenance\", \u00a0 \u00a0 \u00a0 \"aws_attributes\": { \u00a0 \u00a0 \u00a0 \u00a0 \"instance_profile_arn\": \"arn:aws:...\" \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 } \u00a0 ] } Info\nIf the maintenance cluster requires access to storage via an instance profile, you need to specify it with instance_profile_arn. Azure\n{ \u00a0 \"clusters\": [ \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"default\", \u00a0 \u00a0 \u00a0 \"node_type_id\": \"Standard_D3_v2\", \u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"Standard_D3_v2\", \u00a0 \u00a0 \u00a0 \"num_workers\": 20, \u00a0 \u00a0 \u00a0 \"spark_conf\": { \u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\" \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 }, \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"maintenance\" \u00a0 \u00a0 } \u00a0 ] } Info\nIf you need to use Azure Data Lake Storage credential passthrough, or another configuration to access your storage location, specify it for both the default cluster and the maintenance cluster. GCP\n{ \u00a0 \"clusters\": [ \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"default\", \u00a0 \u00a0 \u00a0 \"node_type_id\": \"n1-standard-4\", \u00a0 \u00a0 \u00a0 \"driver_node_type_id\": \"n1-standard-4\", \u00a0 \u00a0 \u00a0 \"num_workers\": 20, \u00a0 \u00a0 \u00a0 \"spark_conf\": { \u00a0 \u00a0 \u00a0 \u00a0 \"spark.databricks.io.parquet.nativeReader.enabled\": \"false\" \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 }, \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"label\": \"maintenance\" \u00a0 \u00a0 } \u00a0 ] } Info\nWhen using cluster policies to configure Delta Live Tables clusters, you should apply a single policy to both the default and maintenance clusters.", "format": "html", "updated_at": "2023-02-02T23:13:36.553Z"}, "author": {"id": 973288, "email": "priyanka.biswas@databricks.com", "name": "priyanka.biswas ", "first_name": "priyanka.biswas", "last_name": "", "role_id": "draft_writer", "created_at": "2022-08-16T20:29:27.862Z", "updated_at": "2023-04-18T21:06:47.768Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256849, "name": "Delta Lake", "codename": "delta", "accessibility": 1, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3009281, "name": "aws"}, {"id": 3009282, "name": "azure"}, {"id": 3009284, "name": "delta"}, {"id": 3009285, "name": "delta live tables"}, {"id": 3009286, "name": "delta tables"}, {"id": 3009283, "name": "gcp"}], "url": "https://kb.databricks.com/delta/delta-live-tables-pipelines-are-not-running-vacuum-automatically"}, {"id": 1672550, "name": "Add custom tags to a Delta Live Tables pipeline", "views": 1429, "accessibility": 1, "description": "Manually edit the JSON configuration file to add custom tags.", "codename": "add-custom-tags-to-a-delta-live-tables-pipeline", "created_at": "2022-12-15T04:12:34.437Z", "updated_at": "2023-02-24T23:17:15.577Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.ac2669f61fd287dfb8d87784c0747ba4\"></div><p><span style=\"background-color:rgb(255,255,255);color:rgb(23,43,77);font-size:16px;\"><span style='-webkit-text-stroke-width:0px;display:inline !important;float:none;font-family:-apple-system, \"system-ui\", \"Segoe UI\", Roboto, Oxygen, Ubuntu, \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif;font-style:normal;font-variant-caps:normal;font-variant-ligatures:normal;font-weight:400;letter-spacing:-0.08px;orphans:2;text-align:start;text-decoration-color:initial;text-decoration-style:initial;text-decoration-thickness:initial;text-indent:0px;text-transform:none;white-space:pre-wrap;widows:2;word-spacing:0px;'>When managing Delta Live Tables pipelines on your clusters, you may want to use custom tags for internal tracking. For example, you may want to use tags to allocate cost across different departments. Or your organization might have a global cluster policy that requires tags on the instances. Failure to comply with a cluster policy can result in cluster start up failures.</span></span></p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><p>The Create Pipeline UI does not have an option to add additional tags. Instead, you must add custom tags manually, by editing the JSON configuration.\u00a0</p><ol>\n<li>Click <strong>Workflows</strong> in the left sidebar menu.</li>\n<li>Click <strong>Delta Live Tables</strong>.</li>\n<li>Click <strong>Create Pipeline</strong>.</li>\n<li>Enter your pipeline information in the UI.</li>\n<li>Click <strong>JSON</strong> in the upper right to switch to the JSON view.</li>\n<li>Add your custom tags to the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">clusters</span> section of the JSON file. The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">custom_tags</span> block should be placed right below the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">label</span> block.</li>\n</ol><pre class=\"language-plain \">{\r\n\u00a0 \u00a0 \"clusters\": [\r\n\u00a0 \u00a0 \u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"label\": \"default\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"custom_tags\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"&lt;custom-tag-name1&gt;\": \"&lt;custom-tag-value1&gt;\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"&lt;custom-tag-name2&gt;\": \"&lt;custom-tag-value2&gt;\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"&lt;custom-tag-name3&gt;\": \"&lt;custom-tag-value3&gt;\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"autoscale\": {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min_workers\": 1,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max_workers\": 5,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mode\": \"ENHANCED\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 ],\r\n\u00a0 \u00a0 \"development\": true,\r\n\u00a0 \u00a0 \"continuous\": false,\r\n\u00a0 \u00a0 \"channel\": \"CURRENT\",\r\n\u00a0 \u00a0 \"edition\": \"ADVANCED\",\r\n\u00a0 \u00a0 \"photon\": false,\r\n\u00a0 \u00a0 \"libraries\": []\r\n}</pre><h2 data-toc=\"true\" id=\"example-1\">Example</h2><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1677012314302-custom-tag-pipeline.gif\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><p>3. For the existing pipelines: click on Delta Live Tables tables UI and select the desired pipeline. Click on settings tab and switch to JSON view as show above.\u00a0</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt notranslate\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-2\">Info</h3>\n<p class=\"hj-alert-text\">You can add custom tags to an existing pipeline by editing the settings and using the JSON view to add the tag information. The JSON view can be used to add or edit any cluster property without using the UI.</p>\n</div>\n</div><p>\u00a0</p>", "body_txt": "When managing Delta Live Tables pipelines on your clusters, you may want to use custom tags for internal tracking. For example, you may want to use tags to allocate cost across different departments. Or your organization might have a global cluster policy that requires tags on the instances. Failure to comply with a cluster policy can result in cluster start up failures. Instructions The Create Pipeline UI does not have an option to add additional tags. Instead, you must add custom tags manually, by editing the JSON configuration.\u00a0 Click Workflows in the left sidebar menu.\nClick Delta Live Tables.\nClick Create Pipeline.\nEnter your pipeline information in the UI.\nClick JSON in the upper right to switch to the JSON view.\nAdd your custom tags to the clusters section of the JSON file. The custom_tags block should be placed right below the label block. { \u00a0 \u00a0 \"clusters\": [ \u00a0 \u00a0 \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"label\": \"default\", \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"custom_tags\": { \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"&lt;custom-tag-name1&gt;\": \"&lt;custom-tag-value1&gt;\", \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"&lt;custom-tag-name2&gt;\": \"&lt;custom-tag-value2&gt;\", \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"&lt;custom-tag-name3&gt;\": \"&lt;custom-tag-value3&gt;\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"autoscale\": { \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min_workers\": 1, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max_workers\": 5, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mode\": \"ENHANCED\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 ], \u00a0 \u00a0 \"development\": true, \u00a0 \u00a0 \"continuous\": false, \u00a0 \u00a0 \"channel\": \"CURRENT\", \u00a0 \u00a0 \"edition\": \"ADVANCED\", \u00a0 \u00a0 \"photon\": false, \u00a0 \u00a0 \"libraries\": [] } Example 3. For the existing pipelines: click on Delta Live Tables tables UI and select the desired pipeline. Click on settings tab and switch to JSON view as show above.\u00a0 Info\nYou can add custom tags to an existing pipeline by editing the settings and using the JSON view to add the tag information. The JSON view can be used to add or edit any cluster property without using the UI. \u00a0", "format": "html", "updated_at": "2023-02-24T23:17:15.563Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256850, "name": "Jobs", "codename": "jobs", "accessibility": 1, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3040262, "name": "aws"}, {"id": 3040263, "name": "azure"}, {"id": 3040271, "name": "delta"}, {"id": 3040270, "name": "delta live tables"}, {"id": 3040272, "name": "dlt"}, {"id": 3040266, "name": "gcp"}, {"id": 3040275, "name": "pipeline"}], "url": "https://kb.databricks.com/jobs/add-custom-tags-to-a-delta-live-tables-pipeline"}, {"id": 1665799, "name": "Remount a storage account after rotating access keys", "views": 3133, "accessibility": 1, "description": "Cannot access storage after rotating access keys until all mount points using the account have been remounted.", "codename": "remount-storage-after-rotate-access-key", "created_at": "2022-12-09T10:32:18.892Z", "updated_at": "2022-12-09T10:41:10.755Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSs3MXJHZGd3a29OeGR2a0hkR0ZQRXkvNFVaMjZmL2FrNm1JZklkNm5yQkJ3QXkrcU4wCmpCTXJFRmxGbk1XOXBJaUllYkRlanNTOWFBL1YzeU5DcXREVTdzOHhqOXYvVTlZR3hUa3ZxWHYrcWMwZApoWGc1YjdmZGk3TGx5M0FhRzVtdzF1ay9ZMWQrUy9pSWJqRHN4ME1OZXo1VkxmelorS2FaUUszdUpXNEMKV3Q2OGNBTjU4TCtmUkU1cDhhclU2N3JKM3NUd2hHT0xRR2hiS0lhSU9JSk1QbFFoMFRtRzluNGJickhGCjFkdnNxaVVlaDZCSjhGQStmOU5hcHJWSUlDdVlyR0c3SStSNDZ2NXBYamk2RFJtaHlHeHdBb09JMzk5egozZGZpVCtFcXVwc1dUTnBlaHRETGNLYzJ2VTNtSkJzV0xNYy9Gc0JVdVdzb3RuRVFEVlJITEx1NGhESGUKR2h2ZkxvUWtrazJBUXAvUWtpUmtJcHhDby9zWlVrMThEWXBkTW9OdHNSOWNoZGkrcHF0d2cvM1BkRWwxCnlPN0lRTkVvcUJYVmlWYjh2YlFPNFhXREVSYTErMG5tb3RtSC9OWCs4MnIyNERvL1BCdUlzRlUzUkZ6KwpuMDZXUVZUWUQvbEU3NjRRME1kQUVOZWxlYjFZNEYxbkliN3V3RnNYT3FaZ2F5dXoveFZaUit2VzNDbDEKT2NXeE9uNXQySmNmNDYxdVozVnhmTTNzVjJxMFNhNUJxT0g1cmJKZ0hOSklKTUtQSTRSSnZyQWhCUENrCllNbDZ5Z2JUczlVb0hZR0lBa25VNnppMDJoWGgzMmUzQUUyaXlpR3RqWXFMYi9wdXJnU0xFOWVTdldKawp1OWxhWmtNQzRzTmxuaCtpWU9lblZwYkxCdnpkNVdWSlJKSUZTMFlmZHo2b3hZU3pNQXFsd21qelFaeWIKVnBMYS9MdFcwOEd6ME5xYkZxWVZ1RUtNVW96SHl0cmg5ZlZBczhqVmtwcWQ3RXlVcUVtSlRMb2hwUnN2CnYrTWpNbGVuRkphKzdLZWtPQy9xMDVLYlgramwrQXlyc2Q2VHBQOUJoTEZTS29WREowWVlocktHbWVMNQp2UXlSVXFmWFNaUzNhVldMV0lQN2hGTFcxTVRlNlJ3Yk9uV0FqOFlDcVdvZHBpZ05vUWZZVHdiL2xTWXcKZU54Y3BDTDBxdFo4d2JIUkRXOC9rV1NJcHM4c1dRMENyR3hPTmxTY2dsL0F0eFEyeTRmbXdQMUhGN3VPCmtmdW1KQTE3NlFBN21aaXFvUjBkTnpWNktYY3c2UjJiK2U4MFNrbG5uK0gxWldFSTNXQXFveVpScGtPMgpsdW9HSENWa2s1RGl4cWw2UHRRek15cERxWTZzOERrMStORWR2Ykx5Wk5QaTM4WFpYNFlBU0tvUVV2b3AKcGVhbyt3dU1XOFVXMmlSa3hnMW1HSWJrcSt2Wjh4Uno3WlB6WHRsaFRyL1BlT3drTlBHMzJDYWNsVy93CjBPcjBjTnRZc2RZNlJsTkhJaDN0NEo2dkhSd3FoZnFNTmtSVWM5bk8yb3FFODZQK0l5SGdYT3ozdUk1Nwo0TUJoLzZ6NmMza1haRTByOVhwM1J6S1k5SWJWZmhCVUNDaWszUlpLdjFqSjc5N0RxZlE4VGpaOU0rN3QKRDZIbE9RaFpjaXQ0YnNGaC9pcTlORzZ5RyttbnhwZnV0aHBlTkZ4K0RlYVpEYnluaWdBdFBXUm93aWJhCkhZc1dad1BZeWlLZlp5LzlSR2ozWG5vaU1zQ0hpTUx2cnZvS0tJMXhXendudm9YZjBuNkJxU1JBOWRuMAo0eU9NaUxuaDBsbGVRU2dVZDZ0RFhYRWRRZ2U4WFFCWm1IbzRCZGFzU0E3MFdjWlYzUT09Cg==.a08ddc4976163009cbc29250b88f59d2\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You have blob storage associated with a storage account mounted, but are unable to access it after access keys are rotated.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">There are multiple mount points using the same storage account.</p><p>Remounting some, but not all, of the mount points with new access keys results in access issues.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><ol>\n<li id=\"isPasted\">Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.fs.mounts()</span> to check all mount points. Review the <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/databricks-utils#--mounts-command-dbutilsfsmounts\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">dbutils.fs.mounts() documentation</a> for usage details.</li>\n<li>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.fs.unmount()</span> to unmount all storage accounts. Review the <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/databricks-utils#--unmount-command-dbutilsfsunmount\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">dbutils.fs.unmount() documentation</a> for usage details.</li>\n<li>Restart the cluster.</li>\n<li>Remount the storage account with new keys. Review the <a href=\"https://learn.microsoft.com/azure/databricks/external-data/azure-storage\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure Data Lake Storage Gen2 and Blob Storage</a> documentation for usage details.</li>\n</ol>", "body_txt": "Problem You have blob storage associated with a storage account mounted, but are unable to access it after access keys are rotated. Cause There are multiple mount points using the same storage account. Remounting some, but not all, of the mount points with new access keys results in access issues. Solution Use dbutils.fs.mounts() to check all mount points. Review the dbutils.fs.mounts() documentation for usage details.\nUse dbutils.fs.unmount() to unmount all storage accounts. Review the dbutils.fs.unmount() documentation for usage details.\nRestart the cluster.\nRemount the storage account with new keys. Review the Azure Data Lake Storage Gen2 and Blob Storage documentation for usage details.", "format": "html", "updated_at": "2022-12-09T10:41:10.740Z"}, "author": {"id": 790221, "email": "dayanand.devarapalli@databricks.com", "name": "dayanand.devarapalli ", "first_name": "dayanand.devarapalli", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T19:20:26.326Z", "updated_at": "2023-02-21T21:31:11.465Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256846, "name": "Databricks File System (DBFS)", "codename": "dbfs", "accessibility": 1, "description": "These articles can help you with the Databricks File System (DBFS).", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2964957, "name": "azure"}], "url": "https://kb.databricks.com/dbfs/remount-storage-after-rotate-access-key"}, {"id": 1665782, "name": "Unable to access Azure Data Lake Storage (ADLS) Gen1 when firewall is enabled", "views": 3654, "accessibility": 1, "description": "Learn how to troubleshoot access issues when connecting to Azure Data Lake Storage Gen 1 from Databricks with a firewall enabled.", "codename": "adls-gen1-firewall-access", "created_at": "2022-12-09T10:20:58.467Z", "updated_at": "2022-12-09T10:23:36.085Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThRQ05uQkFETk4zR0p1Q0psUmF6bHpYUUtjOXphWVE4c1lUakJiTmpORFFoRnZkRUQ1CnB6SFNGQmx3WlRSQmpsTUNRM0NtZmVEY25MM2xUSlQ3Skx1RUk3OVc2Y2tTZyt6VTVxeXdFZWxYYWNNYQpkV0RPZGVZTVM3UzRUUTNwUmx3a0FNVkpackVjZHVhRFRHUHlNanNmbnlPbnF3b2c3SjRPTUZvcmVaNXUKT1d3ZW91TzR5NlhRSjRBbHJMNXhSb1lqakNFWGpQWDNMMGV3MUtiekFJQmNGVEZtaUxoaGdxNFFpUVBaCjEzakNwVC9aTGcxeTdhQlpaZTNaMWJJdGw0VXRWUVZUTWk5L2hzU05qVmFYWDBLOUg1MUZWME9mdFQ2ZgpkSW1sTGkzcWRwQ0N6QUl6YkcvbXVaZnRZQW9UUVVFKy9UVGxjUWFyU2FYTWxwZzRhWTNTZkZ0elQ0OVgKVzJWdFNldFhFOFAwcDF0WUlQeGhxcTFLejQ1cWV0SXZOb0JqMlhnQXBCNGtvZHlOSmF6YVZpMzEwdXhjCi9KZW5ZZGhTNjU0ZGZySzh5VmJWUDYrTWJtNGdoNUVmdGZkV1JmKzM3aWVldFdjU3pNek9hODBJVXl3RgpSSDdkcVpob0s5UmJDNGdBNWlkT1FMZ252ZTZUbkoyUmhUdGR2MXl4Zmx5K0d4L3ZOQjF5eGRMNGN2RmsKRW1oQnJhaVQ3UklyM0NWMktEcWZzTmFBdnNCVVJCVTcrbmpoWlJ0Y2x3WHpSL0EwVjdybnNSV3lwVlVzClEzQVlOemhQSk9qOGd3YTlicmh5eVEvUjlhU3ExNk5QWDM2cmJ6ME0xM0ZZVUlSUVpDanZ6eSs4NndIVQppdXdvTk0xU0crbVlIRDNiUnE2MkgyRTJYalBHN3hlRXFZaW51TFEyemVGcXV4RHpFdVcrU0NDZUwwSXkKL3RRUjh6ZGo2U25QNVVLaUJyWWsrN0VGM1ExMGZHSE1lVC9zTzkxQjBWcDFTVEpKSitDdnpYNUtJTWkzClRUWExiM3EvNmVVdkF1Z0FsTTRTOGNQUU9VZEpMaytGUCtoMHJhLytoNVFIL1ZwTFZZZEVvdGZDQTFaYQo1eFFtSkJaanFMcUlRejB1dm5TZlpLNGV6VzJwczhrUDVRUUF4V1pLVXc0VXFQY0lienlONENaYjNzNnQKWmpldTR2MmVxK2RjSWgzS0NJM2U3c0cvbkZkSTg0SFAvRnRVTmZaWFBSZW51Zi81ckdPU1lYbTMzc0FCCm40a2FyS3oyM1ZNeCs2clpSc1ovaTF6SUc4K25YOVBzb3UxREVNYWtZemxFTVphREZuVGNqMFR4WXFCRApIQ1F6VHZ1MW9GNXlQNzZHWFRlWnRPNXgvejlkVmFRajUxZUpjcHRaNzlOU3BnSnhKTjlQTHdiQXU4QSsKZW4vVEhPR0NIS0VXUElnbDBkSGdTS0IwMlJFNXRwVjZTY1F1c3UyMkswT2xJNTM5c0pSVDBGNTlONVBDCmY5VE5hYUc2R2toZkxEMGNKQzE1Wm5NR2VqdUJ3dGhxNlRTRGlzZnRXeDNZL3hTYnRqT3VKYSt5cjBXWAptM201aGVXUkxFcUwxYWxtQVR1VThMZ1dwRUZJV3dER3l6M2szQnAzdDlVUVRIYm54N0ZUc3BaVDFDREkKWnVpWXBLRFcrY0pyMkxLNWhnZXYxVU8xQWp6c2FzZkp4aVFxKzBRWkc3em5PY3kwSTRqSVRoTVRtbUNZCi8yOD0K.3b06f1c12f183ecdb174adc0f3c06106\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>When you have a firewall enabled on your Azure virtual network (VNet) and you try to access ADLS using the ADLS Gen1 connector, it fails with the error:</p><pre id=\"isPasted\">328 format(target_id, \".\", name), value) 329 else: 330 raise Py4JError(Py4JJavaError:\r\nAn error occurred while calling o196.parquet.: java.lang.RuntimeException:\r\nCould not find ADLS Token at com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider$$anonfun$get Token$1.apply(AdlCredentialContextTokenProvider.scala:18)\r\nat com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider$$anonfun$get\r\nToken$1.apply(AdlCredentialContextTokenProvider.scala:18)\r\nat scala.Option.getOrElse(Option.scala:121)\r\nat com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider.getToken(AdlCredentialContextTokenProvider.scala:18)\r\nat com.microsoft.azure.datalake.store.ADLStoreClient.getAccessToken(ADLStoreClient.java:1036)\r\nat com.microsoft.azure.datalake.store.HttpTransport.makeSingleCall(HttpTransport.java:177)\r\nat com.microsoft.azure.datalake.store.HttpTransport.makeCall(HttpTransport.java:91)\r\nat com.microsoft.azure.datalake.store.Core.getFileStatus(Core.java:655)\r\nat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:735)\r\nat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:718)\r\nat com.databricks.adl.AdlFileSystem.getFileStatus(AdlFileSystem.java:423)\r\nat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\r\nat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:94)</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This is a known issue with the ADLS Gen1 connector. Connecting to ADLS Gen1 when a firewall is enabled is unsupported.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Use\u00a0<a href=\"https://learn.microsoft.com/azure/databricks/external-data/azure-storage\" rel=\"noopener noreferrer\" target=\"_blank\">ADLS Gen2</a> instead.</p>", "body_txt": "Problem When you have a firewall enabled on your Azure virtual network (VNet) and you try to access ADLS using the ADLS Gen1 connector, it fails with the error: 328 format(target_id, \".\", name), value) 329 else: 330 raise Py4JError(Py4JJavaError: An error occurred while calling o196.parquet.: java.lang.RuntimeException: Could not find ADLS Token at com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider$$anonfun$get Token$1.apply(AdlCredentialContextTokenProvider.scala:18) at com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider$$anonfun$get Token$1.apply(AdlCredentialContextTokenProvider.scala:18) at scala.Option.getOrElse(Option.scala:121) at com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider.getToken(AdlCredentialContextTokenProvider.scala:18) at com.microsoft.azure.datalake.store.ADLStoreClient.getAccessToken(ADLStoreClient.java:1036) at com.microsoft.azure.datalake.store.HttpTransport.makeSingleCall(HttpTransport.java:177) at com.microsoft.azure.datalake.store.HttpTransport.makeCall(HttpTransport.java:91) at com.microsoft.azure.datalake.store.Core.getFileStatus(Core.java:655) at com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:735) at com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:718) at com.databricks.adl.AdlFileSystem.getFileStatus(AdlFileSystem.java:423) at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426) at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:94) Cause This is a known issue with the ADLS Gen1 connector. Connecting to ADLS Gen1 when a firewall is enabled is unsupported. Solution Use\u00a0ADLS Gen2 instead.", "format": "html", "updated_at": "2022-12-09T10:23:36.083Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2964956, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/adls-gen1-firewall-access"}, {"id": 1665775, "name": "ADLS and WASB writes are being throttled", "views": 4598, "accessibility": 1, "description": "Learn how to resolve a \"files and folders are being created at too high a rate\" ADLS or WASB storage error.", "codename": "azure-storage-throttling", "created_at": "2022-12-09T10:17:35.481Z", "updated_at": "2022-12-09T10:19:46.632Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTkyc1ZDMEY0K0xjL2Rtdks5dFpOdE8wamw4ZGQ0bmdGTDJRMDdDTXBlM1hVT0l4Z2FrCjJsWWNqY1BwQzRWZHJybjdWam9WWUVQTENzelFVYkpnQ2d5V2laVXl6RXZXc3RNU3hKQyt3MjlYb3dnLwpMUE5rcC9HT0pkL1p1dXZSSXcwUEV6REszaTk3RFFKeERZQUdQaFFETFRqTW9ZZnZSSWNlZlFyVDZPR0gKVWhnVHA3UDlGK1REZGgxVGEvUGlBSHNYdzdEWVc1N09yanBPVnkyUjZVZCtVZ2dBU0RxOXBQczYxcjBICmpPS3dlcmdQUmt6VCtKUnBFdTdPbUxRckNqdjNOOVNxUHVJLzIyZzNlbkpmUUh0VkF5bncyUkh0cTdZdgp4VUZGNWRvbFlvRDU2QVZBVXJjU3I1d21ua1FQWUdhRnVGY0ZIWVR5UzhJK0RBVzdvM25HRXJMaXI5RUIKUmZJZzZzOXVFSExIQjNLQ0xYaW15OThQSEJLbERHUlc5bDZxUTF2bHJ4TmpGRzNFR2VQSjl4RTZpZWFqCm1wcHM3K0VUbklkUWZwTU1ya05wT1k2Z2N0VUVCbmpDSFZLSEs1eWpnZitBeHdKK1paU0Q1Rmxubkh5SApUOFNyUmo3bFZSb0NBbEx6Z01reklRZE1EQU8zTzdtU3VOcnhJako5WE5ZTVhEL05TZDVaRDVjR1BtUVAKRVBHcEdVYlk3ZGw5Uk45ckpOYm9UUi9YOHpES0dlZjVGTzNQS0JudE1EaFlEMzNYdXpDaVJWMmpzM0lSCkF1S01CTVJBWnN2N0J0M3M1VWFVaEtiS2xxZkQ4UzNCeFJ6RysxMnEwZVNYa0Z6cDlWV1ArVmdSaEVRQwp3M2RSSjgzTDFtZlR6ZVdlc1U0RGdxbVFldW1lM3JXQ21WRjB4VnkxVjBoeWFadXlmMndDRlpxaEQ4SkkKamp2Qm51RHl2YWt3Zkh6eFVHWWdNVCt4em4yZlBENDA0MzdYbk13V1hxbjNiNUtoUkhnajlkdUZuZmdWCkpyMFlFTkhPQ3NsUUJua1M5RVlka2EwVVJEMzEwMjNDdHFsRjU0dkVxVFN3L2pUOW9SdTZZL2w5UHRKRAo1NXpUU01Nc3hoUjJ3d1p0eVg2YXZobnhwdUdhVVYzbzJFc0JZV2NWblVtU3dTb3ZjcTRIVjBaQmxGcUMKcHFRaXFkVzBaUW1EN2pJQmRybm5LdzUrbENud1NGQW1vSmVtOVg1NjRWdVZKS1FoVFhRQ3NNZEdtYVV2CndsV2U0Q0dDeUNkVHdGZUlpdHMwRkJqem9LQkFMRTZkWnNCa2RoUXp5OHFWQ3ZNSGh4UDNtcU1LaDBrSQpZdlV1Qld4NkZYc3VKVHJyVmZIa2E2YjR5emwxWnBZYmRoZFRoTXhTTXUrVjhYNGdmZW9YaVNaazg2SXgKUmIrRnFFUFlGSGt5MUFUS01zTmhmWUlDa2gxR25MZ3I0QzU4bUV4dlZQZXRBbDFaRWpHeng5MUkvVytpCjRDcXhTQWhtRHd4d1pyWUFpdFBuMkluQTlIYUo3ckhadEFnQWtIbHh3NGM5d0tUVXU5aW9sam5FNjJWdgo4dWtGN0JvS1cveXZabGh6Q3l3MDBKSS8xRU1yRWZiaktYUFNhVmxIbkNwN21CSnh1YVhJVzZmYzVYNE8KWUtSVVBMSXdJU3NXVEhjZFdSN0YxaEFSbVBtRjQ0bUkreTh6ZllCSEVISGVDYXpteFlIVVdOYkNsU3kzCldYTjZMc2ltK1BNcUpCQ2NNVHM4b3VjTlU1OGxqY3VyaFE3cjhSSjV0M0hHOGZnbFBoYThiWGJWVkhtNgoxTGZiUDlUUXArVXo0a0dNRldFclZZdks2dnBXVXgxaDVJcDlMNS9pUW5uTnRNMjRNdz09Cg==.40d4462ee65419dc4451c9e79b8a4837\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>When accessing data stored on Azure data Lake Storage (ADLS) Windows Azure Storage Blobs (WASB) requests start timing out. You may see an error message indicating that storage is being accessed at too high a rate.</p><pre>Files and folders are being created at too high a rate</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Azure storage subscriptions have a limit on how many files and folders can be accessed over time. If too many requests are made in a given time frame, your account will be subject to throttling in order to keep the requests under the subscription limit.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>To resolve this issue you can either increase the storage limits on your Azure subscription or optimize your Spark code to reduce the number of files created.</p>", "body_txt": "Problem When accessing data stored on Azure data Lake Storage (ADLS) Windows Azure Storage Blobs (WASB) requests start timing out. You may see an error message indicating that storage is being accessed at too high a rate. Files and folders are being created at too high a rate Cause Azure storage subscriptions have a limit on how many files and folders can be accessed over time. If too many requests are made in a given time frame, your account will be subject to throttling in order to keep the requests under the subscription limit. Solution To resolve this issue you can either increase the storage limits on your Azure subscription or optimize your Spark code to reduce the number of files created.", "format": "html", "updated_at": "2022-12-09T10:19:46.630Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2964955, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/azure-storage-throttling"}, {"id": 1665767, "name": "Long jobs fail when accessing ADLS", "views": 4456, "accessibility": 1, "description": "Long running jobs that use Azure AD credential passthrough to access ADLS fail after 1 hour.", "codename": "job-fails-adls-hour", "created_at": "2022-12-09T10:12:23.468Z", "updated_at": "2022-12-09T10:16:39.833Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS83a0ZDbXloVm5tYmwvNWZ4amVsZ01TMm1lMEpHYnlTMkdOYndpam8vWjJNZDJObDZmCno0ZTZuV1N0UUsrZzljNitCMFcxRWJIRENJazNBdjJvc1R5K1ZkcVpybjZKcUo5TGJFTzkza1NUQUwrRwo0OEo1YXN6REdBRGVtRkFXYTRuZVBNUUxHSjVSdUorOUJVZ2tURGRZQ0RTN3RKaWVKdkZ2bkdXd1dVWWIKOWFzaXN4THZwaFRCQ0pKY0lYWU9RM0NHSnRVeWpNZGgwbWRJcXh0Sk12Slkzc3p1dWFuL3hoYkVTZFhECkJpbkp5WnBhV2wwUzM5STFCUXJKWFErdEtxYmlTNk9COHE2RzVScnJDMGpmZ2pRTklQbVprTW1QQ0s2LwpMMkpLWGRKbmdINUZnTHhSVVVwamNXVFdpdnVIUnFtMkpMVUROUjA5WVVjZ3JVWENOcm1YSFdTNWNxQ3QKYVNoZERmSUJRT01JTE9Yb1krakNCeHhvdHczTDEzcEJnd1hSeGRjaXlEYmtQTWlORzdlT3B1Z3AwTTJzCmt1aTZPKzlPRGRrTTRIV2FsL3NqSjY3WDJlSzF1akdvc3dEOE1WMHpaNDcyTVFWelBkUG9yczM2d2IyZQpZamFSOHl1RGozcUdoY1A4SVdVMkdzR3VTclFzM25KYkVmLzVweWdhWXBST2ZzeVVFOHJsOG1hbS9VM3YKa1VSK0hsRyszOUw1aS96VldSMW1ZZDR0YVB4MEZrZDZBQklQMEI2MXJveWJ4MXZXajQ1elA4ZUkrVFNZCi9RczdRdlZ5ZDI4cU53V05VL0FnUGFFaGtSWUw3SEJ6cWh0bVlBakZUS2VUZS9qMzRITTdwcFpCckNNMApKYXZvZ2JEVlcxNDY3Z2F5NWhtMDVZaXBHWmZGd1pReklmaG9sQjAxTWxVUXZhM3RCc2s1dGpTNStXNmcKMmR1ckh6ZlM3UmM0SUV6MXQxMjg1WEFzVkVkZmQ0NHB1eXQvMERKRHYxWHloc2VRcWVHS1F1WmlteUdkCllzTXdYOWFZbFM5SkRzSlVlb1dlZTF1aUhHbERqY2FUTndnUUhHSVJrMmc0VVQzL1l5dFJkeFpTSXNtTApNQXRSclo2ZUpPWTdFcnJsaWlrSngvd1pyU0FIWlAwdk4vSkF6aHNINHBwci9SSzJ0QTBkT0pZUkJldG4Ka0QrMWozQ1lZa1kvNHBkMTNwKzMzSUdJZHRuU2REV1JXd1NUbXM4SkNqWGxxNW1xUm9WWGJxV2VSczdpCnI4OFJsWHZKYzh3d2JVdkpCYU5Panp5cUk1dHQvTms3OUhSaUpOYzI2cFRodHQ5Tmw3NTRXeExZelgwNApVODlMT1JzZXNDZi9yZ05vREVCR0hIc3o2dE1tQ3pXMUxiNUN4MW9ySmg1U1RqUHRqLzRXZGNqcVVhT3kKcVM5MXhFK1pIYjFoSGt1UnJGVnVOMzA4TU9MWHRYR3gybDRpN0U4S1ptbXp4cGUyYXdXS3E3V2VyanlQCnQraE5Ec1pBUHBtbm9oZGFOc2gxQVdCeldqdFQvMXIxMkFLY25kRWN3YzIzVkVoYWZwaGJJY0tKdEM3dwpLcEZSemdwQkRlaUx1QU5qcHlvcm5aQnhEaENaWHVrS0VsSXd3Ri80akNYTnM4WitHTGwyY0FpNVdoeW8KaS9OS1hjekFpd0tMSXl4Ylg0V25rWG5aZGhrWGxPUVF2M0RKTDY2Q3FYaWhMTVF0ZE00M0JEQWl0UTRXCno5S1UrdEtXc1gxZlFwZGxOZE0zUnlyTTJIYjVtKzFmdGhZRVVvNDZhSXhzWlc5ZkRjdUJTZldQNlEwUApzaVBWVVJMd3JySEVicGtxSVNVL2FvazBkYWJICg==.87dcc20ae0cd2edd3dd45494aea2b010\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p id=\"isPasted\">You are using Azure Active Directory (Azure AD) credential passthrough to access Azure Data Lake Storage (ADLS) resources.</p><p>Jobs that run longer than one hour fail with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">HTTP401</span> error message.</p><pre id=\"isPasted\">com.microsoft.azure.datalake.store.ADLException: Error reading from file /local/Users/&lt;path-to-file&gt;\r\n\r\nOperation OPEN failed with HTTP401 : null\r\nLast encountered exception thrown after 5 tries. [HTTP401(null),HTTP401(null),HTTP401(null),HTTP401(null),HTTP401(null)]</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">The lifetime of an Azure AD passthrough token is one hour. When a command is sent to the cluster that takes longer than one hour, it fails if an ADLS resource is accessed after the one hour mark.</p><p>This is a known issue.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">You must rewrite your queries, so that no single command takes longer than an hour to complete.</p><p>It is not possible to increase the lifetime of an Azure AD passthrough token. The token is retrieved by the Azure Databricks replicated principal. You cannot edit its properties.</p><p>Please review the <a href=\"https://learn.microsoft.com/azure/databricks/security/credential-passthrough/adls-passthrough#limitations\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">ADLS credential passthrough limitations</a> documentation for more information.</p>", "body_txt": "Problem You are using Azure Active Directory (Azure AD) credential passthrough to access Azure Data Lake Storage (ADLS) resources. Jobs that run longer than one hour fail with a HTTP401 error message. com.microsoft.azure.datalake.store.ADLException: Error reading from file /local/Users/&lt;path-to-file&gt; Operation OPEN failed with HTTP401 : null Last encountered exception thrown after 5 tries. [HTTP401(null),HTTP401(null),HTTP401(null),HTTP401(null),HTTP401(null)] Cause The lifetime of an Azure AD passthrough token is one hour. When a command is sent to the cluster that takes longer than one hour, it fails if an ADLS resource is accessed after the one hour mark. This is a known issue. Solution You must rewrite your queries, so that no single command takes longer than an hour to complete. It is not possible to increase the lifetime of an Azure AD passthrough token. The token is retrieved by the Azure Databricks replicated principal. You cannot edit its properties. Please review the ADLS credential passthrough limitations documentation for more information.", "format": "html", "updated_at": "2022-12-09T10:16:39.829Z"}, "author": {"id": 790256, "email": "huaming.liu@databricks.com", "name": "huaming.liu ", "first_name": "huaming.liu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T19:51:31.043Z", "updated_at": "2022-03-08T02:15:24.757Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2964954, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/job-fails-adls-hour"}, {"id": 1665758, "name": "Error when reading data from ADLS Gen1 with Sparklyr", "views": 8068, "accessibility": 1, "description": "Learn how to resolve errors that occur when reading data from Azure Data Lake Storage Gen1 with Sparklyr in Databricks.", "codename": "access-adls1-from-sparklyr", "created_at": "2022-12-09T10:02:56.497Z", "updated_at": "2022-12-09T10:16:10.827Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9OeXJBVjlLQnA1RGk2UTNKZnZHQjhkZW5zNTFiWU9SbE9mMmNFTnhXdXFIU0t4MzhNCk5FVG5obzB6TVBnQzdidmdTSFUrK3Y3emNXMDlzUTg0VVZWS2R2ajRoWW41WWhYclBEakRpeTlnMC9hdAp0Qkd0VFE2a21kcHE3WUx0NEpnRGtWV1Z6dHdtcTd0T3A4TWQ0cmo2NjZMTXlOOE8xU1VNbTVuVGVvbXkKOXM5TmdESS9FT3huZExzS3p4T0ErMGZrSXkzSkZUNzR1V2JOVkN1QzRrYmJnaGs1c2FYdTVJYnd5N29VCmcxQnVyb2V5RFlYV2xOZWkzM0o5d2ZLZjkyVTJaUlI0azR5UGx3eGpRY3FnOXF5YzY3alBjNkJ1eEJ3Mwp3cUZFY2I2VUlFQ3czVElMY2FjaUFlZjMyd3p4U1p2TWp4R1IzRllLSENScTRVaU1MejhwRlFXcXJjNkkKR1grOHRSWW1KYnZFQVlMdFd4Q2VEOURWTHpsVVRNZXNmL0NGazI0YnFUTkpncWxvMS84TmlITTdHbW4wCjhnM2lXUmdIeWtmalZRNFgySm1haWdGWTVyeVZGeXRieEFKdnZ2UVBFMlY2UjBJanFRWHE0R20rNFRPSAp4ZmFCZ3JSY3EyY2FPSlJsSDNqb1h5WU40NEJKU0JpUVNiempSRWtoWHFVL3hTTEpVSi9zL2ZZeFBzei8KQjdjVWZyL21jd3BwSW12U00zSHR1RmdkTkhyd2t1ajYzdjNYRTlRTU9JRTJ2MitZME5IUi9sWTByVUZqCk5MT3dKQzczS1JXRXo5VVFoL2FPMno5S3B1cU1uMFpZbFNZMzIvVHEzMG1TNWttY3JINXcwd1RhaU8vMwpTelhMSURSVk5reE1xUHYyVCtjY0UwamxyT2RRdHVzMDQ2WlI2Nk1qQ0FzcDJ3eVYvUUVqcWlTTlhoWGMKVndCQWF2REE0UGtqS0JOYVRzejZXbjlkdm12dlVvM1hSOUM4bFk2SytscE1LS0ZucURReVBaY1FjU2d2CjlBSjBiVjF3SnpBUU9XbGlURVU0enl1dEF3UlBvbFluVU1Hd2VCTmoxa2tPdEsxamZRVmI3c2dFV3lGZwpIc1hLQUQ2L2h6ZHFyNmFYRnlOUDIxZDF4Rk9WcE5aWFFmejFqWGxuOHh5RE5TaFBFa1BXTlZNaGt4UHIKcjkrRlRESTYrQjJvZ3BMdDlEZEw4ZFBhd085QkFnYmRJdFdid1VEcHJWWEtTaHFMbk5JendQc2lUaTFYCkQ0ck9DRnZZMFUxN0ZLZXRaQUFPYUtQdEpidWt4cFZaeGM0ZUhqN2gvNGpZNEhBeUNxWjhDT1hzQXZHKwpqUHFUV0pLSnJqVDVBSXlBOU80K2tQOCs1UXREa0lSaVJDZ1JLNU1YUDcyMC9MZTFXUVZHOFNwQ0tRbUMKNFB0VXZMZSs2Z3pGM1Jxc1BnRXdMZnU1UG9PRE5nbllEdjVJb3lMRCtndVFnU21Ga0Nrd2hwdmlEbStiCnFBVUdlOVlQT1dOeGJJOTcySW9ZUy9mSUV1YWg4azZpdmUxQ1lrdmt2RTZvTEtVVTBjK3JCeVZXVzcrTApnQTNXbFZ5bEs5MlNrT3hwZkVHM1RlVkNHNWYrVWhrbTMvbkRWbWJ0RUp3UTRJWlAycE0xblB0U09KNzgKclJXWWQ1NmN4QitnQ0NJbFQxd2dpeGdCZDdxdWpvOFdZUWp1aXlzb1g0OWE0azNmMWs4SzdtUTJVdjBYClB3bz0K.e062035579a1e835f22f496435d6f402\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p id=\"isPasted\">When using a cluster with Azure AD Credential Passthrough enabled, commands that you run on that cluster are able to read and write your data in Azure Data Lake Storage Gen1 without requiring you to configure service principal credentials for access to storage.</p><p>For example, you can directly access data using</p><pre>%python\r\n\r\nspark.read.csv(\"adl://myadlsfolder.azuredatalakestore.net/MyData.csv\").collect()</pre><p>However, when you try to access data directly using Sparklyr:</p><pre>%r\r\n\r\nspark_read_csv(sc, name = \"air\", path = \"adl://myadlsfolder.azuredatalakestore.net/MyData.csv\")</pre><p>It fails with the error:</p><pre>com.databricks.backend.daemon.data.client.adl.AzureCredentialNotFoundException: Could not find ADLS Gen1 Token</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark_read_csv</span> function in Sparklyr is not able to extract the ADLS token to enable authentication and read data.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>A workaround is to use an Azure <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">application id</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">application key</span>, and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">directory id</span> to mount the ADLS location in DBFS:</p><pre>%python\r\n\r\n# Get credentials and ADLS URI from Azure\r\napplicationId= &lt;application-id&gt;\r\napplicationKey= &lt;application-key&gt;\r\ndirectoryId= &lt;directory-id&gt;\r\nadlURI=&lt;adl-uri&gt;\r\nassert adlURI.startswith(\"adl:\"), \"Verify the adlURI variable is set and starts with adl:\"\r\n\r\n# Mount ADLS location to DBFS\r\ndbfsMountPoint=&lt;mount-point-location&gt;\r\ndbutils.fs.mount(\r\n\u00a0 mount_point = dbfsMountPoint,\r\n\u00a0 source = adlURI,\r\n\u00a0 extra_configs = {\r\n\u00a0 \u00a0 \"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\r\n\u00a0 \u00a0 \"dfs.adls.oauth2.client.id\": applicationId,\r\n\u00a0 \u00a0 \"dfs.adls.oauth2.credential\": applicationKey,\r\n\u00a0 \u00a0 \"dfs.adls.oauth2.refresh.url\": \"<a data-fr-linked=\"true\" href=\"https://login.microsoftonline.com/\">https://login.microsoftonline.com/</a>{}/oauth2/token\".format(directoryId)\r\n\u00a0 })</pre><p>Then, in your R code, read data using the mount point:</p><pre>%r\r\n\r\n# Install Sparklyr\r\n%r\r\ninstall.packages(\"sparklyr\")\r\nlibrary(sparklyr)\r\n# Create a sparklyr connection\r\nsc &lt;- spark_connect(method = \"databricks\")\r\n\r\n# Read Data\r\n%r\r\nmyData = spark_read_csv(sc, name = \"air\", path = \"dbfs:/&lt;mount-point-location&gt;/myData.csv\")</pre><p><br></p>", "body_txt": "Problem When using a cluster with Azure AD Credential Passthrough enabled, commands that you run on that cluster are able to read and write your data in Azure Data Lake Storage Gen1 without requiring you to configure service principal credentials for access to storage. For example, you can directly access data using %python spark.read.csv(\"adl://myadlsfolder.azuredatalakestore.net/MyData.csv\").collect() However, when you try to access data directly using Sparklyr: %r spark_read_csv(sc, name = \"air\", path = \"adl://myadlsfolder.azuredatalakestore.net/MyData.csv\") It fails with the error: com.databricks.backend.daemon.data.client.adl.AzureCredentialNotFoundException: Could not find ADLS Gen1 Token Cause The spark_read_csv function in Sparklyr is not able to extract the ADLS token to enable authentication and read data. Solution A workaround is to use an Azure application id, application key, and directory id to mount the ADLS location in DBFS: %python # Get credentials and ADLS URI from Azure applicationId= &lt;application-id&gt; applicationKey= &lt;application-key&gt; directoryId= &lt;directory-id&gt; adlURI=&lt;adl-uri&gt; assert adlURI.startswith(\"adl:\"), \"Verify the adlURI variable is set and starts with adl:\" # Mount ADLS location to DBFS dbfsMountPoint=&lt;mount-point-location&gt; dbutils.fs.mount( \u00a0 mount_point = dbfsMountPoint, \u00a0 source = adlURI, \u00a0 extra_configs = { \u00a0 \u00a0 \"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\", \u00a0 \u00a0 \"dfs.adls.oauth2.client.id\": applicationId, \u00a0 \u00a0 \"dfs.adls.oauth2.credential\": applicationKey, \u00a0 \u00a0 \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/{}/oauth2/token\".format(directoryId) \u00a0 }) Then, in your R code, read data using the mount point: %r # Install Sparklyr %r install.packages(\"sparklyr\") library(sparklyr) # Create a sparklyr connection sc &lt;- spark_connect(method = \"databricks\") # Read Data %r myData = spark_read_csv(sc, name = \"air\", path = \"dbfs:/&lt;mount-point-location&gt;/myData.csv\")", "format": "html", "updated_at": "2022-12-09T10:16:10.824Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2964950, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/access-adls1-from-sparklyr"}, {"id": 1665302, "name": "Access files written by Apache Spark on ADLS Gen1", "views": 4078, "accessibility": 1, "description": "Configure permissions to allow access to files that Apache Spark writes to ADLS Gen1 storage.", "codename": "spark-default-perms-adls-gen1", "created_at": "2022-12-09T02:50:23.581Z", "updated_at": "2022-12-09T02:55:11.096Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.866dbd5be951fd7fd9d0decc37edcbb3\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p id=\"isPasted\">You are using Azure Databricks and have a Spark job that is writing to ADLS Gen1 storage.</p><p>When you try to manually read, write, or delete data in the folders you get an error message.</p><pre>Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">When writing data to ADLS Gen1 storage, Apache Spark uses the service principal as the owner of the files it creates. The service principal is defined in <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dfs.adls.oauth2.client.id</span>.</p><p>When files are created, they inherit the default permissions from the Hadoop filesystem. The Hadoop filesystem has a default permission of 666 (-rw-rw-rw-) and a default umask of 022, which results in the 644 permission setting as the default for files.</p><p>When folders are created, they inherit the parent folder permissions, which are 770 by default.</p><p>Because the owner is the service principal and not the user, you don\u2019t have permission to access the folder due to the 0 bit in the folder permissions.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><h2 data-toc=\"true\" id=\"option-1-3\">Option 1</h2><p>Make the service principal user part of the same group as the default user. This will allow access when accessing storage through the portal.</p><p>Please reach out to Microsoft support for assistance.</p><h2 data-toc=\"true\" id=\"option-2-4\">Option 2</h2><p>Create a base folder in ADLS Gen1 and set the permissions to 777. Write Spark output under this folder. Because folders created by Spark inherit the parent folder permissions, all folders created by Spark will have 777 permissions. This allows any user to access the folders.</p><h2 data-toc=\"true\" id=\"option-3-5\">Option 3</h2><p>Change the default umask from 022 to 000 on your Azure Databricks clusters.</p><p>Set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.hadoop.fs.permissions.umask-mode 000</span> in the <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>Spark config</strong></a> for your cluster.</p><p>With a umask of 000, the default Hadoop filesystem permission of 666 becomes the default permission used when Azure Databricks creates objects.</p>", "body_txt": "Problem You are using Azure Databricks and have a Spark job that is writing to ADLS Gen1 storage. When you try to manually read, write, or delete data in the folders you get an error message. Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation Cause When writing data to ADLS Gen1 storage, Apache Spark uses the service principal as the owner of the files it creates. The service principal is defined in dfs.adls.oauth2.client.id. When files are created, they inherit the default permissions from the Hadoop filesystem. The Hadoop filesystem has a default permission of 666 (-rw-rw-rw-) and a default umask of 022, which results in the 644 permission setting as the default for files. When folders are created, they inherit the parent folder permissions, which are 770 by default. Because the owner is the service principal and not the user, you don\u2019t have permission to access the folder due to the 0 bit in the folder permissions. Solution Option 1 Make the service principal user part of the same group as the default user. This will allow access when accessing storage through the portal. Please reach out to Microsoft support for assistance. Option 2 Create a base folder in ADLS Gen1 and set the permissions to 777. Write Spark output under this folder. Because folders created by Spark inherit the parent folder permissions, all folders created by Spark will have 777 permissions. This allows any user to access the folders. Option 3 Change the default umask from 022 to 000 on your Azure Databricks clusters. Set spark.hadoop.fs.permissions.umask-mode 000 in the Spark config for your cluster. With a umask of 000, the default Hadoop filesystem permission of 666 becomes the default permission used when Azure Databricks creates objects.", "format": "html", "updated_at": "2022-12-09T02:55:11.092Z"}, "author": {"id": 790221, "email": "dayanand.devarapalli@databricks.com", "name": "dayanand.devarapalli ", "first_name": "dayanand.devarapalli", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T19:20:26.326Z", "updated_at": "2023-02-21T21:31:11.465Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2964862, "name": "azure"}], "url": "https://kb.databricks.com/data/spark-default-perms-adls-gen1"}, {"id": 1664037, "name": "UnknownHostException on cluster launch", "views": 3567, "accessibility": 1, "description": "Troubleshoot an UnknownHostException on cluster launch. This is often a DNS configuration issue.", "codename": "unknown-host-exception-on-launch", "created_at": "2022-12-08T02:21:49.164Z", "updated_at": "2022-12-08T02:31:01.872Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.077ea2b2ebbe49f5b0d45cc01116e063\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p id=\"isPasted\">When you launch an Azure Databricks cluster, you get an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">UnknownHostException</span> error.</p><p>You may also get one of the following error messages:</p><ul>\n<li style='font-family: \"Times New Roman\", Times, serif, -webkit-standard;'>Error: There was an error in the network configuration. databricks_error_message: Could not access worker artifacts.</li>\n<li style='font-family: \"Times New Roman\", Times, serif, -webkit-standard;'>Error: Temporary failure in name resolution.</li>\n<li style='font-family: \"Times New Roman\", Times, serif, -webkit-standard;'>Internal error message: Failed to launch spark container on instance XXX. Exception: Could not add container for XXX with address X.X.X.X.mysql.database.azure.com: Temporary failure in name resolution.</li>\n</ul><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">These errors indicate an issue with DNS settings.</p><ul>\n<li>Primary DNS could be down or unresponsive.</li>\n<li>Artifacts are not being resolved, which results in the cluster launch failure.</li>\n<li>You may have a host record listing the artifact public IP as static, but it has changed.</li>\n</ul><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Identify a working DNS server and update the DNS entry on the cluster.</p><ol>\n<li>Start a <a href=\"https://learn.microsoft.com/azure/virtual-machines/linux/quick-create-portal\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">standalone Azure VM</a> and verify that the artifacts blob storage account is reachable from the instance.<pre>`telnet dbartifactsprodeastus.blob.core.windows.net 443`.</pre>\n</li>\n<li id=\"isPasted\">Verify that you can reach your primary DNS server from a notebook by running a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ping</span> command.</li>\n<li>If your DNS server is not responding, try to reach your secondary DNS server from a notebook by running a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ping</span> command.</li>\n<li>Launch a <a href=\"https://learn.microsoft.com/azure/databricks/clusters/web-terminal\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Web Terminal</a> from the cluster workspace.</li>\n<li>Edit the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/etc/resolv.conf</span> file on the cluster.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nameserver</span> value with your working DNS server.</li>\n<li>Save the changes to the file.</li>\n<li>Restart <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">systemd-resolved</span>.<pre>$ sudo systemctl restart systemd-resolved.service</pre>\n</li>\n</ol><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">This is a temporary change to the DNS and will be lost on cluster restart. After verifying that the custom DNS settings are correct, you can <a href=\"https://learn.microsoft.com/azure/databricks/kb/cloud/custom-dns-routing\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">configure custom DNS settings using dnsmasq</a> to make the change permanent.</p>\n</div>\n</div><h2 data-toc=\"true\" id=\"further-troubleshooting-4\">Further troubleshooting</h2><p id=\"isPasted\">If you are still having DNS issues, you should try the following steps:</p><ul>\n<li>Verify that port 43 (used for whois) and port 53 (used for DNS) are open in your firewall.</li>\n<li>Add the Azure recursive resolver (168,.63.129.16) to the default DNS forwarder. Review the <a href=\"https://learn.microsoft.com/azure/virtual-network/virtual-networks-name-resolution-for-vms-and-role-instances#vms-and-role-instances\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">VMs and role instances</a> documentation for more information.</li>\n<li>Verify that <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nslookup</span> results are identical between your laptop and the default DNS. If there is a mistmatch, your DNS server may have an incorrect host record.</li>\n<li>Verify that everything works with a default Azure DNS server. If it works with Azure DNS, but fails with your custom DNS, your DNS admin should review your DNS server settings.</li>\n</ul><p><br></p>", "body_txt": "Problem When you launch an Azure Databricks cluster, you get an UnknownHostException error. You may also get one of the following error messages: Error: There was an error in the network configuration. databricks_error_message: Could not access worker artifacts.\nError: Temporary failure in name resolution.\nInternal error message: Failed to launch spark container on instance XXX. Exception: Could not add container for XXX with address X.X.X.X.mysql.database.azure.com: Temporary failure in name resolution. Cause These errors indicate an issue with DNS settings. Primary DNS could be down or unresponsive.\nArtifacts are not being resolved, which results in the cluster launch failure.\nYou may have a host record listing the artifact public IP as static, but it has changed. Solution Identify a working DNS server and update the DNS entry on the cluster. Start a standalone Azure VM and verify that the artifacts blob storage account is reachable from the instance.`telnet dbartifactsprodeastus.blob.core.windows.net 443`. Verify that you can reach your primary DNS server from a notebook by running a ping command.\nIf your DNS server is not responding, try to reach your secondary DNS server from a notebook by running a ping command.\nLaunch a Web Terminal from the cluster workspace.\nEdit the /etc/resolv.conf file on the cluster.\nUpdate the nameserver value with your working DNS server.\nSave the changes to the file.\nRestart systemd-resolved.$ sudo systemctl restart systemd-resolved.service Info\nThis is a temporary change to the DNS and will be lost on cluster restart. After verifying that the custom DNS settings are correct, you can configure custom DNS settings using dnsmasq to make the change permanent. Further troubleshooting If you are still having DNS issues, you should try the following steps: Verify that port 43 (used for whois) and port 53 (used for DNS) are open in your firewall.\nAdd the Azure recursive resolver (168,.63.129.16) to the default DNS forwarder. Review the VMs and role instances documentation for more information.\nVerify that nslookup results are identical between your laptop and the default DNS. If there is a mistmatch, your DNS server may have an incorrect host record.\nVerify that everything works with a default Azure DNS server. If it works with Azure DNS, but fails with your custom DNS, your DNS admin should review your DNS server settings.", "format": "html", "updated_at": "2022-12-08T02:31:01.865Z"}, "author": {"id": 789524, "email": "arnab.saha@databricks.com", "name": "arnab.saha ", "first_name": "arnab.saha", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T10:15:07.793Z", "updated_at": "2022-03-08T02:15:24.475Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963773, "name": "azure"}], "url": "https://kb.databricks.com/clusters/unknown-host-exception-on-launch"}, {"id": 1664032, "name": "Adding a configuration setting overwrites all default spark.executor.extraJavaOptions settings", "views": 4663, "accessibility": 1, "description": "Learn how to resolve overwritten configuration settings in Databricks.", "codename": "conf-overwrites-default-settings", "created_at": "2022-12-08T02:07:55.036Z", "updated_at": "2022-12-08T02:14:32.213Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThxQnc4ZFJPMDhUNWo4SlEzMlh3U3FabGJnNkdDd2l1TTNvVnd5TWZZNTJySW15U2JUCkwvN2JTczZtVDVtRTc3T0J6MCtjWnp2NWpXQ3VuSzZ3TkNmOUdKcGE2T1dKQlZoTndIbXlkaHh3a0JGUgpwUm5ZaHhTUjFScWdBQVI4UGtaU0Q2S3Urd3BBVk04LzZWQ1NZMWdZMnpBWWJTUFhWdVhUazQrc1pDR2YKUWF5dGpVekpNNWo4RGIwWUNManpTSGhtTCtVSExNbDZuRW4xRHFFNGYrNGFEZkd1a2gyTDdJakdDV0dKCjZlK1NWRnVvWnpZcSsvRjhaRnVEbzdDbGY4ZWF2YXFFT0RIYXFVTFlsNnV3Nm5uanNCMnFnWXZjWGFIUwpUS1ZqUGdiblNkUzFYd2FiMDlyRjFMQ0FETE5QWU95SHl6RE4wT0srL2kwc2dYVmxKM1dhSlVqNm0raUIKRTRiQU9FdVhWRjVIY1ljbGtzZzBIK3h6Q05hYTljVDdJaG9FN1NsTVQvWDdaUy8zRGJqZFFVUkJTY1hhCjBmWEFLWFdmdklYcm1DTjFYb1YvOHZGUm9rMUpTSEpXRWxmK08rNVNmQkNiZXRMOTB5MWpQdm5sRW80SAoxanFLUnJGc1V1OHk3MCtPZnR1eHR1aE5KUCtwK2kzR1NnU2Irbm1ScTZJM2RmeE1sS2l0S2pnai95M0gKQlMzNnRmUk9UZUNlck5odFgzdndQYndmaXpid284U1lCVHJlb25GYzdnOGt4czVwUS9uU0Rabkt6QVEyCm5NYzNtWkQxWjZQOFVrUHJMelVjWmVHNTZxdzIyYndtM2xqcnpYKytKWjh2SUVBSm5HVDIrVTZ1dWViegprc1JxYS9nR2txd2xhT0I2SlFlZmVSWHA4MzVLank5emJKZG13KzRnMUxhTWxvQ1VOSlZmcFlUcEUwWXcKb2k1bVlVbzNxd0FwRmtBQ3AyOWxpcmMyNHlJVXh1NjFSakZ2c1d2dUsveEFhRStNdEdkZ3ZlMExmak9BClpHT3JLYWJqOXRTRnpJTUFiQ0RRWmhRRkxxeXQvbHJ4U2t3b21KMm9HbGhFcjRhcWhWUVFDKzl0M2w2cQo3cDlkMFFTejNxdmxIVytVTFU0Q0ovekZqK3BjUE9MazJicGdqeUNXMHFyb0lTWUhGVTlGK3VLbXFuK2QKcEpaZ0c2aE9xZitFUHVVbkMvSlZLVUdvRVFhRElhSUU1bGx5Syt5VWtWVG80cGdkQnJ2ZVZyV1dpblVlCjZ3WVB4M0RiZUdIbnFSdHBNU0d0enVBV1haVUpCa0JRNUs5NkFpRGZhSGUycXJaS1R6VlhidUJzTFdxWgpYeXhzYXhONU1nZ3Z5WkRzemFiTnJYbHVGZ25GUlhCZ0FwWUd5UHpuTVRlWEozSU1Ma3dBYWVZSzBKNWkKK29mQ0VUdWhiUm1qcVFtMlRQbGsvUzg3eXE3NnBmNWRjWkZyT1J1ODBNcURHOU1LcFdDdkZLQVR6aTdvCnJ4QjB5SXlXOGE1NlF4MDhVWFlNc0NGZGZpUnRkeXVVY3BIanpia244MkNyN0FuTEFoYU1aU3RHSUtWUwo0R3R3V2ZHYlB3NGZEYnFoLzc0b0tzVVJkRTh5aTBpb0lRbEtRb0hmWHZQbUN4U3ZvU2MvTUVSM2FwS1gKY3JuT1lKNTNPYVNFd2ROb2d4VHFQKzlCTCtBTHNROWpFZWlKYUNmakhBPT0K.1d23528a49a17a7c2a139d727a9110da\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>When you add a configuration setting by entering it in the Apache <strong>Spark config</strong> text area, the new setting replaces existing settings instead of being appended.</p><h1 data-toc=\"true\" id=\"version-1\">Version</h1><p>Databricks Runtime 5.1 and below.</p><h1 data-toc=\"true\" id=\"cause-2\">Cause</h1><p id=\"isPasted\">When the cluster restarts, the cluster reads settings from a configuration file that is created in the <strong>Clusters</strong> UI, and overwrites the default settings.</p><p>For example, when you add the following <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">extraJavaOptions</span> to the <strong>Spark config</strong> text area:</p><pre id=\"isPasted\">spark.executor.extraJavaOptions -\r\njavaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus\r\n_jmx_exporter/jmx_prometheus_javaagent.yml</pre><p>Then, in <strong>Spark UI</strong> &gt; <strong>Environment</strong> &gt; <strong>Spark Properties</strong> under <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.executor.extraJavaOptions</span>, only the newly added configuration setting shows:</p><pre id=\"isPasted\">-javaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus\r\n_jmx_exporter/jmx_prometheus_javaagent.yml</pre><p id=\"isPasted\">Any existing settings are removed.</p><p>For reference, the default settings are:</p><pre id=\"isPasted\">-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -\r\nXX:+UseCodeCacheFlushing -Ddatabricks.serviceName=spark-executor-1 -\r\nDjava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -\r\nXX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -\r\nDjavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.Dataty\r\npeFactoryImpl -\r\nDjavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.Documen\r\ntBuilderFactoryImpl -\r\nDjavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFact\r\noryImpl -\r\nDjavax.xml.validation.SchemaFactory=<a data-fr-linked=\"true\" href=\"https://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory\">https://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory</a> -\r\nDorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -\r\nDorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMX\r\nSImplementationSourceImpl</pre><h1 data-toc=\"true\" id=\"solution-3\">Solution</h1><p id=\"isPasted\">To add a new configuration setting to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.executor.extraJavaOptions</span> without losing the default settings:</p><ol>\n<li>In <strong>Spark UI</strong> &gt; <strong>Environment</strong> &gt; <strong>Spark Properties</strong>, select and copy all of the properties set by default for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.executor.extraJavaOptions</span>.</li>\n<li>Click <strong>Edit</strong>.</li>\n<li>In the <strong>Spark config</strong> text area (<strong>Clusters</strong> &gt; <strong>cluster-name</strong> &gt; <strong>Advanced Options</strong> &gt; <strong>Spark</strong>), paste the default settings.</li>\n<li>Append the new configuration setting below the default settings.</li>\n<li>Click outside the text area, then click <strong>Confirm</strong>.</li>\n<li>Restart the cluster.</li>\n</ol><p>For example, let\u2019s say you paste the following settings into the <strong>Spark config</strong> text area. The new configuration setting is appended to the default settings.</p><pre id=\"isPasted\">spark.executor.extraJavaOptions = -Djava.io.tmpdir=/local_disk0/tmp -\r\nXX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Ddatabricks.serviceName=spark-\r\nexecutor-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -\r\nXX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -\r\nDjavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.Dataty\r\npeFactoryImpl -\r\nDjavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentB\r\nuilderFactoryImpl -\r\nDjavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactor\r\nyImpl -\r\nDjavax.xml.validation.SchemaFactory:<a data-fr-linked=\"true\" href=\"https://www.w3.org/2001/XMLSchema=com.sun.org.apache.xer\">https://www.w3.org/2001/XMLSchema=com.sun.org.apache.xer</a>\r\nces.internal.jaxp.validation.XMLSchemaFactory -\r\nDorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -\r\nDorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplem\r\nentationSourceImpl -\r\njavaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus_jm\r\nx_exporter/jmx_prometheus_javaagent.yml</pre><p>After you restart the cluster, the default settings and newly added configuration setting appear in <strong>Spark UI</strong> &gt; <strong>Environment</strong> &gt;\u00a0<strong>Spark Properties</strong>.</p>", "body_txt": "Problem When you add a configuration setting by entering it in the Apache Spark config text area, the new setting replaces existing settings instead of being appended. Version Databricks Runtime 5.1 and below. Cause When the cluster restarts, the cluster reads settings from a configuration file that is created in the Clusters UI, and overwrites the default settings. For example, when you add the following extraJavaOptions to the Spark config text area: spark.executor.extraJavaOptions - javaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus _jmx_exporter/jmx_prometheus_javaagent.yml Then, in Spark UI &gt; Environment &gt; Spark Properties under spark.executor.extraJavaOptions, only the newly added configuration setting shows: -javaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus _jmx_exporter/jmx_prometheus_javaagent.yml Any existing settings are removed. For reference, the default settings are: -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m - XX:+UseCodeCacheFlushing -Ddatabricks.serviceName=spark-executor-1 - Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal - XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m - Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.Dataty peFactoryImpl - Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.Documen tBuilderFactoryImpl - Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFact oryImpl - Djavax.xml.validation.SchemaFactory=https://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory - Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser - Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMX SImplementationSourceImpl Solution To add a new configuration setting to spark.executor.extraJavaOptions without losing the default settings: In Spark UI &gt; Environment &gt; Spark Properties, select and copy all of the properties set by default for spark.executor.extraJavaOptions.\nClick Edit.\nIn the Spark config text area (Clusters &gt; cluster-name &gt; Advanced Options &gt; Spark), paste the default settings.\nAppend the new configuration setting below the default settings.\nClick outside the text area, then click Confirm.\nRestart the cluster. For example, let\u2019s say you paste the following settings into the Spark config text area. The new configuration setting is appended to the default settings. spark.executor.extraJavaOptions = -Djava.io.tmpdir=/local_disk0/tmp - XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Ddatabricks.serviceName=spark- executor-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security - XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m - Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.Dataty peFactoryImpl - Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentB uilderFactoryImpl - Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactor yImpl - Djavax.xml.validation.SchemaFactory:https://www.w3.org/2001/XMLSchema=com.sun.org.apache.xer ces.internal.jaxp.validation.XMLSchemaFactory - Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser - Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplem entationSourceImpl - javaagent:/opt/prometheus_jmx_exporter/jmx_prometheus_javaagent.jar=9404:/opt/prometheus_jm x_exporter/jmx_prometheus_javaagent.yml After you restart the cluster, the default settings and newly added configuration setting appear in Spark UI &gt; Environment &gt;\u00a0Spark Properties.", "format": "html", "updated_at": "2022-12-08T02:14:32.205Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963772, "name": "azure"}], "url": "https://kb.databricks.com/clusters/conf-overwrites-default-settings"}, {"id": 1664026, "name": "SSH to the cluster driver node", "views": 5132, "accessibility": 1, "description": "How to SSH to the Apache Spark cluster driver node in an Azure virtual network", "codename": "azure-ssh-cluster-driver-node", "created_at": "2022-12-08T01:33:04.749Z", "updated_at": "2023-03-15T18:48:15.219Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9ldDhIaE54ZTg3V294OUdudHY0akgyZHF2eUI0d0dxMkNlYWdpSnlwRkZWdTdBNkpVCjhrNFFKcGU3QTNzR213Q1prZUdTRnVhVGw5SW11ejdIUCtSRDVYN3NUSHNEYWZmWXFabCttbG5MdGRWTgorS1N1K1lONUhQQlNUdDNoVVdaUFBJWk1aUWRhU0h0K0lORWlXY0puOXU2Mk03OXRYRlZwWVd6M0QxTUIKK1BrVzdaQ3VLemRnaVZhRWRtcWdMbTV4REpWa2xRQTA2R1BaZVQ2UHJ5ZndVc0RlazNpNTc4Nmp3VnJaClhEYVVxbHErWm1BWXpzZ3JQM3h5Z0Zybm5tdU5SbTZtVVJqMmlhdjRiVGVwT2FVTlV4cXRzU3RySUZTZgpjQ2xoTk1Wc2xFd1dnQmgzcVRtQlZvQXAvNWYra3VVc2xGRy9YNkhjYUh6eWVSZ3ZjNWRwcERvaWN0ZzIKYWZUSVZHSVQrVU5ZcFpEWjRJV0VvYW5oNjlFbXpDT3hVZStwU1loLzYybHR5Z1hXZGw1YnhpWXdxR1pFCm1EUVNCL0R4OFpPQVN1eThWNkpKWVhydnozNjliMTRMSklJSzNqZmY3S0Y5LzA1L2ZVSEtxRUc3OG1HNwpQODBJcnFwWU1nUUdyUWxIYnhvd1NRU1VtZjZWc2ZFTFk5TTA4SmpHdmNtUGJDS1Vma3hKU2lPMWM5c0MKR1BiU3N2K21QRG8wY3lUd0NNRHQ3L1VPdGhSbGhsVVlJbDkvVTFadm5HckVtUngrWnFRWDltRlVjdm9RClBoSkkwOHRBRDlMRHUzaEJsdmxqbXl2dm1DWlBHbUJ6YzYybEl5NVNkbzRBeTNaTjVzOWxGWjNIRktyYQo4MytPdnBUMm1zUHUrQTl4ZityZGFDaEswNE9Oc1pnVEhCTDI1THJ0VHJsV1MvTGk1UC9OWmRkdXo4QnIKbkpWRi96aDBhSUt5cWJmbTlVaFl2UGk1S1QvWUZETmlXR0FKUFFlVDZuVEZiZXlGRm5SR0hsZ0o2aFI1CmtXMUsxRHByRXE0RHpmb24wYmE0TFJGVUxkNEh0U1Y2cnI4R1Z6dkkzRC9PMi8rT2h4SEs0dXBWc0JWKwpucVJOQTlSN2FxVG9uRUxCbHpPSCsvNDdXN0IwUjFRZmovczAxZFB1eTY1QWtiYVBVRG4wLzdPMEZ5SU4KTE5TQWhRZ2NMSnJ2SkdUU1hwdjB0cjQ3QU9kTFlCTnVjc1RpVmVIbWI0d1BTeVZyQU41UklVcU5FdTR5CnlpTGYxR091NFQvbVE5OWZTc2VuRWl4KytwSDRVOXQzOHR1RDg4b1dIV08vd0M1cW1zLzNERkNzMGdQSQpPdXlRbkVrQUhFR0xBcXNyZ0RaaVVpQXBQQmp5VFNPRlUwTG5hQndCK3lZQi9iSEpXeENpZmMwbXZ0T2MKbzdFNi9GcHBXdThLdm5WaHc1ZS91c0R1Y0Z4eUpHVjZ5ekVVaVNySVp1SGlFVTRTa3o5VjhPTm43STVkCm1MOUpIQTBCNnBkakZrcnFDZlFyWDFJYWdudnlDTTdTMFlLZE1aeUpaRzdzQVVoMUNLWm1kNWlhMFNGawpSQmxiRXhYa0tkYVBxVEpGdTd2Zkc5bi84bXZ0QWEyRTNWNEJYbHVOZWhHclBESGc0R1BhR00wVVJZbloKVTZHVEljbGNyWmwyeHFieTlmWTZ1d1k4Z1ZZaGd5ZEZBblh2Rm1jajM0QXdXcDJhWVV3Mkh6dThLNEVmClJIMW50NVdFMGpIMURvSE45dUNZTXJ4L2p4RVZEb0tQL1JYeDY5djBPRmhyMmUzS1FOL1dsbElMZWFmago5L0w5SzQwU2RIaEhJeFF4bThXSnMrSkJRTlBiRFJSazBsY0JscEdjZnJvdklzR0k5dz09Cg==.75d84e4b6a2596e2e6f280cf049a154a\"></div><p>This article explains how to use SSH to connect to an Apache Spark driver node for advanced troubleshooting and installing custom software.</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-0\">Warning</h3>\n<p class=\"hj-alert-text\">You can only use SSH if your workspace is deployed in an Azure Virtual Network (VNet) under your control. If your workspace is NOT VNet injected, the SSH option will not appear. Additionally, NPIP workspaces do not support SSH.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"configure-an-azure-network-security-group-1\">Configure an Azure network security group</h1><p id=\"isPasted\">The network security group associated with your VNet must allow SSH traffic. The default port for SSH is 2200. If you are using a custom port, you should make note of it before proceeding. You also have to identify a traffic source. This can be a single IP address, or it can be an IP range that represents your entire office.</p><ol>\n<li>In the Azure portal, find the network security group. The network security group name can be found in the public subnet.</li>\n<li>Edit the inbound security rules to allow connections to the SSH port. In this example, we are using the default port.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1670464616538-azure-add-inbound-security-rule.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n</ol><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-2\">Info</h3>\n<p class=\"hj-alert-text\">Make sure that your computer and office firewall rules allow you to send TCP traffic on the port you are using for SSH. If the SSH port is blocked at your computer or office firewall, you cannot connect to the Azure VNet via SSH.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"generate-ssh-key-pair-3\">Generate SSH key pair</h1><ol>\n<li id=\"isPasted\">Open a local terminal.</li>\n<li>Create an SSH key pair by running this command:<pre>ssh-keygen -t rsa -b 4096 -C</pre>\n</li>\n</ol><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-4\">Info</h3>\n<p class=\"hj-alert-text\">You must provide the path to the directory where you want to save the public and private key. The public key is saved with the extension .pub.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"configure-a-new-cluster-with-your-public-key-5\">Configure a new cluster with your public key</h1><ol>\n<li id=\"isPasted\">Copy the ENTIRE contents of the public key file.</li>\n<li>Open the cluster configuration page.</li>\n<li>Click <strong>Advanced Options</strong>.</li>\n<li>Click the <strong>SSH</strong> tab.</li>\n<li>Paste the ENTIRE contents of the public key into the <strong>Public key</strong> field.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1670464834808-azure-cluster-ssh-tab.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>Continue with cluster configuration as normal.</li>\n</ol><h1 data-toc=\"true\" id=\"configure-an-existing-cluster-with-your-public-key-6\">Configure an existing cluster with your public key</h1><p id=\"isPasted\">If you have an existing cluster and did not provide the public key during cluster creation, you can inject the public key from a notebook.</p><ol>\n<li>Open any notebook that is attached to the cluster.</li>\n<li>Copy the following code into the notebook, updating it with your public key as noted:<pre>%scala\r\n\r\nval publicKey = \"&lt;put your public key here&gt;\"\r\n\r\ndef addAuthorizedPublicKey(key: String): Unit = {\r\n\u00a0 val fw = new java.io.FileWriter(\"/home/ubuntu/.ssh/authorized_keys\", /* append */ true)\r\n\u00a0 fw.write(\"\\n\" + key)\r\n\u00a0 fw.close()\r\n}\r\naddAuthorizedPublicKey(publicKey)</pre>\n</li>\n<li>Run the code block to inject the public key.</li>\n</ol><h1 data-toc=\"true\" id=\"ssh-into-the-spark-driver-7\">SSH into the Spark driver</h1><ol>\n<li id=\"isPasted\">Open the cluster configuration page.</li>\n<li>Click <strong>Advanced Options</strong>.</li>\n<li>Click the <strong>SSH</strong> tab.</li>\n<li>Note the <strong>Driver Hostname</strong>.</li>\n<li>Open a local terminal.</li>\n<li>Run the following command, replacing the hostname and private key file path:<pre>ssh ubuntu@&lt;hostname&gt; -p 2200 -i &lt;private-key-file-path&gt;</pre>\n</li>\n</ol><p><br></p>", "body_txt": "This article explains how to use SSH to connect to an Apache Spark driver node for advanced troubleshooting and installing custom software. Warning\nYou can only use SSH if your workspace is deployed in an Azure Virtual Network (VNet) under your control. If your workspace is NOT VNet injected, the SSH option will not appear. Additionally, NPIP workspaces do not support SSH. Configure an Azure network security group The network security group associated with your VNet must allow SSH traffic. The default port for SSH is 2200. If you are using a custom port, you should make note of it before proceeding. You also have to identify a traffic source. This can be a single IP address, or it can be an IP range that represents your entire office. In the Azure portal, find the network security group. The network security group name can be found in the public subnet.\nEdit the inbound security rules to allow connections to the SSH port. In this example, we are using the default port. Info\nMake sure that your computer and office firewall rules allow you to send TCP traffic on the port you are using for SSH. If the SSH port is blocked at your computer or office firewall, you cannot connect to the Azure VNet via SSH. Generate SSH key pair Open a local terminal.\nCreate an SSH key pair by running this command:ssh-keygen -t rsa -b 4096 -C Info\nYou must provide the path to the directory where you want to save the public and private key. The public key is saved with the extension .pub. Configure a new cluster with your public key Copy the ENTIRE contents of the public key file.\nOpen the cluster configuration page.\nClick Advanced Options.\nClick the SSH tab.\nPaste the ENTIRE contents of the public key into the Public key field. Continue with cluster configuration as normal. Configure an existing cluster with your public key If you have an existing cluster and did not provide the public key during cluster creation, you can inject the public key from a notebook. Open any notebook that is attached to the cluster.\nCopy the following code into the notebook, updating it with your public key as noted:%scala val publicKey = \"&lt;put your public key here&gt;\" def addAuthorizedPublicKey(key: String): Unit = { \u00a0 val fw = new java.io.FileWriter(\"/home/ubuntu/.ssh/authorized_keys\", /* append */ true) \u00a0 fw.write(\"\\n\" + key) \u00a0 fw.close() } addAuthorizedPublicKey(publicKey) Run the code block to inject the public key. SSH into the Spark driver Open the cluster configuration page.\nClick Advanced Options.\nClick the SSH tab.\nNote the Driver Hostname.\nOpen a local terminal.\nRun the following command, replacing the hostname and private key file path:ssh ubuntu@&lt;hostname&gt; -p 2200 -i &lt;private-key-file-path&gt;", "format": "html", "updated_at": "2023-03-15T18:48:15.212Z"}, "author": {"id": 868634, "email": "xin.wang@databricks.com", "name": "xin.wang ", "first_name": "xin.wang", "last_name": "", "role_id": "draft_writer", "created_at": "2022-05-11T20:30:41.357Z", "updated_at": "2023-04-27T22:00:47.646Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963771, "name": "azure"}, {"id": 3136278, "name": "ssh cluster"}], "url": "https://kb.databricks.com/clusters/azure-ssh-cluster-driver-node"}, {"id": 1664019, "name": "Custom garbage collection prevents cluster launch", "views": 4277, "accessibility": 1, "description": "Using a custom garbage collection algorithm on Databricks Runtime 10.0 and above prevents the cluster from starting.", "codename": "custom-garbage-collection-prevents-launch-dbr10", "created_at": "2022-12-08T00:54:05.374Z", "updated_at": "2022-12-08T01:00:30.231Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.1e9ad6dafc3dbbd4491a4e27c178f7bc\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p id=\"isPasted\">You are trying to use a custom Apache Spark garbage collection algorithm (other than the default one (parallel garbage collection) on clusters running Databricks Runtime 10.0 and above. When you try to start a cluster, it fails to start. If the configuration is set on an executor, the executor is immediately terminated.</p><p>For example, if you set either of the following custom garbage collection algorithms in your <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\"><strong>Spark config</strong></a>, the cluster creation fails.</p><p>Spark driver</p><pre>spark.driver.extraJavaOptions \u00a0-XX:+UseG1GC</pre><p>Spark executor</p><pre>spark.executor.extraJavaOptions -XX:+UseG1GC</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">A new Java virtual machine (JVM) flag was introduced to set the garbage collection algorithm to parallel garbage collection. If you do not change the default, the change has no impact.</p><p>If you change the garbage collection algorithm by setting <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.executor.extraJavaOptions</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.driver.extraJavaOptions</span> in your <strong>Spark config</strong>, the value conflicts with the new flag. As a result, the JVM crashes and prevents the cluster from starting.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>To work around this issue, you must explicitly remove the parallel garbage collection flag in your <strong>Spark config</strong>. This must be done at the cluster level.</p><pre id=\"isPasted\">spark.driver.extraJavaOptions -XX:-UseParallelGC -XX:+UseG1GC\r\nspark.executor.extraJavaOptions -XX:-UseParallelGC -XX:+UseG1GC</pre><p><br></p>", "body_txt": "Problem You are trying to use a custom Apache Spark garbage collection algorithm (other than the default one (parallel garbage collection) on clusters running Databricks Runtime 10.0 and above. When you try to start a cluster, it fails to start. If the configuration is set on an executor, the executor is immediately terminated. For example, if you set either of the following custom garbage collection algorithms in your Spark config , the cluster creation fails. Spark driver spark.driver.extraJavaOptions \u00a0-XX:+UseG1GC Spark executor spark.executor.extraJavaOptions -XX:+UseG1GC Cause A new Java virtual machine (JVM) flag was introduced to set the garbage collection algorithm to parallel garbage collection. If you do not change the default, the change has no impact. If you change the garbage collection algorithm by setting spark.executor.extraJavaOptions or spark.driver.extraJavaOptions in your Spark config, the value conflicts with the new flag. As a result, the JVM crashes and prevents the cluster from starting. Solution To work around this issue, you must explicitly remove the parallel garbage collection flag in your Spark config. This must be done at the cluster level. spark.driver.extraJavaOptions -XX:-UseParallelGC -XX:+UseG1GC spark.executor.extraJavaOptions -XX:-UseParallelGC -XX:+UseG1GC", "format": "html", "updated_at": "2022-12-08T01:00:30.197Z"}, "author": {"id": 789479, "email": "harikrishnan.kunhumveettil@databricks.com", "name": "harikrishnan.kunhumveettil ", "first_name": "harikrishnan.kunhumveettil", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-26T05:57:22.814Z", "updated_at": "2023-04-07T07:50:41.846Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963770, "name": "azure"}], "url": "https://kb.databricks.com/clusters/custom-garbage-collection-prevents-launch-dbr10"}, {"id": 1664007, "name": "CPU core limit prevents cluster creation", "views": 6472, "accessibility": 1, "description": "Learn how to fix a CPU core quota limit Cloud Provider Launch error when starting a Databricks cluster.", "codename": "azure-core-limit", "created_at": "2022-12-08T00:08:35.508Z", "updated_at": "2022-12-08T00:46:01.627Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStna1FQUlI2S0xCOVM2aFRTTEZuZFk1QlYrOUpwTWdTcDJ4WHJmSjE5R3BUbHBKM2VGCnhRN1VYSDJETk9wVXRubUtGMURtSFlkTWhzaHpnZmFteUJqSHAxbG9DWktpTUczSk8vbjd4M0U3eFVhSQoyNWRsdUVIalpzRjBDYWFXczBDbGZRUzcyNVEzNDRVY01wS1BYV0hCSzdnUDBSS09nYW9laFJTa1NGNk0KK2dTczdJenZCaHJXbzllMXJTSU9IWXZiTzZGS3pxTjVtVzJpWkRzREprM3U1K3o5K2JRZEhhM1k0Zk5UCmdpRThhbW5QLzBPRUo2QnIzalhtMExFbEZGSjNSeWlueGNKRVZlQlFrR3NsQ2k0czhKS2J4QUpmV1VuMgpNUHV0bFY5UTBIUGlSMWVndnN5d2ZOTS9BYkdGcHBjeE9kK1hFcHBYSzlNSDFuVzdZT2VmUVYwM3ArWU8KWmRXVmRQQk4yYzZGanhLbzZPSnIxcjJSNkRDc2orTWF6ZEdldUtyMFJnWUcwUmFiY3hibmtBcXFwc0lxClROQWlFU1dNQWhyMFNMa3hHdVFWRG4reUxic2JrYzVVNldOdC9MRnpMalJMT1F5TVliTFRaVG5Qd2ZKeQord0JKaC9KWFVXL05RRnRBY2N6aEgvamlHUFppRzdMYUVsMlBlK3NFT1pMTVZER1kzODFhNXBBVWViUmsKNGFJV1owSXM4elRNUGJOLzNYbWc5MUJYdVd6RmkxcGlQOG90am9UUGNuMmlMMlVEbVNzbmZTMENIS1pOClVuME9ZVkd0VytFYjhnM01xRTRRNG5YUjk2dEUwMENUZTJmZDdMaU1pQVRZY0JGN2VZSnJHa1Fsek1wcQpIYU04NjVHRmFFakNmM2RBZ0NFcFJyUlhiQkJBWS9XYU94U20ycTREVjlRQUJ6N3IyaFRQN2xwbUtoYmIKWEhVR1h1KzZsaUxIMWcwZ2dDSHY0YUtTWmRsamZpT1UxVVRqQjh5WFMybGl1cEpZRjZnTWtyZWQyZVB1CnQzTkQ2anJrNnVPTEF1MGhPREx0TWNIbnhYOXViQ29uWFpNaHhaS1k4cjNGQWhJeDZsZDYrUnNTbklZSQpmR2FzV2xnTFdER1IyWDlZYUc2N3lYUU16OFVnbTJBTVNwVFpWQ2RrUEdnTG1HUTlHbHZEei8wNytYOGUKUGZLSXVlL3NvNXorbHEzbXNtM3BDaEh5b1FQWHlBaVhJeUpNNXppTXMrTDlJcFhCVHZCVkxxWmRBOUZMCjZ4ME96cjE4ZTJsK0dHcWtmSnBJeHd5em9sSzlsTVZrRHl0OG1FejVWRzBBdStNM05kSlRQblN4Q0NaOApud0xIWjRpTWtLRzNjaEdvNmNNZ1FVMWtublRCUUd1Uy9sZEt5cmJiVWdlaUJoaW5JN0g4emw2Mlp2NVYKdlRIVUV1eXBUZGpua0dZUCttNWMwUHdQMFJLYUtxN2doRGEyMGJRNFlUUElTWm5UcWhQaGVXb1hTai8wClRGSG5scmplakdCYTlaanBIYWh1b1ZIZTlvWXVrTENPODFYeU5xWlRrOHR5c3N2QXE1TFA5VXVQR0RIOQp4Vk85MVpGRVR5TWs5YTRZWngyWndwMXFGYy9yaVUwZlFpL3NFbFRIQmpmS3VUc0lXL3RuQ0lXaVRtZEEKa3JDUWxsWXk0alVpZlhvVkUwanlDTFFvWk5ZMEQwL1QyWXNaeDZyOFMyMVUvaU1oemRHSzQ2L05raERrCmJ0dTNSdERZTWVjNFAzYlpZZlBsUzBhSCtudU94THozME80UEIvdmwwRUc4MGk1L1dFYTZQOUN4eFB4VApMZ3pMNXEzaUhSa1VoYi95OHIrZ1YvR0UvSCsrCg==.06aa94096dfa160a76ba16a625629a11\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p id=\"isPasted\">Cluster creation fails with a message about a cloud provider error when you hover over cluster state.</p><pre>Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster.</pre><p>When you <a href=\"https://learn.microsoft.com/azure/databricks/clusters/clusters-manage#view-a-cluster-event-log\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">view the cluster event log</a> to get more details, you see a message about core quota limits.</p><pre>Operation results in exceeding quota limits of Core. Maximum allowed: 350, Current in use: 350, Additional requested: 4.</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">Azure subscriptions have a CPU core quota limit which restricts the number of CPU cores you can use. This is a hard limit. If you try to start a cluster that would result in your account exceeding the CPU core quota the cluster launch will fail.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">You can either free up resources or request a quota increase for your account.</p><ul>\n<li>Stop inactive clusters to free up CPU cores for use.</li>\n<li>Open an Azure support case with a request to increase the CPU core quota limit for your subscription.</li>\n</ul>", "body_txt": "Problem Cluster creation fails with a message about a cloud provider error when you hover over cluster state. Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster. When you view the cluster event log to get more details, you see a message about core quota limits. Operation results in exceeding quota limits of Core. Maximum allowed: 350, Current in use: 350, Additional requested: 4. Cause Azure subscriptions have a CPU core quota limit which restricts the number of CPU cores you can use. This is a hard limit. If you try to start a cluster that would result in your account exceeding the CPU core quota the cluster launch will fail. Solution You can either free up resources or request a quota increase for your account. Stop inactive clusters to free up CPU cores for use.\nOpen an Azure support case with a request to increase the CPU core quota limit for your subscription.", "format": "html", "updated_at": "2022-12-08T00:46:01.622Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963762, "name": "azure"}], "url": "https://kb.databricks.com/clusters/azure-core-limit"}, {"id": 1664005, "name": "IP address limit prevents cluster creation", "views": 5036, "accessibility": 1, "description": "Learn how to fix a public IP address quota limit Cloud Provider Launch error when starting a Databricks cluster.", "codename": "azure-ip-limit", "created_at": "2022-12-08T00:02:47.222Z", "updated_at": "2022-12-08T00:07:03.742Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStlQ0g5MWlFck43bXl6UmIxR3lnVnlZUU1hV1BCK04za24zTXJPV1YyUDhpK2RnOXhrCngyQ2x5dWoxUit0TmdjYU1QSHY2YTc5Q2FQWGxod0xzeERHbVBFRCs3ODcrS0JmdDNxQWVoS1JXYjZzNgpDSVJXcVRXSHBoekhKSGZteGpTOE5nNVAvbGpnTHNXRUhHcXVZQkV3a21KZFFQdjg5em5SQVV6MlIzSW4KdDlySkJkYmI3VW1WbXp2cjA5M3hOdi8vR1R2eWRWK2RDR0Yvd0thOXN3RHhabWFIQXBRdGZoOXgxbmtSCllZWENSUWNYS0VLWUhqdkpucGxYYXlZU203MnNUSTE0UmhOV1A4bVZycE5OK3BLQUZNZG94QlByYk1CYgp4blNzTkJZSWw3TTFsdlNBMnZVYlpaVUh5N1J0RXJvRkFQWkxsWTF5VGJjaU5VTXFUcjhUczZWM3pZQksKQ09rY2VBSkdoSzFkdHM3eWIxM0ppMlNrYzdabVU4YVU3ZnNObnhUVnMxbHROczZQWGJQV1kzSEtnTCs3Ck5ISXRMS1dlVUkxNmxIdTYvWnJxSlRYRVBSZkZJNzVaV1lta3VrRXlMQmpubjJVcnRxeWxpZzV0M3AyWApVRjF2M2Q4ekgvekdRdHVoU2pVMDhMS1dCaGp6UmFLMi9aTU10Z3dtRTQwL2JlUHNwWTFIVEpVdXBoQlUKNzEzclFwZEdVVjd3RDZnc0NaSmdpMlZYT3N1NlpkZTBlaG1JNFcvWG00UzRHZkpaYm5kMExzeEQxZXRaCi9ScDAvaW5XaWFkS3JmWEtnTCsyT0NiUXJUcHNnTy9BekMrejQzYlJnYkJucXY3dWJQdDc0bXRlQjMrOApzdmphQm13TWJsK3h6VVZpS1FPRTNkaWxxQ1J4OTNSSTZGRCs4MWJ3UEhaV3Y3MFBtdDZhMEYwcll5RUoKcEU5dnc1Vk8wMURxTitXdjFjQVJXS0NCTk5qd0hEZDJUV3BNTkF3TmxORGVGZzNhZnFZaHpuenpnK0VOCnByenZHclFuS0xxTHU4b1ZyK1pTand4ZDVPNVI2Q1Nua2xsSmtwQkpIZndVOTRJbkUwMW9aaXVxQUNRVgpYb3d2UlFSN0RMeWJIQ21ESnFZVFR2b0RaS1RZVks2ZVFMa2syanBqeTNZWC85S2ovUE9DQ2tuT01uZHYKRW5SMnNKeHJYWlIvMEdUOEd5N2pweEJIWkorMkdSb2ltUkk3MElEVVcyUEJSSlB4UzlTb3pPU1F0SW9CCmh0anZTSFFBcy9Wa0VYTnlQdkh2ZW5PekU1NXQyOHJ6MjY4NTJ0MVR3NmtZTG1ZU3VHak5YaEJQVnlVQwphcm5hK2o0cmEraHo2U2s5eitYVy9RaytGRHNaS0tVYnZDZHI4dFY3TG1HZWxoTDFpYlMyTDlMOXlvRzgKazg2NmxwZmpSYUZXRnpxRzhMcnpmZTJtQ1NVNkwzdC9XT3hpUElBc25iemJrV2hYMnNXdVZ4Umpsb2xVCjVoUkR6K00vcW5qWStrbDZDT3hTME1xck1KVU9hZ1lFNTlHVHE1aG9mSVk2L1JSanNMbFNMdVVObGFrdApjVGNRN216Z1pUNjJsYzJxNUxFcENIOWRjSnRoN0R3R2MvQjdEM0NpeFQwSWFqck9NRjZ5SkRLT0ZrMGYKaFF0UWYrSGtsSGRvOHRyR0xmUkdaNzk3d0w5ZzJOWVErdFY2Q1NQWGpFWHFBM1MzSmw2eWFxRXdLcnFnCkpqNEJ0MTU1bTJ5SE1JN3FSdG1UbWFSc2FKWFl1RlV3L0VCVUlLbzJnbnBFUDEweGQxZDFyeWlhdzVFbQpsNXFXUURxNW9KWUo2YkZKR21YTndYNHYxS2pNCg==.fb51cb2f7bbf248e7489a131efab6dbe\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Cluster creation fails with a message about a cloud provider error when you hover over cluster state.</p><pre>Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster.</pre><p>When you <a href=\"https://learn.microsoft.com/azure/databricks/clusters/clusters-manage#view-a-cluster-event-log\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">view the cluster event log</a> to get more details, you see a message about <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">publicIPAddresses</span> limits.</p><pre>ResourceQuotaExceeded Azure error message: Creating the resource of type 'Microsoft.Network/publicIPAddresses' would exceed the quota of '800' resources of type 'Microsoft.Network/publicIPAddresses' per resource group. The current resource count is '800', please delete some resources of this type before creating a new one.'</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Azure subscriptions have a public IP address limit which restricts the number of public IP addresses you can use. This is a hard limit. If you try to start a cluster that would result in your account exceeding the public IP address quota the cluster launch will fail.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">You can either free up resources or request a quota increase for your account.</p><ul>\n<li>Stop inactive clusters to free up public IP addresses for use.</li>\n<li>Open an Azure support case with a request to increase the public IP address quota limit for your subscription.</li>\n</ul>", "body_txt": "Problem Cluster creation fails with a message about a cloud provider error when you hover over cluster state. Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster. When you view the cluster event log to get more details, you see a message about publicIPAddresses limits. ResourceQuotaExceeded Azure error message: Creating the resource of type 'Microsoft.Network/publicIPAddresses' would exceed the quota of '800' resources of type 'Microsoft.Network/publicIPAddresses' per resource group. The current resource count is '800', please delete some resources of this type before creating a new one.' Cause Azure subscriptions have a public IP address limit which restricts the number of public IP addresses you can use. This is a hard limit. If you try to start a cluster that would result in your account exceeding the public IP address quota the cluster launch will fail. Solution You can either free up resources or request a quota increase for your account. Stop inactive clusters to free up public IP addresses for use.\nOpen an Azure support case with a request to increase the public IP address quota limit for your subscription.", "format": "html", "updated_at": "2022-12-08T00:07:03.740Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963748, "name": "azure"}], "url": "https://kb.databricks.com/clusters/azure-ip-limit"}, {"id": 1664003, "name": "Slow cluster launch and missing nodes", "views": 4731, "accessibility": 1, "description": "Learn how to resolve a \"nodes could not be acquired\" error when starting a Databricks .", "codename": "azure-nodes-not-acquired", "created_at": "2022-12-07T23:58:58.195Z", "updated_at": "2022-12-08T00:07:30.600Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThVSUFTWTV1ZTVidFNLMXVXL3RkVXF0UGhVS0ltYWRCa0ZodVRicGJORWRzalJoSTA4CmNmeXJyUy9VRjNRaGZVMFMyZVpaTVM0TzFsQ2VkTU9qS2pEZGhnSUZTdDBMMTBSR3I3NlZEd2Q4NFByQwo5VnpxTm56UG1zZXlHdUI3SkthZUxqN01naVVkNGRsRXc0clU3N2w2NDNlNC9NUk1sZzJIMHZZUHhJT1YKSGZtMk9wd0F1eWdnT1B2aU5rMkM4M2pUOG5PRmNuOFExL3JLcW8vK1FXemdzQ1ZZNzRuOHR3RnZPc3pQCkxsRDhIcGZJckVzTFF1cXBDU0pqbWlTR3Npc3BQMmR2U3llMkF1VUdxTU15Sm85MFhEdTQrWU01ZkRYNQpGNk11ZEFrYml2RDFmNUJtZEJvVTREOFlyWWUzMk1LbG81R0NPNUhZMXB0YlI1NVJENHFxZW9wODVUYnAKVkU3VDUrUzlXWDJMcGFpTlpCNVlBQVp2N0p4VEQ1Q2FrbG5vaVZqNlNWRmFJd0dmZzZUMThGMHRXbFc2CnBWbzRLUGJJTlNtS1lCdjY5d3NrcHFyUWJLWmEwVE9TK1JxOFEvVHlkZ0ZQbFZGR2JRbDJDWDl1MTh0VQpmRW1NZitJMnErQkxnV1FxZEhoTU1YV2tiSFh6VUJIUVRtYjkwamxJVmxKeHhRbTRFU1l1d2NJU3lYVTcKNnBiZU9vcERyN3hROHpjQTVjRFBnRzArd3hYZWFQMTFTRFovekU1c2FLMUtPd0dlUE9acmdZZ2xqUG1SCnNtNFljcjNVaEEvSGhKUHdMelI0cDJnTUlZb1d3YnhHOE5nU3ZUeDBqSzRvaE1tc25VQy8yYnFUZjVOZApBM0VNbGlnRzVrVFB5OVY2cEQxellhVXRybnYzZzlLelJXR2hXNnNnT1lpeGlkdUY4YW5DWEZ0MUNJWjIKV0k4T1kzZm9qNVlZaVRacU9QU3lodGw4TFh1NEtPTUdibmluYW5yU0greFd2dE5ieGg3SSsvNnBiRVl4CjNUSmVsYTlEa1ZpbmowQ0hUWlYwanZXN1E0ZDZyZVJvNE5WK3NpVXF2Rmtoai9ldVVZNUlKYkxvUXEyNwpmNCtkUnRnV0tuU1lzcE1DemJhTFJ2NXFSc2dSVmE1alpYalYwNVVsMHk4UXB0SGZ0ZTg2ZVBtdVIvRjEKQWh4TGVGNmI0VGpoV1BCVlg0TlhoM2VEZ0o3amtvVjFQQlZGUWpXRkhlZmVSd09kSDBySmdjeFhqNFZuCno3em5jRGtjQ3BvNk91bWpFY0h4dzhhY0R5ODNLcG12RC9Pb2w2QU9LQTd0Z2ZPL3dzRDYzQnhuQU1obApiQk9WZ2tDbGZFVSsrYk9xbHVCYnF4amR3QTcyT0ljdDlMcDRDUDByeHUwL2I4aVAvemorQldaWTBZZ0MKUnNoL3NOWHhBeGx3QjQrNGttUXdiQlVnVThEamFvM1JLZDhEdHhibVRNYW1lMGdzNTZGTGh2T2hPcWNvCjNWdVNkcjN6RzJqS09nQStrUXBrT2tpa1Z3WHdDeFNzODVXeXEzWGp5M1hkRlg1cG1GK3BPWDYyeGVvTwo3VzkreEpMWUh2RTlBZ29aUEtDWmdMYlcvemVvZ0RKU3l3ekRHdGdVNWVGYUErZ1J3NVVmMmRxcnZKMTcKdE5vc0k5SjJ1OW5iQVcwTkxUN09ndWZmTkNuWnFPRm5IY0lXS1V3aFZRPT0K.657094d2f76a79b42e148bb24d875744\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>A cluster takes a long time to launch and displays an error message similar to the following:</p><pre>Cluster is running but X nodes could not be acquired</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Provisioning an Azure VM typically takes 2-4 minutes, but if all the VMs in a cluster cannot be provisioned at the same time, cluster creation can be delayed. This is due to Azure Databricks having to reissue VM creation requests over a period of time.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">If a cluster launches without all of the nodes, Azure Databricks automatically tries to acquire the additional nodes and will update the cluster once available.</p><p>To workaround this, you should configure a cluster with a bigger instance type and a smaller number of nodes.</p>", "body_txt": "Problem A cluster takes a long time to launch and displays an error message similar to the following: Cluster is running but X nodes could not be acquired Cause Provisioning an Azure VM typically takes 2-4 minutes, but if all the VMs in a cluster cannot be provisioned at the same time, cluster creation can be delayed. This is due to Azure Databricks having to reissue VM creation requests over a period of time. Solution If a cluster launches without all of the nodes, Azure Databricks automatically tries to acquire the additional nodes and will update the cluster once available. To workaround this, you should configure a cluster with a bigger instance type and a smaller number of nodes.", "format": "html", "updated_at": "2022-12-08T00:07:30.593Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963747, "name": "azure"}], "url": "https://kb.databricks.com/clusters/azure-nodes-not-acquired"}, {"id": 1663995, "name": "SAS requires current ABFS client", "views": 3255, "accessibility": 1, "description": "SAS requires the current ABFS client; old clients generate an `IllegalArgumentException` error.", "codename": "sas-requires-current-abfs-client", "created_at": "2022-12-07T23:23:20.714Z", "updated_at": "2022-12-07T23:28:12.966Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.0c2deb6a47611ba212c6ce531518455a\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>While using SAS token authentication, you encounter an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IllegalArgumentException</span> error.</p><pre>IllegalArgumentException: No enum constant shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AuthType.SAS</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>SAS requires the current ABFS client. Previous ABFS clients do not support SAS.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">You must use the current ABFS client (<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem</span>) to use SAS.</p><p>This ABFS client is available by default in Databricks Runtime 7.3 LTS and above.</p><p>If you are using an old ABFS client, you should update your code so it references the current ABFS client.</p>", "body_txt": "Problem While using SAS token authentication, you encounter an IllegalArgumentException error. IllegalArgumentException: No enum constant shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AuthType.SAS Cause SAS requires the current ABFS client. Previous ABFS clients do not support SAS. Solution You must use the current ABFS client (shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem) to use SAS. This ABFS client is available by default in Databricks Runtime 7.3 LTS and above. If you are using an old ABFS client, you should update your code so it references the current ABFS client.", "format": "html", "updated_at": "2022-12-07T23:28:12.964Z"}, "author": {"id": 789487, "email": "kavya.parag@databricks.com", "name": "kavya.parag ", "first_name": "kavya.parag", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T06:35:45.179Z", "updated_at": "2023-03-29T14:10:28.525Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256028, "name": "Cloud infrastructure", "codename": "cloud", "accessibility": 1, "description": "These articles can help you manage the configuration for your Databricks workspaces.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963740, "name": "azure"}], "url": "https://kb.databricks.com/cloud/sas-requires-current-abfs-client"}, {"id": 1663992, "name": "Jobs are not progressing in the workspace", "views": 4312, "accessibility": 1, "description": "Learn how to troubleshoot a Databricks workspace in a virtual network using Azure Firewall. \"WARN TaskSchedulerImpl Initial job has not accepted any resources\" error message.", "codename": "azure-vnet-jobs-not-progressing", "created_at": "2022-12-07T23:16:28.717Z", "updated_at": "2022-12-07T23:28:38.400Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStrMFZYWjNSRndOZXdTSHoraEk4VUpoS0JCUnZZU1JtdFJHWWRpTGxnbXliV0h3RjViCm5QUmYzd01ESEZ1T2Zzdm93RnNZL3ZRL0EzMFRqS2NZcDRUWVpNSFhSaFhSY2R4S1ZuMXBOYjFiZ1ppZApCMEtEM1pOcktlWFU1WS9sWFBCd3NPS1NhTHh6ZFlOMVkxRm82a0JVUVJyM05taFIxUmVvWXJWaDJpbFYKc1M1bnQ0bVlXaC9pZ1pRdFhGZ2dRTU1rSHNLQms3TVJWSzF5c01iK0VIb3hOZnNnZEFyYUtBWXRjdk1MCjhsRmxuZzRXRVI5ZHA4SFMrV2ZlbjhqVkFoUUlFNXpGN0JnUk1Xc3ZaNTFaU2hiVkdzQUVBQnlqcjVYeQpRSGJxNGVGbCt4ZGVqMitIdHdTeTM2dzJLZ1BodFpaajVQZXNVYitkZGpRbVlJQm1WV2RNVm93VGsyN2oKd1VOdW5PMnJZbzN3aHVLam5nelpFWnJ6aUxyeWx4V2FoWHBHQkJxKzdyVTdSQ0NPSzVVbjN4VmUzeTFHCkNqZDNDakdpL2s0SXZYMHl6M1BkRkFtaytPd3k0Qjc5b1hKYXpzeEJpeXVYV3RLa1FIUkdKNUV4UmszdQp2RWtoa1Y3VCtjcGFYRDRZRU92TUs2c2phZk5WTUtlWEVQTHRhdVFhMjhGREdBRW9IbXVVMVRQK2F4dG0KTGFuYUVTclgxVGw0azVuSVB4Y3Z3cjZ2d3RUMnhQUkNCLzNEWENERDU4VWREcVR6Z0FDT1IyRXhnL2VSCjJLdVh1YzVobCtaTW50TG43dFkwUytzSmdQeXFzRjdTN1BOdTRqTHJueFh4UXNvU2Vwb0o5aEs4aTdUdgpydTZ0bjFicVVHZmJXdlE0TDhEakF3eVdRdWpOM2J5VW5DMlIzVTJ3U3YyTjUvRzBQdlFsczZ4TjRSRXcKaGFvaTJueXFlYVNNa2FiVXErRHlRTW1DQ0FqeFFkMFdEYTZCeUNoS2JhVjBMUFhsSTVaT3lQMjdrMDlCCkNYMDhmZ283dVR4bG4weUg2Mi9YU25kZit2MlJYcVZxOTgrTnplUW9QV01NWUkvN1BtMjBpc3dFWG01ZwpjbXZtR3B0WVNWZFhoR0xTNDZlRGVFTkpkZ3ZrUHlqZmNJN29aaUE4d2U5V2hqM0NvY0NCeFZTYS9ocnIKdk9xTlpQTTduYms3YUdoYlcyNjR4R0pSVHVEZXFkdVRHRnE4MGN0M21lMFZhMmhOc1NLRHM4YXp4UE9PCkVNekkxTU03c1I1NjhpY1V5RHNrQ090bS9sTTFGVXAvbmU1WWlxL2RHbGdsNGlWaXNZVnNmVncyVVN2MgpvL09IdE5sRlBnTDBRYUVFbGh5d2MvTU5yN2dXM1ZmRFI1Yi9GZEhHZE1XMC9WZHdVRURjVjNYMGdLY3YKeENBdVJvVE4wRndlUW1kdnhPcEhQK1lETzJpdWdmbmgwcUViUVJ1WHh2MTl0YjJKWlRxa0Y4TmE3emNuCnpmREJmSkdxTVE1MnRPMHd2cHZsYXRVYUxqNU5YbmV3cDVweVo4Q2Y4RUVJbkpNRWw5U1BhcEFjTjZHaQpQMVdjbGc4RGFSRlFOdGs5dXpKbm1JMFNCRlY1K0dXVDZpN2UzVFdNWHpSeXd6c0JnNHVEZnljdE9zVlcKK09KUUNlMHdsYzgzeEdMRkZRSWNUWEtxNXl4cnV2R0xFTjJMNWpKSEhoRnRlMFdzb3V4ekNCQVo0bGpnCjMxND0K.a631ff439a7727aae6fe3c555820c8ee\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Jobs fail to run on any cluster in the workspace.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">This can happen if you have changed the VNet of an existing workspace. Changing the VNet of an existing Azure Databricks workspace is not supported.</p><p>Review <a href=\"https://learn.microsoft.com/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Deploy Azure Databricks in your Azure virtual network (VNet injection)</a> for more details.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><ol>\n<li id=\"isPasted\">Open the cluster driver logs in the Azure Databricks UI.</li>\n<li>Search for the following WARN messages:<pre>19/11/19 16:50:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\r\n19/11/19 16:50:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\r\n19/11/19 16:50:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources</pre>If this error is present, it is likely that the VNet of the Azure Databricks workspace was changed.</li>\n<li>Revert the change to restore the original VNet configuration that was used when the Azure Databricks workspace was created.</li>\n<li>Restart the running cluster.</li>\n<li>Resubmit your jobs.</li>\n<li>Verify the jobs are getting resources.</li>\n</ol>", "body_txt": "Problem Jobs fail to run on any cluster in the workspace. Cause This can happen if you have changed the VNet of an existing workspace. Changing the VNet of an existing Azure Databricks workspace is not supported. Review Deploy Azure Databricks in your Azure virtual network (VNet injection) for more details. Solution Open the cluster driver logs in the Azure Databricks UI.\nSearch for the following WARN messages:19/11/19 16:50:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources 19/11/19 16:50:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources 19/11/19 16:50:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resourcesIf this error is present, it is likely that the VNet of the Azure Databricks workspace was changed.\nRevert the change to restore the original VNet configuration that was used when the Azure Databricks workspace was created.\nRestart the running cluster.\nResubmit your jobs.\nVerify the jobs are getting resources.", "format": "html", "updated_at": "2022-12-07T23:28:38.397Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256028, "name": "Cloud infrastructure", "codename": "cloud", "accessibility": 1, "description": "These articles can help you manage the configuration for your Databricks workspaces.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963739, "name": "azure"}], "url": "https://kb.databricks.com/cloud/azure-vnet-jobs-not-progressing"}, {"id": 1663982, "name": "Network configuration of Azure Data Lake Storage Gen1 causes ADLException: Error getting info for file", "views": 3943, "accessibility": 1, "description": "Learn how to resolve credential passthrough failure when using Azure Data Lake Storage Gen1 with Databricks.", "codename": "azure-vnet-gen1-issue", "created_at": "2022-12-07T22:43:58.127Z", "updated_at": "2022-12-07T23:27:54.969Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9mTjdCQ2oyMkZFV09iSmllUTZhZVhvdUc2SDljMFovdEJKZEpEVmdXaFJ5TFFCTXgxClhrUjBBd0p5OEpkaTJvOUJGazFTemNRZWtXSVVEVzRPbHI3bEtLVWFyMyt4UWZGVEM1UW1kVXR5eW0rcQpFdWpCcCtLK3lHOEtsWXI5RjFuWll5UEtmTXBjMjNQUHBLUGQzK2JnWDdrVSszdytQcVFNUnNOcFRJVm0KcUxtcjRTS2EzSXo0T2xVdFgvZ1gxK3c1MFpPcVFvN281VEx6OCtyMGgyU3NVRVZoS1g1ckhyc3ZJbkc4CkdBMUt5LzVWMUVPWEFwWHNzZHhIMjE1L1hqcmdQVDQ5RGxrcWpRWHA5NHNTeGZ5NW9INzRPQTQ1VDF3bgpPT1VJdFp0QUlwVDNsd2w4Y1E2cU5FcUw3T1JlM2VTUVBIKzNDNXlUa1MwWmJ3N2sxZ3hRNVdPdS84dVUKMGo0anU5Wi9zZDE0emZQSGR4Vm9kMnF1MW94SlV3SWV5Tis5WnlWRks2aWVRZUQzdkZMOVIxK045a1VrClFQTnlkeFcvUU1mUWxmVTZNWUVyeGFLMnVCTHA3eXJrRFEzMjNKUGcwaGxzNTdNbEt2NitQSFpaYzZWNgpuZG1ZUDNXR1RHNksxVDZuenVWZ09xTkE4RDl4aWltd253aGxGWXgvNHZLZTFIdVg0MXkvMC81dHh5ejkKT0p6cUUvYWNQRlRsc29Gb0U1QkFmZncra3NHdkhHMnlRN3Eralp5TENoN2xLOGRuKzhyUVgrdi9DWmNYCmF3YTJNbVJzekpTNEkzZHNHMWxYSnJDUGZPdUNNV21zOERDNVJ2bkhxbzQ4aXhwaUp1SGhEYmVMbDJqcwpqN3lURW5aRWVNdlRYSFRLYXhKMUt1ajJvbEN1RFBPWEJ5OUZRSW1TVEN2WWFVc0tUTHFKUkJiQUVGWGIKN0dFckhCTzU3cTVhNlBhd3Y1d3g2MzRmQk9jQU9ydVRXaHJVSFpqWFpydjlJQ21RTnpaTjd5dWJJV0kwCjgrSHJtQ0JmVVdZNkhzT3NIekpvUGxOaE80Q0xhZko1eno0NkNWSlo0dVAzb0pjcUpjMnNuRi8rRmZUcwpONk92U2FhNDkwbzc3a1ZXQWFPTTNPQ2N5UGJTN0ZZTi9vYllhUXFvUTFJeW9KVWhrbU5MMVdhc3BFU2MKcW9HeFVEd1FWdmxJMk1LRmltODVURFR4Smp1LzBDNnhxM2ZhL2dtLzNZMVpmUVIyOFNaQ29XcTBYanBsClV1Q0pJWU9SOXcyR2pQOVRHMHBuWDh5MFRYNHQ5MDFTTHQ4bmVpNS84ZW5wVU05OVo5Qkh5NXlDQnQ5UAp1Ykg5bHVOcjQzV24ydnFSZ2ltYUhTYlEvS2xmaGw5ajNOTG9tTHBuTnk4UDJ2aksvYUZ2dDNpdEFBY1MKeGp2QVFNVkRSelozVEtCTjZsUTdTb09lVm9yUW5MNDV3NGJwazdFeEJjZjVDYVRvdWpQK1lXZHc5Z2RPClF3WmVEaUZJZE8rMWp1VERFUGFlMmtpTnZTbEZJVnUyNUwwQUQ5elZqcFpiQmUrckQ5M3JNUmxaT2huTAp3dmVWZ2NRK3o4TW1aaExpL2VJeEFQT0pRVFRFTFhQbFNjYVVqUjVTa21qZEpMNXM3N2I5dXZYeUFlVzAKTEZ3N2swUmdrd0N1cktnczMwQm1jV0pjaWUwZVNIWUUzRGRLRmhWUTl6Z2NUQmgzcGowd3hWOE5qUVRrCmlwWlZSVVI4b3M4bDB4UW0vakY2QzFlcgo=.3eba416b177e437bdc43f07e76ff4a94\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Access to Azure Data Lake Storage Gen1 (ADLS Gen1) fails with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ADLException: Error getting info for file &lt;filename&gt;</span> when the following network configuration is in place:</p><ul>\n<li>Azure Databricks workspace is deployed in your own virtual network (uses VNet injection).</li>\n<li>Traffic is allowed via Azure Data Lake Storage credential passthrough.</li>\n<li>ADLS Gen1 storage firewall is enabled.</li>\n<li>Azure Active Directory (Azure AD) service endpoint is enabled for the Azure Databricks workspace\u2019s virtual network.</li>\n</ul><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1670453128753-azure-gen1-access-issue.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">Azure Databricks uses a control plane located in its own virtual network, and the control plane is responsible for obtaining a token from Azure AD. ADLS credential passthrough uses the control plane to obtain Azure AD tokens to authenticate the interactive user with ADLS Gen1.</p><p>When you deploy your Databricks workspace in your own virtual network (using VNet injection), Azure Databricks clusters are created in your own virtual network. For increased security, you can restrict access to the ADLS Gen 1 account by configuring the ADLS Gen1 firewall to allow only requests from your own virtual network, by implementing service endpoints to Azure AD.</p><p>However, ADLS credential passthrough fails in this case. The reason is that when ADLS Gen1 checks for the virtual network where the token was created, it finds the network to be the Azure Databricks control plane and not the customer-provided virtual network where the original passthrough call was made.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">To use ADLS credential passthrough with a service endpoint, storage firewall, and ADLS Gen1, enable <strong>Allow access to Azure services</strong> in the <a href=\"https://learn.microsoft.com/azure/data-lake-store/data-lake-store-security-overview#network-isolation\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">firewall settings</a>.</p><p>If you have security concerns about enabling this setting in the firewall, you can upgrade to ADLS Gen2. ADLS Gen2 works with the network configuration described above.</p><p>For more information, see:<a href=\"https://learn.microsoft.com/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject\" title=\"\" id=\"\" target=\"_blank\" rel=\"noopener noreferrer\"></a></p><ul>\n<li><a href=\"https://learn.microsoft.com/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Deploy Azure Databricks in your Azure virtual network (VNet injection)</a></li>\n<li><a href=\"https://learn.microsoft.com/azure/databricks/security/credential-passthrough/adls-passthrough\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Access Azure Data Lake Storage using Azure Active Directory credential passthrough</a></li>\n</ul><p><br></p>", "body_txt": "Problem Access to Azure Data Lake Storage Gen1 (ADLS Gen1) fails with ADLException: Error getting info for file &lt;filename&gt; when the following network configuration is in place: Azure Databricks workspace is deployed in your own virtual network (uses VNet injection).\nTraffic is allowed via Azure Data Lake Storage credential passthrough.\nADLS Gen1 storage firewall is enabled.\nAzure Active Directory (Azure AD) service endpoint is enabled for the Azure Databricks workspace\u2019s virtual network. Cause Azure Databricks uses a control plane located in its own virtual network, and the control plane is responsible for obtaining a token from Azure AD. ADLS credential passthrough uses the control plane to obtain Azure AD tokens to authenticate the interactive user with ADLS Gen1. When you deploy your Databricks workspace in your own virtual network (using VNet injection), Azure Databricks clusters are created in your own virtual network. For increased security, you can restrict access to the ADLS Gen 1 account by configuring the ADLS Gen1 firewall to allow only requests from your own virtual network, by implementing service endpoints to Azure AD. However, ADLS credential passthrough fails in this case. The reason is that when ADLS Gen1 checks for the virtual network where the token was created, it finds the network to be the Azure Databricks control plane and not the customer-provided virtual network where the original passthrough call was made. Solution To use ADLS credential passthrough with a service endpoint, storage firewall, and ADLS Gen1, enable Allow access to Azure services in the firewall settings. If you have security concerns about enabling this setting in the firewall, you can upgrade to ADLS Gen2. ADLS Gen2 works with the network configuration described above. For more information, see: Deploy Azure Databricks in your Azure virtual network (VNet injection) Access Azure Data Lake Storage using Azure Active Directory credential passthrough", "format": "html", "updated_at": "2022-12-07T23:27:54.967Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256028, "name": "Cloud infrastructure", "codename": "cloud", "accessibility": 1, "description": "These articles can help you manage the configuration for your Databricks workspaces.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963737, "name": "azure"}], "url": "https://kb.databricks.com/cloud/azure-vnet-gen1-issue"}, {"id": 1663970, "name": "Assign a single public IP for VNet-injected workspaces using Azure Firewall", "views": 4819, "accessibility": 1, "description": "Learn how to assign a single public IP address for a Databricks workspace in a virtual network using Azure Firewall.", "codename": "azure-vnet-single-ip", "created_at": "2022-12-07T22:31:21.150Z", "updated_at": "2022-12-07T23:29:03.860Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlJVEFWcmpPN0dPRXFjcnYxRHF3YWZydFJUT1d2UzY4MWJKdUFJZE42cUhYeE1lcU12Ck45MkcvUzlkYnJRQzVJS0oxZnRqR095Z29VZk1UL2J4TzhJbkhhSFNhYjE1UGVBeFh0R2JsRHFVdTNjSAorL0VsS0tUT2hUV1VPNm1lcDdBcXd0SVBqWmhkRm92aHd0SWtmcjNCNkUyNkxuZytkZHMzQXM1ZWJJT2wKc0ZzZE14NlYyeUR2VXJhLzRuODlsdDFPM01PS2syVmphM0dNZUxmRFZ4RWdYOXM4WlRicnB0SlhYbklPClRnajBkcFoydjFSY1NzUHNCakZDc3IyaGE5ZGM0Ym4rK0dBRmNsQmtZamtlWnA4M05jbTlQSzZoYWk4TwpZQmY0RGpVQWE4bHE5SnJXcG9tZVlWNDB5NUVlem92YXdaekVlbVFkUFRNREJjZXpJTWtSWEhxWXlpb3AKdS9hSjFTRkZQbkEwcUU0M3h3THBKcndlKzdrdFlMZlhwdHo0NEM5cmZBS1d6S0EvTkVCSWsxdkFoRVFZCmxVaGY1bGErc3kzRTU0UEV3bmZnMVV1YjhaVTBTLytKaFBJbzlxQnZJRFZNeEpjSm43U1BrL21TK3pTSQpxZW9DMEVsblJxbXc2bUlTMEkxREFycmtjaWV5bnBLVloxT0ExL3RQZkVYZzRWNCtDRWZKemtKL05GaE4KNGhNWUpVS29vVy9zdVJzKzJCSHdFNVpNZVYrRVU3QkF5aVJ6WVErYStsYkdHVzJ5QzBTT0xaQVN2ZTJrCmc2THNRT1l6TGIwMTlybkEraDNkMnVnWTJ3V3lubFNjVDJaUjRCL3ZlejQ4SVR5NmtZWm1CMHErNW02bwpGVnFWWG5rV1k5UEhCcjlTREZPQ0xEK2Y0VGFXeTdKeVdKVm1kYzdWS2hubVdXd0Jveis3VENsM0lreEUKbTdFNHNhdFJwRzhuWlVLQ0FQd3lJU3ZWMWcxRXNibDVRaGxUVk8vclVTaTNoWDVFRVh0MDBMa1FRQjdBCjdNcGRFUzJDeTZ1dGt0TDBaSkJkYW51UEszWVZOMkF1SHRoQ05lbnRrWlhmSEtnak5PQ3AxSVQ3TGphUApJaUpUTWoxQTVkUG01Zmd2Vlh5Y212TE5UaCtEd0VhSGtkUHJyUHVNalQ4TUpNZ0FHajBudURDTlM0K1QKQzM2MzR6RnNEYndUM0M0eFovaTZLbnhLQUVTQmJMUDltR3FlUnR3Tmljb2l6enc2b3hZWTZNQjNVd2NYCmgxcWovSnV5cVI3QXFzU05TQWNXRXFJR3lmQUFFVk01Yi9sOHRYVlh6MnVoOWxpNThDZUYydWtGcE5WcApKMGpVRWMwbjJraW4zTi9IUDliNjl0R3Y0eHFPZjNvTHdJQnNwNi9Gelh2VWJyaG1TaVA1YTVpWXZPK3cKQ0dYVG9qOXRncTh0Ykgvb3E4SEVPWE9QbnNoaCsxYWVDd01uN2puM1RvVTY1dWk1OUtUWXo1NnZDRTU3CkFYbENnMWFvZnBGWG9VQVJ4Z3c0UFZrYnFJb3ZlRW9POER6OE9GbUlHSENOZjhySmRZT1RxNC90OHBDVQovdkYxWWRPL1ZWNUZJS0lmOXozcGdvcmxEMnRvQTRjUnFSVS9YZVY0bjVtaGYwTjlKVDh5eGRpVjZrb0wKSGZCejhUZ2RBdVBkVEpmclhScTdBVVNuMWNCaVZ1b3k1c2hVTWNoMmwzODhHUW8wNk9ReXhpK1FBdTlKCkdxaWtyUUdGUU01VnMxeXZxZUswTGJIMwo=.b55971c1b8a840031840ab03d87238b6\"></div><p id=\"isPasted\">You can use an Azure Firewall to create a <a href=\"https://learn.microsoft.com/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">VNet-injected workspace</a> in which all clusters have a single IP outbound address. The single IP address can be used as an additional security layer with other Azure services and applications that allow access based on specific IP addresses.</p><p>1. Set up an Azure Databricks Workspace in your own virtual network.</p><p>2. Set up a firewall within the virtual network. See <a href=\"https://learn.microsoft.com/azure/virtual-network/tutorial-create-route-table-portal#create-an-nva-virtual-machine\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Create an NVA</a>. When you create the firewall, you should:</p><ul>\n<li>Note both the private and public IP addresses for the firewall for later use.</li>\n<li>\n<a href=\"https://learn.microsoft.com/azure/firewall/tutorial-firewall-deploy-portal#configure-a-network-rule\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Create a network rule</a> for the public subnet to forward all traffic to the internet:<ul>\n<li>Name: any arbitrary name</li>\n<li>Priority: 100</li>\n<li>Protocol: Any</li>\n<li>Source Addresses: IP range for the public subnet in the virtual network that you created</li>\n<li>Destination Addresses: 0.0.0.0/1</li>\n<li>Destination Ports: *</li>\n</ul>\n</li>\n</ul><p>3. Create a Custom Route Table and associate it with the public subnet.</p><p style=\"margin-left: 20px;\">a. Add custom routes, also known as user-defined routes (<a href=\"https://learn.microsoft.com/azure/virtual-network/virtual-networks-udr-overview#custom-routes\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">UDR</a>) for the following services. Specify the <a href=\"https://learn.microsoft.com/azure/databricks/administration-guide/cloud-configurations/azure/udr#--control-plane-nat-webapp-and-extended-infrastructure-ip-addresses\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure Databricks region addresses</a> for your region. For <strong>Next hop type</strong>, enter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Internet</span>, as shown in <a href=\"https://learn.microsoft.com/azure/virtual-network/tutorial-create-route-table-portal#create-a-route\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">creating a route table</a>.</p><ul style=\"margin-left: 20px ;\">\n<li>Control Plane NAT VIP</li>\n<li>Webapp</li>\n<li>Metastore</li>\n<li>Artifact Blob Storage</li>\n<li>Logs Blob Storage</li>\n</ul><p style=\"margin-left: 20px;\">b. Add a custom route for the firewall with the following values:</p><ul style=\"margin-left: 20px ;\">\n<li>Address prefix: 0.0.0.0./0</li>\n<li>Next hop type: Virtual appliance</li>\n<li>Next hop address: The private IP address for the firewall.</li>\n</ul><p style=\"margin-left: 20px;\">c. Associate the route table with the public subnet.</p><p>4. Validate the setup</p><ul>\n<li>Create a cluster in the Azure Databricks workspace.</li>\n<li>Next, query blob storage to your own paths or run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">%fs ls</span> in a cell.</li>\n<li>If it fails, confirm that the route table has all required UDRs (including Service Endpoint instead of the UDR for Blob Storage)</li>\n</ul><p>For more information, see <a href=\"https://learn.microsoft.com/azure/databricks/administration-guide/cloud-configurations/azure/on-prem-network#route-via-firewall\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Route Azure Databricks traffic using a virtual appliance or firewall</a>.</p>", "body_txt": "You can use an Azure Firewall to create a VNet-injected workspace in which all clusters have a single IP outbound address. The single IP address can be used as an additional security layer with other Azure services and applications that allow access based on specific IP addresses. 1. Set up an Azure Databricks Workspace in your own virtual network. 2. Set up a firewall within the virtual network. See Create an NVA. When you create the firewall, you should: Note both the private and public IP addresses for the firewall for later use. Create a network rule for the public subnet to forward all traffic to the internet:\nName: any arbitrary name\nPriority: 100\nProtocol: Any\nSource Addresses: IP range for the public subnet in the virtual network that you created\nDestination Addresses: 0.0.0.0/1\nDestination Ports: * 3. Create a Custom Route Table and associate it with the public subnet. a. Add custom routes, also known as user-defined routes (UDR) for the following services. Specify the Azure Databricks region addresses for your region. For Next hop type, enter Internet, as shown in creating a route table. Control Plane NAT VIP\nWebapp\nMetastore\nArtifact Blob Storage\nLogs Blob Storage b. Add a custom route for the firewall with the following values: Address prefix: 0.0.0.0./0\nNext hop type: Virtual appliance\nNext hop address: The private IP address for the firewall. c. Associate the route table with the public subnet. 4. Validate the setup Create a cluster in the Azure Databricks workspace.\nNext, query blob storage to your own paths or run %fs ls in a cell.\nIf it fails, confirm that the route table has all required UDRs (including Service Endpoint instead of the UDR for Blob Storage) For more information, see Route Azure Databricks traffic using a virtual appliance or firewall.", "format": "html", "updated_at": "2022-12-07T23:29:03.856Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256028, "name": "Cloud infrastructure", "codename": "cloud", "accessibility": 1, "description": "These articles can help you manage the configuration for your Databricks workspaces.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963734, "name": "azure"}], "url": "https://kb.databricks.com/cloud/azure-vnet-single-ip"}, {"id": 1663899, "name": "Power BI proxy and SSL configuration", "views": 4912, "accessibility": 1, "description": "Learn how to set up Power BI with a proxy or VPN.", "codename": "powerbi-proxy-ssl-configuration", "created_at": "2022-12-07T21:59:29.459Z", "updated_at": "2022-12-07T22:18:20.782Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.a1f4a46a553b7f704dc6c7fb72138851\"></div><h1 data-toc=\"true\" id=\"driver-configurations-0\">Driver configurations</h1><p>You can set driver configurations using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">microsoft.sparkodbc.ini</span> file which can be found in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ODBC Drivers\\Simba Spark ODBC Driver</span> directory. The absolute path of the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">microsoft.sparkodbc.ini</span> directory depends on whether you are using Power BI Desktop or on-premises Power BI Gateway:</p><ul>\n<li>Power BI Desktop: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">C:\\Program Files\\Microsoft Power BI Desktop\\bin\\ODBC Drivers\\Simba Spark ODBC Driver\\microsoft.sparkodbc.ini</span>\n</li>\n<li>Power BI Gateway: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">m\\ODBC Drivers\\Simba Spark ODBC Driver\\microsoft.sparkodbc.ini</span>, where <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">m</span> is placed inside the gateway installation directory.</li>\n</ul><h1 data-toc=\"true\" id=\"set-driver-configurations-1\">Set driver configurations</h1><ol>\n<li>Check if the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">microsoft.sparkodbc.ini</span> file was already created. If it is then jump to step 3.</li>\n<li>Open <strong>Notepad</strong> or <strong>File Explorer</strong> as <strong>Run As Administrator</strong> and create a file at <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ODBC Drivers/Simba Spark ODBC Driver/microsoft.sparkodbc.ini</span>.</li>\n<li>Add the new driver configurations to the file below the header <strong>[Driver]</strong> by using the syntax =. Configuration keys can be found in the manual provided with the installation of the <a href=\"https://www.databricks.com/spark/odbc-drivers-download\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Databricks ODBC Driver</a>. The manual is located at <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">C:\\Program Files\\Simba Spark ODBC Driver\\Simba Apache Spark ODBC Connector Install and Configuration Guide.html</span>.</li>\n</ol><h1 data-toc=\"true\" id=\"configuring-a-proxy-2\">Configuring a proxy</h1><p>To configure a proxy, add the following configurations to the driver configuration in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">microsoft.sparkodbc.ini</span> file:</p><pre id=\"isPasted\">[Driver]\r\nUseProxy=1\r\nProxyHost=&lt;proxy.example.com&gt;\r\nProxyPort=&lt;port&gt;\r\nProxyUID=&lt;username&gt;\r\nProxyPWD=&lt;password&gt;</pre><p>Depending on the firewall configuration it might also be necessary to add:</p><pre id=\"isPasted\">[Driver]\r\nCheckCertRevocation=0</pre><h1 data-toc=\"true\" id=\"troubleshooting-3\">Troubleshooting</h1><h2 data-toc=\"true\" id=\"error-ssl_connect-certificate-verify-failed-4\">Error: SSL_connect: certificate verify failed</h2><p>When SSL issues occur, the ODBC driver returns a generic error <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SSL_connect: certificate verify failed</span>. You can get more detailed SSL debugging logs by setting in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ODBC Drivers/Simba Spark ODBC Driver/microsoft.sparkodbc.ini</span> file the following two configurations:</p><pre id=\"isPasted\">[Driver]\r\nAllowDetailedSSLErrorMessages=1\r\nEnableCurlDebugLogging=1</pre><h2 data-toc=\"true\" id=\"diagnose-issues-by-analyzing-cryptoapi-logs-5\">Diagnose issues by analyzing CryptoAPI logs</h2><p>Most issues can be diagnosed by using Windows CryptoAPI logs, which can be found in the Event Viewer. The following steps describe how to capture these logs.</p><ol>\n<li>Open <strong>Event Viewer</strong> and go to <strong>Applications and Services Logs</strong> &gt; <strong>Microsoft</strong> &gt; <strong>Windows</strong> &gt; <strong>CAPI2</strong> &gt; <strong>Operational</strong>.</li>\n<li>In <strong>Filter Current Log</strong>, check the boxes <strong>Critical</strong>, <strong>Error</strong>, and <strong>Warning</strong> and click <strong>OK</strong>.</li>\n<li>In the <strong>Event Viewer</strong>, go to <strong>Actions</strong> &gt; <strong>Enable Log</strong> to start collecting logs.</li>\n<li>Connect <strong>Power BI</strong> to Azure Databricks to reproduce the issue.</li>\n<li>In the Event Viewer, go to <strong>Actions</strong> &gt; <strong>Disable Log</strong> to stop collecting logs.</li>\n<li>Click <strong>Refresh</strong> to retrieve the list of collected events.</li>\n<li>Export logs by clicking <strong>Actions</strong> &gt; <strong>Save Filtered Log File As</strong>.</li>\n</ol><h2 data-toc=\"true\" id=\"diagnose-build-chain-or-verify-chain-policy-event-errors-6\">Diagnose Build Chain or Verify Chain Policy event errors</h2><p>If the collected logs contain an error on the <strong>Build Chain</strong> or <strong>Verify Chain Policy</strong> events, this likely points to the issue. More details can be found by selecting the event and reading the <strong>Details</strong> section. Two fields of interest are <strong>Result</strong> and <strong>RevocationResult</strong>.</p><ul>\n<li>The revocation status of the certificate or one of the certificates in the certificate chain is unknown.<ul>\n<li>\n<strong>CAPI2 error</strong>: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RevocationResult: [80092013] The revocation function was unable to check revocation because the revocation server was offline.</span>\n</li>\n<li>\n<strong>Cause</strong>: The revocation check failed due to an unavailable certificate revocation server.</li>\n<li>\n<strong>Resolution</strong>: Disable certificate revocation checking.</li>\n</ul>\n</li>\n<li>The certificate chain is not complete.<ul>\n<li>\n<strong>CAPI2 error</strong>: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Result: [800B010A] A certificate chain could not be built to a trusted root authority.</span>\n</li>\n<li>\n<strong>Cause</strong>: The certificate advertised by the VPN or proxy server is incomplete and does not contain a full chain to the trusted root authority.</li>\n<li>\n<strong>Resolution</strong>: The preferred solution is to configure the VPN or proxy server to advertise the full chain. If this is not possible, a workaround is to obtain the intermediate certificates for the Databricks workspace, and install these in the Intermediate Certification Authorities store, to enable Windows to find the unadvertised certificates. If possible, it is recommended to install these certificates for all Power BI users using a group policy in Windows. This has to be set up by the system administrator.</li>\n</ul>\n</li>\n</ul><h2 data-toc=\"true\" id=\"certificate-configurations-7\">Certificate configurations</h2><h3 data-toc=\"true\" id=\"disable-certificate-revocation-checking-8\">Disable certificate revocation checking</h3><p>If the ODBC driver is unable to reach the certificate revocation list server, for example because of a firewall configuration, it will fail to validate the certificate. This can be resolved by disabling this check. To disable certificate revocation checking, set the configuration <strong>CheckCertRevocation=0</strong> to the <strong>microsoft.sparkodbc.ini</strong> file.</p><h3 data-toc=\"true\" id=\"install-intermediate-certificates-9\">Install intermediate certificates</h3><ol>\n<li>Open your Azure Databricks workspace URL in Chrome and go to <strong>View site information</strong> by clicking the padlock icon in the address bar.</li>\n<li>Click <strong>Certificate</strong> &gt; <strong>Certificate Path</strong> and repeat steps 3 to 6 for every intermediate certificate in the chain.</li>\n<li>Choose an intermediate certificate and go to <strong>Details</strong> &gt; <strong>Copy to File</strong> &gt; <strong>Next</strong> to export the certificate.</li>\n<li>Select the location of the certificate and click <strong>Finish</strong>.</li>\n<li>Open the exported certificate and click <strong>Install Certificate</strong> &gt; <strong>Next</strong>.</li>\n<li>From the <strong>Certificate Import Wizard</strong> click <strong>Place all certificates in the following store</strong> &gt; <strong>Browse and choose Intermediate Certification Authorities</strong>.</li>\n</ol><p><br></p>", "body_txt": "Driver configurations You can set driver configurations using the microsoft.sparkodbc.ini file which can be found in the ODBC Drivers\\Simba Spark ODBC Driver directory. The absolute path of the microsoft.sparkodbc.ini directory depends on whether you are using Power BI Desktop or on-premises Power BI Gateway: Power BI Desktop: C:\\Program Files\\Microsoft Power BI Desktop\\bin\\ODBC Drivers\\Simba Spark ODBC Driver\\microsoft.sparkodbc.ini Power BI Gateway: m\\ODBC Drivers\\Simba Spark ODBC Driver\\microsoft.sparkodbc.ini, where m is placed inside the gateway installation directory. Set driver configurations Check if the microsoft.sparkodbc.ini file was already created. If it is then jump to step 3.\nOpen Notepad or File Explorer as Run As Administrator and create a file at ODBC Drivers/Simba Spark ODBC Driver/microsoft.sparkodbc.ini.\nAdd the new driver configurations to the file below the header [Driver] by using the syntax =. Configuration keys can be found in the manual provided with the installation of the Databricks ODBC Driver. The manual is located at C:\\Program Files\\Simba Spark ODBC Driver\\Simba Apache Spark ODBC Connector Install and Configuration Guide.html. Configuring a proxy To configure a proxy, add the following configurations to the driver configuration in the microsoft.sparkodbc.ini file: [Driver] UseProxy=1 ProxyHost=&lt;proxy.example.com&gt; ProxyPort=&lt;port&gt; ProxyUID=&lt;username&gt; ProxyPWD=&lt;password&gt; Depending on the firewall configuration it might also be necessary to add: [Driver] CheckCertRevocation=0 Troubleshooting Error: SSL_connect: certificate verify failed When SSL issues occur, the ODBC driver returns a generic error SSL_connect: certificate verify failed. You can get more detailed SSL debugging logs by setting in the ODBC Drivers/Simba Spark ODBC Driver/microsoft.sparkodbc.ini file the following two configurations: [Driver] AllowDetailedSSLErrorMessages=1 EnableCurlDebugLogging=1 Diagnose issues by analyzing CryptoAPI logs Most issues can be diagnosed by using Windows CryptoAPI logs, which can be found in the Event Viewer. The following steps describe how to capture these logs. Open Event Viewer and go to Applications and Services Logs &gt; Microsoft &gt; Windows &gt; CAPI2 &gt; Operational.\nIn Filter Current Log, check the boxes Critical, Error, and Warning and click OK.\nIn the Event Viewer, go to Actions &gt; Enable Log to start collecting logs.\nConnect Power BI to Azure Databricks to reproduce the issue.\nIn the Event Viewer, go to Actions &gt; Disable Log to stop collecting logs.\nClick Refresh to retrieve the list of collected events.\nExport logs by clicking Actions &gt; Save Filtered Log File As. Diagnose Build Chain or Verify Chain Policy event errors If the collected logs contain an error on the Build Chain or Verify Chain Policy events, this likely points to the issue. More details can be found by selecting the event and reading the Details section. Two fields of interest are Result and RevocationResult. The revocation status of the certificate or one of the certificates in the certificate chain is unknown. CAPI2 error: RevocationResult: [80092013] The revocation function was unable to check revocation because the revocation server was offline. Cause: The revocation check failed due to an unavailable certificate revocation server. Resolution: Disable certificate revocation checking. The certificate chain is not complete. CAPI2 error: Result: [800B010A] A certificate chain could not be built to a trusted root authority. Cause: The certificate advertised by the VPN or proxy server is incomplete and does not contain a full chain to the trusted root authority. Resolution: The preferred solution is to configure the VPN or proxy server to advertise the full chain. If this is not possible, a workaround is to obtain the intermediate certificates for the Databricks workspace, and install these in the Intermediate Certification Authorities store, to enable Windows to find the unadvertised certificates. If possible, it is recommended to install these certificates for all Power BI users using a group policy in Windows. This has to be set up by the system administrator. Certificate configurations Disable certificate revocation checking If the ODBC driver is unable to reach the certificate revocation list server, for example because of a firewall configuration, it will fail to validate the certificate. This can be resolved by disabling this check. To disable certificate revocation checking, set the configuration CheckCertRevocation=0 to the microsoft.sparkodbc.ini file. Install intermediate certificates Open your Azure Databricks workspace URL in Chrome and go to View site information by clicking the padlock icon in the address bar.\nClick Certificate &gt; Certificate Path and repeat steps 3 to 6 for every intermediate certificate in the chain.\nChoose an intermediate certificate and go to Details &gt; Copy to File &gt; Next to export the certificate.\nSelect the location of the certificate and click Finish.\nOpen the exported certificate and click Install Certificate &gt; Next.\nFrom the Certificate Import Wizard click Place all certificates in the following store &gt; Browse and choose Intermediate Certification Authorities.", "format": "html", "updated_at": "2022-12-07T22:18:20.778Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:26.664Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256842, "name": "Business intelligence tools", "codename": "bi", "accessibility": 1, "description": "These articles can help you manage your business intelligence (BI) tool integrations with Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963729, "name": "azure"}], "url": "https://kb.databricks.com/bi/powerbi-proxy-ssl-configuration"}, {"id": 1663840, "name": "Configure Simba JDBC driver using Azure AD", "views": 4431, "accessibility": 1, "description": "Access Databricks with a Simba JDBC driver using an Azure user account and Azure AD authentication.", "codename": "configure-simba-azure-ad-creds", "created_at": "2022-12-07T21:32:17.738Z", "updated_at": "2022-12-07T22:28:13.523Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.10f00bc9cf0cdb689a5169d0d080becf\"></div><p id=\"isPasted\">This article describes how to access Azure Databricks with a Simba JDBC driver using Azure AD authentication.</p><p>This can be useful if you want to use an Azure AD user account to connect to Azure Databricks.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-0\">Info</h3>\n<p class=\"hj-alert-text\">Power BI has native support for Azure AD authentication with Azure Databricks. Review the <a href=\"https://learn.microsoft.com/azure/databricks/partners/bi/power-bi\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Power BI</a> documentation for more information.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"create-a-service-principal-1\">Create a service principal</h1><p id=\"isPasted\">Create a service principal in Azure AD. The service principal obtains an access token for the user.</p><ol>\n<li>Open the <a href=\"https://portal.azure.com/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure Portal</a>.</li>\n<li>Open the <strong>Azure Active Directory</strong> service.</li>\n<li>Click <strong>App registrations</strong> in the left menu.</li>\n<li>Click <strong>New registration</strong>.</li>\n<li>Complete the form and click <strong>Register</strong>.</li>\n</ol><p>Your service principal has been successfully created.</p><h1 data-toc=\"true\" id=\"configure-service-principal-permissions-2\">Configure service principal permissions</h1><ol>\n<li id=\"isPasted\">Open the service principal you created.</li>\n<li>Click <strong>API permissions</strong> in the left menu.</li>\n<li>Click <strong>Add a permission</strong>.</li>\n<li>Click <strong>Azure Rights Management Services</strong>.</li>\n<li>Click <strong>Delegated permissions</strong>.</li>\n<li>Select <strong>user_impersonation</strong>.</li>\n<li>Click <strong>Add permissions</strong>.</li>\n<li>The <strong>user_impersonation</strong> permission is now assigned to your service principal.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1670449025548-azure-sp-user-impersonation-permission.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n</ol><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">If <strong>Grant admin consent</strong> is not enabled, you may encounter an error later on in the process.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"update-service-principal-manifest-4\">Update service principal manifest</h1><ol>\n<li id=\"isPasted\">Click <strong>Manifest</strong> in the left menu.</li>\n<li>Look for the line containing the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">\"allowPublicClient\"</span> property.</li>\n<li>Set the value to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span>.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1670449099741-azure-sp-manifest-editor.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>Click <strong>Save</strong>.</li>\n</ol><h1 data-toc=\"true\" id=\"download-and-configure-the-jdbc-driver-5\">Download and configure the JDBC driver</h1><ol>\n<li id=\"isPasted\">Download the <a href=\"https://www.databricks.com/spark/jdbc-drivers-download\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Databricks JDBC Driver</a>.</li>\n<li>\n<a href=\"https://learn.microsoft.com/azure/databricks/integrations/jdbc-odbc-bi\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Configure the JDBC driver</a> as detailed in the documentation.</li>\n</ol><h1 data-toc=\"true\" id=\"obtain-the-azure-ad-token-6\">Obtain the Azure AD token</h1><p id=\"isPasted\">Use the sample code to obtain the Azure AD token for the user.</p><p>Replace the variables with values that are appropriate for your account.</p><pre id=\"isPasted\">%python\r\n\r\nfrom adal import AuthenticationContext\r\n\r\nauthority_host_url = \"https://login.microsoftonline.com/\"\"\r\n# Application ID of Azure Databricks\r\nazure_databricks_resource_id = \"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d\"\r\n\r\n# Required user input\r\nuser_parameters = {\r\n\u00a0 \u00a0\"tenant\" : \"&lt;tenantId&gt;\",\r\n\u00a0 \u00a0\"client_id\" : \"&lt;clientId&gt;\",\r\n\u00a0 \u00a0\"username\" : \"&lt;user@domain.com&gt;\",\r\n\u00a0 \u00a0\"password\" : &lt;password&gt;\r\n}\r\n\r\n# configure AuthenticationContext\r\n# authority URL and tenant ID are used\r\nauthority_url = authority_host_url + user_parameters['tenant']\r\ncontext = AuthenticationContext(authority_url)\r\n\r\n# API call to get the token\r\ntoken_response = context.acquire_token_with_username_password(\r\n\u00a0 azure_databricks_resource_id,\r\n\u00a0 user_parameters['username'],\r\n\u00a0 user_parameters['password'],\r\n\u00a0 user_parameters['client_id']\r\n)\r\n\r\naccess_token = token_response['accessToken']\r\nrefresh_token = token_response['refreshToken']</pre><h1 data-toc=\"true\" id=\"pass-the-azure-ad-token-to-the-jdbc-driver-7\">Pass the Azure AD token to the JDBC driver</h1><p id=\"isPasted\">Now that you have the user\u2019s Azure AD token, you can pass it to the JDBC driver using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Auth_AccessToken</span> in the JDBC URL as detailed in the <a href=\"https://learn.microsoft.com/azure/databricks/integrations/jdbc-odbc-bi#--building-the-connection-url-for-the-databricks-driver\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Building the connection URL for the Databricks driver</a> documentation.</p><p>This sample code demonstrates how to pass the Azure AD token.</p><pre id=\"isPasted\">%python\r\n\r\n# Install jaydebeapi pypi module (used for demo)\r\n\r\nimport jaydebeapi\r\nimport pandas as pd\r\n\r\nimport os os.environ[\"CLASSPATH\"] = \"&lt;path to downloaded Simba Spark JDBC/ODBC driver&gt;\"\r\n\r\n# JDBC connection string\r\nurl=\"jdbc:spark://adb-111111111111xxxxx.xx.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/&lt;workspaceId&gt;/&lt;clusterId&gt;;AuthMech=11;Auth_Flow=0;Auth_AccessToken={0}\".format(access_token)\r\n\r\ntry:\r\n\u00a0 conn=jaydebeapi.connect(\"com.simba.spark.jdbc.Driver\", url)\r\n\u00a0 cursor = conn.cursor()\r\n\r\n\u00a0 # Execute SQL query\r\n\u00a0 sql=\"select * from &lt;tablename&gt;\"\r\n\u00a0 cursor.execute(sql)\r\n\u00a0 results = cursor.fetchall()\r\n\u00a0 column_names = [x[0] for x in cursor.description]\r\n\u00a0 pdf = pd.DataFrame(results, columns=column_names)\r\n\u00a0 print(pdf.head())\r\n\r\n\u00a0# Uncomment the following two lines if this code is running in the Databricks Connect IDE or within a workspace notebook.\r\n\u00a0# df = spark.createDataFrame(pdf)\r\n\u00a0# df.show()\r\n\r\nfinally:\r\n\u00a0 \u00a0 \u00a0 \u00a0 if cursor is not None:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cursor.close()</pre><p><br></p>", "body_txt": "This article describes how to access Azure Databricks with a Simba JDBC driver using Azure AD authentication. This can be useful if you want to use an Azure AD user account to connect to Azure Databricks. Info\nPower BI has native support for Azure AD authentication with Azure Databricks. Review the Power BI documentation for more information. Create a service principal Create a service principal in Azure AD. The service principal obtains an access token for the user. Open the Azure Portal.\nOpen the Azure Active Directory service.\nClick App registrations in the left menu.\nClick New registration.\nComplete the form and click Register. Your service principal has been successfully created. Configure service principal permissions Open the service principal you created.\nClick API permissions in the left menu.\nClick Add a permission.\nClick Azure Rights Management Services.\nClick Delegated permissions.\nSelect user_impersonation.\nClick Add permissions.\nThe user_impersonation permission is now assigned to your service principal. Info\nIf Grant admin consent is not enabled, you may encounter an error later on in the process. Update service principal manifest Click Manifest in the left menu.\nLook for the line containing the \"allowPublicClient\" property.\nSet the value to true. Click Save. Download and configure the JDBC driver Download the Databricks JDBC Driver. Configure the JDBC driver as detailed in the documentation. Obtain the Azure AD token Use the sample code to obtain the Azure AD token for the user. Replace the variables with values that are appropriate for your account. %python from adal import AuthenticationContext authority_host_url = \"https://login.microsoftonline.com/\"\" # Application ID of Azure Databricks azure_databricks_resource_id = \"2ff814a6-3304-4ab8-85cb-cd0e6f879c1d\" # Required user input user_parameters = { \u00a0 \u00a0\"tenant\" : \"&lt;tenantId&gt;\", \u00a0 \u00a0\"client_id\" : \"&lt;clientId&gt;\", \u00a0 \u00a0\"username\" : \"&lt;user@domain.com&gt;\", \u00a0 \u00a0\"password\" : &lt;password&gt; } # configure AuthenticationContext # authority URL and tenant ID are used authority_url = authority_host_url + user_parameters['tenant'] context = AuthenticationContext(authority_url) # API call to get the token token_response = context.acquire_token_with_username_password( \u00a0 azure_databricks_resource_id, \u00a0 user_parameters['username'], \u00a0 user_parameters['password'], \u00a0 user_parameters['client_id'] ) access_token = token_response['accessToken'] refresh_token = token_response['refreshToken'] Pass the Azure AD token to the JDBC driver Now that you have the user\u2019s Azure AD token, you can pass it to the JDBC driver using Auth_AccessToken in the JDBC URL as detailed in the Building the connection URL for the Databricks driver documentation. This sample code demonstrates how to pass the Azure AD token. %python # Install jaydebeapi pypi module (used for demo) import jaydebeapi import pandas as pd import os os.environ[\"CLASSPATH\"] = \"&lt;path to downloaded Simba Spark JDBC/ODBC driver&gt;\" # JDBC connection string url=\"jdbc:spark://adb-111111111111xxxxx.xx.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/&lt;workspaceId&gt;/&lt;clusterId&gt;;AuthMech=11;Auth_Flow=0;Auth_AccessToken={0}\".format(access_token) try: \u00a0 conn=jaydebeapi.connect(\"com.simba.spark.jdbc.Driver\", url) \u00a0 cursor = conn.cursor() \u00a0 # Execute SQL query \u00a0 sql=\"select * from &lt;tablename&gt;\" \u00a0 cursor.execute(sql) \u00a0 results = cursor.fetchall() \u00a0 column_names = [x[0] for x in cursor.description] \u00a0 pdf = pd.DataFrame(results, columns=column_names) \u00a0 print(pdf.head()) \u00a0# Uncomment the following two lines if this code is running in the Databricks Connect IDE or within a workspace notebook. \u00a0# df = spark.createDataFrame(pdf) \u00a0# df.show() finally: \u00a0 \u00a0 \u00a0 \u00a0 if cursor is not None: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cursor.close()", "format": "html", "updated_at": "2022-12-07T22:28:13.505Z"}, "author": {"id": 789819, "email": "arvind.ravish@databricks.com", "name": "arvind.ravish ", "first_name": "arvind.ravish", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T14:57:00.920Z", "updated_at": "2023-04-12T21:20:46.123Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 7942, "name": "costCenter.711-SupportEngineering"}]}, "category": {"id": 256842, "name": "Business intelligence tools", "codename": "bi", "accessibility": 1, "description": "These articles can help you manage your business intelligence (BI) tool integrations with Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2963718, "name": "azure"}], "url": "https://kb.databricks.com/bi/configure-simba-azure-ad-creds"}, {"id": 1663538, "name": "SQL access control error when using Snowflake as a data source", "views": 3547, "accessibility": 1, "description": "Snowflake does not officially support schema as an option; you must use sfschema.", "codename": "sql-access-control-error-when-using-snowflake-as-a-data-source", "created_at": "2022-12-07T17:50:36.355Z", "updated_at": "2023-01-20T12:54:25.090Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlWYWhjL0F2eUNwTm9CUTV3emVZL1dSTUVYT3lldENQWUlVNnh3Q01TWGxQRGRtSWlsCmxTSWxzZ3NmSEFOSmEyeUVFQmt5bldYZlc2RS9pYWxLbWVDOE1rOEpGRmVhZ0lrYUxiMzdCTndESm5QZQo2QjdRKy9PZTlNUkRvSTlxN3VtQmVRMzRwNURjaHFrUGFXejNQcENiVnI5SXJock5ZMUhlLzV1Y1AvekcKaXBQTkRqb0lGRWcxTGozdmVZRkZGbzlZS2tJTGxRK0ltbG5JNU5HRFBKdGpIVkxRZDZLdFRRMHpaUlRICmNsenlRREJTQ0dZWUNQZENIUVVUWFlCeUI2VXBqbVlIUVpBcmFUU0htdld6QXNIcjZVOXJNSTZCQmhmUQpLK3dPQnQzalRBZHNibFFteEtFeGVwSUJZNzl3ZDNRUHBnYWpBMmF6ZTBnczdiUWdrVzlSTWJiWm0wby8KVTcxbjBmNytlazJrRllpTHhpdDk3ZDFLQ0Erd09Qck0wYUJKeXFqQ0pROHl5Mk9ueFFiSS90RzY5ckhSCmtVdytJWVpWdUxkdGpycFVnWENYQ1RLclV3ampBUzlGY21pYXFXZnFVN09ZS045RjNvckZKbFByYlFpZwpYZDNMb0NLZVlPRGYzOFZkWFdrTjg0ak45SzNwaDNleFl2a0JHNVlKdG5XMGZLYVphYUFWaCtIcno2b2UKOEpteGNCVFVGamhOWFhlRUt1SksyeHBqUWxqSGZJSVFxZnd6VVFOVmVyMWd0djdnc0F0bC9sc2x3d2JkClJjbW81MmkyZWFVNzVIUEhLdWx6LzJRM1ZyK2gwaktnQ3NVRDlFSDBRRHZ5dEM3UisyTlN2OTNRY0pQcwpQbFNzNjZzbnZXRkd5Qk14eFE1d053RENkbWc1amY1azczL0JCaUVnTGpaNnBRVFkzdC9CN1VyekhUNmcKekdEdURhQ09PU0c5eWEzL0lOK3FNSVNSbUdLekhENWNoSUxFekhyT2pOOG15N2ZnN0dXbkI0TlhzMnI1CkFOZTllSFo3NXBDRjhoa21RUDB0L3ZldU5mT2doZ1NSVWpWMklJQnFOUXdYMWdUT2k1RUZuV3h5Zm4zYQpuUmw0SWhJMU5hK2tIRVRZZExHSUVncFpyQWh4TWg3RzMzK3hVeXdBL0VhK0lHVWQ5Q2RLSFdodXlsa0YKUWVCR3JaY1R2YUR0ZDYzcjN1VEkrOWprN0d1RXJrN2RSTnkvVFFwNFJMS3AzbURTQmpDdFc1UUdTWTVWCjVqQS9kK1I3ZitOdVJnYUNnZTFwOEFia21TUWhZWHBFdWdPMG4xcXVjS1lkN3NJb3EwdWRidmxadmRHcgpVSVZONHlhaTBPSTF5cnZzY09QT0wva2tWc1F3RHQvWDlLeWc3TGRHdDBPOWszQmNONGZVeFhKNHN6eXEKeEJuMG9JQnZ2YTAyTjllckI1YXVqQjVPWnRudDRuUUE0QkhROG04Y3Fmc1FpSWE0T1FYUVJFY29SdXRlCm9MbzRKWGNkWkhHZkhLTnkrN0N4RHhqdjlsVGtTdHJkdUREcDdCcTZ1T2RlUkFaMUgrR252ai9vUHhRNApFMU5zaTZscGRlZTNtVmpaYjFwWnRwM3R5QVJmbStSNUxQZUJVaUhQb1ZhNkRQZ2NSckxKcGQwbDRCVWwKVGNJelB2NC94VHc5dHlTdDNuK040RjFnUzY1UXQ5dFpNRU55Uk84OS9kNEFmbFdQRFRSUWFRY2I0WFdxCnZSeHFyL0NWc3ZVK2Y0OFN6enozTWpETzdDb05EZCtCVitkMU9HcjVabkdoZnc9PQo=.27dda3c1784055da6aa42ccc1465d3e8\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>The <a href=\"https://docs.snowflake.com/en/user-guide/spark-connector.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Snowflake Connector for Spark</a> is used to read data from, and write data to, Snowflake while working in Databricks. The connector makes Snowflake look like another Spark data source.</p><p>When you try to query Snowflake, your get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SnowflakeSQLException</span> error message.</p><pre>SnowflakeSQLException:\u00a0SQL access control error:\u00a0Insufficient privileges to operate on schema\u00a0'&lt;SCHEMA&gt;'\u00a0\r\n2Insufficient privileges to operate on schema\u00a0'&lt;SCHEMA&gt;'\u00a0at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:127)</pre><h1 data-toc=\"true\" id=\"cause-1\"><span style=\"font-size: 36px;\">Cause</span></h1><p>When you attempted to <strong>read and write data from Snowflake</strong> (<a href=\"https://docs.databricks.com/external-data/snowflake.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"read and write data from Snowflake\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/external-data/snowflake\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"read and write data from Snowflake\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/external-data/snowflake.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"read and write data from Snowflake\">GCP</a>) you used <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">schema</span> instead of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">sfschema</span>.</p><pre id=\"isPasted\">%python\r\n\r\nsnowflake_table = (spark.read<br id=\"isPasted\">\u00a0 .format(\"snowflake\")\r\n\u00a0 .option(\"dbtable\", &lt;table_name&gt;)\r\n\u00a0 .option(\"sfUrl\", &lt;database_host_url&gt;)\r\n\u00a0 .option(\"sfUser\", &lt;username&gt;)\r\n\u00a0 .option(\"sfPassword\", &lt;password&gt;)\r\n\u00a0 .option(\"sfDatabase\", &lt;database_name&gt;)\r\n\u00a0 .option(\"Schema\", &lt;schema_name&gt;)\r\n\u00a0 .option(\"sfWarehouse\", &lt;warehouse_name&gt;)\r\n\u00a0 .load()\r\n)</pre><p>Snowflake does not officially support <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">schema</span> as an option.</p><p>In some cases, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">schema</span> is treated as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">sfschema</span>, but there is no guarantee that this will happen.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>When reading or writing data from Snowflake you must use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">sfschema</span> instead of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">schema</span> in Snowflake options.</p><pre id=\"isPasted\">%python\r\n\r\nsnowflake_table = (spark.read\r\n  .format(\"snowflake\")\r\n  .option(\"dbtable\", &lt;table_name&gt;)\r\n  .option(\"sfUrl\", &lt;database_host_url&gt;)\r\n  .option(\"sfUser\", &lt;username&gt;)\r\n  .option(\"sfPassword\", &lt;password&gt;)\r\n  .option(\"sfDatabase\", &lt;database_name&gt;)\r\n  .option(\"sfSchema\", &lt;schema_name&gt;)\r\n  .option(\"sfWarehouse\", &lt;warehouse_name&gt;)\r\n  .load()\r\n)</pre><p><br>Please review the Snowflake <a href=\"https://docs.snowflake.com/en/user-guide/spark-connector-use.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Using the Spark Connector</a> documentation for more information.</p>", "body_txt": "Problem The Snowflake Connector for Spark is used to read data from, and write data to, Snowflake while working in Databricks. The connector makes Snowflake look like another Spark data source. When you try to query Snowflake, your get a SnowflakeSQLException error message. SnowflakeSQLException:\u00a0SQL access control error:\u00a0Insufficient privileges to operate on schema\u00a0'&lt;SCHEMA&gt;'\u00a0 2Insufficient privileges to operate on schema\u00a0'&lt;SCHEMA&gt;'\u00a0at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:127) Cause When you attempted to read and write data from Snowflake (AWS | Azure | GCP) you used schema instead of sfschema. %python snowflake_table = (spark.read\u00a0 .format(\"snowflake\") \u00a0 .option(\"dbtable\", &lt;table_name&gt;) \u00a0 .option(\"sfUrl\", &lt;database_host_url&gt;) \u00a0 .option(\"sfUser\", &lt;username&gt;) \u00a0 .option(\"sfPassword\", &lt;password&gt;) \u00a0 .option(\"sfDatabase\", &lt;database_name&gt;) \u00a0 .option(\"Schema\", &lt;schema_name&gt;) \u00a0 .option(\"sfWarehouse\", &lt;warehouse_name&gt;) \u00a0 .load() ) Snowflake does not officially support schema as an option. In some cases, schema is treated as sfschema, but there is no guarantee that this will happen. Solution When reading or writing data from Snowflake you must use sfschema instead of schema in Snowflake options. %python snowflake_table = (spark.read .format(\"snowflake\") .option(\"dbtable\", &lt;table_name&gt;) .option(\"sfUrl\", &lt;database_host_url&gt;) .option(\"sfUser\", &lt;username&gt;) .option(\"sfPassword\", &lt;password&gt;) .option(\"sfDatabase\", &lt;database_name&gt;) .option(\"sfSchema\", &lt;schema_name&gt;) .option(\"sfWarehouse\", &lt;warehouse_name&gt;) .load() ) Please review the Snowflake Using the Spark Connector documentation for more information.", "format": "html", "updated_at": "2023-01-20T12:54:25.085Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2994268, "name": "aws"}, {"id": 2994269, "name": "azure"}, {"id": 2994273, "name": "data"}, {"id": 2994272, "name": "data source"}, {"id": 2994274, "name": "error"}, {"id": 2994270, "name": "gcp"}, {"id": 2994271, "name": "snowflake"}], "url": "https://kb.databricks.com/data-sources/sql-access-control-error-when-using-snowflake-as-a-data-source"}, {"id": 1662086, "name": "Slowness when fetching results in Databricks SQL", "views": 2185, "accessibility": 1, "description": "Ensure that cloud fetch is enabled for best performance when using ODBC/JDBC to fetch results.", "codename": "slowness-when-fetching-results-in-databricks-sql", "created_at": "2022-12-06T20:34:15.942Z", "updated_at": "2023-02-03T02:33:37.772Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlJZ3ZtLzd1ekxycjkzTEM5TU5xU0I0NEx3cjhXN1l6N0hrZVQ1Lzlub3AvNW1qVjZVCkFFM3BzWG1VeTIyeWNsMzVwMmU0Nk9SMjVIcmdTeFZUSndQNXM2cFlLNVZhWTRPdUZuNEF3YWY2OWZ4UAp0RWJZYmsrQlV2bTJJSEttTnlzZ0hNNnNhYVpSc21aSXkyMWw0Y0g5QXY1SEs5SGZvcDU2V2N2eUVnR1EKZ3JZcWdKcjZ2Q1dmLzVQR2l2aG43ZnY3VC9hQzBSY3lCMDN1WHNES1hSeFlJNm94VU9FMHRkZkdCODAwCnZQcTRqdUpmQjJZUk01Wkp1N3laNXpzT21uR0psUm1CWndjWEFkWlB0aE5OWlpIaWVNOFhxTHM5clVmWAphUytzaHVBOFIza2VYUmxRTWgyUG1OcEUxaFZ3T0xDTFNxaHh1cU9oRVFkWG5MTHR3UW8ybE1abmJHQk8KWXFPRjNkckR0Qk44NE9DYUpjOHRpY3dveGk0WUt3eERkOWFhQWoxTUlYeVBMaHRuZmVPVWVyL1JaMjN6ClJ0c3UydWtUTXRLNDB5VVZLMzJJeFMzNzBPdkxIQVozZG92amNlVmFaRUZNSFZ6S3FWd2RZVDNhSzBadwphZHF5Q2tUb0U2bnYxbm9HYU0xMVR0bk91VjR5K3pXNHgyaVdrQkxabUpxQ2YyYUs3SEhyMDNRVmNnbmMKVFpkNEhjUUdjdWZkNjJMeU9UTGxSZ3RjaThxcDlMRFhGV0pxNDF3WnlOOHE4OTB0amdHRS9PbEtVNXM3CnN5QVUrZ1RveXl0SXMvenVVL25XMUFnUk5tTWMxamRuWUhpNkVkYjFJV1d6c0FvYnBIRnRuWlN6bERFLwpzREFoSWZGSTdFT25nSXgyVkJvUnhJSzQ0ZTdEM3RxUFZwQi9WaTMwV2tZbFExbTduTFVVaThPWnZibmkKS2ZpekIzMVh4SFE0Y3puRUdySnRWM0lzeGc5TnErUEFwSVBXY2R1RzJQV25tKzBjZnVEOE9hUXd3YkZ3CnVkOStRU1dnYnN4U1hhUXdWdWdlREVKUEFaV0xaS1ByVXA1a3V6V29IZDFBb2NrK1BIZjlwWUdzUmRwUgpNeWROMWNVSTMwQTJ3dFUwNjV5ZzNhOXRVK2kwY2YvZS9DMDdZa3BVRWdQSGRBWGhNb0VpcWNmdzdYSEkKOGNvNUNqSUNlWGpSUWZMUmttaHlaR1hxMW84c2RPTVl2NHhUUUMzTkdsdVZ0YkR2RnlMMVZiU0xSNHEwCjdaYWVrTVZsdWJuWWJkb1h1MkJIdFA4cDdXc1RUeWYzYTRCRXFuckwwWDZRdkRzOVdqSzErNEdIQytVNQpvaEpqNDYzU2VqSFlOQ01SOVU2VXdvNDN0Q2JOMTErODNDd1RCQ3d5SU43R2xtQVk5bFRPYnpKZEpaRjIKRHpXaEwvanJ3eHRSd2NMSkpxYkJIVHRFWVExMDA0cTlJd01JRHFraGo0U1o1bXJGbURVajY1YXIyWFQwClRzc3NzZGJ6SnN2ODBmMGRETTFZajI5YmZlWkYwckFSUEdHRDZQUU8wVVlwR3JJd0ljYTkzRkhHeEY2TApVRXJqeEtlTk1sZnliang5c21rcCtjc0I1NGtGRjNGOW1mZXF0a21pSmMzREtOZUIrNE0yZmYxaE4zRlMKZkZYRzI5SHBmSkFlR29NbW9kTGxYbkFvbDc5SGNDTDRWUmp4ekFyRzVORkF0dGFFSnZwTHNCSGhVY09uCkdFMHNDYjRCMGU1MVBlQitiM25KM1pIZFRGbThHbzI4SG9POFJiU2RSSmV0cDVMMHNOejdiN25hTHZNOQpqTENmbUUrVEFrdW9Wdk5hTXA4KzF0RXBHd1piUFNIUHVMWElLS1luWUZZTlJlUmhlcERDd3o2ZUpZT1MKR2dFaWhBeW1IeVI5eG1PWG5GTjZKN3gyRWZEMTNQL3EK.c593ed3e5a26e72195af9653f9278cde\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Databricks SQL uses cloud fetch to increase query performance. This is done by default.</p><p>Instead of using single threaded queries, cloud fetch retrieves data in parallel from cloud storage buckets (such as AWS S3 and Azure Data Lake Storage). Compared to a standard, single threaded fetch, you can see up to a 10X increase in performance using cloud fetch.</p><p>If you are seeing slowness when fetching results in Databricks SQL it is likely that cloud fetch is disabled.</p><p>The following symptoms indicate an issue with cloud fetch:</p><ul>\n<li>Slowness when retrieving results over ODBC/JDBC</li>\n<li>Your BI tools frequently get fetch time-outs while waiting for query results</li>\n<li>The SQL warehouse query editor is slow</li>\n</ul><h1 data-toc=\"true\" id=\"causes-1\">Causes</h1><p>Some common issues that can result in cloud fetch being disabled:</p><ul>\n<li>Using an ODBC driver version below 2.6.17</li>\n<li id=\"isPasted\">Using a JDBC driver version below 2.6.18</li>\n<li>Firewall/ACL issues between your workspace and cloud storage</li>\n<li>Cloud provider versioning is enabled on the cloud storage you are using</li>\n</ul><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><ul>\n<li>Ensure you are using a <a href=\"https://www.databricks.com/spark/odbc-drivers-download\" rel=\"noopener noreferrer\" target=\"_blank\">Databricks ODBC driver</a> version 2.6.17 or above.</li>\n<li>Ensure you are using a <a href=\"https://www.databricks.com/spark/jdbc-drivers-download\" rel=\"noopener noreferrer\" target=\"_blank\">Databricks JDBC driver</a> version 2.6.18 or above.</li>\n<li>Ensure your ODBC/JDBC <strong>Authentication requirements</strong> (<a href=\"https://docs.databricks.com/integrations/jdbc-odbc-bi.html#authentication-requirements\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Authentication requirements\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/integrations/jdbc-odbc-bi#--authentication-requirements\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Authentication requirements\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/integrations/jdbc-odbc-bi.html#authentication-requirements\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Authentication requirements\">GCP</a>) are properly configured.</li>\n<li>Disable storage bucket versioning on the cloud storage you are using to store your data.</li>\n</ul>", "body_txt": "Problem Databricks SQL uses cloud fetch to increase query performance. This is done by default. Instead of using single threaded queries, cloud fetch retrieves data in parallel from cloud storage buckets (such as AWS S3 and Azure Data Lake Storage). Compared to a standard, single threaded fetch, you can see up to a 10X increase in performance using cloud fetch. If you are seeing slowness when fetching results in Databricks SQL it is likely that cloud fetch is disabled. The following symptoms indicate an issue with cloud fetch: Slowness when retrieving results over ODBC/JDBC\nYour BI tools frequently get fetch time-outs while waiting for query results\nThe SQL warehouse query editor is slow Causes Some common issues that can result in cloud fetch being disabled: Using an ODBC driver version below 2.6.17\nUsing a JDBC driver version below 2.6.18\nFirewall/ACL issues between your workspace and cloud storage\nCloud provider versioning is enabled on the cloud storage you are using Solution Ensure you are using a Databricks ODBC driver version 2.6.17 or above.\nEnsure you are using a Databricks JDBC driver version 2.6.18 or above.\nEnsure your ODBC/JDBC Authentication requirements (AWS | Azure | GCP) are properly configured.\nDisable storage bucket versioning on the cloud storage you are using to store your data.", "format": "html", "updated_at": "2023-02-03T02:33:37.760Z"}, "author": {"id": 951639, "email": "emad.rizkallah@databricks.com", "name": "emad.rizkallah ", "first_name": "emad.rizkallah", "last_name": "", "role_id": "draft_writer", "created_at": "2022-07-06T16:13:16.701Z", "updated_at": "2023-04-25T14:56:51.165Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256847, "name": "Databricks SQL", "codename": "dbsql", "accessibility": 1, "description": "These articles can help you with Databricks SQL.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3002639, "name": "aws"}, {"id": 3002640, "name": "azure"}, {"id": 3002646, "name": "cloud fetch"}, {"id": 3002644, "name": "fetch"}, {"id": 3002641, "name": "gcp"}, {"id": 3002647, "name": "results"}, {"id": 3002645, "name": "slow"}, {"id": 3002642, "name": "sql"}, {"id": 3002643, "name": "versioning"}], "url": "https://kb.databricks.com/dbsql/slowness-when-fetching-results-in-databricks-sql"}, {"id": 1659596, "name": "Job timeout when connecting to a SQL endpoint over JDBC", "views": 2553, "accessibility": 1, "description": "Increase the SocketTimeout value in the JDBC connection URL to prevent thread requests from timing out.", "codename": "job-timeout-when-connecting-to-a-sql-endpoint-over-jdbc", "created_at": "2022-12-05T18:47:31.289Z", "updated_at": "2023-01-20T12:50:12.255Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThsN0RabVdIcy9Md3JnUEh1d1NLQzJqbHNHcVY5aTBnZTh3UTdJVnhHckRLNXcrMldKClk4WWVLVVVOM3VwUGdyZUYxL2pWTmV4b0NjbElFdWZPSzJpdXNJVUk4SmF2dVVOYWxzY0M2NHMrYjVHSwpXU3pqYzJ5MFN5U0ppdFFYekpYOHhPL091K3E2WGZOQkJIMVhyemc2QXZiY2N2UG9SNmVaY0V1VG9tek4KajBQREhDZzZRdS9Rb09Pa21tbklacmEvS2Rwelh1NVJZWEJVQXNxSWx1NkhHQjRadldjeTRJR0hIZlFTCmRxazhsVTNSWEcvTnY0VE15UkRtUElPWjY2WnN2Z1NPWjI0ays4WUdnS2hyUWZGelBSZHVOWVR5NTVnbAp5WHB5cHF1bHJLRW9tUml4c3c5WG13aTM5NTQ2Vi9EUDZpTUlDRXJoeGtHNTFZL1p3KzNLS0dhUVFlZHgKTnQ1MXVMbnBwSUdUSWpYSWpPS05DWWltUUpxbi9sa09IVG9MdEpUMkFkM3ZobndFYXU1amI1NGloT0dyCnM0QzJsWmVEaGZ6S2QyVGdqMzBkVW0xVEhQQU9KMEFhcnhsRk03cnpNcHpkWFFQaDBpVzFMdzM4MHhPeAp6L3JDOVFHeUFLVTBZZG9pTE0xbjFEVU56bmlSNUZadWI3Lzc0VVVRblQvZUo5VWl6UDlENXoxa3lDaHYKMFRoM2dSUXNqY2o0MDAxVEVFcjVCVkMxQkdZT3dwYk1sc2xOUnVvMnJiSTZvWkpncWYxMXl2KzJabXYzCjIzMXdHYVBGbXBMZjI0eTFTV1cxckozWjVoWEVQS05qeHNJNXVrOU5WeUZWazliL2dRVm9JM2VvUTh2KwpFbUJKZDJFYUxhczRjcUJXSHcvYi9QRldyQ0R4NVYxZFhSQzBCUVVLRWorUTNMMlN2MmxxMUQ2RVZWRVAKU1ROUkczMkRscjF5UTdIU3FGMC9BVXNtalFBTTkyMXBXaVFpQWJQQUZ3elhhdkZjOW9vYmlLeEhubHR3CkMzZVhUSFhLQjl6ZVRQZHp4Sk5HdElsdzh4S3JqOFdKSnNZQm1yTGhuM2RDUzk4TGN6MFRMdWdrM2ZwUQo1c2pEditET0VtTWZoQk8zU0pKSkJQa0FpV3FZV29nc3pVVjl6U2tZMGVuWWJFdGgxMDduMFVqaTR3elMKNm5LSnhidE45UkYxbkZsNmVXaVUycnY4bU5najlPaitvek1nSmZXSTlyMmpqbU0zUXpDWlpUeEhwVVV4CmRGNXRJK0JnUHBObi8xV2lxMEliMzJkaWsyb2UyRERSbDNUYjdmZUJVaTZXMEJuUTIzeDEvMTdmTUdoYwpVSElIOFV6NGo2Tk15bnhhSlZ3emFHZmlaL0RmelBsVURGanY2ZFhUZlBkaURmbkdTMGl5L2l3YkJqSm4KOVF6bzJIdyt2YnRDSkp4cENDSlJUZW5CNnphMm1tWlM4bEx0WDZYc2g1NHYxMy9qdlZpNUZBWkVWdHZLClc0Qk8xcXdFdHVaaXdvWUJtV0ZBSGc3eldtaVdFNWMxRVJGNjdveVhtOEk4Y2JtK3N2bVY4MlhZUGdlQQpLWG8yK2ZRRWUzU3JUNnRIaS8zbzVZNnowZlYwalZTQVZPd1doaTdUQ1g4QmhEUFMzYUk1VldBajM5bjAKKy8wWUlBQi91MWdWMEZydElzNzMraVhOQkVhbm5xaWhwOWhDbGxweGpwcE1qSi9xT2dPcjkyVGxxbWVRCjExQmphOUVFZWN3NndHemJTM2FBNG1IQm51WlB1SjY5NUxsUE9OUXhQR01HQlE9PQo=.c82933c9ebf2fc56b925cfbe15210bb4\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You have a job that is reading and writing to an SQL endpoint over a JDBC connection.</p><p>The SQL warehouse fails to execute the job and you get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.net.SocketTimeoutException: Read timed out</span> error message.</p><pre>2022/02/04 17:36:15 - TI_stg_trade.0 - Caused by: com.simba.spark.jdbc42.internal.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\r\n2022/02/04 17:36:15 - TI_stg_trade.0 - at com.simba.spark.hivecommon.api.TETHttpClient.flushUsingHttpClient(Unknown Source)\r\n2022/02/04 17:36:15 - TI_stg_trade.0 - at com.simba.spark.hivecommon.api.TETHttpClient.flush(Unknown Source)\r\n2022/02/04 17:36:15 - TI_stg_trade.0 - at com.simba.spark.jdbc42.internal.apache.thrift.TServiceClient.sendBase(TServiceClient.java:73)\r\n2022/02/04 17:36:15 - TI_stg_trade.0 - at com.simba.spark.jdbc42.internal.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62)</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Each incoming request requires a thread for the duration of the request. When the number of simultaneous requests is greater than the number of available threads, a timeout can occur. This can occur during long running queries.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Increase the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SocketTimeout</span> value in the JDBC connection URL.</p><p>In this example, the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SocketTimeout</span> is set to 300 seconds:</p><div><pre>jdbc:spark://&lt;server-hostname&gt;:443;HttpPath=&lt;http-path&gt;;TransportMode=http;SSL=1[;property=value[;property=value]];SocketTimeout=300</pre></div><p><br>For more information, review the <strong>Building the connection URL for the legacy Spark driver</strong> (<a href=\"https://docs.databricks.com/integrations/jdbc-odbc-bi.html#building-the-connection-url-for-the-legacy-spark-driver\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Building the connection URL for the legacy Spark driver\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/integrations/jdbc-odbc-bi#--building-the-connection-url-for-the-legacy-spark-driver\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Building the connection URL for the legacy Spark driver\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/integrations/jdbc-odbc-bi.html#building-the-connection-url-for-the-legacy-spark-driver\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Building the connection URL for the legacy Spark driver\">GCP</a>) documentation.</p>", "body_txt": "Problem You have a job that is reading and writing to an SQL endpoint over a JDBC connection. The SQL warehouse fails to execute the job and you get a java.net.SocketTimeoutException: Read timed out error message. 2022/02/04 17:36:15 - TI_stg_trade.0 - Caused by: com.simba.spark.jdbc42.internal.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out 2022/02/04 17:36:15 - TI_stg_trade.0 - at com.simba.spark.hivecommon.api.TETHttpClient.flushUsingHttpClient(Unknown Source) 2022/02/04 17:36:15 - TI_stg_trade.0 - at com.simba.spark.hivecommon.api.TETHttpClient.flush(Unknown Source) 2022/02/04 17:36:15 - TI_stg_trade.0 - at com.simba.spark.jdbc42.internal.apache.thrift.TServiceClient.sendBase(TServiceClient.java:73) 2022/02/04 17:36:15 - TI_stg_trade.0 - at com.simba.spark.jdbc42.internal.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) Cause Each incoming request requires a thread for the duration of the request. When the number of simultaneous requests is greater than the number of available threads, a timeout can occur. This can occur during long running queries. Solution Increase the SocketTimeout value in the JDBC connection URL. In this example, the SocketTimeout is set to 300 seconds: jdbc:spark://&lt;server-hostname&gt;:443;HttpPath=&lt;http-path&gt;;TransportMode=http;SSL=1[;property=value[;property=value]];SocketTimeout=300 For more information, review the Building the connection URL for the legacy Spark driver (AWS | Azure | GCP) documentation.", "format": "html", "updated_at": "2023-01-20T12:50:12.228Z"}, "author": {"id": 818781, "email": "atanu.sarkar@databricks.com", "name": "Atanu.Sarkar ", "first_name": "Atanu.Sarkar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-05T13:43:53.249Z", "updated_at": "2023-03-24T21:50:55.464Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256847, "name": "Databricks SQL", "codename": "dbsql", "accessibility": 1, "description": "These articles can help you with Databricks SQL.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2996944, "name": "aws"}, {"id": 2996945, "name": "azure"}, {"id": 2996947, "name": "gcp"}, {"id": 2998854, "name": "jdbc"}, {"id": 2998855, "name": "spark driver"}, {"id": 2998856, "name": "sql"}, {"id": 2998857, "name": "timeout"}], "url": "https://kb.databricks.com/dbsql/job-timeout-when-connecting-to-a-sql-endpoint-over-jdbc"}, {"id": 1647208, "name": "Unable to access Delta Sharing tables with a Python client", "views": 965, "accessibility": 1, "description": "You must ensure that your client IP is whitelisted in the Azure firewall.", "codename": "unable-to-access-delta-sharing-tables-with-a-python-client", "created_at": "2022-11-30T19:18:41.719Z", "updated_at": "2023-02-24T23:19:28.459Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStYYVBOeWM0dFpudFdFWGxEMyt3QmQ3TXRhTEpTbStieEdQNDBLbjFOQXlCWWRtcHpGCklVQUdBNTYyZlp5M3hRZm8vSVR3TzFTTzRkbVpQWUt2Ymt4aDNFTUg4MlVRRzBUa1RqVTJ3K2VhWDJGbgpmR1hsTnVqdysrbCtyV0M1bGsyeCtWNFNEOXFYYWpsMkhCZ3pGRW53SjIyM1ZnL3AreCs0TmJwU0grTUkKbHp2K2I1Z3RGNVphRTgrSDZQOWQwem8yaHJkcWVNL20rbzRrcG1ua1hKeXlBK1hKaCtaN21JcERySi95CmVUY1ZyVkhkeFkxMmd6cldSV2pZVUptNVp3cWJpdFVHQm1KV2RSOXlMU0tmZXhRRndiZkZ5cjBwWStqego5RkNDT2owdXZ6OFFQcnlUbGpyUlRuRUgyUnZwWGxYVEFqMFFadUF6Rk1oMkhqVS9velBtelNKSGgvdWMKdlFZZG1LOUx2dXNFSXhBbDNEQzhrUisrb3VMS2tLVnJuVlg0Y2ExVmp6cGZvRjNvSFJHc3F4Q0xya3lvCnVTZnhsazdTcE5va3g4MmVBd1FrNW9sOWYvSlVoS1BCdDI0bWlNQ0NMd2F6bVJPTXlsb2U5ZzFkejBaMAp0czVhU1pyVnpHa3Q4VzhvQWdsZW44ZXFvRWp3VWVlSWNpcUNJVGVIUVdWUXU1bE5TdlhmM1Z5Ukp0Wm8KVDBKRmp0cjI0NTRwQzdGWER0ak5iYkJmVENuRzVqb0oycXhhTE52ajRzdUozUmpVdHYvTXp3OFRVajQ1CkxDSEVLTEhqMnEwd3hNMHBETEhZTzFGbmZGS0ZOMTQ5ckE4UzVBY0V5d3h4YWp3UitZa2FxZWZUUi83Swp4ZTA0MWdpNkxnZDhzZlpCUVoyZ1NVc1F0c2hXbFFYZG1tMG1YaTRMZk5RaG1KekN2MldoV095R0J2NmIKUUpkby9XWUpycnpzWFl6ODkzdUZubms2Q01yMlNwb2kyRmhGZXJDQjYyS2hwbm1pZ09qU3U0akRwZ0tOCnBzYTZXUndiVW9DS28yNTJwZVB1ajhMQVAzWXNUTDhYcjljYnhDbUdSdlpja2pFQ3ZzZEk5N0Y0Z3ZkbwpjeUdWMHZXNjdWUGNaQmtOMHozNitEZ0RXSlRFQ2s2cTBKTGl1MXNwblI3bllvTkxtemE4cTBocytySTEKU3dpQ01VMFQ5c3ArWUJEK0hVV2luWEtrRVFZTyt6d3d5aTl3RWowaVNmdTIvT2gySndQN1ZLNDlIRklBCkU4THdxRFFCU29XK1B5RkZRc0V5RjFSbVR2cm9WeFB6ekdySDlRRk9zOFBvUkhRcW1wbStpSWtMdFd4MwpMc2dyVFFlVUt6MzFwVFZiZXR2TFQ3OFEzZk9iNGRsV1VTcWxFZjM0ZExJa1NFeEZubWsrazcwczBjaW4KdFNsMzg2R1FzYUlVSWtvckw4QnVXQzB6bW1TbWJtTGxJOC9aVU53eGd4U1hGdXpIa0VGME5CbmtjRVM4Cldac09uSTZpdVpObXRSQWVaNHdsVUozRFhuTS9hS2FPOExIQm1mcG5JODFtbVdsSmpNckRHaFdHYWZpMwoycG5LbUp3c1ZZay95RExIOHg4RnE2NmdlTXd5ZEhwdHNpUmVxUT09Cg==.82b8d9e07aa4acc6ea701588bb82b450\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p><a href=\"https://delta.io/sharing\" rel=\"noopener noreferrer\" target=\"_blank\">Delta Sharing</a> is a platform independent <a href=\"https://github.com/delta-io/delta-sharing/blob/main/PROTOCOL.md\" rel=\"noopener noreferrer\" target=\"_blank\">open protocol</a> that is used to securely share data with other organizations.</p><p>When using an open sharing model, recipients can access shared data in a read-only format using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">delta-sharing</span> Python library.</p><p>When trying to access a shared table using any Python client, you get an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SSLCertVerificationError.</span></p><pre id=\"isPasted\">SSLCertVerificationError \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n\u00a0 \u00a0 669 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Make the request on the httplib connection object.\r\n--&gt; 670 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 httplib_response = self._make_request(\r\n\u00a0 \u00a0 671 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conn,\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\r\n\u00a0 \u00a0 380 \u00a0 \u00a0 \u00a0 \u00a0 try:\r\n--&gt; 381 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self._validate_conn(conn)\r\n\u00a0 \u00a0 382 \u00a0 \u00a0 \u00a0 \u00a0 except (SocketTimeout, BaseSSLError) as e:\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in _validate_conn(self, conn)\r\n\u00a0 \u00a0 975 \u00a0 \u00a0 \u00a0 \u00a0 if not getattr(conn, \"sock\", None): \u00a0# AppEngine might not have \u00a0`.sock`\r\n--&gt; 976 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conn.connect()\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py in connect(self)\r\n\u00a0 \u00a0 360\r\n--&gt; 361 \u00a0 \u00a0 \u00a0 \u00a0 self.sock = ssl_wrap_socket(\r\n\u00a0 \u00a0 362 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sock=conn,\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data)\r\n\u00a0 \u00a0 376 \u00a0 \u00a0 \u00a0 \u00a0 if HAS_SNI and server_hostname is not None:\r\n--&gt; 377 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return context.wrap_socket(sock, server_hostname=server_hostname)\r\nC:\\ProgramData\\Anaconda3\\lib\\ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\r\n\u00a0 \u00a0 499 \u00a0 \u00a0 \u00a0 \u00a0 # ctx._wrap_socket()\r\n--&gt; 500 \u00a0 \u00a0 \u00a0 \u00a0 return self.sslsocket_class._create(\r\n\u00a0 \u00a0 501 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sock=sock,\r\nC:\\ProgramData\\Anaconda3\\lib\\ssl.py in _create(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\r\n\u00a0 \u00a01039 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\r\n-&gt; 1040 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.do_handshake()\r\n\u00a0 \u00a01041 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 except (OSError, ValueError):</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The client IP address is not whitelisted in the storage account firewall.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>If firewall is enabled on the storage account, ensure that client IP address is whitelisted in the firewall.</p><ol id=\"isPasted\">\n<li><p>Sign in to the <a href=\"https://portal.azure.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Azure Portal</a>.</p></li>\n<li><p>Expand the left sidebar menu and click <strong id=\"isPasted\">Storage accounts</strong>. to display a list of your storage accounts. If the portal menu isn't visible, click the menu button to toggle it on.</p></li>\n<li><p>Click the name of the storage account you want to edit.</p></li>\n<li><p>Click <strong>Networking</strong> under the <strong>Security + networking</strong> header.</p></li>\n<li><p>Make sure <strong>Firewalls and virtual networks</strong> is selected.</p></li>\n<li><p>Select\u00a0<strong>Enabled from selected virtual networks and IP addresses</strong>.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1677018908904-azure-firewalls-and-virtual-networks-config.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Azure Firewalls and virtual networks configuration panel.\"></p></li>\n<li><p>Under <strong>Firewall</strong> ensure that a check mark appears next to\u00a0<strong>Add your client IP address</strong>.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1677018929363-azure-firewall-client-ip-address.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Enter client IP address to whitelist in the firewall.\"></p></li>\n<li><p>Enter your client IP address in the <strong>Address range</strong> field.</p></li>\n<li><p>Select\u00a0<strong>Save</strong> to apply your changes.</p></li>\n<li><p>Wait two minutes to ensure the changes have propagated.</p></li>\n</ol><p>You should now be able to access the Delta Sharing tables with your local Python client.</p><p><br></p>", "body_txt": "Problem Delta Sharing is a platform independent open protocol that is used to securely share data with other organizations. When using an open sharing model, recipients can access shared data in a read-only format using the delta-sharing Python library. When trying to access a shared table using any Python client, you get an SSLCertVerificationError. SSLCertVerificationError \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Traceback (most recent call last) C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw) \u00a0 \u00a0 669 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Make the request on the httplib connection object. --&gt; 670 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 httplib_response = self._make_request( \u00a0 \u00a0 671 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conn, C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw) \u00a0 \u00a0 380 \u00a0 \u00a0 \u00a0 \u00a0 try: --&gt; 381 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self._validate_conn(conn) \u00a0 \u00a0 382 \u00a0 \u00a0 \u00a0 \u00a0 except (SocketTimeout, BaseSSLError) as e: C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py in _validate_conn(self, conn) \u00a0 \u00a0 975 \u00a0 \u00a0 \u00a0 \u00a0 if not getattr(conn, \"sock\", None): \u00a0# AppEngine might not have \u00a0`.sock` --&gt; 976 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conn.connect() C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py in connect(self) \u00a0 \u00a0 360 --&gt; 361 \u00a0 \u00a0 \u00a0 \u00a0 self.sock = ssl_wrap_socket( \u00a0 \u00a0 362 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sock=conn, C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data) \u00a0 \u00a0 376 \u00a0 \u00a0 \u00a0 \u00a0 if HAS_SNI and server_hostname is not None: --&gt; 377 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return context.wrap_socket(sock, server_hostname=server_hostname) C:\\ProgramData\\Anaconda3\\lib\\ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session) \u00a0 \u00a0 499 \u00a0 \u00a0 \u00a0 \u00a0 # ctx._wrap_socket() --&gt; 500 \u00a0 \u00a0 \u00a0 \u00a0 return self.sslsocket_class._create( \u00a0 \u00a0 501 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sock=sock, C:\\ProgramData\\Anaconda3\\lib\\ssl.py in _create(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session) \u00a0 \u00a01039 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\") -&gt; 1040 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.do_handshake() \u00a0 \u00a01041 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 except (OSError, ValueError): Cause The client IP address is not whitelisted in the storage account firewall. Solution If firewall is enabled on the storage account, ensure that client IP address is whitelisted in the firewall. Sign in to the Azure Portal. Expand the left sidebar menu and click Storage accounts. to display a list of your storage accounts. If the portal menu isn't visible, click the menu button to toggle it on. Click the name of the storage account you want to edit. Click Networking under the Security + networking header. Make sure Firewalls and virtual networks is selected. Select\u00a0Enabled from selected virtual networks and IP addresses. Under Firewall ensure that a check mark appears next to\u00a0Add your client IP address. Enter your client IP address in the Address range field. Select\u00a0Save to apply your changes. Wait two minutes to ensure the changes have propagated. You should now be able to access the Delta Sharing tables with your local Python client.", "format": "html", "updated_at": "2023-02-24T23:19:28.453Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 313834, "name": "Unity Catalog", "codename": "unity-catalog", "accessibility": 1, "description": "These articles can help you with Unity Catalog.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3040784, "name": "azure"}, {"id": 3040787, "name": "delta"}, {"id": 3040785, "name": "firewall"}, {"id": 3040788, "name": "ip"}, {"id": 3040786, "name": "whitelist"}], "url": "https://kb.databricks.com/unity-catalog/unable-to-access-delta-sharing-tables-with-a-python-client"}, {"id": 1645970, "name": "Connection retries take a long time to fail", "views": 3471, "accessibility": 1, "description": "The default Apache Hadoop values for connection timeout and retry are high, reduce the values for quicker failures.", "codename": "connection-retries-take-a-long-time-to-fail", "created_at": "2022-11-30T05:19:41.446Z", "updated_at": "2022-12-21T11:00:16.771Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.0993b2928883435a27cce27e39a6187b\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to access a table on a remote HDFS location or an object store that you do not have permission to access. The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SELECT</span> command should fail, and it does, but it does not fail quickly. It can take up to ten minutes, sometimes more, to return a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ConnectTimeoutException</span> error message.</p><pre>The error message they eventually receive is : \"\r\nError in SQL statement: ConnectTimeoutException: Call From 1006-163012-faded894-10-133-241-86/127.0.1.1 to\u00a0<a href=\"https://analytics.aws.healthverity.com/\" rel=\"noopener\" target=\"_blank\">analytics.aws.healthverity.com</a>:8020 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=<a href=\"https://analytics.aws.healthverity.com/10.24.12.199:8020\" rel=\"noopener\" target=\"_blank\">analytics.aws.healthverity.com/10.24.12.199:8020</a>]; For more details see: SocketTimeout - HADOOP2 - Apache Software Foundation \"</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Everything is working as designed, however the default Apache Hadoop values for connection timeout and retry are high, which is why the connection does not fail quickly.</p><pre>ipc.client.connect.timeout 20000\r\nipc.client.connect.max.retries.on.timeouts 45</pre><p data-toc=\"true\" id=\"-2\">Review the <a href=\"https://hadoop.apache.org/docs/r2.6.4/hadoop-project-dist/hadoop-common/core-default.xml\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">complete list of Hadoop common core-default.xml values</a>.</p><p data-toc=\"true\">Review the <a href=\"https://cwiki.apache.org/confluence/display/HADOOP2/SocketTimeout\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">SocketTimeout</a> documentation for more details.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">You can resolve the issue by reducing the values for connection timeout and retry.</p><ul>\n<li>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ipc.client.connect.timeout</span> value is in seconds.</li>\n<li>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ipc.client.connect.max.retries.on.timeouts</span> value is the number of times to retry before failing.</li>\n</ul><p>Set these values in your cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a>).</p><p>If you are not sure what values to use, these are Databricks recommended values:</p><pre>ipc.client.connect.timeout 5000\r\nipc.client.connect.max.retries.on.timeouts 3</pre><p><br></p>", "body_txt": "Problem You are trying to access a table on a remote HDFS location or an object store that you do not have permission to access. The SELECT command should fail, and it does, but it does not fail quickly. It can take up to ten minutes, sometimes more, to return a ConnectTimeoutException error message. The error message they eventually receive is : \" Error in SQL statement: ConnectTimeoutException: Call From 1006-163012-faded894-10-133-241-86/127.0.1.1 to\u00a0analytics.aws.healthverity.com:8020 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=analytics.aws.healthverity.com/10.24.12.199:8020]; For more details see: SocketTimeout - HADOOP2 - Apache Software Foundation \" Cause Everything is working as designed, however the default Apache Hadoop values for connection timeout and retry are high, which is why the connection does not fail quickly. ipc.client.connect.timeout 20000 ipc.client.connect.max.retries.on.timeouts 45 Review the complete list of Hadoop common core-default.xml values. Review the SocketTimeout documentation for more details. Solution You can resolve the issue by reducing the values for connection timeout and retry. The ipc.client.connect.timeout value is in seconds.\nThe ipc.client.connect.max.retries.on.timeouts value is the number of times to retry before failing. Set these values in your cluster's Spark config (AWS | Azure). If you are not sure what values to use, these are Databricks recommended values: ipc.client.connect.timeout 5000 ipc.client.connect.max.retries.on.timeouts 3", "format": "html", "updated_at": "2022-12-21T11:00:16.720Z"}, "author": {"id": 791192, "email": "sivaprasad.cs@databricks.com", "name": "sivaprasad.cs ", "first_name": "sivaprasad.cs", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T19:34:47.857Z", "updated_at": "2023-03-20T13:02:42.882Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256865, "name": "Security and permissions", "codename": "security", "accessibility": 1, "description": "These articles can help you with access control lists (ACLs), secrets, and other security- and permissions-related functionality.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2978766, "name": "aws"}, {"id": 2978767, "name": "azure"}, {"id": 2978842, "name": "hadoop"}, {"id": 2978936, "name": "retry"}, {"id": 2978935, "name": "timeout"}], "url": "https://kb.databricks.com/security/connection-retries-take-a-long-time-to-fail"}, {"id": 1644204, "name": "Multi-task workflows using incorrect parameter values", "views": 2558, "accessibility": 1, "description": "If parallel tasks running on the same cluster use Scala companion objects the wrong values can be used due to sharing a single class in the JVM.", "codename": "multi-task-workflows-using-incorrect-parameter-values", "created_at": "2022-11-29T19:01:43.955Z", "updated_at": "2022-12-05T18:29:49.371Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"5bccb792cae9d\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.f199d800f1a4783c6ef103e534a5e894\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Using key-value parameters in a multi task workflow is a common use case. It is normal to have multiple tasks running in parallel and each task can have different parameter values for the same key. These key-value parameters are read within the code and used by each task.</p><p>For example, assume you have four tasks: <span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard;'>task1</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">task2</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">task3</span>, and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">task4</span> within a workflow job. <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">table-name</span> is the parameter key and the parameter values are <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">employee</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">department</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">location</span>, and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">contacts</span>.</p><p>When you run the job, you expect each task to get its own parameters. However if the application code uses Scala companion objects, you may notice one of the task parameters gets applied to all other tasks, instead of the respective parameters for each task getting applied. This produces inconsistent results.</p><p>Using our example, if the tasks are run in parallel using Scala companion objects, any one task parameter (for example, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">task4</span> parameter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">contacts</span>) may get passed as the table name to the other three tasks.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>When companion objects are used within application code, there is a mutable state in the companion object that is modified concurrently. Since all tasks run on the same cluster, this class is loaded once and all tasks run under the same Java virtual machine (JVM).</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You can mitigate the issue by applying one of these solutions. The best choice depends on your specific use case.</p><ul>\n<li>Run the jobs sequentially (add dependencies in tasks).</li>\n<li>Schedule each task on a different cluster.</li>\n<li>Rewrite the code that loads the configuration so you are explicitly creating a new object and not using the companion object's shared state.</li>\n</ul>", "body_txt": "Problem Using key-value parameters in a multi task workflow is a common use case. It is normal to have multiple tasks running in parallel and each task can have different parameter values for the same key. These key-value parameters are read within the code and used by each task. For example, assume you have four tasks: task1, task2, task3, and task4 within a workflow job. table-name is the parameter key and the parameter values are employee, department, location, and contacts. When you run the job, you expect each task to get its own parameters. However if the application code uses Scala companion objects, you may notice one of the task parameters gets applied to all other tasks, instead of the respective parameters for each task getting applied. This produces inconsistent results. Using our example, if the tasks are run in parallel using Scala companion objects, any one task parameter (for example, task4 parameter contacts) may get passed as the table name to the other three tasks. Cause When companion objects are used within application code, there is a mutable state in the companion object that is modified concurrently. Since all tasks run on the same cluster, this class is loaded once and all tasks run under the same Java virtual machine (JVM). Solution You can mitigate the issue by applying one of these solutions. The best choice depends on your specific use case. Run the jobs sequentially (add dependencies in tasks).\nSchedule each task on a different cluster.\nRewrite the code that loads the configuration so you are explicitly creating a new object and not using the companion object's shared state.", "format": "html", "updated_at": "2022-12-05T18:29:49.361Z"}, "author": {"id": 965161, "email": "rajeevkannan.thangaiah@databricks.com", "name": "Rajeev kannan Thangaiah", "first_name": "Rajeev kannan", "last_name": "Thangaiah", "role_id": "draft_writer", "created_at": "2022-08-03T00:44:50.300Z", "updated_at": "2023-04-20T21:52:06.303Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256850, "name": "Jobs", "codename": "jobs", "accessibility": 1, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2957140, "name": "aws"}, {"id": 2957141, "name": "azure"}, {"id": 2957142, "name": "gcp"}], "url": "https://kb.databricks.com/jobs/multi-task-workflows-using-incorrect-parameter-values"}, {"id": 1642933, "name": "SSL exception when connecting to GCP secret manager", "views": 4068, "accessibility": 1, "description": "GCP secret manager requires GCM cipher suites to be installed on your cluster. Databricks Runtime 10.4 LTS and above have GCM cipher suites enabled by default.", "codename": "ssl-exception-when-connecting-to-gcp-secret-manager", "created_at": "2022-11-28T19:29:10.092Z", "updated_at": "2023-01-20T13:00:44.661Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9qc2lXVFNNS2pIYndwejFkaVlGVUlSNnd2TXJBTSt3U3RuV0ZFbnp0QXJGdnFSS3pHCklxcHJHdWw4RU9XUjdheEV5c1dUM0paS1UvUFc2V3orNkJ5clJXT0Vkdk0zUnlLdTMxdm5nZlM1QUc4LwpxTHZ3cFp4RnE1RUREbndmMjdZL1ErdVJrT201Y1pBbkR3bXVIbW85Ky9iOHBjbFExQzZWUzFnaVgyWnkKLzJwelBkR0RabkJObnM5SFJ4dE02NGcwUUg4S3FlN3BNSkUvRWlmVnpqWEJRRHVhaWpsUWg4QnhhUHBCCmF3NWFWanVaWG5hSkFHSUtvSm5ScnZkTGttTmdUVU1WbDFpS2NITmkxUENHRnFVSHRZckF0N2htMm9XTQp6ZjFia2NWZGFWamNidzhDWkVnSEp1OEtYbHRXVXl4bWs0dUVUbVp0cklHUDhpNlFlb0RSVzhTMDlrL0kKYmg4SVBsTmJUdDljM09NdDQrZCtyRUp0dStXNnVnbnVWWWt6T3VjRnFsMkxQM0FnSzFXVjdPc0ViakUzCkExTWhzcTZnTGVmL1A3Ynp2dVpjSUFpbTI5cE1Xc2Z3Y3h6S21IVXI1YzNnaXlKVzRLMEVoNUxOS2dyVQpoajRBblVWdW0rRitWbFJuT2RHZzF0a2R1VUh3NUVFeGcwMENtdEg5eW05WUp4ckFzbXlYMWtjeTlIUFkKQmxjbjZ4bEV3VjBWR2tEWU0welkwcE5zcFBrUnNGMlkvR0k1SmozRjlzdU03RGgxZlptSXpJeXkyOTN5CmNjMm1GVGFCRm4wSFppcGRmeHVvdXJrelFsYXNkNnhlZ3UzNUxsZytBUG15QTJjWnJKTnB2ajRFZVBjaAozT3pwUHcvc2IzL3VuODhLMVhNUmpCNlNDMGN6dGptU0dTaGVkb01KWlNHRVdqUXFON1BYdGhHdjlzQVcKaGxMbXhESUZaU3dYYVZONm5mVVBlTXFwWHQxME0rNW5Ea1Z5V1Z5dEMzQmtXaW9tSVpVczZ4QUFWbU10CjhBejVmb0dUNWlTYjJYTGE1RFVFZEFxM29FYTkwZE5hVzdhdEJ4eVkrMmdLR010cmgwUDNPbDBFcE4wRwpGMXQ0d1JudWFPemxmK1htekVZSzZrdnU3RTJyZlJjanN6ZTBaeXVHOUFWeDJWbTBCRTdCVDZ3UFNkUDMKc0Z4U1FzWjRXSytiV1lzUUJxY0hvWjNyZnFVbkRadU0zL0lIbG1jT3lzVldtelBjVEErQ0lkUG1IdjdhCm1QY2R2NVdFd3oyU280NkVOdktBSHdkdzl0V2xFSEJMMGMzbFFWSGlCZytQTTdnWElNRk1KNzl2QWNCNwpaaGZNTC9tTTFLbmxCeVZ2dlF1eEhnaGlwUzQ1MmxxMFlzU2lhOFNxQ0VOZDFBeGw3NGI1SXJhaWRoaksKdUxxaWpKNVFLUy8wcjRtUlppVE9ybCthUXAxMTZYQmd4TnJSYTYraGk2NUROTnhmOUx2d2E0ZTZGRXYwCm9ib3FvcFh1V3NFVVg3RG9mYTk1K1NhQ2dhMFpFMlR5QXZHbUNwUkJ5dU1Lc1ZDckJaVy9HQkQ5SnU2RwpuenhPeDQ0MVV5WWJZWEtXTDFibEpBVFYvWitoQlY0RmxvVlhoWDJ5S0lMMGhEbW9GbHhiYzZXTTd5NGcKSDN0eGxyc2p0dU5ZeUFyTEVXTlAK.7216f5f056a57fc8a20d566a0d78d33f\"></div><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-0\">Info</h3>\n<p class=\"hj-alert-text\">This article applies to clusters using Databricks Runtime 7.3 LTS and 9.1 LTS.\u00a0</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"problem-1\">Problem</h1><p>Secrets stored in the GCP secret manager service can be retrieved using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">google-cloud-secret-manager</span> client library.</p><p>Your code may fail with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SSLHandshakeException</span> error message on Databricks Runtime 9.1 LTS and below.</p><h2 data-toc=\"true\" id=\"sample-code-2\">Sample code:</h2><pre id=\"isPasted\">import com.google.cloud.secretmanager.v1.AccessSecretVersionResponse;\r\nimport com.google.cloud.secretmanager.v1.ProjectName;\r\nimport com.google.cloud.secretmanager.v1.Replication;\r\nimport com.google.cloud.secretmanager.v1.Secret;\r\nimport com.google.cloud.secretmanager.v1.SecretManagerServiceClient;\r\nimport com.google.cloud.secretmanager.v1.SecretPayload;\r\nimport com.google.cloud.secretmanager.v1.SecretVersion;\r\nimport com.google.protobuf.ByteString;\r\nimport com.google.cloud.secretmanager.v1.SecretName;\r\nimport com.google.cloud.secretmanager.v1.SecretManagerServiceSettings\r\nimport com.google.api.gax.core.FixedCredentialsProvider;\r\nimport com.google.auth.Credentials\r\n\r\ndef access_secret_version(secret_id, version_id=\"latest\"):\r\n\u00a0 \u00a0 # Create the Secret Manager client.\r\n\u00a0 \u00a0 client = secretmanager.SecretManagerServiceClient()\r\n\r\n\u00a0 \u00a0 # Build the resource name of the secret version.\r\n\u00a0 \u00a0 name = f\"projects/{PROJECT_ID}/secrets/{secret_id}/versions/{version_id}\"\r\n\r\n\u00a0 \u00a0 # Access the secret version.\r\n\u00a0 \u00a0 response = client.access_secret_version(name=name)\r\n\r\n\u00a0 \u00a0 # Return the decoded payload.\r\n\u00a0 \u00a0 return response.payload.data.decode('UTF-8')\u00a0 \u00a0\u00a0\r\nimport hashlib\r\n\r\ndef secret_hash(secret_value):\u00a0\r\n\u00a0 # return the sha224 hash of the secret value\r\n\u00a0\r\n return hashlib.sha224(bytes(secret_value, \"utf-8\")).hexdigest()</pre><h2 data-toc=\"true\" id=\"error-message-3\">Error message:</h2><pre>UnavailableException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception<br id=\"isPasted\">Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\r\nCaused by: StatusRuntimeException: UNAVAILABLE: io exception\r\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\r\nCaused by: SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)\r\n\r\nCaused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\r\n\r\nCaused by: javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)\r\nat sun.security.ssl.HandshakeContext.&lt;init&gt;(HandshakeContext.java:171)</pre><h1 data-toc=\"true\" id=\"cause-4\">Cause</h1><p id=\"isPasted\">GCM (Galois/Counter Mode) cipher suites are not enabled by default on Databricks Runtime 9.1 LTS and below.</p><p>Without the GCM cipher suites, there is no protocol available to establish the expected SSL connection to the GCP secret manager.</p><p>You can use the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nmap\u00a0</span>utility to verify which cipher suites are required by the external server.</p><pre id=\"isPasted\">%sh\r\n\r\nnmap --script ssl-enum-ciphers -p 443 secretmanager.googleapis.com</pre><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1671063562402-1671063562402.png\" class=\"fr-fic fr-dib fr-fil\"></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-5\">Info</h3>\n<p class=\"hj-alert-text\">If <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nmap</span> is not installed, run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">sudo apt-get install -y nmap</span> to install it on your cluster.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"solution-6\">Solution</h1><p data-toc=\"true\" id=\"you-must-enable-gcm-cipher-suites-on-your-cluster-to-connect-to-the-gcp-secret-manager-service-that-requires-gcm-cipher-suites-4\">You must enable GCM cipher suites to connect to the GCP secret manager service.</p><p data-toc=\"true\">If you upgrade to Databricks Runtime 10.4 LTS and above, GCM cipher suites are enabled by default.</p><p data-toc=\"true\">If you stay on Databricks Runtime 9.1 LTS and below, you should follow the instructions in the <a href=\"/_questions/1273723\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Enable GCM cipher suites</a> knowledge base article to install GCM cipher suites on your cluster.</p><p data-toc=\"true\" id=\"once-you-have-verified-that-gcm-cipher-suites-are-installed-on-your-cluster-try-to-access-the-secrets-7\">Once you have enabled GCM cipher suites, you can connect to the GCP secret manager service.</p>", "body_txt": "Info\nThis article applies to clusters using Databricks Runtime 7.3 LTS and 9.1 LTS.\u00a0 Problem Secrets stored in the GCP secret manager service can be retrieved using the google-cloud-secret-manager client library. Your code may fail with an SSLHandshakeException error message on Databricks Runtime 9.1 LTS and below. Sample code: import com.google.cloud.secretmanager.v1.AccessSecretVersionResponse; import com.google.cloud.secretmanager.v1.ProjectName; import com.google.cloud.secretmanager.v1.Replication; import com.google.cloud.secretmanager.v1.Secret; import com.google.cloud.secretmanager.v1.SecretManagerServiceClient; import com.google.cloud.secretmanager.v1.SecretPayload; import com.google.cloud.secretmanager.v1.SecretVersion; import com.google.protobuf.ByteString; import com.google.cloud.secretmanager.v1.SecretName; import com.google.cloud.secretmanager.v1.SecretManagerServiceSettings import com.google.api.gax.core.FixedCredentialsProvider; import com.google.auth.Credentials def access_secret_version(secret_id, version_id=\"latest\"): \u00a0 \u00a0 # Create the Secret Manager client. \u00a0 \u00a0 client = secretmanager.SecretManagerServiceClient() \u00a0 \u00a0 # Build the resource name of the secret version. \u00a0 \u00a0 name = f\"projects/{PROJECT_ID}/secrets/{secret_id}/versions/{version_id}\" \u00a0 \u00a0 # Access the secret version. \u00a0 \u00a0 response = client.access_secret_version(name=name) \u00a0 \u00a0 # Return the decoded payload. \u00a0 \u00a0 return response.payload.data.decode('UTF-8')\u00a0 \u00a0\u00a0 import hashlib def secret_hash(secret_value):\u00a0 \u00a0 # return the sha224 hash of the secret value \u00a0 return hashlib.sha224(bytes(secret_value, \"utf-8\")).hexdigest() Error message: UnavailableException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exceptionChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0] Caused by: StatusRuntimeException: UNAVAILABLE: io exception Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0] Caused by: SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate) Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0] Caused by: javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate) at sun.security.ssl.HandshakeContext.&lt;init&gt;(HandshakeContext.java:171) Cause GCM (Galois/Counter Mode) cipher suites are not enabled by default on Databricks Runtime 9.1 LTS and below. Without the GCM cipher suites, there is no protocol available to establish the expected SSL connection to the GCP secret manager. You can use the nmap\u00a0utility to verify which cipher suites are required by the external server. %sh nmap --script ssl-enum-ciphers -p 443 secretmanager.googleapis.com Info\nIf nmap is not installed, run sudo apt-get install -y nmap to install it on your cluster. Solution You must enable GCM cipher suites to connect to the GCP secret manager service. If you upgrade to Databricks Runtime 10.4 LTS and above, GCM cipher suites are enabled by default. If you stay on Databricks Runtime 9.1 LTS and below, you should follow the instructions in the Enable GCM cipher suites knowledge base article to install GCM cipher suites on your cluster. Once you have enabled GCM cipher suites, you can connect to the GCP secret manager service.", "format": "html", "updated_at": "2023-01-20T13:00:44.584Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256865, "name": "Security and permissions", "codename": "security", "accessibility": 1, "description": "These articles can help you with access control lists (ACLs), secrets, and other security- and permissions-related functionality.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2998862, "name": "aes-gcm"}, {"id": 2992293, "name": "cipher suites"}, {"id": 2992294, "name": "encryption"}, {"id": 2992291, "name": "gcm"}, {"id": 2991972, "name": "gcp"}, {"id": 2998861, "name": "secret manager"}, {"id": 2998860, "name": "secrets"}, {"id": 2992292, "name": "ssh"}], "url": "https://kb.databricks.com/security/ssl-exception-when-connecting-to-gcp-secret-manager"}, {"id": 1641321, "name": "Failed to install Elasticsearch via Maven", "views": 1544, "accessibility": 1, "description": "If library dependencies are already installed, it can result in a library installation failure.", "codename": "failed-to-install-elasticsearch-via-maven", "created_at": "2022-11-28T05:01:51.136Z", "updated_at": "2023-03-17T15:00:01.088Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.1017d5b1f8ebb0c2d6bb816d0604852f\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to install Elasticsearch via Maven when you get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DRIVER_LIBRARY_INSTALLATION_FAILURE</span> error message saying that the library resolution failed.</p><pre id=\"isPasted\">Error Code: DRIVER_LIBRARY_INSTALLATION_FAILURE. Error Message: Library resolution failed. Cause: java.lang.RuntimeException: org.slf4j:slf4j-api download failed.</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The Elasticsearch library is trying to install dependencies that are already installed, resulting in a conflict.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>This can be resolved by excluding the dependencies before starting the install.</p><ol>\n<li>Select <strong>Compute</strong> from the left side menu.</li>\n<li>Click on the name of the cluster you want to modify.</li>\n<li>Click <strong>Libraries</strong>.</li>\n<li>Click <strong>Install new</strong>.</li>\n<li>Click <strong>Maven</strong>.</li>\n<li>Enter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">org.elasticsearch:elasticsearch-spark-30_2.12:7.17.6</span> in the <strong>Coordinates</strong> text box.</li>\n<li>Enter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">commons-logging:commons-logging,org.slf4j:slf4j-api,com.google.protobuf:protobuf-java,javax.xml.bind:jaxb-api</span> in the <strong>Exclusions</strong> text box.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1670160205636-1670160205636.png\" style=\"width: 486px;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Maven install library screen with Elasticsearch and the necessary exclusions entered.\">\n</li>\n<li>Click <strong>Install</strong>.</li>\n</ol><p>You should now be able to start your cluster and successfully complete the Elasticsearch install.</p>", "body_txt": "Problem You are trying to install Elasticsearch via Maven when you get a DRIVER_LIBRARY_INSTALLATION_FAILURE error message saying that the library resolution failed. Error Code: DRIVER_LIBRARY_INSTALLATION_FAILURE. Error Message: Library resolution failed. Cause: java.lang.RuntimeException: org.slf4j:slf4j-api download failed. Cause The Elasticsearch library is trying to install dependencies that are already installed, resulting in a conflict. Solution This can be resolved by excluding the dependencies before starting the install. Select Compute from the left side menu.\nClick on the name of the cluster you want to modify.\nClick Libraries.\nClick Install new.\nClick Maven.\nEnter org.elasticsearch:elasticsearch-spark-30_2.12:7.17.6 in the Coordinates text box.\nEnter commons-logging:commons-logging,org.slf4j:slf4j-api,com.google.protobuf:protobuf-java,javax.xml.bind:jaxb-api in the Exclusions text box. Click Install. You should now be able to start your cluster and successfully complete the Elasticsearch install.", "format": "html", "updated_at": "2023-03-17T15:00:01.083Z"}, "author": {"id": 831505, "email": "ankitha.vijayanandana@databricks.com", "name": "ankitha.vijayanandana ", "first_name": "ankitha.vijayanandana", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T05:54:50.837Z", "updated_at": "2023-02-23T04:37:07.455Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256852, "name": "Libraries", "codename": "libraries", "accessibility": 1, "description": "These articles can help you manage libraries in Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3146521, "name": "aws"}, {"id": 3146524, "name": "azure"}, {"id": 2960290, "name": "elasticsearch"}, {"id": 3146525, "name": "gcp"}, {"id": 2960288, "name": "library"}, {"id": 2960291, "name": "maven"}], "url": "https://kb.databricks.com/libraries/failed-to-install-elasticsearch-via-maven"}, {"id": 1639053, "name": "Cannot access Databricks secrets when using a \"No isolation shared\" cluster", "views": 2424, "accessibility": 1, "description": "You cannot use dbutils.secrets.get() when admin protection for No isolation shared clusters is enabled in your account.", "codename": "cannot-access-databricks-secrets-when-using-a-no-isolation-shared-cluster", "created_at": "2022-11-25T12:57:21.423Z", "updated_at": "2023-03-31T04:29:06.849Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSs4bm1zZEY5NnRRYWF3WHFMcDVCTlJtL0xrVHF0cHAyczlHc2RONGlyWDZJSlhKT3E5CkpXQ3M4bWNVT3NPN29ackkzUURzWEx5MlFlaXJHSFB5eUx2aTZSZ1ZZYnB3TnQ0M2V3alhncjhNYUFCeQpoVUhiYlpSd3p4QmxvZ3c2UmIzVjV2K3FQVXpFbVBzb0VSZGFEU1RVWjdVVmFZV0Zsa3RtdlJIZ2J6UXkKTmt0aFB2RU1nNlA3ZURXQWt3RnRtVUVhNkg1Q0MyT0VzYkJzdDVUSFZJTGNmVVRGekxjOXVRb3Y2RHM4ClhjZHRieXVYVGhoekNRY1BzM1o5MCtsTjdLSldmdUNOUGJQYWp5cE44b0tpaVRUVTJBY1hIa25kSytnawpJVWgxOFlSYmU4MEJXQkdUZnNTMklEZ1lObEpDSms2K1JZRWE2aUtEdVZFcDQwbGFPZDZJWUY3UGdZLzIKd2pDa0F5OFVNbmhhRmFuS3pJQUNWVlUyRWtnU3EyRms4NnV3TW8zTEx4Y2Jwa1lCTU8rbklBVGV5emM3CnV6cVRWUVgyeUxwRDhMTWNLOGFyWmlDL2N1clp0WE9sOVJuK3NtQ1RYcDBacnd6YUhQTUtMaGd5a3BwZQpwUStObSt0c1FpNHRXWXQvaGtDQTV5em8wNUMraFJxQUpLRERNRkMxNFdJUC9OWGJRL1lkOGpleXpabEcKWWtYVDhGVlhsdG1ZOXZGTnI5eGN5ZDFDVXR0SW41NmpxMloycS90TkNLQ0lWeWdrdVJ5R3BYbTZQSGdwCmx0dWdSdXl0dXVkaXYzNDM1YTEzL0RZUTluK00ybFIzNytIUTBHUGxSeW52UGFoQ3dNaFJ3TFZjRGx1bQo1SEZFL1B6dWVyRHVSam5CaXBUbWxNOG95V3RKN3U3R2tvTU01Q2lqNmxqcUloQlBFQ0dLLzNMaEtscksKWEtPcmRiMzl2VmU4NEY4ZDNEYWIwZnRnYWlHeDVNT1VvTDJKQUpqOEx3eTVtNmNNOTlQWlFlcGNESXZaCkg1YXJFMWFYZUk2bXZzV0VmNGFoUVd1KzZmcFNBTVdGWEN0ZHUwRE1VbXdHWjlLZGwxbW5hZEhCQUhEQgpYanZvVUs1dHQ3bENKVnJRc1VENjVCZ050dEZwRTZDTUQ1anV4S3V0T3ErYTJURTlvQ3gwams1a1c3V3oKUlZqZHRoVnd2cnNBM1RHNTUvZXo2b1Z6Mm02RnNjbHU4ZUhUOG1vZDdTb3NSdDNVTGlBRWxkZmF2dlpGCnVCL1djeVg5dzM2T2M5MENYbUNFRHRtdHZ6QmxVdWJacy9tN0hoZVRtd1UrVmNEMlZLUXZkdUk3WDRwcQpiblpBK0RPTHR1QWFnRDRRbkVlOHJsU0cwRThYc2VhVkJZRjdhR29kbVBDaU5KdmtpTnV3WDBVbXZHczgKNDdHZzNGU1YrTkJOTG9qdXdGcU5GSnVzMTNIa08zVDJFVktMWVVIRjN1WFIzd29MU3ZaclVEVEJ5RG4xCnFQKzh5TEFRSjVMWkVLNW5aeStrTHlnU3hIbzM3Q0JDNy9zUDAzZGdIdERjaXdZZ2wrT0FZVjRpRnFNOApTVHdtK05CSVJPQkFFTjAwczNNWlk5RzNlOFJ4VUpHZHJ5T2hicE0yZ2xkSHdnYUFrME13MXZ5UGs5VGgKWUtmeVZFSTVyUHFDTXB1dngveksvSFRhSThNaEJpRjJ0NlFNaTRWZFZqbFBabk9FKzFZbnJQV21YZnc2CmY3NmlOallveHAwOElTeFFlK01XV1BVNzMwSTV4YzM3eFYrc3gvK2ZxclRRVGc9PQo=.4dbc148d06fb2f3bdbe9d4e8bf88a9c5\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are using a using a <strong>No isolation shared</strong> cluster. When you run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.secrets.get()</span> it returns a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">401 Unauthorized</span> error message.</p><p><img src=\"https://s3.amazonaws.com/helpjuice-static/helpjuice_production%2Fuploads%2Fupload%2Fimage%2F10723%2Fdirect%2F1669442495931-dbutils.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Admin protection for <strong>No isolation shared</strong> clusters is enabled in your account.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>An account admin has to disable admin protection for <strong>No isolation shared</strong> clusters.</p><ol>\n<li>Log in to the Account Console.</li>\n<li>Click <strong>Settings</strong>.</li>\n<li>Click the <strong>Feature enablement</strong> tab.</li>\n<li>Under <strong>Enable Admin Protection for \u201cNo Isolation Shared\u201d Clusters</strong>, click the setting to disable this feature.</li>\n</ol><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1669382043086-admin_protection.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">You MUST be a Databricks account admin to disable admin protection. A regular user cannot disable this feature.</p>\n</div>\n</div><p><br>For more information, review the <strong>Enable admin protection for \u201cNo isolation shared\u201d clusters on your account</strong> (<a href=\"https://docs.databricks.com/administration-guide/account-settings/no-isolation-shared.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/administration-guide/account-settings/no-isolation-shared\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/administration-guide/account-settings/no-isolation-shared.html\" rel=\"noopener noreferrer\" target=\"_blank\">GCP</a>) documentation.</p>", "body_txt": "Problem You are using a using a No isolation shared cluster. When you run dbutils.secrets.get() it returns a 401 Unauthorized error message. Cause Admin protection for No isolation shared clusters is enabled in your account. Solution An account admin has to disable admin protection for No isolation shared clusters. Log in to the Account Console.\nClick Settings.\nClick the Feature enablement tab.\nUnder Enable Admin Protection for \u201cNo Isolation Shared\u201d Clusters, click the setting to disable this feature. Info\nYou MUST be a Databricks account admin to disable admin protection. A regular user cannot disable this feature. For more information, review the Enable admin protection for \u201cNo isolation shared\u201d clusters on your account (AWS | Azure | GCP) documentation.", "format": "html", "updated_at": "2023-03-31T04:29:06.845Z"}, "author": {"id": 791192, "email": "sivaprasad.cs@databricks.com", "name": "sivaprasad.cs ", "first_name": "sivaprasad.cs", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T19:34:47.857Z", "updated_at": "2023-03-20T13:02:42.882Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 254612, "name": "Databricks administration", "codename": "administration", "accessibility": 1, "description": "These articles can help you administer your Databricks workspace, including user and group management, access control, and workspace storage.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3145110, "name": "aws"}, {"id": 3145111, "name": "azure"}, {"id": 3097995, "name": "cluster access"}, {"id": 3097996, "name": "databricks secrets"}, {"id": 3145112, "name": "gcp"}], "url": "https://kb.databricks.com/administration/cannot-access-databricks-secrets-when-using-a-no-isolation-shared-cluster"}, {"id": 1634082, "name": "Job fails with Java IndexOutOfBoundsException error", "views": 2179, "accessibility": 1, "description": "When groupby() is used along with applyInPandas it generates an exception due to an arrow buffer limitation.", "codename": "job-fails-with-java-indexoutofboundsexception-error", "created_at": "2022-11-22T07:39:46.529Z", "updated_at": "2022-12-21T09:56:40.094Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStUN1hqalpXZ0ExNDlGRWwySEhrQlpCd0laZ0pWTDk1cWsva1hwdzhXcXZKSWhucVpZClpqcERsakp5NXBTU0RvbzMrTG9tTEMrWjdmc2ZFZEkwcGFscGJ1bXNXTFFCcU9JV1UrL2xPeVZocVZUNAo0Tk9RMGFKUmxaM2Z5U1BKUjRFK1I1RmFlNEZXWlNsbmovUFN2WmNnejhEeVlNVWhDZTZ0VEI3dHFZc1AKdmFWRUZDOW1nUjVXd3BsamtXcFdCejYwSG1Pbkw2a05vRGI1eDFnaXh3M1RTUzgxVFJKVUFTUDBMaUU2CkZVeldhdHd0RTEzTjdBdUZrVnZmTEMyRk1maHd0Y1pVMWZ5aFNzYXhxNWlKZEo5aHlSdjFrY1hXaGxpYgo3ZUxHOU90WW5KdUFKaEt6Wk9LaTRIMHBQeklOU0IzZ3RrQzVFTjQ3NnR3WG56VStaclhZUFB4NGpuSjkKUXJiYVF3ZEl6TXlNOXBqd2J0WUtKbHJWRnVwSm1mVTl0M0ZMSHJqRFlVbHpIbVlQcFpxRlg0b1Yzd2N4CnV6OFRhZ0FEN2NiZFoxTHk2VDFRMlFYOEZIVTFvS1JIdjRwdUtWQVRNVFBYVEdEZ24xOGd1a3Z6Q0xLUQp5cUUrK1lyMCtPNTZkVkVndEsyWTVrOE9MV1lTUUNrdlJQOVlBOUxhM3RHVTl4YzVYNmVSQ1J6dkEycjUKRWJHSHlLV285SUR3MS9menVOd1VCUmpGZndTWjBOMTFjWDB0ZGVrY0o3b1RsKzZRYkRET3YrY29tVXJmCkJrcEEvOWZ1RlB0eDg0VHk0YmpMbjFHUDRGaHBDQitTT1RoVU1WWGg4cXUrcy9DRnc5WlNaRmFQRE5ieQo2WmorWTFIMmwrZUlxcU5zU0I1RjRXeGlUK2g3TlQrZGliRWlDVER1Y3JBMjVHMENQRE13cndxbVZWamsKVU9nOHJqMzNWUlFmZW00b1hXWTBiNmN3L3VTMzY0WmRoREtUMFBlcjQ0b0EvM21UYXRXbUVpK1FSOTFXCkV4M1FTaWt3UkpSZXVRZXgwNkJCNE8xTC9DRm1MdEZOS2E5YXpPRFJjMUc5dFl0SjRrMU5hSkNIQUdhNgpQTHFYRUswRTIyWlRQYVpWeUc5MUtpRGxicjNocGM4OVd1QUpLSmFITmhjdmcxTTlLSzF4d2h4TzNNZWIKemJoWGcyMDAwRHpVUEp2YVZNLzR5ZEVNaklaQTlSNkp3eEtYU1VPa21LVGRyQ0F2TWFrWWpzdWhENzNQCjZxeGgvTFFPU3l6R05OS3NtMkxkaEJVV2lpdXNHOXpxdjlkaXRUZW9ibkdoQkxvVUs4S2RoZXhoVVZqcwpWRXljaU5ITi9oeStzajBkbU44MUh5R2JFZnJNTDVaazYrZVE2a25QSmY1aFR1ZEJRSXR1VFFqUm1FTjgKT3hhRGVuWmtMUC95OFhOelJoeU1ScUE4WXZTcHNZVDJJU0h4dVJmbDRRdTdSdDMvM1ZZelBlaHNEVzlRCmN0OHdwbFVUODF1NFY5QzJEMkh4bm8zekJSOUVLL2dPWGg3eGJGRjlHbmxQYXdrL2N0S0IzakRuWldnQwpObnFiN3BzRXBQT3k3TWQzT3FpY1pjTGxnVkxuemFrY295SVljTFdYZ3RUc2VXdUJFT2pleFdQdDljYWkKVmVlYU1WeENWV3VYYUZham05c2tPMjhXQ1NrODZsR04xQmRpcllMOE1FdjdoU2pyM2tpZ1RxZ09tQkZyCk9aWmNDdlZvNDlIbkdzRk51QkF0M0hLYgo=.8e8aeb7141c3c4fa5d4de856792a1688\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Your job fails with a Java <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IndexOutOfBoundsException</span> error message:</p><pre>java.lang.IndexOutOfBoundsException: index: 0, length: &lt;number&gt; (expected: range(0, 0))</pre><p>When you review the stack trace you see something similar to this:</p><pre>Py4JJavaError: An error occurred while calling o617.count.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 2195, 10.207.235.228, executor 0): java.lang.IndexOutOfBoundsException: index: 0, length: 1073741824 (expected: range(0, 0))\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at io.netty.buffer.ArrowBuf.checkIndex(ArrowBuf.java:716)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at io.netty.buffer.ArrowBuf.setBytes(ArrowBuf.java:954)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:508)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1239)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1066)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:287)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:151)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:105)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:100)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:122)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:478)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2146)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\r\n\r\n\r\nDriver stacktrace:\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at scala.Option.foreach(Option.scala:407)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.IndexOutOfBoundsException: index: 0, length: 1073741824 (expected: range(0, 0))\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at io.netty.buffer.ArrowBuf.checkIndex(ArrowBuf.java:716)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at io.netty.buffer.ArrowBuf.setBytes(ArrowBuf.java:954)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:508)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1239)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1066)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:287)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:151)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:105)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:100)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:122)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:478)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2146)\r\n<span style=\"white-space:pre;\">\u00a0 \u00a0\u00a0</span>at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This error occurs due to an <a href=\"https://issues.apache.org/jira/browse/ARROW-15983\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">arrow buffer limitation</a>. When <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">groupby()</span> is used along with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">applyInPandas</span> it results in this error.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You can work around the issue by setting the following value in your cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>):</p><pre>spark.databricks.execution.pandasZeroConfConversion.groupbyApply.enabled=true</pre><p>This setting allows <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">groupby()</span> to function correctly with pandas operations.</p>", "body_txt": "Problem Your job fails with a Java IndexOutOfBoundsException error message: java.lang.IndexOutOfBoundsException: index: 0, length: &lt;number&gt; (expected: range(0, 0)) When you review the stack trace you see something similar to this: Py4JJavaError: An error occurred while calling o617.count. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 2195, 10.207.235.228, executor 0): java.lang.IndexOutOfBoundsException: index: 0, length: 1073741824 (expected: range(0, 0)) \u00a0 \u00a0\u00a0at io.netty.buffer.ArrowBuf.checkIndex(ArrowBuf.java:716) \u00a0 \u00a0\u00a0at io.netty.buffer.ArrowBuf.setBytes(ArrowBuf.java:954) \u00a0 \u00a0\u00a0at org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:508) \u00a0 \u00a0\u00a0at org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1239) \u00a0 \u00a0\u00a0at org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1066) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:287) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:151) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:105) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:100) \u00a0 \u00a0\u00a0at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) \u00a0 \u00a0\u00a0at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:122) \u00a0 \u00a0\u00a0at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:478) \u00a0 \u00a0\u00a0at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2146) \u00a0 \u00a0\u00a0at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270) Driver stacktrace: \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460) \u00a0 \u00a0\u00a0at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) \u00a0 \u00a0\u00a0at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) \u00a0 \u00a0\u00a0at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152) \u00a0 \u00a0\u00a0at scala.Option.foreach(Option.scala:407) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668) \u00a0 \u00a0\u00a0at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656) \u00a0 \u00a0\u00a0at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) Caused by: java.lang.IndexOutOfBoundsException: index: 0, length: 1073741824 (expected: range(0, 0)) \u00a0 \u00a0\u00a0at io.netty.buffer.ArrowBuf.checkIndex(ArrowBuf.java:716) \u00a0 \u00a0\u00a0at io.netty.buffer.ArrowBuf.setBytes(ArrowBuf.java:954) \u00a0 \u00a0\u00a0at org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:508) \u00a0 \u00a0\u00a0at org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1239) \u00a0 \u00a0\u00a0at org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1066) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:287) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:151) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:105) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:100) \u00a0 \u00a0\u00a0at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) \u00a0 \u00a0\u00a0at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581) \u00a0 \u00a0\u00a0at org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:122) \u00a0 \u00a0\u00a0at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:478) \u00a0 \u00a0\u00a0at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2146) \u00a0 \u00a0\u00a0at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270) Cause This error occurs due to an arrow buffer limitation. When groupby() is used along with applyInPandas it results in this error. Solution You can work around the issue by setting the following value in your cluster's Spark config (AWS | Azure | GCP): spark.databricks.execution.pandasZeroConfConversion.groupbyApply.enabled=true This setting allows groupby() to function correctly with pandas operations.", "format": "html", "updated_at": "2022-12-21T09:56:40.091Z"}, "author": {"id": 488150, "email": "rakesh.parija@databricks.com", "name": "rakesh.parija ", "first_name": "rakesh.parija", "last_name": "", "role_id": "admin", "created_at": "2021-10-07T02:59:41.577Z", "updated_at": "2023-04-21T14:21:18.111Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 256868, "name": "Python with Apache Spark", "codename": "python", "accessibility": 1, "description": "These articles can help you to use Python with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2978885, "name": "arrow"}, {"id": 2978880, "name": "aws"}, {"id": 2978881, "name": "azure"}, {"id": 2978882, "name": "gcp"}, {"id": 2978884, "name": "groupby"}, {"id": 2978883, "name": "pandas"}], "url": "https://kb.databricks.com/python/job-fails-with-java-indexoutofboundsexception-error"}, {"id": 1634039, "name": "Cannot delete Unity Catalog metastore using Terraform", "views": 3085, "accessibility": 1, "description": "Set force_destroy in the databricks_metastore section of the Terraform configuration to delete a metastore and its catalog.", "codename": "cannot-delete-unity-catalog-metastore-using-terraform", "created_at": "2022-11-22T06:48:51.907Z", "updated_at": "2022-12-21T08:14:47.758Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9VaXFXWXNETS8yV1NxdUplQW9OL0V1dlBVV3lRWExkWEFDOFJpdEVwcFZhTFUyckp5CnlLMDROQzFtVlZEZnhsVkpDaDI2NE9mM1U3TFpCdiswRk9xQUE3Sllwb3drK0xhK2Zab3FpNTZNY2d4QgoxT1JMTTlvQ0k3SDVQYkd3WXZFMVl5UnNvcnRLM1NiSzE3eTNTMkZncm9yYmtycHVjb000d1gxSXExajkKc253WDhoRkVENU4wRW5ESytVQm1ndnBXM2I1NzlmZDQyTnBjRkZscGJLeWxvbUpCMU5SYUk3bUlpdkxjClJDR1I0dm1pQVpoQTZaZHJQT2RJaUR2b2NjOVk5cUYvc1pLWlVESUhPSXVIUi9Fb3RwNWVHeW1pKzVETQpySENBaXlTN0tvaExiaTlJSHp0NlBZelhwc0xROGRXN0lWb3FYczVveDJ5MmdOSmUyWnhMVnMrTXdvTEQKQzhOeTIyeXgzUXRaajZMUU9BU1RZeGNJMllpVGN0bGg2eVlnbERBYmhhWjJiTkkzWmpCTXYrbElkZXBmCkI1ODRmV0FocEZWTFYySFRJRmdZTDZVb3ZZZStrK1F2cDJRKzZhTE1YaVV6cFpXMnNueDMwVjJiTmpvZgpxQnhvTUtUQnpMaDFsdUNaZXdRNWVLdTlKYXQzSGFhUTFFL0dtNFBRVlkycmlnSHJHdnVBenJLYVRvaVIKb01IM1ZMbFZyWXM5RE1Fa0hSK3FWY1ZHK09IMmI2dzBtNEdtbTQrcjRnMitvaUU0cGZpVlI3RlBMZ1YzCm8rUXVRZy90TVFrVXZzWUlFeVcyUmxFbG9pVm5Hb0o0QlhDZVZNNlVMWXBqUzNFOThCNEFFY0NSalp6agpaZnI5Y3dGZFk2NE81aHYxWnQ2UWNlTm52K2lKYkNpZ3V5Z2xCeGVZM1AwUngyMnhPV3hiZThSU1IxRlUKdFNRMG43TlYxTFBCNklZRll3UUFQekRKUm44QnZoV2lFdGU2WUlBZ29uODhJOWZxa1N4dy9RS29ybTRwCnpwbFBTa0ZwUjRud21tY0VuZWV5QjNLWStwOG1qMTkyUU50ZnFGajJPSE9hWHp1cmNCWkNrNVBzSG9aVwowK201WFFvL3Q5UVBLNjI2TzV1S3Nzek9GTGxoUkVyVHpGZUlGYU5reWh3RFpsUWJFa25UWklvMWpldC8KemRYdjd6N0hWVEMrZXB0a3g2NVdzSW9GZjRJY2M1SWROd2ZuYkdnV29iWndRN3hkUEd6RUtUWW5TQityCmpHK1Jvcm55RDFWTWhjWXRGdzROekx5a1lDckZiQXBPWjNkQzVqQU8yamFUaU9RU2hndnFvNVdsUVNWTQpWQjZ3U3NQaU5yazViaytIdjl4Vm5QMWcyM1E3a1lwUmFlL0duRjYzQjJaWDczYy9wK1YxWE1vSm40UlcKdXZHaHp4Tnp1aEJjK3dZSmdUeE81UUZiL0tOajNiclM4SWJUNkI0eXNqcjMxMWZWNzUwRDZjY3hYRkcyCkl1Q1RJZmdOVlQwUks5K1UweXhDWWNGUHkxcWRtNE53YVFnNkZsdXVKQ1dzNXFpUm1aMXZvUkg3eWg0NwpkQzJ3czVjb0xzVDBpT2NvcU9Ody9OaDlIZFBhZU9zaDBsRnFnNTYvc3JRczlEWWlZamphUnNnUnJiR20KL290enBXZGt4QndsSXBPUWc0OHZXc3hHYW1FRU5yc2c0MzIyZzBPRTArVGMwUzRpTzRyREk4Qi9taG1RCmVpcjVJVWpQYTBCVXNCLzBkbVhubk9KM3NBRUsxVGl0eWhhMndsUTRGampPOTVYTkNRR2lUaEdZUHhsdgpTcHo2M0l1d3YvZEMrRXNFMkVmTklJYlBVVnRZMDk3bEVkaXlwQitTNjJNdnFnd0dTNW9uV0I5ZFprRHkKclQ4T2RGSmliOXM9Cg==.1b7b4db1a1fe91877be257b563a2e4a6\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You cannot delete the Unity Catalog metastore using Terraform.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The default catalog is auto-created with a metastore. As a result, you cannot delete the metastore without first wiping the catalog.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">force_destory = true</span> in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">databricks_metastore</span> section of the Terraform configuration to delete the metastore and the corresponding catalog.</p><h2 data-toc=\"true\" id=\"example-code-3\">Example code:</h2><pre id=\"isPasted\">resource \"databricks_metastore\" \"this\" {\r\n\u00a0 provider \u00a0 \u00a0 \u00a0= databricks.workspace\r\n\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"primary\"\r\n\u00a0 storage_root \u00a0= \"s3://${aws_s3_bucket.metastore.id}/metastore\"\r\n\u00a0 owner \u00a0 \u00a0 \u00a0 \u00a0 = var.unity_admin_group\r\n\u00a0 <strong><span style=\"font-size: 14px;\">force_destroy = true</span></strong>\r\n}\r\n\r\n\r\nresource \"databricks_metastore_data_access\" \"this\" {\r\n\u00a0 provider \u00a0 \u00a0 = databricks.workspace\r\n\u00a0 metastore_id = databricks_metastore.this.id\r\n\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 = aws_iam_role.metastore_data_access.name\r\n\u00a0 aws_iam_role {\r\n\u00a0 \u00a0 role_arn = aws_iam_role.metastore_data_access.arn\r\n\u00a0 }\r\n\u00a0 is_default = true\r\n}\r\n\r\n\r\nresource \"databricks_metastore_assignment\" \"default_metastore\" {\r\n\u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = databricks.workspace\r\n\u00a0 for_each \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = toset(var.databricks_workspace_ids)\r\n\u00a0 workspace_id \u00a0 \u00a0 \u00a0 \u00a0 = each.key\r\n\u00a0 metastore_id \u00a0 \u00a0 \u00a0 \u00a0 = databricks_metastore.this.id\r\n\u00a0 default_catalog_name = \"hive_metastore\"\r\n}</pre><p><br></p>", "body_txt": "Problem You cannot delete the Unity Catalog metastore using Terraform. Cause The default catalog is auto-created with a metastore. As a result, you cannot delete the metastore without first wiping the catalog. Solution Set force_destory = true in the databricks_metastore section of the Terraform configuration to delete the metastore and the corresponding catalog. Example code: resource \"databricks_metastore\" \"this\" { \u00a0 provider \u00a0 \u00a0 \u00a0= databricks.workspace \u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"primary\" \u00a0 storage_root \u00a0= \"s3://${aws_s3_bucket.metastore.id}/metastore\" \u00a0 owner \u00a0 \u00a0 \u00a0 \u00a0 = var.unity_admin_group \u00a0 force_destroy = true } resource \"databricks_metastore_data_access\" \"this\" { \u00a0 provider \u00a0 \u00a0 = databricks.workspace \u00a0 metastore_id = databricks_metastore.this.id \u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 = aws_iam_role.metastore_data_access.name \u00a0 aws_iam_role { \u00a0 \u00a0 role_arn = aws_iam_role.metastore_data_access.arn \u00a0 } \u00a0 is_default = true } resource \"databricks_metastore_assignment\" \"default_metastore\" { \u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = databricks.workspace \u00a0 for_each \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = toset(var.databricks_workspace_ids) \u00a0 workspace_id \u00a0 \u00a0 \u00a0 \u00a0 = each.key \u00a0 metastore_id \u00a0 \u00a0 \u00a0 \u00a0 = databricks_metastore.this.id \u00a0 default_catalog_name = \"hive_metastore\" }", "format": "html", "updated_at": "2022-12-21T08:14:47.747Z"}, "author": {"id": 791192, "email": "sivaprasad.cs@databricks.com", "name": "sivaprasad.cs ", "first_name": "sivaprasad.cs", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T19:34:47.857Z", "updated_at": "2023-03-20T13:02:42.882Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 313834, "name": "Unity Catalog", "codename": "unity-catalog", "accessibility": 1, "description": "These articles can help you with Unity Catalog.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2971714, "name": "aws"}, {"id": 2971715, "name": "azure"}, {"id": 2971718, "name": "delete"}, {"id": 2971717, "name": "metastore"}, {"id": 2971716, "name": "terraform"}], "url": "https://kb.databricks.com/unity-catalog/cannot-delete-unity-catalog-metastore-using-terraform"}, {"id": 1632499, "name": "Single scheduled job tries to run multiple times", "views": 1703, "accessibility": 1, "description": "Ensure your cron syntax is correct when scheduling jobs. A wildcard in the wrong space can produce unexpected results.", "codename": "single-scheduled-job-tries-to-run-multiple-times", "created_at": "2022-11-21T16:51:59.691Z", "updated_at": "2023-01-20T12:45:49.508Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.80c8741b0d43e180e096ab0183a6e0ad\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You <strong>schedule a job</strong> (<a href=\"https://docs.databricks.com/workflows/jobs/jobs.html#schedule-a-job\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Schedule a job\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/workflows/jobs/jobs#--schedule-a-job\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Schedule a job\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/workflows/jobs/jobs.html#schedule-a-job\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Schedule a job\">GCP</a>) to run once per day, using Quartz Cron Syntax, but the job tries to run multiple times on the same day.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>When the job was configured, it was scheduled by manually entering the cron syntax and a special character <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">*</span> was accidentally set for the seconds value. This tells the cron scheduler to run the job once every second.</p><p>Cron syntax specifies a time in the format <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;seconds&gt; &lt;minutes&gt; &lt;hours&gt; &lt;day-of-month&gt; &lt;month&gt; &lt;day-of-week&gt;</span>. Numbers are used for the values and special characters can be used for multiple values.</p><p>For example, the cron syntax <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">* 07 04 * * ?</span> instructs the system to attempt to start the job once every second from 04:07:00 to 04:07:59, every day.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1669049625616-Screen%20Shot%202022-10-11%20at%2011.20.51%20AM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">You need to specify a value for the seconds field. By default, Databricks uses 10 for the seconds field.</p><p>By changing <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">*</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">10</span> in the previous example, the cron scheduler only runs the job once per day, at 04:07:10.\u00a0</p><p>For more information, review the Quartz Job Scheduler <a href=\"https://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html\" rel=\"noopener noreferrer\" target=\"_blank\">CronTrigger Tutorial</a> documentation.\u00a0</p><p><br></p>", "body_txt": "Problem You schedule a job (AWS | Azure | GCP) to run once per day, using Quartz Cron Syntax, but the job tries to run multiple times on the same day. Cause When the job was configured, it was scheduled by manually entering the cron syntax and a special character * was accidentally set for the seconds value. This tells the cron scheduler to run the job once every second. Cron syntax specifies a time in the format &lt;seconds&gt; &lt;minutes&gt; &lt;hours&gt; &lt;day-of-month&gt; &lt;month&gt; &lt;day-of-week&gt;. Numbers are used for the values and special characters can be used for multiple values. For example, the cron syntax * 07 04 * * ? instructs the system to attempt to start the job once every second from 04:07:00 to 04:07:59, every day. Solution You need to specify a value for the seconds field. By default, Databricks uses 10 for the seconds field. By changing * to 10 in the previous example, the cron scheduler only runs the job once per day, at 04:07:10.\u00a0 For more information, review the Quartz Job Scheduler CronTrigger Tutorial documentation.\u00a0", "format": "html", "updated_at": "2023-01-20T12:45:49.373Z"}, "author": {"id": 966503, "email": "monica.cao@databricks.com", "name": "monica.cao ", "first_name": "monica.cao", "last_name": "", "role_id": "draft_writer", "created_at": "2022-08-04T20:42:23.711Z", "updated_at": "2023-04-11T14:20:27.282Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256850, "name": "Jobs", "codename": "jobs", "accessibility": 1, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2997015, "name": "aws"}, {"id": 2998846, "name": "azure"}, {"id": 2998851, "name": "cron"}, {"id": 2998852, "name": "cronjob"}, {"id": 2998853, "name": "cron job"}, {"id": 2998847, "name": "gcp"}, {"id": 2998850, "name": "quartz"}, {"id": 2998849, "name": "quartz cron syntax"}, {"id": 2998848, "name": "quartz job scheduler"}], "url": "https://kb.databricks.com/jobs/single-scheduled-job-tries-to-run-multiple-times"}, {"id": 1621134, "name": "Unpin cluster configurations using the API", "views": 3172, "accessibility": 1, "description": "Unpin compute cluster configurations using the API.", "codename": "unpin-cluster-configurations-using-the-api", "created_at": "2022-11-14T05:51:52.209Z", "updated_at": "2022-12-21T09:37:40.258Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"U2FsdGVkX1+6Zn9wz6/OTcW9TDMLyV9qzQo4sIN/omO3BDQjYs5l3rcz5dOo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.8fdbe31b70159309e2a1d7fb690bd496\"></div><p id=\"isPasted\">Normally, cluster configurations are automatically deleted 30 days after the cluster was last terminated.</p><p>If you want to keep specific cluster configurations, you can <a class=\"fr-strong\" href=\"https://databricks.helpjuice.com/admin/questions/1501547-pinning-cluster-through-api?utm_medium=email&amp;utm_source=email&amp;version=1\" id=\"\" title=\"\">pin them</a>. Up to 100 clusters can be pinned.</p><p>If you not longer need a pinned cluster, you can unpin it. If you have pinned 100 clusters, you must unpin a cluster before you can pin another one.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"note-0\">Note</h3>\n<p class=\"hj-alert-text\">You must be a Databricks administrator to unpin a cluster.</p>\n</div>\n</div><p data-toc=\"true\" id=\"-1\">You can easily unpin a cluster (<a href=\"https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster\" id=\"isPasted\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pin a cluster\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/clusters-manage#--pin-a-cluster\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pin a cluster\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/clusters-manage.html#pin-a-cluster\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pin a cluster\">GCP</a>) via the workspace UI, but if you are managing your clusters via the API, you can also use the Unpin endpoint (<a href=\"https://docs.databricks.com/dev-tools/api/latest/clusters.html#unpin\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Unpin endpoint\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/clusters#--unpin\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Unpin endpoint\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/api/latest/clusters.html#pin\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Pin endpoint\">GCP</a>) in the Clusters API.</p><h1 data-toc=\"true\" id=\"instructions-1\">Instructions</h1><h2 data-toc=\"true\" id=\"unpin-all-pinned-clusters-2\">Unpin all pinned clusters</h2><p id=\"isPasted\"><i data-id=\"7894850386-jxyj7\"></i>Use the following sample code to unpin all pinned clusters in your workspace.</p><p>Before running the sample code, you will need a personal access token (<a href=\"https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"personal access token\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/authentication#--generate-a-personal-access-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"personal access token\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"personal access token\">GCP</a>) and your workspace domain. The workspace domain is just the domain name.</p><ol>\n<li>Copy and paste the sample code into a notebook cell.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;workspace-domain-without-trailing-backlash&gt;</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;personal-access-token&gt;</span> values.</li>\n<li>Run the cell to unpin all pinned clusters in your workspace.</li>\n</ol><pre data-stringify-type=\"pre\" id=\"isPasted\">%python\r\n\r\nimport requests\r\nworkspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\"\r\naccess_token = \"&lt;personal-access-token&gt;\"\r\n\r\nurl = workspace_url + \"/api/2.0/clusters/list\"\r\n\r\nheaders = {\r\n 'Authorization': 'Bearer ' + access_token\r\n}\r\n\r\ncluster = requests.request(\"GET\", url, headers=headers).json()\r\nfor pinned in cluster[\"clusters\"]:\r\n    if 'pinned_by_user_name' in pinned :\r\n        print(\"Unpinning\"+\" , \"+ pinned[\"default_tags\"]['ClusterName'])\r\n        url = workspace_url + \"/api/2.0/clusters/unpin\"\r\n        requests.post(url,json={\"cluster_id\" : pinned[\"cluster_id\"]},headers=headers)</pre><h2 data-toc=\"true\" id=\"unpin-a-cluster-by-name-3\">Unpin a cluster by name</h2><p>Use the following sample code to unpin a specific cluster in your workspace.</p><p>Before running the sample code, you will need a personal access token and your workspace domain. The workspace domain is just the domain name.</p><ol>\n<li>Copy and paste the sample code into a notebook cell.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;workspace-domain-without-trail-backlash&gt;</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;personal-access-token&gt;</span> values.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;cluster-name-to-unpin&gt;</span> value with the name of the cluster you want to pin.</li>\n<li>Run the cell to unpin the selected cluster in your workspace.</li>\n</ol><pre data-stringify-type=\"pre\" id=\"isPasted\">%python\r\n\r\nimport requests\r\nworkspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\"\r\naccess_token = \"&lt;personal-access-token&gt;\"\r\n\r\nurl = workspace_url + \"/api/2.0/clusters/list\"\r\n\r\nheaders = {\r\n 'Authorization': 'Bearer ' + access_token\r\n}\r\n\r\ncluster = requests.request(\"GET\", url, headers=headers).json()\r\nfor pinned in cluster[\"clusters\"]:\r\n    if 'pinned_by_user_name' in pinned :\r\n        \u00a0if pinned[\"default_tags\"]['ClusterName'] == \"&lt;cluster-name-to-unpin&gt;\" :<br id=\"isPasted\">\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Unpinning\"+\" , \"+ pinned[\"default_tags\"]['ClusterName'])\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 url = workspace_url + \"/api/2.0/clusters/unpin\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests.post(url,json={\"cluster_id\" : pinned[\"cluster_id\"]},headers=headers)</pre><h2 data-toc=\"true\" id=\"unpin-all-clusters-by-a-specific-user-4\">Unpin all clusters by a specific user</h2><p>Use the following sample code to unpin a specific cluster in your workspace.</p><p>Before running the sample code, you will need a personal access token and your workspace domain. The workspace domain is just the domain name.</p><ol>\n<li>Copy and paste the sample code into a notebook cell.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;workspace-domain-without-trailing-backlash&gt;</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;personal-access-token&gt;</span> values.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;cluster-creator-username&gt;</span> value with the name of the user whose clusters you want to unpin.</li>\n<li>Run the cell to unpin the selected clusters in your workspace.</li>\n</ol><pre id=\"isPasted\">%python\r\n\r\nimport requests\r\nworkspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\"\r\naccess_token = \"&lt;personal-access-token&gt;\" \u00a0\r\n\r\nurl = workspace_url + \"/api/2.0/clusters/list\"\r\n\r\nheaders={\r\n\u00a0'Authorization': 'Bearer ' + access_token\r\n}\r\n\r\ncluster = requests.request(\"GET\", url, headers=headers).json()\r\n\r\nfor pinned in cluster[\"clusters\"]:\r\n\u00a0 \u00a0 \u00a0if 'pinned_by_user_name' in pinned :\r\n\u00a0 \u00a0 \u00a0 \u00a0 if pinned[\"creator_user_name\"] == \"&lt;cluster-creator-username&gt;\" :\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 url = workspace_url + \"/api/2.0/clusters/unpin\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests.post(url,json={\"cluster_id\" : pinned[\"cluster_id\"]},headers=headers)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0print(\"Unpinning\"+\" , \"+ pinned[\"default_tags\"]['ClusterName'])</pre><p><br></p>", "body_txt": "Normally, cluster configurations are automatically deleted 30 days after the cluster was last terminated. If you want to keep specific cluster configurations, you can pin them. Up to 100 clusters can be pinned. If you not longer need a pinned cluster, you can unpin it. If you have pinned 100 clusters, you must unpin a cluster before you can pin another one. Note\nYou must be a Databricks administrator to unpin a cluster. You can easily unpin a cluster (AWS | Azure | GCP) via the workspace UI, but if you are managing your clusters via the API, you can also use the Unpin endpoint (AWS | Azure | GCP) in the Clusters API. Instructions Unpin all pinned clusters Use the following sample code to unpin all pinned clusters in your workspace. Before running the sample code, you will need a personal access token (AWS | Azure | GCP) and your workspace domain. The workspace domain is just the domain name. Copy and paste the sample code into a notebook cell.\nUpdate the &lt;workspace-domain-without-trailing-backlash&gt; and &lt;personal-access-token&gt; values.\nRun the cell to unpin all pinned clusters in your workspace. %python import requests workspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\" access_token = \"&lt;personal-access-token&gt;\" url = workspace_url + \"/api/2.0/clusters/list\" headers = { 'Authorization': 'Bearer ' + access_token } cluster = requests.request(\"GET\", url, headers=headers).json() for pinned in cluster[\"clusters\"]: if 'pinned_by_user_name' in pinned : print(\"Unpinning\"+\" , \"+ pinned[\"default_tags\"]['ClusterName']) url = workspace_url + \"/api/2.0/clusters/unpin\" requests.post(url,json={\"cluster_id\" : pinned[\"cluster_id\"]},headers=headers) Unpin a cluster by name Use the following sample code to unpin a specific cluster in your workspace. Before running the sample code, you will need a personal access token and your workspace domain. The workspace domain is just the domain name. Copy and paste the sample code into a notebook cell.\nUpdate the &lt;workspace-domain-without-trail-backlash&gt; and &lt;personal-access-token&gt; values.\nUpdate the &lt;cluster-name-to-unpin&gt; value with the name of the cluster you want to pin.\nRun the cell to unpin the selected cluster in your workspace. %python import requests workspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\" access_token = \"&lt;personal-access-token&gt;\" url = workspace_url + \"/api/2.0/clusters/list\" headers = { 'Authorization': 'Bearer ' + access_token } cluster = requests.request(\"GET\", url, headers=headers).json() for pinned in cluster[\"clusters\"]: if 'pinned_by_user_name' in pinned : \u00a0if pinned[\"default_tags\"]['ClusterName'] == \"&lt;cluster-name-to-unpin&gt;\" :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Unpinning\"+\" , \"+ pinned[\"default_tags\"]['ClusterName']) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 url = workspace_url + \"/api/2.0/clusters/unpin\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests.post(url,json={\"cluster_id\" : pinned[\"cluster_id\"]},headers=headers) Unpin all clusters by a specific user Use the following sample code to unpin a specific cluster in your workspace. Before running the sample code, you will need a personal access token and your workspace domain. The workspace domain is just the domain name. Copy and paste the sample code into a notebook cell.\nUpdate the &lt;workspace-domain-without-trailing-backlash&gt; and &lt;personal-access-token&gt; values.\nUpdate the &lt;cluster-creator-username&gt; value with the name of the user whose clusters you want to unpin.\nRun the cell to unpin the selected clusters in your workspace. %python import requests workspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\" access_token = \"&lt;personal-access-token&gt;\" \u00a0 url = workspace_url + \"/api/2.0/clusters/list\" headers={ \u00a0'Authorization': 'Bearer ' + access_token } cluster = requests.request(\"GET\", url, headers=headers).json() for pinned in cluster[\"clusters\"]: \u00a0 \u00a0 \u00a0if 'pinned_by_user_name' in pinned : \u00a0 \u00a0 \u00a0 \u00a0 if pinned[\"creator_user_name\"] == \"&lt;cluster-creator-username&gt;\" : \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 url = workspace_url + \"/api/2.0/clusters/unpin\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests.post(url,json={\"cluster_id\" : pinned[\"cluster_id\"]},headers=headers) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0print(\"Unpinning\"+\" , \"+ pinned[\"default_tags\"]['ClusterName'])", "format": "html", "updated_at": "2022-12-21T09:37:40.244Z"}, "author": {"id": 790705, "email": "simran.arora@databricks.com", "name": "simran.arora ", "first_name": "simran.arora", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T08:29:23.302Z", "updated_at": "2023-04-25T08:04:15.648Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2978868, "name": "100"}, {"id": 2978860, "name": "aws"}, {"id": 2978861, "name": "azure"}, {"id": 2978865, "name": "cluster"}, {"id": 2978866, "name": "compute"}, {"id": 2978862, "name": "gcp"}, {"id": 2978867, "name": "limit"}, {"id": 2978863, "name": "pin"}, {"id": 2978864, "name": "unpin"}], "url": "https://kb.databricks.com/clusters/unpin-cluster-configurations-using-the-api"}, {"id": 1610825, "name": "How do I request a certification voucher?", "views": 2091, "accessibility": 1, "description": "Gain information on how to acquire certification vouchers to start your certification process.", "codename": "how-do-i-request-a-certification-voucher", "created_at": "2022-11-04T18:57:54.778Z", "updated_at": "2023-03-28T06:37:04.177Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"color: rgb(0, 0, 0);\">Discounted</span> c<span style=\"color: rgb(0, 0, 0);\">ertification vouchers are reserved for Databricks events, beta exams, and partner organizations or can be redeemed using pre-purchased credits.\u00a0</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><br></p><p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><strong><u><span style=\"color: rgb(0, 0, 0);\">Customers</span></u></strong></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 1:\u00a0</strong>Verify if your company has any pre-purchased credits to be used\u00a0</span></p><ul>\n<li dir=\"ltr\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; color: rgb(0, 0, 0);\">If yes, please submit a request here: <a href=\"https://www.databricks.com/learn/training/public-training-requests-form\" rel=\"noopener noreferrer\" target=\"_blank\"><u>https://www.databricks.com/learn/training/public-training-requests-form</u></a>\n</li>\n<li dir=\"ltr\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; color: rgb(0, 0, 0);\">If no, please proceed to Databricks Academy and purchase a certification<br><br>\n</li>\n</ul><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><strong><span style=\"color: rgb(0, 0, 0);\"><u>Partners</u></span></strong></p><p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 1:\u00a0</strong>Navigate to the\u00a0</span><a href=\"https://help.databricks.com/s/contact-us?ReqType=training\" rel=\"noopener noreferrer\" style=\"text-decoration:none;\" target=\"_blank\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Databricks Help Center</span></a></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 2:\u00a0</strong>Provide the requested information</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 3:</strong> For the Training Issue, select \u201cCertifications\u201d</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 4:\u00a0</strong>For Subject, please type \u201cPartner Certification Voucher Request - [Certification Name]\u201d</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 5:\u00a0</strong>For Message, please type \u201cI am with [Name of Partner]. I am requesting a partner certification voucher for [Certification Name] from the recent course I took, [Course Name] on [Course Date]\u201d</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><br></p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-0\">Warning</h3>\n<p class=\"hj-alert-text\">Vouchers will only be sent to an email address that includes a partner email domain - do not use a personal email address.</p>\n</div>\n</div><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><br></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"color: rgb(0, 0, 0);\">If you're interested in any Databricks events and/or beta exams, please submit a request through the\u00a0</span><a href=\"https://help.databricks.com/s/contact-us?ReqType=training\" rel=\"noopener noreferrer\" style=\"text-decoration:none;\" target=\"_blank\"><u>Databricks Help Center</u></a>.</p>", "body_txt": "Discounted certification vouchers are reserved for Databricks events, beta exams, and partner organizations or can be redeemed using pre-purchased credits.\u00a0 Customers Step 1:\u00a0Verify if your company has any pre-purchased credits to be used\u00a0 If yes, please submit a request here: https://www.databricks.com/learn/training/public-training-requests-form If no, please proceed to Databricks Academy and purchase a certification Partners Step 1:\u00a0Navigate to the\u00a0 Databricks Help Center Step 2:\u00a0Provide the requested information Step 3: For the Training Issue, select \u201cCertifications\u201d Step 4:\u00a0For Subject, please type \u201cPartner Certification Voucher Request - [Certification Name]\u201d Step 5:\u00a0For Message, please type \u201cI am with [Name of Partner]. I am requesting a partner certification voucher for [Certification Name] from the recent course I took, [Course Name] on [Course Date]\u201d Warning\nVouchers will only be sent to an email address that includes a partner email domain - do not use a personal email address. If you're interested in any Databricks events and/or beta exams, please submit a request through the\u00a0 Databricks Help Center .", "format": "html", "updated_at": "2023-03-28T06:37:03.785Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3113926, "name": "certification request"}, {"id": 3113927, "name": "request voucher"}, {"id": 3113932, "name": "training"}], "url": "https://kb.databricks.com/training/how-do-i-request-a-certification-voucher"}, {"id": 1610821, "name": "How do I reschedule a certification exam with Webassessor?", "views": 1699, "accessibility": 1, "description": "Learn how to easily and quickly reschedule your certification exam online with Webassessor.", "codename": "how-do-i-reschedule-a-certification-exam-with-webassessor", "created_at": "2022-11-04T18:55:08.424Z", "updated_at": "2023-03-28T06:35:03.221Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'><strong>Login to your\u00a0</strong></span><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'><a href=\"https%3A/www.webassessor.com/databricks\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>Webassessor</strong></a></span><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'><strong>\u00a0account.</strong></span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\">\n<li aria-level=\"1\" dir=\"ltr\" style='list-style-type: disc; font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;'><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>Open the My Assessments tab.</span></p></li>\n<li aria-level=\"1\" dir=\"ltr\" style='list-style-type: disc; font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;'><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>In Scheduled Exams, click the Reschedule/Cancel link.</span></p></li>\n</ul><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><br></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>To Reschedule</span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\">\n<li aria-level=\"1\" dir=\"ltr\" style='list-style-type: disc; font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;'><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>On the new screen, click the \"Reschedule\" button and follow the prompts.</span></p></li>\n<li aria-level=\"1\" dir=\"ltr\" style='list-style-type: disc; font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;'><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-family: \"DM Sans\", sans-serif; color: rgb(14, 16, 26); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>After rescheduling, click the \"Done\" button.</span></p></li>\n</ul>", "body_txt": "Login to your\u00a0 Webassessor \u00a0account. Open the My Assessments tab. In Scheduled Exams, click the Reschedule/Cancel link. To Reschedule On the new screen, click the \"Reschedule\" button and follow the prompts. After rescheduling, click the \"Done\" button.", "format": "html", "updated_at": "2023-03-10T02:03:13.177Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3113877, "name": "reschedule exam"}, {"id": 3113878, "name": "training"}, {"id": 3113876, "name": "webassessor"}], "url": "https://kb.databricks.com/training/how-do-i-reschedule-a-certification-exam-with-webassessor"}, {"id": 1610819, "name": "Where can I find my badges and certifications?", "views": 1669, "accessibility": 1, "description": "Learn how to locate and access your badges and certifications.", "codename": "where-can-i-find-my-badges-and-certifications", "created_at": "2022-11-04T18:54:23.983Z", "updated_at": "2023-03-28T06:34:47.540Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>We have moved to a digital certificate and badging system, which is more secure and allows for digital verification.</span></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-0\">Info</h3>\n<p class=\"hj-alert-text\"><strong>Please note that badges and certifications may take up to 48 hours to be sent to your email.</strong></p>\n</div>\n</div><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>\u00a0</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'><strong>Step 1:\u00a0</strong>Please go to</span><span style=\"font-size: 16px;\"><a href=\"http://credentials.databricks.com/\" rel=\"noopener noreferrer\" style=\"text-decoration:none;\" target=\"_blank\"><span style='font-family: \"DM Sans\", sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>\u00a0</span><span style='font-family: \"DM Sans\", sans-serif; color: rgb(17, 85, 204); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;'>credentials.databricks.com</span></a></span><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'><strong>Step 2:\u00a0</strong>Click on the Sign In option from the upper right-hand corner.</span></p><p><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'><strong>Step 3:\u00a0</strong>On the Sign In page, you can either provide your email and password if you have an Accredible account (we use Accredible to store your credentials) or, you can retrieve your credentials without a password by validating your email address.</span></p><p><span style=\"color: rgb(0, 0, 0);\">If you have not received your badge and/or certification within 48 hours of completing an accreditation or exam, please proceed with <span id=\"isPasted\" style=\"font-size: 16px;\"><a href=\"https://help.databricks.com/s/contact-us?ReqType=training\" rel=\"noopener noreferrer\" style=\"text-decoration:none;\" target=\"_blank\"><span style='font-family: \"DM Sans\", sans-serif; color: rgb(17, 85, 204); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;'>submitting a support ticket.</span></a></span><span style='font-size: 16px; font-family: \"DM Sans\", sans-serif; color: rgb(0, 0, 0); background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;'>.</span></span></p>", "body_txt": "We have moved to a digital certificate and badging system, which is more secure and allows for digital verification. Info Please note that badges and certifications may take up to 48 hours to be sent to your email. \u00a0 Step 1:\u00a0Please go to \u00a0 credentials.databricks.com . Step 2:\u00a0Click on the Sign In option from the upper right-hand corner. Step 3:\u00a0On the Sign In page, you can either provide your email and password if you have an Accredible account (we use Accredible to store your credentials) or, you can retrieve your credentials without a password by validating your email address. If you have not received your badge and/or certification within 48 hours of completing an accreditation or exam, please proceed with submitting a support ticket. .", "format": "html", "updated_at": "2023-03-10T01:58:42.202Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3113855, "name": "badges"}, {"id": 3113854, "name": "certifications"}, {"id": 3113858, "name": "training"}], "url": "https://kb.databricks.com/training/where-can-i-find-my-badges-and-certifications"}, {"id": 1610816, "name": "How can I edit my email address, first name, last name, language, and/or time zone in Databricks Academy?", "views": 1567, "accessibility": 1, "description": "Learn how to easily change your email address, name, preferred language, and time zone in Databricks Academy.", "codename": "how-can-i-edit-my-email-address-first-name-last-name-language-andor-time-zone-in-databricks-academy", "created_at": "2022-11-04T18:53:41.601Z", "updated_at": "2023-03-28T06:34:16.189Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 1:</strong> Log in to your\u00a0</span><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><a href=\"https://www.databricks.com/learn/training/login\" rel=\"noopener noreferrer\" target=\"_blank\">account</a>.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 2:</strong> Click on the user menu from the\u00a0top left.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 3:\u00a0</strong>Click on the edit option/My profile next to your name or email address.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 4:\u00a0</strong>Make the changes as per your requirement and click on save changes.</span></p>", "body_txt": "Step 1: Log in to your\u00a0 account. Step 2: Click on the user menu from the\u00a0top left. Step 3:\u00a0Click on the edit option/My profile next to your name or email address. Step 4:\u00a0Make the changes as per your requirement and click on save changes.", "format": "html", "updated_at": "2023-03-10T01:57:00.097Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3113844, "name": "databricks"}, {"id": 3113845, "name": "edit email"}, {"id": 3113843, "name": "training"}], "url": "https://kb.databricks.com/training/how-can-i-edit-my-email-address-first-name-last-name-language-andor-time-zone-in-databricks-academy"}, {"id": 1610815, "name": "How can I enroll in a course?", "views": 1613, "accessibility": 1, "description": "Learn the steps needed to enroll in a course and get valuable insights on the best method to do so.", "codename": "how-can-i-enroll-in-a-course", "created_at": "2022-11-04T18:52:50.187Z", "updated_at": "2023-03-28T06:33:47.779Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 1:</strong> Log in to your Academy account.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 2:</strong> Click on the course catalog and select the course or</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; margin-left: 60px;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Click on course catalog and click on the drop-down next to cards, click on the calendar\u00a0</span><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">and select the course.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 3:</strong> Select the session as per your preferred time zone and then click on Add to Cart.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 4:</strong> Click on view cart and enter the coupon code (if you have one) and click on checkout.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 5:</strong> Fill out all the mandatory billing details and click on save.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 6:</strong> Click on confirm the order.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><strong>Step 7:</strong> Fill out payment details, click on next it will redirect to review the payment details.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:12pt;font-family:'DM Sans',sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">\u00a0</span></p>", "body_txt": "Step 1: Log in to your Academy account. Step 2: Click on the course catalog and select the course or Click on course catalog and click on the drop-down next to cards, click on the calendar\u00a0 and select the course. Step 3: Select the session as per your preferred time zone and then click on Add to Cart. Step 4: Click on view cart and enter the coupon code (if you have one) and click on checkout. Step 5: Fill out all the mandatory billing details and click on save. Step 6: Click on confirm the order. Step 7: Fill out payment details, click on next it will redirect to review the payment details. \u00a0", "format": "html", "updated_at": "2023-03-10T01:55:51.776Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3113840, "name": "course"}, {"id": 3113841, "name": "enroll"}, {"id": 3113842, "name": "training"}], "url": "https://kb.databricks.com/training/how-can-i-enroll-in-a-course"}, {"id": 1610813, "name": "How can I reset my password in Databricks Academy?", "views": 1571, "accessibility": 1, "description": "Learn step-by-step instructions for resetting your password in Databricks Academy.", "codename": "how-can-i-reset-my-password-in-databricks-academy", "created_at": "2022-11-04T18:51:36.581Z", "updated_at": "2023-03-28T06:05:42.410Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 1</strong>: Navigate to your Academy login page.</span></p><ul>\n<li>\n<a href=\"https://customer-academy.databricks.com/learn\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#0000ff;font-size:16px;\"><u>https://customer-academy.databricks.com/learn</u></span></a><span style=\"background-color:transparent;color:#000000;font-size:16px;\">\u00a0</span>\n</li>\n<li><a href=\"https://partner-academy.databricks.com/learn\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#0000ff;font-size:16px;\"><u>https://partner-academy.databricks.com/learn</u></span></a></li>\n<li><a href=\"https://microsoft-academy.databricks.com/learn\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#1155cc;font-size:16px;\"><u>https://microsoft-academy.databricks.com/learn</u></span></a></li>\n</ul><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 2: </strong>Select forgot password.</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 3:</strong> Enter the email address and click on send reset link.</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 4:</strong> Go to your email inbox.</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 5: </strong>Find the link and reset the password.</span></p><p><br>\u00a0</p>", "body_txt": "Step 1: Navigate to your Academy login page. https://customer-academy.databricks.com/learn \u00a0 https://partner-academy.databricks.com/learn https://microsoft-academy.databricks.com/learn Step 2: Select forgot password. Step 3: Enter the email address and click on send reset link. Step 4: Go to your email inbox. Step 5: Find the link and reset the password. \u00a0", "format": "html", "updated_at": "2023-03-10T01:54:55.163Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3111823, "name": "databricks academy"}, {"id": 3111824, "name": "reset password"}, {"id": 3113839, "name": "training"}], "url": "https://kb.databricks.com/training/how-can-i-reset-my-password-in-databricks-academy"}, {"id": 1610812, "name": "How can I unenroll from an instructor led training?", "views": 1188, "accessibility": 1, "description": "Learn the steps needed to unenroll from an instructor led training course.", "codename": "how-can-i-unenroll-from-an-instructor-led-training", "created_at": "2022-11-04T18:50:01.749Z", "updated_at": "2023-03-28T06:05:17.941Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>As a partner:\u00a0</strong></span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 1:</strong>\u00a0</span><span style=\"background-color:transparent;color:#1d1c1d;font-size:16px;\">Log in to your </span><a href=\"about:blank\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#1155cc;font-size:16px;\"><u>Academy account</u></span></a><span style=\"background-color:transparent;color:#1d1c1d;font-size:16px;\">.</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 2:\u00a0</strong></span><span style=\"background-color:transparent;color:#1d1c1d;font-size:16px;\">Go to my courses and learning plans or directly select the course you wish to unenroll.</span></p><p><span style=\"background-color:transparent;color:#1d1c1d;font-size:16px;\"><strong>Step 3: </strong>Click on the unenroll from the course to the right top.</span></p><p><br><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>As a customer, </strong>please review our </span><a href=\"https://www.databricks.com/learn/training/training-faq#cancellation\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#1155cc;font-size:16px;\"><u>cancellation policy</u></span></a><span style=\"background-color:transparent;color:#000000;font-size:16px;\"> and proceed with\u00a0</span><a href=\"https://help.databricks.com/s/contact-us?ReqType=training\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#1155cc;font-size:16px;\"><u>submitting a support ticket</u></span></a><span style=\"background-color:transparent;color:#000000;font-size:16px;\">.</span></p>", "body_txt": "As a partner:\u00a0 Step 1:\u00a0 Log in to your Academy account . Step 2:\u00a0 Go to my courses and learning plans or directly select the course you wish to unenroll. Step 3: Click on the unenroll from the course to the right top. As a customer, please review our cancellation policy and proceed with\u00a0 submitting a support ticket .", "format": "html", "updated_at": "2023-03-09T22:35:59.968Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3113159, "name": "remove course"}, {"id": 3113172, "name": "training"}, {"id": 3113158, "name": "unregister"}], "url": "https://kb.databricks.com/training/how-can-i-unenroll-from-an-instructor-led-training"}, {"id": 1610809, "name": "How can I locate all of the courses that are available to me?", "views": 1817, "accessibility": 1, "description": "Find out which courses your school offers and get tips on how to most efficiently search for available courses.", "codename": "how-can-i-locate-all-of-the-courses-that-are-available-to-me", "created_at": "2022-11-04T18:47:45.285Z", "updated_at": "2023-03-28T06:04:50.184Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 1: </strong>Log into your Databricks Academy Account.</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 2: </strong>Click on the menu from the left top.</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 3: </strong>Select the course catalog and find all the courses available for you.</span></p>", "body_txt": "Step 1: Log into your Databricks Academy Account. Step 2: Click on the menu from the left top. Step 3: Select the course catalog and find all the courses available for you.", "format": "html", "updated_at": "2022-11-04T18:49:56.125Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3109074, "name": "courses"}, {"id": 3109073, "name": "locate"}, {"id": 3113134, "name": "training"}], "url": "https://kb.databricks.com/training/how-can-i-locate-all-of-the-courses-that-are-available-to-me"}, {"id": 1610806, "name": "How do I create an Academy account?", "views": 1707, "accessibility": 1, "description": "Customer, Partner, and Microsoft Academy", "codename": "how-do-i-create-an-academy-account", "created_at": "2022-11-04T18:41:00.671Z", "updated_at": "2023-03-28T06:04:36.384Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 1:\u00a0</strong></span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\">For customers, click on: </span><a href=\"https://customer-academy.databricks.com/learn\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#0000ff;font-size:16px;\"><u>https://customer-academy.databricks.com/learn</u></span></a></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\">For partners, click on:\u00a0</span><a href=\"https://partner-academy.databricks.com/learn\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#0000ff;font-size:16px;\"><u>https://partner-academy.databricks.com/learn</u></span></a><span style=\"background-color:transparent;color:#000000;font-size:16px;\">\u00a0</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\">For Microsoft employees, click on:\u00a0</span><a href=\"https://microsoft-academy.databricks.com/learn\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"background-color:transparent;color:#1155cc;font-size:16px;\"><u>https://microsoft-academy.databricks.com/learn</u></span></a></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 2: </strong>Click on Register and fill in all the mandatory details.</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 3: </strong>Select the next option to enter the company email address.</span></p><p><span style=\"background-color:transparent;color:#000000;font-size:16px;\"><strong>Step 4: </strong>Click on register to complete the registration process.</span></p><p><br><span style=\"font-size:16px;\">\u00a0</span></p>", "body_txt": "Step 1:\u00a0 For customers, click on: https://customer-academy.databricks.com/learn For partners, click on:\u00a0 https://partner-academy.databricks.com/learn \u00a0 For Microsoft employees, click on:\u00a0 https://microsoft-academy.databricks.com/learn Step 2: Click on Register and fill in all the mandatory details. Step 3: Select the next option to enter the company email address. Step 4: Click on register to complete the registration process. \u00a0", "format": "html", "updated_at": "2023-03-09T20:19:55.608Z"}, "author": {"id": 1006593, "email": "naomi.chiu@databricks.com", "name": "naomi.chiu ", "first_name": "naomi.chiu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-10-31T18:06:59.696Z", "updated_at": "2023-03-27T14:33:58.341Z", "groups": [{"id": 7950, "name": "costCenter.704-Education"}]}, "category": {"id": 363752, "name": "Training", "codename": "training", "accessibility": 1, "description": "Information and help with Databricks Training", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3112070, "name": "academy signup"}, {"id": 3112071, "name": "create account"}, {"id": 3113145, "name": "training"}], "url": "https://kb.databricks.com/training/how-do-i-create-an-academy-account"}, {"id": 1605815, "name": "Users unable to view job results when using remote Git source", "views": 3903, "accessibility": 1, "description": "Databricks does not manage permission for remote repos, so you must sync changes with a local notebook so non-admin users can view results.", "codename": "users-unable-to-view-job-results-when-using-remote-git-source", "created_at": "2022-11-02T05:23:48.204Z", "updated_at": "2023-03-07T02:42:16.242Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.18752c7d82ebd1bc0f1cfb255339253f\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are running a job using notebooks that are stored in a remote Git repository (<a href=\"https://docs.databricks.com/workflows/jobs/jobs.html#run-jobs-using-notebooks-in-a-remote-git-repository\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"running a job with notebooks that are stored in a remote Git repository\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/workflows/jobs/jobs#--run-jobs-using-notebooks-in-a-remote-git-repository\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"running a job with notebooks that are stored in a remote Git repository\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/workflows/jobs/jobs.html#run-jobs-using-notebooks-in-a-remote-git-repository\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"running a job with notebooks that are stored in a remote Git repository\">GCP</a>). Databricks users with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Can View</span> permissions (who are not a workspace admin or owners of the job) cannot access or view the results of ephemeral jobs submitted via <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.notebook.run()</span> from parent notebook.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>When job visibility control (<a href=\"https://docs.databricks.com/administration-guide/access-control/jobs-acl.html#prevent-users-from-seeing-jobs-they-do-not-have-access-to\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Prevent users from seeing jobs they do not have access to\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks//administration-guide/access-control/jobs-acl#--prevent-users-from-seeing-jobs-they-do-not-have-access-to\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Prevent users from seeing jobs they do not have access to\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/administration-guide/access-control/jobs-acl.html#prevent-users-from-seeing-jobs-they-do-not-have-access-to\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Prevent users from seeing jobs they do not have access to\">GCP</a>) is enabled in the workspace, users can only see jobs permitted by their access control level. This works as expected with notebooks stored in the workspace. However, Databricks does not manage access control for your remote Git repo, so it does not know if there are any permission restrictions on notebooks stored in Git. The only Databricks user that definitely has permission to access notebooks in the remote Git repo is the job owner. As a result, other non-admin users are blocked from viewing, even if they have <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Can View</span> permissions in Databricks.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You can work around this issue by configuring the job notebook source as your Databricks workspace and make the first task of your job fetch the latest changes from the remote Git repo.</p><p>This allows for scenarios where the job has to ensure it is using the latest version of a notebook stored in a shared, remote Git repo.</p><figure class=\"image\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/2493633/image.png\" class=\"fr-fic fr-dii\" alt=\"Job workflow showing the job start by syncing changes from a remote repo to a local notebook.\"></figure><p>This image illustrates the two-part process. First, you fetch the latest changes to the notebook from the remote Git repo. Then, after the latest version of the notebook has been synced, you start running the notebook as part of the job.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">When you create your job in the UI, make sure <strong>Type</strong> is set to <strong>Notebook</strong> and <strong>Source</strong> is set to <strong>Workspace</strong>. Select\u00a0<strong>Repos</strong> when choosing the notebook path.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1671606572927-create-sample-job.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Sample job creation with Type set to Notebook and Source set to Workspace.\"></p>\n</div>\n</div><h2 data-toc=\"true\" id=\"configure-secret-access-4\">Configure secret access</h2><h3 data-toc=\"true\" id=\"create-a-databricks-personal-access-token-5\">Create a Databricks personal access token</h3><p>Follow the <strong>Personal access tokens for users</strong> (<a href=\"https://docs.databricks.com/dev-tools/auth.html#personal-access-tokens-for-users\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Personal access tokens for users\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/auth#--azure-databricks-personal-access-tokens\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Azure Databricks personal access tokens\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/auth.html#personal-access-tokens-for-users\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Personal access tokens for users\">GCP</a>) documentation to create a personal access token.</p><h3 data-toc=\"true\" id=\"create-a-secret-scope-6\">Create a secret scope</h3><p>Follow the <strong>Create a Databricks-backed secret scope</strong> (<a href=\"https://docs.databricks.com/security/secrets/secret-scopes.html#create-a-databricks-backed-secret-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a Databricks-backed secret scope\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/security/secrets/secret-scopes#create-a-databricks-backed-secret-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a Databricks-backed secret scope\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/security/secrets/secret-scopes.html#create-a-databricks-backed-secret-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a Databricks-backed secret scope\">GCP</a>) documentation to create a secret scope.</p><h3 data-toc=\"true\" id=\"store-your-personal-access-token-and-your-databricks-instance-in-the-secret-scope-7\">Store your personal access token and your Databricks instance in the secret scope</h3><p>Follow the <strong>Create a secret in a Databricks-backed scope</strong> (<a href=\"https://docs.databricks.com/security/secrets/secrets.html#create-a-secret-in-a-databricks-backed-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a secret in a Databricks-backed scope\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/security/secrets/secrets#create-a-secret-in-a-databricks-backed-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a secret in a Databricks-backed scope\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/security/secrets/secrets.html#create-a-secret-in-a-databricks-backed-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a secret in a Databricks-backed scope\">GCP</a>) documentation to store the personal access token you created and your Databricks instance as new secrets within your secret scope.</p><p>Your Databricks instance is the hostname for your workspace, for example, xxxxx.cloud.databricks.com.</p><h2 data-toc=\"true\" id=\"use-a-script-to-sync-the-latest-changes-8\">Use a script to sync the latest changes\u00a0</h2><p>This sample Python code pulls the latest revision from the remote Git repo and syncs it with the local notebook. This ensures the local notebook is up-to-date before processing the job.</p><p id=\"isPasted\">You need to replace the following values in the script before running:</p><ul>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;repo-id&gt;</span> - The name of the remote Git repo.</li>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;scope-name&gt;</span> - The name of your scope that holds the secrets.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;secret-name-1&gt;</span> - The name of the secret that holds your Databricks instance.</li>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;secret-name-2&gt;</span> - The name of the secret that holds your personal access token.</li>\n</ul><p><br></p><pre class=\"language-python\">%python\r\n\r\nimport requests\r\nimport json\r\ndatabricks_instance = dbutils.secrets.get(scope = \"&lt;scope-name&gt;\", key = \"&lt;secret-name-1&gt;\")\r\ntoken = dbutils.secrets.get(scope = \"&lt;scope-name&gt;\", key = \"&lt;secret-name-2&gt;\")\r\n\r\nurl = f\"{databricks_instance}/api/2.0/repos/&lt;repo-id&gt;\"  # Use repos API to get repo id https://docs.databricks.com/dev-tools/api/latest/repos.html#operation/get-repos\r\npayload = json.dumps({\r\n    \"branch\": \"main\"  # use branch/tag. Refer https://docs.databricks.com/dev-tools/api/latest/repos.html#operation/update-repo\r\n})\r\nheaders = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\r\n\r\nresponse = requests.request(\"PATCH\", url, headers=headers, data=payload, timeout=60)\r\nprint(response.text)\r\nif response.status_code != 200:\r\n\u00a0\u00a0\u00a0\u00a0raise Exception(f\"Failure during fetch operation. Response code: {response}\")</pre>", "body_txt": "Problem You are running a job using notebooks that are stored in a remote Git repository (AWS | Azure | GCP). Databricks users with Can View permissions (who are not a workspace admin or owners of the job) cannot access or view the results of ephemeral jobs submitted via dbutils.notebook.run() from parent notebook. Cause When job visibility control (AWS | Azure | GCP) is enabled in the workspace, users can only see jobs permitted by their access control level. This works as expected with notebooks stored in the workspace. However, Databricks does not manage access control for your remote Git repo, so it does not know if there are any permission restrictions on notebooks stored in Git. The only Databricks user that definitely has permission to access notebooks in the remote Git repo is the job owner. As a result, other non-admin users are blocked from viewing, even if they have Can View permissions in Databricks. Solution You can work around this issue by configuring the job notebook source as your Databricks workspace and make the first task of your job fetch the latest changes from the remote Git repo. This allows for scenarios where the job has to ensure it is using the latest version of a notebook stored in a shared, remote Git repo. This image illustrates the two-part process. First, you fetch the latest changes to the notebook from the remote Git repo. Then, after the latest version of the notebook has been synced, you start running the notebook as part of the job. Info\nWhen you create your job in the UI, make sure Type is set to Notebook and Source is set to Workspace. Select\u00a0Repos when choosing the notebook path. Configure secret access Create a Databricks personal access token Follow the Personal access tokens for users (AWS | Azure | GCP) documentation to create a personal access token. Create a secret scope Follow the Create a Databricks-backed secret scope (AWS | Azure | GCP) documentation to create a secret scope. Store your personal access token and your Databricks instance in the secret scope Follow the Create a secret in a Databricks-backed scope (AWS | Azure | GCP) documentation to store the personal access token you created and your Databricks instance as new secrets within your secret scope. Your Databricks instance is the hostname for your workspace, for example, xxxxx.cloud.databricks.com. Use a script to sync the latest changes\u00a0 This sample Python code pulls the latest revision from the remote Git repo and syncs it with the local notebook. This ensures the local notebook is up-to-date before processing the job. You need to replace the following values in the script before running: &lt;repo-id&gt; - The name of the remote Git repo. &lt;scope-name&gt; - The name of your scope that holds the secrets. &lt;secret-name-1&gt; - The name of the secret that holds your Databricks instance. &lt;secret-name-2&gt; - The name of the secret that holds your personal access token. %python import requests import json databricks_instance = dbutils.secrets.get(scope = \"&lt;scope-name&gt;\", key = \"&lt;secret-name-1&gt;\") token = dbutils.secrets.get(scope = \"&lt;scope-name&gt;\", key = \"&lt;secret-name-2&gt;\") url = f\"{databricks_instance}/api/2.0/repos/&lt;repo-id&gt;\" # Use repos API to get repo id https://docs.databricks.com/dev-tools/api/latest/repos.html#operation/get-repos payload = json.dumps({ \"branch\": \"main\" # use branch/tag. Refer https://docs.databricks.com/dev-tools/api/latest/repos.html#operation/update-repo }) headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"} response = requests.request(\"PATCH\", url, headers=headers, data=payload, timeout=60) print(response.text) if response.status_code != 200: \u00a0\u00a0\u00a0\u00a0raise Exception(f\"Failure during fetch operation. Response code: {response}\")", "format": "html", "updated_at": "2023-03-07T02:42:16.225Z"}, "author": {"id": 789489, "email": "ravirahul.padmanabhan@databricks.com", "name": "ravirahul.padmanabhan ", "first_name": "ravirahul.padmanabhan", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:02:48.540Z", "updated_at": "2023-04-21T14:21:18.115Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 256850, "name": "Jobs", "codename": "jobs", "accessibility": 1, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2935173, "name": "aws"}, {"id": 2935174, "name": "azure"}, {"id": 2935175, "name": "gcp"}, {"id": 2935177, "name": "jobs"}, {"id": 2978809, "name": "remote"}, {"id": 2935176, "name": "repos"}, {"id": 2978808, "name": "sync"}], "url": "https://kb.databricks.com/jobs/users-unable-to-view-job-results-when-using-remote-git-source"}, {"id": 1590044, "name": "R commands fail on custom Docker cluster", "views": 2269, "accessibility": 1, "description": "R version 4.2.0 changed the way Renviron.site is initialized, so you must set an environment variable when using custom Docker clusters.", "codename": "r-commands-fail-on-custom-docker-cluster", "created_at": "2022-10-24T18:30:31.447Z", "updated_at": "2023-01-20T12:27:59.625Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.b9b7a9e9a8d43a5688e2afbdaa8771cd\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to run R notebooks on a <strong>custom Docker cluster</strong> (<a href=\"https://docs.databricks.com/clusters/custom-containers.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Customize containers with Databricks Container Services\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/custom-containers\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Customize containers with Databricks Container Services\">Azure</a>), but they immediately fail.</p><p>When you try to execute an R notebook, it returns an error saying the notebook was cancelled.</p><p>When you review the <strong>Cluster driver and worker logs</strong> (<a href=\"https://docs.databricks.com/clusters/clusters-manage.html#cluster-driver-and-worker-logs\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Cluster driver and worker logs\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/clusters-manage#--cluster-driver-and-worker-logs\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Cluster driver and worker logs\">Azure</a>) you see a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">there is no package called 'Rserve'</span> error.</p><pre id=\"isPasted\">Tue Aug 30 16:24:34 UTC 2022 Starting R processing from BASH\u00a0\r\n\u00a0\u00a0\r\nTue Aug 30 16:24:34 UTC 2022 R script: /local_disk0/tmp/_rServeScript.r6851825576782071270resource.r\u00a0\r\n\u00a0\u00a0\r\nTue Aug 30 16:24:34 UTC 2022 Port number: 1108\u00a0\r\n\u00a0\u00a0\r\nTue Aug 30 16:24:34 UTC 2022 cgroup: None\u00a0\r\n\u00a0\u00a0\r\n2022-08-30 16:24:34 R process started with pid 1462\u00a0\r\n\u00a0\u00a0\r\nError in loadNamespace(x) : there is no package called 'Rserve'\u00a0\r\n\u00a0\u00a0\r\nCalls: loadNamespace -&gt; withRestarts -&gt; withOneRestart -&gt; doWithOneRestart\u00a0\r\n\u00a0\u00a0\r\nExecution halted.\u00a0</pre><p><br>When you check for the Python libraries, they are all present.\u00a0</p><p>When you check the R version in a notebook, it returns the version information so you know R is installed.</p><pre id=\"isPasted\">%sh\r\n\r\nR --version</pre><pre>R version 4.2.0 (2022-04-22) -- \"Vigorous Calisthenics\"\u00a0\r\nCopyright (C) 2022 The R Foundation for Statistical Computing\u00a0\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\u00a0\r\n\u00a0\u00a0\r\nR is free software and comes with ABSOLUTELY NO WARRANTY.\u00a0\r\nYou are welcome to redistribute it under the terms of the\u00a0\r\nGNU General Public License versions 2 or 3.\u00a0\r\nFor more information about these matters see\u00a0\r\nhttps://www.gnu.org/licenses/.</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Databricks Runtimes use R version 4.1.3 by default. If you start a standard cluster from the <strong>Compute</strong> menu in the workspace and check the version, it returns R version 4.1.3.</p><p>When you build a custom cluster with Docker, it is possible to use a different R version. In the example used here, we see that the custom Docker cluster is running R version 4.2.0.</p><p>R version 4.2.0 changed the way <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Renviron.site</span> is initialized, which implicitly modifies the behavior of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">--vanilla</span>.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>If you want to use R version 4.2.0 on a custom Docker cluster with Databricks Runtime 11.3 and below, you must set the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DATABRICKS_ENABLE_RPROFILE=true</span> <strong>environment variable</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#environment-variables\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Environment variables\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#--environment-variables\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Environment variables\">Azure</a>) on the cluster.</p><p>If you want to use R version 4.2.0 on a custom Docker cluster with Databricks Runtime 12.0 and above, you can use <strong>R session customization</strong> (<a href=\"https://docs.databricks.com/sparkr/index.html#r-session-customization\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"R session customization\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sparkr/#r-session-customization\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"R session customization\">Azure</a>) to set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DATABRICKS_ENABLE_RPROFILE=true</span> in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.Rprofile</span> file.</p><p>For more information on installing R, please review the <strong>Install RStudio Server Open Source Edition</strong> (<a href=\"https://docs.databricks.com/sparkr/rstudio.html#install-rstudio-server-open-source-edition\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Install RStudio Server Open Source Edition\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sparkr/rstudio#install-rstudio-server-open-source-edition\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Install RStudio Server Open Source Edition\">Azure</a>) documentation.</p>", "body_txt": "Problem You are trying to run R notebooks on a custom Docker cluster (AWS | Azure), but they immediately fail. When you try to execute an R notebook, it returns an error saying the notebook was cancelled. When you review the Cluster driver and worker logs (AWS | Azure) you see a there is no package called 'Rserve' error. Tue Aug 30 16:24:34 UTC 2022 Starting R processing from BASH\u00a0 \u00a0\u00a0 Tue Aug 30 16:24:34 UTC 2022 R script: /local_disk0/tmp/_rServeScript.r6851825576782071270resource.r\u00a0 \u00a0\u00a0 Tue Aug 30 16:24:34 UTC 2022 Port number: 1108\u00a0 \u00a0\u00a0 Tue Aug 30 16:24:34 UTC 2022 cgroup: None\u00a0 \u00a0\u00a0 2022-08-30 16:24:34 R process started with pid 1462\u00a0 \u00a0\u00a0 Error in loadNamespace(x) : there is no package called 'Rserve'\u00a0 \u00a0\u00a0 Calls: loadNamespace -&gt; withRestarts -&gt; withOneRestart -&gt; doWithOneRestart\u00a0 \u00a0\u00a0 Execution halted.\u00a0 When you check for the Python libraries, they are all present.\u00a0 When you check the R version in a notebook, it returns the version information so you know R is installed. %sh R --version R version 4.2.0 (2022-04-22) -- \"Vigorous Calisthenics\"\u00a0 Copyright (C) 2022 The R Foundation for Statistical Computing\u00a0 Platform: x86_64-pc-linux-gnu (64-bit)\u00a0 \u00a0\u00a0 R is free software and comes with ABSOLUTELY NO WARRANTY.\u00a0 You are welcome to redistribute it under the terms of the\u00a0 GNU General Public License versions 2 or 3.\u00a0 For more information about these matters see\u00a0 https://www.gnu.org/licenses/. Cause Databricks Runtimes use R version 4.1.3 by default. If you start a standard cluster from the Compute menu in the workspace and check the version, it returns R version 4.1.3. When you build a custom cluster with Docker, it is possible to use a different R version. In the example used here, we see that the custom Docker cluster is running R version 4.2.0. R version 4.2.0 changed the way Renviron.site is initialized, which implicitly modifies the behavior of --vanilla. Solution If you want to use R version 4.2.0 on a custom Docker cluster with Databricks Runtime 11.3 and below, you must set the DATABRICKS_ENABLE_RPROFILE=true environment variable (AWS | Azure) on the cluster. If you want to use R version 4.2.0 on a custom Docker cluster with Databricks Runtime 12.0 and above, you can use R session customization (AWS | Azure) to set DATABRICKS_ENABLE_RPROFILE=true in the .Rprofile file. For more information on installing R, please review the Install RStudio Server Open Source Edition (AWS | Azure) documentation.", "format": "html", "updated_at": "2023-01-20T12:27:59.620Z"}, "author": {"id": 818781, "email": "atanu.sarkar@databricks.com", "name": "Atanu.Sarkar ", "first_name": "Atanu.Sarkar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-05T13:43:53.249Z", "updated_at": "2023-03-24T21:50:55.464Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2998841, "name": "aws"}, {"id": 2998842, "name": "azure"}, {"id": 2998843, "name": "docker"}, {"id": 2998845, "name": "rstudio"}, {"id": 2998844, "name": "r studio"}], "url": "https://kb.databricks.com/clusters/r-commands-fail-on-custom-docker-cluster"}, {"id": 1584863, "name": "Permission denied error when creating external location", "views": 2235, "accessibility": 1, "description": "You must be a metastore admin or have the CREATE EXTERNAL LOCATION privilege.", "codename": "permission-denied-error-when-creating-external-location", "created_at": "2022-10-20T16:55:45.393Z", "updated_at": "2023-01-11T20:56:41.696Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStUeEVoMmV3WW95aC9SNkxIVDZaTTgvSU9oU0FaSStBc0tqSjdoMDhqUm12ckRTMWNnCjNiZVBqQTU0R1hPSVFxU0VIZTZrSzFTdjlCK0JUbGdDckhjRGJLckIxOXF1aE4vVGN2eWl1WTIwMTVuQQpieU9hUVMzUzBJdDVuWkdoT2lmc1A0TmFJWVJhYVFpWFlBTk42NnJCOXZrWGJjd3B4RUlTaGpJZHFiTG8KQ0VZUUxERTJJQ1lNRDlveFE3a1dkVWNiMG0wZ0MxOUNpQnV3YkFJZHNhQURxYzdzbWtOZThmZFVlMGJ6ClFyM3F6NnhBMnBSc3F4MW1aQWRFT3ozZVFrK2dBQ1lIZEN1K2tHNVpCTUJYVnF1UEZEdDJaOEhmUEZmZgpOWmJld0IvdVV5L29GYnUvSHNqYzdhTlNERGd6Ti8vVXFuVStJY0J6WVUyamZ0Rm5HY1lxbUpKa2w3Z1YKWURHT2V5bDliQ0hQcHZpdXB6ZEV4MmZ6Z1BnNHVEaDVyM3c1L3hKWkx6RVllandOZ01FVjB4SHVmNHFjCm5wRC90ZHRRYkJOT3l3Ri9kMFRpQVV5OVdRaWMxRVI3QU05d0tSTDBSUGczNkNkbmdINUZjRHRFbnZqTwprbEdOYXZUL052RVcrQkp1eGVVRHQvL28xMEdFMHZJUGp5eTUzem5xUXB3V0ZaT0E5Z1dYVUZnb01iVzEKU1FQTndvMWV5ZlR6aWlDaEdpejU2L3k4aFh4RzUzUmdrK3BSUUc2NS9ZcEdoQmkzZkpyNWMwS3l6Mk1ZCk9zWnhSYllRSXBqYlVYYTJmajFUcFV5cGdjRmd6VG9xMWFDVFprcysrTnRkODNvelQwajNVcmN4TkhWSQpFYVlPT2hvVVRpQ25OWGR6MU0ySUVsME5SbkpSVEZuYWt1RHRWbUw4WTJOVE1ybmlSWi9hN2dwR1ZQVkQKTjQ5VXp5RGFZTTZNL2NLOG9aOVV2T1YxaWN3M1VyeExoYjhRZng5V05ORmRHZk96L0t4NzRYTElPR294CnJ5emtNRC8rZlZCSnk1Z3FkNmg0SkhaSjIxbm5wSGlRWmxtTTBtRVBuOTNvNVlMNGtNQzd1WTJBd0NoVQpyWFN6UlBacFVmWGRNS3pTUEYxSDJ0MWlWR1BpWjRqYXdvaHpiL09lNUdvbUNLWk40ZGdSRDFtY1hIUC8KUHFjT1BkVnVBV3Z5bzZ1WEpVSWhUVlROZTZTWEFSZUNHZ2craThmYmtHakgyUzhkQjY1NTg4U2dRNTROCkl6Mk52ejlZZTY0Rnc4bHhJei95WTBIejNVSHRIaGdSRjQvYXEzU0xlTm9mTHVVR2g5a1V1SVhyYVNMSwpxbVV1bmljbHNwaGU5ZGhwM0VPZS9BTnJtS09MK25pNEZWMEJGY004aDRiSUhUSFU5Ti9uRndaelZXVG4KUXRMVlpncUMrSGZzbkFWRmdxeC9NSWdGYTJGZHlZRGRrS0RSL0tuNWRhdG05cEdnVERCdTBmQTY2K1YwClduNmNOT3ZHU0I4eDJ0Zzh1VFVhV1hIVXNuOThJeUVUV3BBM1pOSDRFVEdlN2M2Qzdma3U5cnM5TzBIMQpnS0h3Y3BjaFdSTVp4YjNvZlBKWmFYY0JGbkwrTTE4dDlPWlNFMmlFT1VrTU13Z09LYVR6Sit5NnZmM1MKTExoNU82UlpsTlJyT3hGTFFadDMK.062d6191c835c141e631086424606a66\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>An external location is a storage location, such as an S3 bucket, on which external tables or managed tables can be created. A user or group with permission to use an external location can access any storage path within the external location without direct access to the storage credential. Review the <a href=\"https://docs.databricks.com/data-governance/unity-catalog/manage-external-locations-and-credentials.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Manage external locations and storage credentials</a> documentation for more information.</p><p>You are trying to create an external location when it fails due to a permission denied error message.</p><pre>PERMISSION_DENIED: User does not have CREATE EXTERNAL LOCATION on Metastore &lt;metastore_name&gt;</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>You do not have the necessary permissions to create an external location.</p><p>The user should be a metastore admin or he/she should have the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">CREATE EXTERNAL LOCATION</span> privilege in order to create external locations.\u00a0</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Ask a metastore admin to give you the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">CREATE EXTERNAL LOCATION</span> privilege on the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">METASTORE</span>.</p><ul><li>Show the existing permissions for the specified user on the metastore.</li></ul><pre>%sql\r\n\r\nSHOW GRANTS `&lt;user-name&gt;` on METASTORE;</pre><ul><li>Give the specified user <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">CREATE EXTERNAL LOCATION</span> permissions on the metastore.</li></ul><pre id=\"isPasted\">%sql\r\n\r\nGRANT CREATE EXTERNAL LOCATION ON METASTORE TO `&lt;user-name&gt;`;</pre><ul><li>Show all permissions for access to the metastore.</li></ul><pre id=\"isPasted\">%sql\u00a0\r\n\r\nSHOW GRANTS ON METASTORE;</pre><p><br></p>", "body_txt": "Problem An external location is a storage location, such as an S3 bucket, on which external tables or managed tables can be created. A user or group with permission to use an external location can access any storage path within the external location without direct access to the storage credential. Review the Manage external locations and storage credentials documentation for more information. You are trying to create an external location when it fails due to a permission denied error message. PERMISSION_DENIED: User does not have CREATE EXTERNAL LOCATION on Metastore &lt;metastore_name&gt; Cause You do not have the necessary permissions to create an external location. The user should be a metastore admin or he/she should have the CREATE EXTERNAL LOCATION privilege in order to create external locations.\u00a0 Solution Ask a metastore admin to give you the CREATE EXTERNAL LOCATION privilege on the METASTORE. Show the existing permissions for the specified user on the metastore. %sql SHOW GRANTS `&lt;user-name&gt;` on METASTORE; Give the specified user CREATE EXTERNAL LOCATION permissions on the metastore. %sql GRANT CREATE EXTERNAL LOCATION ON METASTORE TO `&lt;user-name&gt;`; Show all permissions for access to the metastore. %sql\u00a0 SHOW GRANTS ON METASTORE;", "format": "html", "updated_at": "2023-01-11T20:56:41.687Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 313834, "name": "Unity Catalog", "codename": "unity-catalog", "accessibility": 1, "description": "These articles can help you with Unity Catalog.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2991950, "name": "admin"}, {"id": 2991949, "name": "administrator"}, {"id": 2974654, "name": "aws"}, {"id": 2974655, "name": "metastore"}, {"id": 2974657, "name": "permissions"}, {"id": 2974660, "name": "s3"}, {"id": 2974658, "name": "storage"}, {"id": 2974656, "name": "unity catalog"}], "url": "https://kb.databricks.com/unity-catalog/permission-denied-error-when-creating-external-location"}, {"id": 1584859, "name": "Search audit logs for connections from prohibited IP addresses", "views": 2490, "accessibility": 1, "description": "Use audit logs to review and validate connection attempts to your workspace.", "codename": "search-audit-logs-for-connections-from-prohibited-ip-addresses", "created_at": "2022-10-20T16:52:08.965Z", "updated_at": "2023-01-20T13:11:56.749Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStTZjBvU0doSVBSMkdIWFJUMW5sbm1TNUE5eDRyakpHbG1jT1Y4NzhtM1pHNHlZOGRlCk5XWDlFWU5pSnhQMWxjcVBvV3Y4VDdmeENSU1o0bTFSVDBCYWV1akxqUHBNbk5nRlM5WUlHS29XNGVybQprVUYyMXgvUkhndHoxb2pUUnJNMllES1BXMWp6OXkwbGpUc2EvN1VtU0dCMXZielNSWC9GMEREQ0FPb3MKMjhndE5jdXlveUZZS3ROdktDZW05cmFLOGduZk1zRWdic1RUdXhxOG5aWllEOHVCMnVubzg4U0Rmb0ZLClBnMlp1cjhLMGd2c2dUMTZuUFd2ekdhSkx4eGFoZEhwd1dqQlBXTzAyMDFEWkRDVUFieEVwSndLNnlWRQoxQnRJV09FdlEwbkI4UGdacGlGUlRRU1pybjVYTzROUisrVUVsN1NJUEJaTlRoaDkyS2dER1ppNmF3T0MKU1ZCZDBGY1RiaExLaitiUnQwYmFIeXZ1QmpVSTFjQVJXRXZheGYxTDFHR0VkQjFYOGV1R2J4dmkyc0lVCkM0bisvQ3hxZWVRblZUTWJTL3UzRTFCQUt4ZGQzYjFwd0JFQ0FyNWR5QjBNaE5MWmlDRUN1bFlyTndrZwp0SFRXMm92MXJXb2dMZ2kxTzVzNFA0aU15Tys1NS8zREIrZXk3VGk3V1Bsek5kNi9kRDlvQ2JFZ1dZREYKemgvWXpmLzNMbXdoOXJpWG1qYTRXMk16SWxsL2hmaVllQmZUbEhXcjRRaUYzYk9mck5kemZncnB2aHhTCjRtMjBoeGZSNTVocjlpekxKYlAzZDl6R2NwUUUyUUxYYUs1YUF5VjJiT29JT3BDYkcrYjVGYjIvY3RZUwo4dWd2anB2d3A1cE5zYjRZbGZOREVrR0JpZkR4bHlnV2xsa1IxL1lScVp4SUNwN1VuTnoxb0duN3BOVWwKNkNIOXZIYVU0ajFmTWEvNVZKeWZGQTdZWDlrR3R6dUVKM2JRSlZDZE1rZEd2Q1hxR0tSbytxeURyMlF1CmRCN3ZWNW9aUWZuRFBsK08zeUlud01va0UzSlRuSkZZZ29NT01WMVp0RzAzRzl5VkRSR1hXQjk3NkZLbAprZVBoZ0wra2xuLzdnaFVUcUZqbEtnNEp0RERQeEtPQy9kMlFKOXhjNEh6K0NVT0R0aEF0MkR0WlUyb1EKbGVHMEN3M0FUMUpYU3lHM3lsdUdBZ3JiZWczc3MvL0U4KzBVQ1VCQURhdHk4ODd1Vm45Q2d0VnExZUI3CnhaMzFsd1VCaGdSTmVRV3UzN2tnWVMvVlE1aTJiaHZHaVV0M1JPQkM5Tk1NVEh3Qi9sV2FaYW5kZWF0bgpYeHFrZDViRkVReFh1S3ZpeStIbkpaelc2RkRkMjFPaXptYy9XYmJjY2JSemdXcU82R2gwRS9IbW5ydTgKV1NYWUo5dTJ6Z0RrU09nV3IzSUx5MTlLSDlZbjM4Y3gycnJPOFRuTEJVeDcxeGxwdGZCcFY1UHlFb00zCjRhYUI0QWh3eDNFVGk2a0hqM0g2VnY0VUt3M0ErTHdWZmZUZmlXTzZNV1RoaDYraXpWUFhMazdoUnk4SQpRNWlnNUFPSmh1MWV4WnhCZStjQzU5QkRZdXk0TXJsd3NQVVZlSmZNMGY3VUFFMk50Mk1yb1RLakVTRW0KUnREUCt0dlBOeUxOZjh6SEhZbmYwWU9XY1dZekM1ZnJlNmRuK0x2RHUrSHMzYmFUSmVBLzh6aUEzUGlVCjZ1K3pIeFJVdXNudmd4aTlWRHZ3TkVHTENTcTJQRHhtTW9nMmFDc0MwTndNZzk4RkQ1d1IzYWcxWVF6SApuUWpEdU90T3ZNVk41MGdvYStYWlJlTWV6b1VpSnAxcUVCYm1MeXFCclMvVGUxNFYxL1lobC8xYXA1QnAKKzRRVTI4RGlLMXd4eEVnYXUzU0E0OUtaeUJNb1k0b20K.f7b528af7a82245c1f894f7ce1b27ed0\"></div><p>IP access lists can be used to restrict access to Databricks based on known network locations. Once enabled, an IP access list requires uses to login from an allowed address. If a user attempts to login from any IP address not on the access list, the login is denied. Review the <a href=\"https://docs.databricks.com/security/network/ip-access-list.html\" rel=\"noopener noreferrer\" target=\"_blank\">IP access list</a> documentation for more details.</p><p>Best practices involve periodically reviewing the IP access logs to see if any login attempts were made from outside the permitted range. After you identify access attempts made from prohibited IP addresses, you can follow up with appropriate action. For example, if the attempts were made by a legitimate use, you may want to permit additional IP ranges in the access list. If the attempts were not made by legitimate users, you may want to review your security profile.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><p>Information on access attempts is stored in the Databricks audit logs. You can use Databricks notebooks to analyze the audit logs and track activities performed by users. This example shows you how to search the audit logs for times when someone tried to login to your workspace from a prohibited IP address.\u00a0</p><ol>\n<li id=\"isPasted\">Load the audit logs as a DataFrame and register the DataFrame as a temp table. You will need to enter the S3 <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;bucket-name&gt;</span> and the full <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;path-to-audit-logs&gt;</span>. Review the <a href=\"https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#configure-audit-log-delivery\" rel=\"noopener noreferrer\" target=\"_blank\">Configure audit log delivery</a> documentation for more information.<pre>%scala\r\n\r\nval df = spark.read.format(\"json\").load(\"s3a://&lt;bucket-name&gt;/&lt;path-to-audit-logs&gt;\")\r\ndf.createOrReplaceTempView(\"audit_logs\")</pre>\n</li>\n<li>Query the audit log based on the date range and accounts that attempted access from a prohibited IP address. You will need to enter the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;start-date&gt;</span> and the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;end-date&gt;</span>of the date range to search before running the sample code.<pre>%sql\r\n\r\nselect\r\n\u00a0 date,\r\n\u00a0 eventTime,\r\n\u00a0 orgId,\r\n\u00a0 sourceIPAddress,\r\n\u00a0 actionName,\r\n\u00a0 userAgent,\r\n\u00a0 get_json_object(rawMessage, '$.response.statusCode') StatusCode,\r\n\u00a0 get_json_object(rawMessage, '$.response.errorMessage') AS errorMessage\r\nfrom\r\n\u00a0 audit_logs\r\nwhere\r\n\u00a0 date &gt;= \"&lt;start-date&gt;\" #Date <span style=\"background-color: rgb(239, 239, 239);\">in yyyy-MM-dd format\r\n  and date &lt;= \"&lt;end-date&gt;\" #Date in yyyy-MM-dd format</span>\r\n\u00a0 and serviceName = \"accounts\"\r\n\u00a0 and actionName = \"IpAccessDenied\"\u00a0\r\norder by\r\n\u00a0 eventTime</pre>\n</li>\n<li>The results display all instances when a user was denied access based on their IP address.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1669849244341-1669849244341.png\" class=\"fr-fic fr-dib\">\n</li>\n<li>Take appropriate action based on the results and you use case.</li>\n</ol><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-1\">Info</h3>\n<p class=\"hj-alert-text\">You can modify this sample code to search based on other <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">serviceName</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">actionName</span> values as required.</p>\n</div>\n</div><p><br></p><p><br></p><p><br></p><p><br></p>", "body_txt": "IP access lists can be used to restrict access to Databricks based on known network locations. Once enabled, an IP access list requires uses to login from an allowed address. If a user attempts to login from any IP address not on the access list, the login is denied. Review the IP access list documentation for more details. Best practices involve periodically reviewing the IP access logs to see if any login attempts were made from outside the permitted range. After you identify access attempts made from prohibited IP addresses, you can follow up with appropriate action. For example, if the attempts were made by a legitimate use, you may want to permit additional IP ranges in the access list. If the attempts were not made by legitimate users, you may want to review your security profile. Instructions Information on access attempts is stored in the Databricks audit logs. You can use Databricks notebooks to analyze the audit logs and track activities performed by users. This example shows you how to search the audit logs for times when someone tried to login to your workspace from a prohibited IP address.\u00a0 Load the audit logs as a DataFrame and register the DataFrame as a temp table. You will need to enter the S3 &lt;bucket-name&gt; and the full &lt;path-to-audit-logs&gt;. Review the Configure audit log delivery documentation for more information.%scala val df = spark.read.format(\"json\").load(\"s3a://&lt;bucket-name&gt;/&lt;path-to-audit-logs&gt;\") df.createOrReplaceTempView(\"audit_logs\") Query the audit log based on the date range and accounts that attempted access from a prohibited IP address. You will need to enter the &lt;start-date&gt; and the &lt;end-date&gt;of the date range to search before running the sample code.%sql select \u00a0 date, \u00a0 eventTime, \u00a0 orgId, \u00a0 sourceIPAddress, \u00a0 actionName, \u00a0 userAgent, \u00a0 get_json_object(rawMessage, '$.response.statusCode') StatusCode, \u00a0 get_json_object(rawMessage, '$.response.errorMessage') AS errorMessage from \u00a0 audit_logs where \u00a0 date &gt;= \"&lt;start-date&gt;\" #Date in yyyy-MM-dd format and date &lt;= \"&lt;end-date&gt;\" #Date in yyyy-MM-dd format \u00a0 and serviceName = \"accounts\" \u00a0 and actionName = \"IpAccessDenied\"\u00a0 order by \u00a0 eventTime The results display all instances when a user was denied access based on their IP address. Take appropriate action based on the results and you use case. Info\nYou can modify this sample code to search based on other serviceName and actionName values as required.", "format": "html", "updated_at": "2023-01-20T13:11:56.746Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256865, "name": "Security and permissions", "codename": "security", "accessibility": 1, "description": "These articles can help you with access control lists (ACLs), secrets, and other security- and permissions-related functionality.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2998865, "name": "access"}, {"id": 2998863, "name": "audit"}, {"id": 2961291, "name": "aws"}, {"id": 2998864, "name": "logs"}, {"id": 2998866, "name": "security"}], "url": "https://kb.databricks.com/security/search-audit-logs-for-connections-from-prohibited-ip-addresses"}, {"id": 1584844, "name": "Reading a table fails due to AAD token timeout on ADLS Gen2", "views": 3854, "accessibility": 1, "description": "Accessing ADLS Gen2 storage fails if the AAD service principal token is expired or invalid.", "codename": "reading-a-table-fails-due-to-aad-token-timeout-on-adls-gen2", "created_at": "2022-10-20T16:48:50.107Z", "updated_at": "2022-11-30T20:31:27.011Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStwbGpNZlI4MFRQemFBUDJOZGlCYXp3UkVkTjdkeS9Nb3VwMzZGVFRMSTQxTmFyb2tqCkgreHJTQmxyNXhOSlNvMXRIbGFXcTVvQzAwWkRlNTdmNFlzS2ZSdmhkSFZwZXJtTWVmbElWYWl2ZS9NUgpGNTNCdXRwZlNNQTVRN3kxZHRGWjZtLzlla1VmbmdQdTE1VW5mc0ZHTU5rbnRlc25OR2hzVjNsQXdORnEKVDQwbnIzZjllTzUzMWFhK3haNDY4cWIvb0RCYWZXbmo3ckNHYmMvdWVlWVNDYWh0OFhRTWg2UXQ2Q2JKCk42Y3RoeW44eVRKSFFta3ZZUzhkOWdUekhtaEgxMnJQRk41R1g5NGFzdnZZTzRnUEFZQXVHV3NkVHNpbApVT0VhTzJ2N2E4d2RMLytPQ0dleHRyWkJ6aUhZd1NBQVZPOHIvVGk3bmZhbEJLaDl0cjBIYkpwTDdWdnoKQWU2RWY3VzBBYk1CM0Q4d3lob0dkYzdkb00rMTcyTGIzUkxTQXBlRThNTFYrMzlCS0c3dG41SWRCV01ZCkF3MWhmK3VCVExId0tSNlIxcURKWUVIdkdmQUZXTHdxQy9acjJpN0xaVmVTM1dUUzBqcEpaVERzN054aAoxWkJ2bDFxQ1RMNG9peHJESjJwY0tCdXBKY3loZGUxNEw3VllRSWFtV0dXdjhXbmk2NGZEUURyU0NkeUoKN2cxTVpBd1AwZ3Z4LzZZejRBMUwrWEkybTMrK1VsQXQvbWY3OG1lc2puWGQrNnBWaTZSUlZBTEdJUFpPCnpXYnloVjlRdCtWWnM5eHNqTnZXcEtKaGt4UWZ5aFFaWEVKU09PYkJhWkNhQ2huZVZ1VmZsZDdYWnQyQQpDQUliQUVOejEvbFdTMG5xVkpRaFlldWM2Y0NEMVRSQzZhY3Zib0prZ0pDdm5YWWpqR3gzQ2lNLzJKdkIKS3pvZjdzbWtjejI5L0dvY1RySnRVTEVlem9EVnBFUXVRQzg5Z0N0MlphQUNHVEJLQVlTNk1TOEdTT0d6CmxhTEFyYXdaMVZiY2kxSTBDT2ppTXZmTEUyZ0luWlE0bUcvM3d3S2tsUitPbU04bHhlbDBBWU9hMmx5VQpQQ2FDR0RFZVVmNzZsWW1pTGxkWUs2NzVBeTE0UlVoekpPdnNERHdPUGFnSmhmMTY5aTROMThzWCtmbnYKdGU3NVhiSjdiZ1huejRXd2lkSngvc2cyWDlCK2gwVTVpaGFKeGtjWkJSZGVHdVVmQ0VYVWVwTXlmb3pICmJ3OHRFTEdTcUJRK1VLY0RDSWhjdXhUMWdpc0pCaytScHVhQmMvZWkwMGJvS1BSMXY3bkEyY2lYVkRjTAp3SjZkWlVxV1NVOG9BZ2NHYjdMa0VBMytISE52dzVBc3hiVCtYejR4TSsrdTBLbHFHNXFmUHNaZllldUwKNm9LWlZxUS9uclB5STJmKzIyYWhadUhPc2o5Z0R0VlpRaXVCR1lUTXNTcnJWc0JtamdqekpUYS8zZ1o4CjNDZTRlUWR4UGh0dUFzNzZNUTJEZzZLNnhkQlN1MVM5L3FRUzg0WExoaU8rZlFzZTh1eE1udncyMlNsdwpNWFhFWnlsREtmNEJDeDFNRXFWZnhxdHY2Mi9SVnFDWm15eDNKeDV0aGM3RjBHSVBGaitDQzJoRXZMRUsKUUFlRWRGdkVtM2ZVb2h2a0FhWFoK.60e175641fbd1f63120f5a7ff6672f52\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Access to ADLS Gen2 storage can be configured using OAuth 2.0 with an Azure service principal. You can securely access data in an Azure storage account using OAuth 2.0 with an Azure Active Directory (Azure AD) application service principal for authentication.</p><p>You are trying to access external tables (tables stored outside of the root storage location) which are stored on ADLS Gen2. Access fails with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ADLException</span> error and an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IOException : AADToken</span> timeout error.</p><pre id=\"isPasted\">WARN DeltaLog: Failed to parse dbfs:/mnt/&lt;table path in ADLS Gen2 storage container&gt;. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\r\ncom.microsoft.azure.datalake.store.ADLException: Error getting info for file &lt;table path in ADLS Gen2 storage container&gt;\r\nError fetching access tokenOperation null failed with exception java.io.IOException : AADToken: HTTP connection failed for getting token from AzureAD due to timeout. Client Request Id :&lt;directory-id&gt; Latency(ns) : 180152012\r\nLast encountered exception thrown after 5 tries. [java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException]\r\n[ServerRequestId:null]\r\nCaused by: java.io.IOException: Server returned HTTP response code: 401 for URL: <a data-fr-linked=\"true\" href=\"https://login.microsoftonline.com/&lt;\">https://login.microsoftonline.com/&amp;lt</a>;directory-id&gt;/oauth2/token \u00a0\r\nat sun.reflect.GeneratedConstructorAccessor118.newInstance(Unknown Source) \u00a0at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nCaused by: java.io.IOException: Server returned HTTP response code: 401 for URL: <a data-fr-linked=\"true\" href=\"https://login.microsoftonline.com/&lt;\">https://login.microsoftonline.com/&amp;lt</a>;directory-id&gt;/oauth2/token\r\n\u00a0 at sun.reflect.GeneratedConstructorAccessor118.newInstance(Unknown Source)\r\n\u00a0 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Access to ADLS Gen2 storage fails if the client secret token associated with the Azure Active Directory (Azure AD) application service principal is expired or invalid.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Review the storage account access setup and verify that the client secret is expired. Create a new client secret token and then remount the ADLS Gen2 storage container using the new secret, or update the client secret token with the new secret in the ADLS Gen2 storage account configuration.</p><h2 data-toc=\"true\" id=\"review-existing-storage-account-secrets-3\">Review existing storage account secrets</h2><p>Check to see if the existing client secret is expired.</p><ol>\n<li>Open the <a href=\"https://portal.azure.com/#home\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Azure portal\">Azure portal</a>.</li>\n<li>Click <strong>Azure Active Directory</strong>.</li>\n<li>In the menu on the left, look under <strong>Manage</strong> and click <strong>App registrations.</strong>\n</li>\n<li>On the all applications tab, locate the application created for Azure Databricks. You can search the app registrations by Display name or by Application (client) ID.</li>\n<li>Click on your application. \u00a0 <a aria-controls=\"s1\" aria-expanded=\"false\" data-mc-state=\"closed\" data-mc-targets=\"s1\" href=\"https://help.campusmanagement.com/Renee/Content/SecretExpired.htm#\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1667833776458-1667833776458.png\" alt=\"Closed\" width=\"40\" height=\"16\" class=\"fr-fic fr-dii\"></a>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0</li>\n<li>In the menu on the left, look under <strong>Manage\u00a0</strong>and click <strong>Certificates &amp; secrets</strong>.</li>\n<li>Review the <strong>Client secrets</strong> section and check the date in the <strong>Expires\u00a0</strong>column.</li>\n</ol><h2 data-toc=\"true\" id=\"create-a-new-secret-token-4\">Create a new secret token</h2><p>If the existing client secret is expired, you must create a new token.</p><ol>\n<li>Click <strong>New client secret</strong>.</li>\n<li>Enter a description and a duration for the secret.</li>\n<li>Click <strong>Add</strong>.</li>\n<li>The client secret is displayed. Copy the <strong>Value</strong>. It cannot be retrieved after you leave the page.</li>\n</ol><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-5\">Warning</h3>\n<p class=\"hj-alert-text\">If you forget to copy the secret <strong>Value</strong>, you must repeat these steps. The <strong>Value</strong> cannot be retrieved once you leave the page. Returning to the page displays a masked version of the <strong>Value</strong>.</p>\n</div>\n</div><h2 data-toc=\"true\" id=\"remount-adls-gen2-storage-with-new-secret-6\">Remount ADLS Gen2 storage with new secret</h2><p>Once you have generated a new client secret, you can unmount the existing ADLS Gen2 storage, update the secret information, and then remount the storage.</p><ol>\n<li>\n<p>Unmount the existing mount point.</p>\n<pre>%python\r\n\r\ndbutils.fs.unmount(\"/mnt/&lt;mount-name&gt;\")</pre>\n<p>Review the <a href=\"https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-utils#--unmount-command-dbutilsfsunmount\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"dbutils.fs.unmount\">dbutils.fs.unmount</a> documentation for more information.</p>\n</li>\n<li><p>Remount the storage account with the new client secret.</p></li>\n<li>\n<p id=\"isPasted\">Replace</p>\n<ul>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;application-id&gt;</span> with the <strong>Application (client) ID</strong> for the Azure Active Directory application</li>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;container-name&gt;</span> with the name of the container</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;directory-id&gt;</span> with the <strong>Directory (tenant) ID</strong> for the Azure Active Directory application</li>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;scope-name&gt;</span> with the Databricks secret scope name</li>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;service-credential-key&gt;</span> with the name of the key containing the client secret</li>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;storage-account&gt;\u00a0</span>with the name of the Azure storage account</li>\n</ul>\n<p><br></p>\n<pre>%python\r\n\r\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\r\n          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\r\n          \"fs.azure.account.oauth2.client.id\": \"&lt;application-id&gt;\",\r\n          \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"&lt;scope-name&gt;\",key=\"&lt;service-credential-key&gt;\"),\r\n          \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token\"}\r\n\r\n# Optionally, you can add &lt;directory-path&gt; to the source URI of your mount point.\r\ndbutils.fs.mount(\r\n  source = \"abfss://&lt;container-name&gt;@&lt;storage-account&gt;.dfs.core.windows.net/&lt;directory-path&gt;\",\r\n  mount_point = \"/mnt/&lt;mount-name&gt;\",\r\n\u00a0 extra_configs = configs)</pre>\n<p>Review the <a href=\"https://learn.microsoft.com/en-gb/azure/databricks/dbfs/mounts\" rel=\"noopener noreferrer\" target=\"_blank\">mount an Azure Blob storage container</a> documentation for more information.</p>\n</li>\n</ol><h2 data-toc=\"true\" id=\"replace-client-secret-in-the-storage-account-config-7\" tabindex=\"0\">Replace client secret in the storage account config</h2><p>As an alternative to updating individual mounts, you can replace the client secret in the storage account authentication configuration. The storage account must be set up for direct access.\u00a0</p><pre>%python\r\n\r\nspark.conf.set(\"fs.azure.account.auth.type.&lt;storage-account&gt;.dfs.core.windows.net\", \"OAuth\")\r\nspark.conf.set(\"fs.azure.account.oauth.provider.type.&lt;storage-account&gt;.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\r\nspark.conf.set(\"fs.azure.account.oauth2.client.id.&lt;storage-account&gt;.dfs.core.windows.net\", \"&lt;application-id&gt;\")\r\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.&lt;storage-account&gt;.dfs.core.windows.net\", \"&lt;service_credential_key_name&gt;\")\r\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.&lt;storage-account&gt;.dfs.core.windows.net\", \"https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token\")</pre><p id=\"isPasted\">Replace</p><ul>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;application-id&gt;</span> with the <strong>Application (client) ID</strong> for the Azure Active Directory application</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;directory-id&gt;</span> with the <strong>Directory (tenant) ID</strong> for the Azure Active Directory application</li>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;service-credential-key&gt;</span> with the name of the key containing the client secret</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;storage-account&gt;</span> with the name of the Azure storage account</li>\n</ul><p>Review the <a href=\"https://learn.microsoft.com/en-us/azure/databricks/external-data/azure-storage\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"access ADLS Gen2\">access ADLS Gen2</a> documentation for more information.</p>", "body_txt": "Problem Access to ADLS Gen2 storage can be configured using OAuth 2.0 with an Azure service principal. You can securely access data in an Azure storage account using OAuth 2.0 with an Azure Active Directory (Azure AD) application service principal for authentication. You are trying to access external tables (tables stored outside of the root storage location) which are stored on ADLS Gen2. Access fails with an ADLException error and an IOException : AADToken timeout error. WARN DeltaLog: Failed to parse dbfs:/mnt/&lt;table path in ADLS Gen2 storage container&gt;. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again. com.microsoft.azure.datalake.store.ADLException: Error getting info for file &lt;table path in ADLS Gen2 storage container&gt; Error fetching access tokenOperation null failed with exception java.io.IOException : AADToken: HTTP connection failed for getting token from AzureAD due to timeout. Client Request Id :&lt;directory-id&gt; Latency(ns) : 180152012 Last encountered exception thrown after 5 tries. [java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException] [ServerRequestId:null] Caused by: java.io.IOException: Server returned HTTP response code: 401 for URL: https://login.microsoftonline.com/&amp;lt;directory-id&gt;/oauth2/token \u00a0 at sun.reflect.GeneratedConstructorAccessor118.newInstance(Unknown Source) \u00a0at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) Caused by: java.io.IOException: Server returned HTTP response code: 401 for URL: https://login.microsoftonline.com/&amp;lt;directory-id&gt;/oauth2/token \u00a0 at sun.reflect.GeneratedConstructorAccessor118.newInstance(Unknown Source) \u00a0 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) Cause Access to ADLS Gen2 storage fails if the client secret token associated with the Azure Active Directory (Azure AD) application service principal is expired or invalid. Solution Review the storage account access setup and verify that the client secret is expired. Create a new client secret token and then remount the ADLS Gen2 storage container using the new secret, or update the client secret token with the new secret in the ADLS Gen2 storage account configuration. Review existing storage account secrets Check to see if the existing client secret is expired. Open the Azure portal.\nClick Azure Active Directory.\nIn the menu on the left, look under Manage and click App registrations. On the all applications tab, locate the application created for Azure Databricks. You can search the app registrations by Display name or by Application (client) ID.\nClick on your application. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\nIn the menu on the left, look under Manage\u00a0and click Certificates &amp; secrets.\nReview the Client secrets section and check the date in the Expires\u00a0column. Create a new secret token If the existing client secret is expired, you must create a new token. Click New client secret.\nEnter a description and a duration for the secret.\nClick Add.\nThe client secret is displayed. Copy the Value. It cannot be retrieved after you leave the page. Warning\nIf you forget to copy the secret Value, you must repeat these steps. The Value cannot be retrieved once you leave the page. Returning to the page displays a masked version of the Value. Remount ADLS Gen2 storage with new secret Once you have generated a new client secret, you can unmount the existing ADLS Gen2 storage, update the secret information, and then remount the storage. Unmount the existing mount point.\n%python dbutils.fs.unmount(\"/mnt/&lt;mount-name&gt;\")\nReview the dbutils.fs.unmount documentation for more information. Remount the storage account with the new client secret. Replace &lt;application-id&gt; with the Application (client) ID for the Azure Active Directory application &lt;container-name&gt; with the name of the container &lt;directory-id&gt; with the Directory (tenant) ID for the Azure Active Directory application &lt;scope-name&gt; with the Databricks secret scope name &lt;service-credential-key&gt; with the name of the key containing the client secret &lt;storage-account&gt;\u00a0with the name of the Azure storage account %python configs = {\"fs.azure.account.auth.type\": \"OAuth\", \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\", \"fs.azure.account.oauth2.client.id\": \"&lt;application-id&gt;\", \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"&lt;scope-name&gt;\",key=\"&lt;service-credential-key&gt;\"), \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token\"} # Optionally, you can add &lt;directory-path&gt; to the source URI of your mount point. dbutils.fs.mount( source = \"abfss://&lt;container-name&gt;@&lt;storage-account&gt;.dfs.core.windows.net/&lt;directory-path&gt;\", mount_point = \"/mnt/&lt;mount-name&gt;\", \u00a0 extra_configs = configs)\nReview the mount an Azure Blob storage container documentation for more information. Replace client secret in the storage account config As an alternative to updating individual mounts, you can replace the client secret in the storage account authentication configuration. The storage account must be set up for direct access.\u00a0 %python spark.conf.set(\"fs.azure.account.auth.type.&lt;storage-account&gt;.dfs.core.windows.net\", \"OAuth\") spark.conf.set(\"fs.azure.account.oauth.provider.type.&lt;storage-account&gt;.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") spark.conf.set(\"fs.azure.account.oauth2.client.id.&lt;storage-account&gt;.dfs.core.windows.net\", \"&lt;application-id&gt;\") spark.conf.set(\"fs.azure.account.oauth2.client.secret.&lt;storage-account&gt;.dfs.core.windows.net\", \"&lt;service_credential_key_name&gt;\") spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.&lt;storage-account&gt;.dfs.core.windows.net\", \"https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token\") Replace &lt;application-id&gt; with the Application (client) ID for the Azure Active Directory application &lt;directory-id&gt; with the Directory (tenant) ID for the Azure Active Directory application &lt;service-credential-key&gt; with the name of the key containing the client secret &lt;storage-account&gt; with the name of the Azure storage account Review the access ADLS Gen2 documentation for more information.", "format": "html", "updated_at": "2022-11-30T20:31:26.988Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2955796, "name": "adls gen2"}, {"id": 2932994, "name": "azure"}, {"id": 2955798, "name": "security"}, {"id": 2955797, "name": "token"}], "url": "https://kb.databricks.com/data-sources/reading-a-table-fails-due-to-aad-token-timeout-on-adls-gen2"}, {"id": 1555295, "name": "Error when creating a user, group, or service principal at the account level with Terraform", "views": 7399, "accessibility": 1, "description": "You must include your account_id in the Terraform Databricks provider block to manage users, groups, and service principals.", "codename": "error-when-creating-a-user-group-or-service-principal-at-the-account-level-with-terraform", "created_at": "2022-10-07T23:24:27.302Z", "updated_at": "2022-10-28T13:22:18.952Z", "next_expiration_on": null, "published": true, "answer": {"body": "<p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS95dnFSaUJxMldId1BNOGVlV0pybkhyV2ZVVTRweWdheXowUXM4S0dFUEtmUTBvSzRPCmRwa0Z1K1RFOUcraG02WmVqTHozRzdENmFMNEhsbTNjckg5UmgyV0VvcjVqdHg2cmdiNGlFNkxoZS9jOApDYm9QL0piOXBZdWhWM0pVSlRaK0xUQTlJNW5qM3IxYXVLU2p4a29WVkN1TU1wRXJpaG5STHhTTzRkbWkKdTRyM3cwVHJVQzdTVytDSm9oLzhLZ09YN2ZmakozTG8xUXgvS0ROY2d0bWZIMENYanNPZVo1Rm5YZ21aCi94bUtDK0RadEhFL0g2ZDg4L2RRejRSWEppUTBmbzc2TTFibnBCVy8xczYrd0c4UW1hL2Q3TUx5RE9KNgpqRktjM1c1OTVXcEpJZU1MZU9oU3ZmNDg3V0JyK3FIdktnRVN4NTZSajRnVEc5ZW9odHRMcGtnTmxWOTMKVkYyUEc5ZGFxM2dmbWxETWoyS0ZqNWViL1Uwbk1VM2lrb2llZXVUUDd3Y2gvbm16emNqbk1LdEJiREJsCjlkWkcwTGtHb0hqemIwaC9Mamc5VncwcnVteUo1cStnSVRmWDVCQkYzaDZNZHBmbzlsYmJvbEUyMDJ1SQpzQ1hEeGgvREJNdTM0L21qcEd3T1dqalRXZEhNeDZlK2ZPUjhManlVTFNqVndqMzBpTFRCN1QwQVVpS2cKZ3ZHYmJ5NVkwSWszQjZTUFlZN3lTRWZKVmZ6WTNJczB5bG5BR085eXFHcFpwOWw0R3QxeDVySnVvWGFWCkltV0drb3hka2xhRVg2eGVXdzFUSHk1RmhVN1BpZ0dnVnI4MG1ZSzZLZjNvNU4vdG1aR0NHUFlLVi9TeApFQmRXMzJ2QVpQcnhUVFpTM0hicHBtdkt1Ri91UFQ1SmtCc3h1eHU0RnRwajlsOFB4bWNWbHVsN3VOOU8KeVNpeU1rWmdINEhkbnVySW9NcDhEb3BSR1QyNHVmTFlDZHJEcURnSXFTdEt5bUIxWHRhMTgwZEF4MXNYClloRXVubWZuc2M2dmlCeTVjY0R6TEZHakhBTUxqSlZTaWVXYWFFU1pFZVdJbmVod3FTZ3I4eFpiSHc5KwpiOUlsTnRVdkd3NUovUHZ4ZzlId3ZmaEtjL2xRUUtSeVBDQ1VNZVpCL0xXR3A0MlZGNmFETmNBNXZ6UDMKWG9TWW1hQ0FVTmM4eTRLL3NBTU9rOGFkdWtqaHhjMUhCeWpDWU5aZlpjY2tpdXNqR3BVQjNHL0pWaFk3CklLWDBUbFFsTXBMTzVXSzR1amJ2SndkUkJDYkwrbW5oYkRPR050dmNpUVBoWGNFcTBEY05oRnZoY0k4cgpXd1JLYkhXSlAwZjcrazVvcDNaSGxxMXhIbXh4dWcyOEJnWE0xMHhRN3F6SElyL2t5eS92bDBnZTVzRjQKZHZ6WnZvZUkK.03b86a5ec81256938143c2a07cc8c1ba\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Unity Catalog uses Databricks account identities to resolve users, service principals, and groups, and to enforce permissions. These identities can be managed using Terraform.</p><p>You are trying to create users, service principals, or groups at the account level when your Terraform code fails with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">set `host` property</span> error message.</p><pre id=\"isPasted\">2022-10-06T15:20:46.816+0300 [INFO] \u00a0Starting apply for databricks_group.bfr_databricks_groups\r\n2022-10-06T15:20:46.817+0300 [DEBUG] databricks_group.bfr_databricks_groups: applying the planned Create change\r\n2022-10-06T15:20:46.818+0300 [INFO] \u00a0provider.terraform-provider-databricks_v1.4.0: Using directly configured basic authentication: timestamp=2022-10-06T15:20:46.817+0300\r\n2022-10-06T15:20:46.818+0300 [INFO] \u00a0provider.terraform-provider-databricks_v1.4.0: Configured basic auth: host=<a data-fr-linked=\"true\" href=\"https://accounts.cloud.databricks.com\">https://accounts.cloud.databricks.com</a>, username=, password=***REDACTED***: timestamp=2022-10-06T15:20:46.818+0300\r\n2022-10-06T15:20:46.818+0300 [DEBUG] provider.terraform-provider-databricks_v1.4.0: POST /api/2.0/preview/scim/v2/Groups {\r\n\u00a0 \"displayName\": \"test\",\r\n\u00a0 \"schemas\": [\r\n\u00a0 \u00a0 \"urn:ietf:params:scim:schemas:core:2.0:Group\"\r\n\u00a0 ]\r\n}: timestamp=2022-10-06T15:20:46.818+0300\r\n2022-10-06T15:20:48.283+0300 [DEBUG] provider.terraform-provider-databricks_v1.4.0: 405 Method Not Allowed [non-JSON document of 334 bytes]: timestamp=2022-10-06T15:20:48.283+0300\r\n2022-10-06T15:20:48.283+0300 [WARN] \u00a0provider.terraform-provider-databricks_v1.4.0: /api/2.0/preview/scim/v2/Groups:405 - Databricks API (/api/2.0/preview/scim/v2/Groups) requires you to set `host` property (or DATABRICKS_HOST env variable) to result of `databricks_mws_workspaces.this.workspace_url`. This error may happen if you're using provider in both normal and multiworkspace mode. Please refactor your code into different modules. Runnable example that we use for integration testing can be found in this repository at <a data-fr-linked=\"true\" href=\"https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/aws-workspace\">https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/aws-workspace</a>: timestamp=2022-10-06T15:20:48.283+0300</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">account_id</span> parameter is missing in the Terraform Databricks provider block.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You must add your Databricks <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">account_id</span> to the Terraform Databricks provider block.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">You must be an admin user to get your Databricks account ID.</p>\n<ol>\n<li class=\"hj-alert-text\">Login to the Databricks account console (<a href=\"https://accounts.cloud.databricks.com/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"account console\">AWS</a> | <a href=\"https://accounts.azuredatabricks.net/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"account console\">Azure</a>).</li>\n<li class=\"hj-alert-text\">Click <strong>User Profile</strong>.</li>\n<li class=\"hj-alert-text\">Look for the <strong>Account ID</strong> value in the pop-up.</li>\n</ol>\n</div>\n</div><p><br></p><pre id=\"isPasted\">// initialize provider at account-level\r\nprovider \"databricks\" {\r\n  alias      = \"mws\"\r\n  host       = \"https://accounts.cloud.databricks.com\"\r\n  account_id = \"&lt;databricks-account-id&gt;\r\n  username   = var.databricks_account_username\r\n  password   = var.databricks_account_password\r\n}</pre><p><br>Please review the Terraform <a href=\"https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/group\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">databricks_group Resource</a> documentation for more details.</p>", "body_txt": "Problem Unity Catalog uses Databricks account identities to resolve users, service principals, and groups, and to enforce permissions. These identities can be managed using Terraform. You are trying to create users, service principals, or groups at the account level when your Terraform code fails with a set `host` property error message. 2022-10-06T15:20:46.816+0300 [INFO] \u00a0Starting apply for databricks_group.bfr_databricks_groups 2022-10-06T15:20:46.817+0300 [DEBUG] databricks_group.bfr_databricks_groups: applying the planned Create change 2022-10-06T15:20:46.818+0300 [INFO] \u00a0provider.terraform-provider-databricks_v1.4.0: Using directly configured basic authentication: timestamp=2022-10-06T15:20:46.817+0300 2022-10-06T15:20:46.818+0300 [INFO] \u00a0provider.terraform-provider-databricks_v1.4.0: Configured basic auth: host=https://accounts.cloud.databricks.com, username=, password=***REDACTED***: timestamp=2022-10-06T15:20:46.818+0300 2022-10-06T15:20:46.818+0300 [DEBUG] provider.terraform-provider-databricks_v1.4.0: POST /api/2.0/preview/scim/v2/Groups { \u00a0 \"displayName\": \"test\", \u00a0 \"schemas\": [ \u00a0 \u00a0 \"urn:ietf:params:scim:schemas:core:2.0:Group\" \u00a0 ] }: timestamp=2022-10-06T15:20:46.818+0300 2022-10-06T15:20:48.283+0300 [DEBUG] provider.terraform-provider-databricks_v1.4.0: 405 Method Not Allowed [non-JSON document of 334 bytes]: timestamp=2022-10-06T15:20:48.283+0300 2022-10-06T15:20:48.283+0300 [WARN] \u00a0provider.terraform-provider-databricks_v1.4.0: /api/2.0/preview/scim/v2/Groups:405 - Databricks API (/api/2.0/preview/scim/v2/Groups) requires you to set `host` property (or DATABRICKS_HOST env variable) to result of `databricks_mws_workspaces.this.workspace_url`. This error may happen if you're using provider in both normal and multiworkspace mode. Please refactor your code into different modules. Runnable example that we use for integration testing can be found in this repository at https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/aws-workspace: timestamp=2022-10-06T15:20:48.283+0300 Cause The account_id parameter is missing in the Terraform Databricks provider block. Solution You must add your Databricks account_id to the Terraform Databricks provider block. Info\nYou must be an admin user to get your Databricks account ID. Login to the Databricks account console (AWS | Azure).\nClick User Profile.\nLook for the Account ID value in the pop-up. // initialize provider at account-level provider \"databricks\" { alias = \"mws\" host = \"https://accounts.cloud.databricks.com\" account_id = \"&lt;databricks-account-id&gt; username = var.databricks_account_username password = var.databricks_account_password } Please review the Terraform databricks_group Resource documentation for more details.", "format": "html", "updated_at": "2022-10-28T13:22:18.944Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 313835, "name": "Terraform", "codename": "terraform", "accessibility": 1, "description": "These articles can help you with Terraform.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2921646, "name": "aws"}, {"id": 2921647, "name": "azure"}, {"id": 2924759, "name": "group"}, {"id": 2924762, "name": "service principal"}, {"id": 2924761, "name": "user"}], "url": "https://kb.databricks.com/terraform/error-when-creating-a-user-group-or-service-principal-at-the-account-level-with-terraform"}, {"id": 1551335, "name": "Set an unlimited lifetime for service principal access token", "views": 4605, "accessibility": 1, "description": "Configure an extended or unlimited lifetime for a service principal access token.", "codename": "set-an-unlimited-lifetime-for-service-principal-access-token", "created_at": "2022-10-04T17:00:03.652Z", "updated_at": "2023-04-11T14:19:27.509Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTljdnBUQ1VhWU0yUnlqR0hNUmRFZ0VKWnJIYVNiUXIvZ3JVOVRVWUhFL1VZcnZCT2ZPCnhSSFhML1kvZWExR2NPcDBFYVpCaVBYZldkUzJ6Z2ZGZGtSSUZoZ0ZhcExwT1diR1VDVEZlVWVjelZMeApxakU2dmJyTmxKTjB3b1AvYnN1TmhuSGw3NHYrOXZCQlJDWmVkUWhXKzVPcVZ4NFA5aHR6SGdLRGJyRCsKbUljK1N4dC9QSWlHMjk5cEl1bnI5elNsMlNqQmcvODVBK0RuNDdXQzU2Ukl4VENRWnBIRVNsMUdXYVB5CldQQVhyVTl1WGRYMTk0QnlhWmVvSStscWR5c3kzTEwrU0o3cjd0eU1rQTAzdVlnSFV5QlRYNTdqQnZTOApGSGpqYTFLd3NtN21KeGFOeWwrOEdqaVh2Z0t5bGdzNGFLYVV3VFMvd1NoS1ZKWGhvUWRVUVo3TVlFNVkKWkR6bng0RkN0M1VNakRHMDdQWitCd0huNVlDTk1QanRsVWRsNUVnMUhOdFVVRzZKcVIrK0dLUHc5cnpvCnNMbDQ0YVZmK1pnTDJlM2VMVTRSdzJZKytSV2RRREhuUk5RMytGdmdUV2xPWWdnMXBpYVh0dURyVDBYYgpjbHRtNm9ZSDI5RFRLT1Z3ZHNCRGNrbGhIckRDQzVuNUpRYjBxQXVNcis5cTAzZWpXY0dodFVrcUJZVHIKL1ZONm5hWHpIM3AzY2JWWXZPeG4zYlAvSW1hWHNuZ1VvaytOMWtNTHJDdnhYdTJLOWVuUFVtdVZVTmVCCmI4SUdZME0rTXhMVDZJSW1RT1RTUWM0cGExcHh3YW1ESERaZUZZQncydEloanBGSUdlTkd3WkdJcThIUQplOC93RTNsYWlNT1VteHlYSU13VWY1bkNzYmVFRlZpeXo2anBGOXlWbVp6SUltdTFlWEV6dVhnb3NTRkgKWHBaYWtBVGlWV2tvek1ya1p2R3VKbWxsc2FEY2FGRTVEL3FET2poVnRsTVFLcXE5aXAxMlBqY3BNZGg2CnFRdDN2V2RKNm1EbkJEemRhSVd3SEJuL2JXZlZWRk5KeUZTWFVYTFcyLzlnREdFZnJ5ak00TG9rZmZRZgprdWt2dVREbWlQRW1OenNyb3p4TkNNQTBWVzRaZVNxNTdtbXpoOEhPcVVYYlJzOGpaOGV1YU43amZkbU4KL09uRlhuTUJBbEdXd1A5ZzNBQnlPR05rWlJ0cXFwRk9QbTNsdytiSDdYc01pYWViSXg5WHFXYUdUZmIwCkFPTE5SNFdLNmZwYnE2Z3JmbTUzRVJtc05WdWQ3SFhCRHJsbVl0QTZvT2MwTkVyNDFIZDRjS2htOE10RQpFTnZCOFRrcjFmdDh0US9YSW55V1RlVktWM1ZacWJCeC9uVzF4bTlzMy9YSGg4N21oZmVzV2tCSTVDYnMKaGQ3Q0l0cStxNVJjd1RqYnBZYnBlTG9QN2k4V2FLZTdnL2x4bTFYR1dJNFBwZytjOEhuOG5VdHV5b1BvCk4xYjVFcm4vbys3OXZ2ekkya0xWMXhURFVwbXRCNStQVFFlQmNCSGZzdzE0eUliS09lMkhxcDUrYUtkUQpTRU9wK1RXNmhkWUJ3NDhxNGxSRHA3R1FNVFlXY0t5cVRNcTVEcGh0ejJHbld5Zm9BSGZDMnNtcHE1djgKa0NQVnZQQlQ4TW43OERCbjdVNUdXS2xLbVpmSmEyaGEzK1dZdUt2MGxoVFVSRGhUblFnOGNBSTFxbkw5Clhyckw3a0Qyb0tETEdoWk0vLzFYaytVYwo=.9b62172aa9ed09e8c02a160e74679119\"></div><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-0\">Info</h3>\n<p class=\"hj-alert-text\">This article applies to clusters using Databricks Runtime 11.2 and above on AWS.</p>\n</div>\n</div><p id=\"isPasted\">You\u00a0are using Databricks service principals to delegate permissions to automated tools and systems. In order to secure the service principals, Databricks recommends creating an access token for the service principal.</p><p>Please review the <a href=\"https://docs.databricks.com/dev-tools/service-principals.html#create-a-databricks-access-token-for-a-databricks-service-principal\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"create a Databricks access token for a Databricks service principal\">create a Databricks access token for a Databricks service principal</a> documentation for more information.</p><p>You should also review the <a href=\"seconds%20when%20creating%20the%20security%20token.\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"manage service principals and personal access tokens\">manage service principals and personal access tokens</a> documentation which covers using service principals with Partner Connect.</p><p>By default, the access token has a limited lifespan, defined in seconds. This is defined in the JSON block when calling the <a href=\"https://docs.databricks.com/dev-tools/api/latest/token-management.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Token Management API\">Token Management API</a> to create the access token for the service principal.</p><pre>{\r\n\"application_id\": \"&lt;application-id&gt;\",\r\n\"comment\": \"&lt;comment&gt;\",\r\n\"lifetime_seconds\": 1209600\r\n}</pre><h1 data-toc=\"true\" id=\"instructions-1\">Instructions</h1><p>There are some use cases, such as setting up an automation pipeline, where you may want the service principal to have an access token with a long expiration.</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-2\">Warning</h3>\n<p class=\"hj-alert-text\">Although it is possible to configure service principals with access tokens that do not expire, it is not recommended for standard use cases. Access tokens that do not expire do not follow security best practices. They may also cause issues during a compliance audit.</p>\n</div>\n</div><h2 data-toc=\"true\" id=\"long-life-access-token-3\">Long life access token</h2><p>To configure a security principal access token for a long life, determine the number of days the token needs to be active and multiply it by 86400. 86400 is the number of seconds in one day.</p><p>For example, if you want a security token to last for 30 days, set the lifetime to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2592000</span> (30 x 86400) seconds when creating the security token.</p><pre id=\"isPasted\">{\r\n\"application_id\": \"&lt;application-id&gt;\",\r\n\"comment\": \"&lt;comment&gt;\",\r\n\"lifetime_seconds\": 2592000\r\n}</pre><h2 data-toc=\"true\" id=\"unlimited-life-access-token-4\">Unlimited life access token</h2><p>To configure a security principal access token that does not expire, set the lifetime to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">-1</span> seconds when creating the security token.</p><pre id=\"isPasted\">{\r\n\"application_id\": \"&lt;application-id&gt;\",\r\n\"comment\": \"&lt;comment&gt;\",\r\n\"lifetime_seconds\": -1\r\n}</pre><p>\u00a0</p>", "body_txt": "Info\nThis article applies to clusters using Databricks Runtime 11.2 and above on AWS. You\u00a0are using Databricks service principals to delegate permissions to automated tools and systems. In order to secure the service principals, Databricks recommends creating an access token for the service principal. Please review the create a Databricks access token for a Databricks service principal documentation for more information. You should also review the manage service principals and personal access tokens documentation which covers using service principals with Partner Connect. By default, the access token has a limited lifespan, defined in seconds. This is defined in the JSON block when calling the Token Management API to create the access token for the service principal. { \"application_id\": \"&lt;application-id&gt;\", \"comment\": \"&lt;comment&gt;\", \"lifetime_seconds\": 1209600 } Instructions There are some use cases, such as setting up an automation pipeline, where you may want the service principal to have an access token with a long expiration. Warning\nAlthough it is possible to configure service principals with access tokens that do not expire, it is not recommended for standard use cases. Access tokens that do not expire do not follow security best practices. They may also cause issues during a compliance audit. Long life access token To configure a security principal access token for a long life, determine the number of days the token needs to be active and multiply it by 86400. 86400 is the number of seconds in one day. For example, if you want a security token to last for 30 days, set the lifetime to 2592000 (30 x 86400) seconds when creating the security token. { \"application_id\": \"&lt;application-id&gt;\", \"comment\": \"&lt;comment&gt;\", \"lifetime_seconds\": 2592000 } Unlimited life access token To configure a security principal access token that does not expire, set the lifetime to -1 seconds when creating the security token. { \"application_id\": \"&lt;application-id&gt;\", \"comment\": \"&lt;comment&gt;\", \"lifetime_seconds\": -1 } \u00a0", "format": "html", "updated_at": "2022-11-08T09:16:29.735Z"}, "author": {"id": 966503, "email": "monica.cao@databricks.com", "name": "monica.cao ", "first_name": "monica.cao", "last_name": "", "role_id": "draft_writer", "created_at": "2022-08-04T20:42:23.711Z", "updated_at": "2023-04-11T14:20:27.282Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256865, "name": "Security and permissions", "codename": "security", "accessibility": 1, "description": "These articles can help you with access control lists (ACLs), secrets, and other security- and permissions-related functionality.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2933022, "name": "aws"}, {"id": 3239773, "name": "lifetime token"}], "url": "https://kb.databricks.com/security/set-an-unlimited-lifetime-for-service-principal-access-token"}, {"id": 1541001, "name": "OPTIMIZE is only supported for Delta tables error on Delta Lake", "views": 2031, "accessibility": 1, "description": "Use CREATE OR REPLACE TABLE when moving Delta tables from one storage location to another.", "codename": "optimize-is-only-supported-for-delta-tables-error-on-delta-lake", "created_at": "2022-09-27T20:34:36.384Z", "updated_at": "2023-02-03T01:47:13.059Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9NcjNFWWtIcmc4Ums2ZTJieVNVUmtsMUp0V2pOemhVaWpYWVlhUnVnRzFoVG91MDZTCm1QTFhNS1k0TVpnMThsRnV6SWpuM3V3V0dLYS8yZ2pOekVBVmtqU2lmcjdZOHB4RmR6Z0xkSzhTVTNYNApwSjJzZk5rclA2cTRJYkpkVUtJQ3RwVG03aVJJOTdJbFZyQ1h1QU51YXI0OW5xVDE4NU85c0lCNUFuVHEKVnhTRzBocUNxRzBCTUNlbmRiai9meUtGcGRsK1RqeVprVU1HcjI2Z3BvSjVaYnczWUZId2FSbkRGYjcxCkgvYXcvRlF2MXNQb0x0cjZCMm5ZNVVKU2tNYmhhRkhNRjVYMTNvdVBEL0dwYk1yVWQ5QXhNREtTWGJ3dApFZUhOMTJXSFQrUGwxQzRkU3VaS2NrQUlnUnFuaW9VZUoyT3J6NjdodW1sZzZ3OG9lTGxiVDl3NzZjOVkKclhYZU5ycUtDM3RlU2ZBc2hkTEE3a0FoOTJjQlllRkdReHRodmlmWVVOWGo0RXpFMEVYUUUzQkdvZExhCmtBZ3BGMXFpVEp2WldSUFRRbTMwdEFVcm5RaUVoUUNVbGs1WHlkS3NsbEVIN1dzRWQ0cTF5UFBXTUI1aQplSVVEWTdjSEhzUllMQlRrT3ZFUURDaHJIdHJOVUttNjVzaXBtM2tzendHL1E5RUVXKzlPSVpsQnkwZ2oKL0tUQlhlSTdoSFJURHhtWjE4UVdnSEg1dFNyUFNudWJPNVcrWUFDSTkwMndwNEJ3TDc2aDZBUUw1azg2ClpaK3lsUmlkUnVaVkIrQzhQblRhY3Z5Y0kwT2dzWE11c3I1SFkrZU9mL3RORjZkVGlKaXJtMGVmY2VNVAo3VUFGeFloMjR3QjBzQ1RtRnVkVWpvRkF1Vm5vZmJYR2poN2s0UytCa3RsenRJeDRld3BtKzk3SmFNb1IKWG4yTUJnakxQanVrNDFSSVlZdUdKU2E3VmZZOWw4Z2g0RGtFOHBrdWVQbDh4NlJXODhxeHBicTJqTlNuCkdzRHh1VmZiV21CTFhOL1I0Si9IY1RlTDBtWUtrekpXMUg4dTk5Vkc0WlMyNkFmbTFIWEpwRjVFOXBibwphdHlSYmhnbzR3ZnVWc2I2U3lvdllpR2dPajRWR0xxOG1DbTJDYy8vcEtudWxudnA5Yjc3dE1lMkFHZWMKWUdxREhGSitmdWl3UFk4blhMbVNQVm5yQ0Y1elB4UVhBZVF0ZjVZNThkNm0wWFYyQzdQYkMrbHdIdFlOCm4vMHB0a1FtcURXUnZjVDRSOXBVMS9qYmlEQm1mUnlkZWlVUC9kTzF5dk9tbk53VVpVR1JTNTBlZ2F4RwpsQXNvanVXTmg4NGs5Y1JkTmU5bDdZS0VXNTdPTmlUK20vZlF2QmdJUURzbUxxem5ZcURGUy9nM2lvY2MKQmtXY3llMStCM2VLTWtseWd0RWdLVXNQUDhkb3BPcjV2dnZKL1dTZGtDMGxjL0xvVlVLSzR6V20wRjRNClRuaW5qZjZIZE9WK2FWZ0U0dUFMb2xhMUNYQ0RIYWtYKzQzcG9wVmlESXpFVzR0cE53ZFFJOUQ2ZVJrKwpvVnMyMEJlaTA3R1ZnYldOK2dCRlV0TmZRSW55SnY1eldua0RGNXpaN2JBOE9HL05TNjBjOU04Rk44aFUKWlBnVVk4bG12VktpeVNubnVmTVV6SW9xTStzWG5xTW5uRlM5YlFzWk13WCt0ajVUTE5TU1RWU2dZMFdPCkUyWEV1dm5iSFptVUJ6c0IwNzBjLzR1dW5KWWJhWE1tRFlXNGxRQjhHUGhhYnE5RHJOSTR6TTBXUENlSApXS1NYcnVwV3pRRXlPZ3RIcUdVb0pGbk5UUnI0akE0QWYyVk1LbDVaclppUVp4N3pXOVVMcVBSODgxN2cKSUZRVHdZeEdNUm89Cg==.3a7c9949f99412e1aa5a99136c768637\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> on a Delta table and get an error message saying it is only supported on Delta tables.</p><pre>Error: `&lt;database-name&gt;`.`&lt;table-name&gt;`is not a Delta table. OPTIMIZE is only supported for Delta tables.</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This can happen if the target table's storage location was modified and the table was recreated with a new storage location before you tried to run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span>.</p><p>If you review the driver logs, you see that there is no Delta log for the table at the old location.</p><pre>INFO DeltaLog: No delta log found for the Delta table at &lt;old-location&gt;</pre><p data-toc=\"true\" id=\"-2\">This means the metadata is still pointing to the old table location. It has not been updated with the new (current) table location.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><ol>\n<li>Ensure the Delta table is recreated in the new location using <strong><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">CREATE OR REPLACE TABLE</span></strong> (<a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"CREATE TABLE [USING]\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/sql-ref-syntax-ddl-create-table-using\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"CREATE TABLE [USING]\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"CREATE TABLE [USING]\">GCP</a>). This replaces the Delta table.</li>\n<li>After the Delta table has been moved, run <strong><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">FSCK REPAIR TABLE</span></strong> (<a href=\"https://docs.databricks.com/sql/language-manual/delta-fsck.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"FSCK REPAIR TABLE\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/delta-fsck\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"FSCK REPAIR TABLE\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/sql/language-manual/delta-fsck.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"FSCK REPAIR TABLE\">GCP</a>).<pre>FSCK REPAIR TABLE `&lt;database-name&gt;`.`&lt;table-name&gt;`</pre>\n</li>\n<li>Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE\u00a0</span>to optimize the Delta table. It should run successfully complete.<pre>OPTIMIZE `&lt;database-name&gt;``&lt;table-name&gt;</pre>\n</li>\n</ol><p><br></p>", "body_txt": "Problem You run OPTIMIZE on a Delta table and get an error message saying it is only supported on Delta tables. Error: `&lt;database-name&gt;`.`&lt;table-name&gt;`is not a Delta table. OPTIMIZE is only supported for Delta tables. Cause This can happen if the target table's storage location was modified and the table was recreated with a new storage location before you tried to run OPTIMIZE. If you review the driver logs, you see that there is no Delta log for the table at the old location. INFO DeltaLog: No delta log found for the Delta table at &lt;old-location&gt; This means the metadata is still pointing to the old table location. It has not been updated with the new (current) table location. Solution Ensure the Delta table is recreated in the new location using CREATE OR REPLACE TABLE (AWS | Azure | GCP). This replaces the Delta table.\nAfter the Delta table has been moved, run FSCK REPAIR TABLE (AWS | Azure | GCP).FSCK REPAIR TABLE `&lt;database-name&gt;`.`&lt;table-name&gt;` Run OPTIMIZE\u00a0to optimize the Delta table. It should run successfully complete.OPTIMIZE `&lt;database-name&gt;``&lt;table-name&gt;", "format": "html", "updated_at": "2023-02-03T01:47:13.055Z"}, "author": {"id": 790229, "email": "mathan.pillai@databricks.com", "name": "mathan.pillai ", "first_name": "mathan.pillai", "last_name": "", "role_id": "admin", "created_at": "2022-01-26T19:28:01.464Z", "updated_at": "2023-04-28T22:28:56.701Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256849, "name": "Delta Lake", "codename": "delta", "accessibility": 1, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3002657, "name": "aws"}, {"id": 3002658, "name": "azure"}, {"id": 3002660, "name": "delta lake"}, {"id": 3002661, "name": "delta table"}, {"id": 3002659, "name": "gcp"}, {"id": 3002662, "name": "move"}, {"id": 3002663, "name": "optimize"}], "url": "https://kb.databricks.com/delta/optimize-is-only-supported-for-delta-tables-error-on-delta-lake"}, {"id": 1533667, "name": "Decreased performance when using DELETE with a subquery on Databricks Runtime 10.4 LTS", "views": 532, "accessibility": 1, "description": "Auto optimize should be disabled when you have a DELETE with a subquery where one side is small enough to be broadcast.", "codename": "decreased-performance-when-using-delete-with-a-subquery-on-databricks-runtime-104-lts", "created_at": "2022-09-26T08:09:25.561Z", "updated_at": "2023-04-21T16:48:28.678Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"85ac73b85bf89\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThSOUtuUXliZWJab1ZuNFl5ZktXT1VJSVloSE5CSENIRWhDK05RYkNLamU2YWluWDN6CnVGYzk3azhkcldzQ0xKR3BOcTJkTnFjR08zOXFHWEFVQnBaNjFEL0ErOWhCVlFwdDRSOVpHMTZCT05ncgpOeWd0U3lIaFE4ZWV5aVR4RVNqNnZEQW1kcWw4bm1KYzlVb1NoVkRoRDZhVDd3WVlwcmEvQnB1OHE4dysKMlFVaFVQODhxbUNHNGVDdi9mck1iT0xqNkwzZGp3aGFqS2dQYS9BTWZxK0Fob2MrSVJhUHdBWTVZRXhVCkI1S0J0aUo4dURrY3pHZkJYZGIvWlN4NTRlbTNTcXRsRWlycmdsLzA0ZmE4ZjNEK0RHOGV0UDQ4Z1FSRgp2b0xSOHJ5SUZXdkRUcFJsdUt3bkNMOWNOKy9qc2l6UjBpTVRSK0pTTUo1MjdsYkFiS0ZDMFdveklKZFEKek5RZTJEU2Q3M3R2VkhJZFhCdHlMazJWSWlKdnRlc3JSakVyY0lSMDJTZ25FZUI1UlVOZnhOcmo3em91CkZxQmViajhud0VtRUJpUmxaMUhsWXRBRFRNb2d2ZWc2bUV5UzBLT2ZGdGJZSG94TFZzbUpUQm1IdERlcAo4UGhEai9WQk4xdGxIdHVheFRTalRRb2hNTjY1cmo3bi9uYjg1UnovOU1sdVJtSW5TQ0JBallOd2VJeEYKTDdCanBhVHRxdGgxS3h2MFRTVGJBNWRxNHFIclQ5RE1iMFpndlhvL0NiQnd5WW01UEsyelR4VjFwNnpLCjcya0dtN25TZE9hQ3ZBWElha21rMXpnWHBnZ0RKcStmU1ZPME9yZTJSTDloVFRmQjFuQ0htKzhsN044OQp0Mk45MWJOY3U2dm1kakRNNndDRGlLUUw3V05seUIzdDhYSkU2b09TYUV1aE0vSWpQRDdRUGJSY0RnREkKcGtZVW9DVXlKR3NhYjgwMm5xdE1qRmNXZjBLZVJKNWZqeGRPTGxaWWFFa2xaR2RXYjU3clR6cmNmbGJCCjdmblZmeXcxdWNncW5ZanJzQmw3R3ZZRERFZnNIR2FVTUpmdGJySllFcFoyQ1F0am9sWktEeG1rQXZQZwoycFRkd0gvZUR3cVdFRWdpc0xsNzg3a0tiVC9OV0JiRENmRHBvalhKYW1PRGVrRHRDemNxTUkxNkFMOGQKd0lyWklnQXNiRmFhbStRRUhaV21tK0s1blpVa1gxMkFTano4Y3EwSXFOa0VXZjRpWFEydVlpMFhNR3pmCmF5NWpIckNHSGF4b2RsMXF5MVprMG1RZXAwWnhudEgyekF3OGVrTTFicUdMeGhHZGhlWEFPdXpkR0QxVQpUN0FiRzErWXBaNTVrOXhadm1zMGxJMHFCVmVDMVVwWnhtYWhtczFLLzV5RG0rZWRUY1Y5NzdscVpmMHkKdmRVMnB5U09MdzJBUUFzM1o3VmhXU0twUVhnaEdRPT0K.18ecce608225416853013b6ffff90733\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p><strong>Auto optimize on Databricks</strong> (<a href=\"https://docs.databricks.com/optimizations/auto-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/optimizations/auto-optimize\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/optimizations/auto-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>) is an optional set of features that automatically compact small files during individual writes to a Delta table. Paying a small cost during writes offers significant benefits for tables that are queried actively.</p><p>Although auto optimize can be beneficial in many situations, you can see decreased performance on Databricks Runtime 10.4 LTS when you have a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DELETE</span> with a subquery where one side is small enough to be broadcast.</p><p>For instance the query may look like follows:</p><pre id=\"isPasted\">DELETE FROM &lt;tableToDelete&gt;\u00a0\r\nWHERE Date = &lt;'SampleDate'&gt;\r\nAND SampleID IN (\r\n\u00a0 \u00a0 \u00a0 \u00a0 SELECT MatchId FROM &lt;OtherTable&gt; WHERE MatchId = 'Value')</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">Optimized writes are enabled by default for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DELETE</span> with a subquery, on Databricks Runtime 10.4 LTS, on the assumption the data will be shuffled. In situations where one side is small enough to be broadcast, this does not happen and you may see a performance hit.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>If you encounter this issue, and you do not want to upgrade to a newer Databricks Runtime, you should disable auto optimize in your Delta table by setting <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">delta.autoOptimize.optimizeWrite = false</span> in the table properties.</p><p>You should also set this value in your cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>):</p><pre>spark.databricks.delta.delete.forceOptimizedWrites = false</pre><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">Databricks Runtime 11.2 and above disables auto optimized writes for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DELETE</span> with subqueries by default.</p>\n</div>\n</div>", "body_txt": "Problem Auto optimize on Databricks (AWS | Azure | GCP) is an optional set of features that automatically compact small files during individual writes to a Delta table. Paying a small cost during writes offers significant benefits for tables that are queried actively. Although auto optimize can be beneficial in many situations, you can see decreased performance on Databricks Runtime 10.4 LTS when you have a DELETE with a subquery where one side is small enough to be broadcast. For instance the query may look like follows: DELETE FROM &lt;tableToDelete&gt;\u00a0 WHERE Date = &lt;'SampleDate'&gt; AND SampleID IN ( \u00a0 \u00a0 \u00a0 \u00a0 SELECT MatchId FROM &lt;OtherTable&gt; WHERE MatchId = 'Value') Cause Optimized writes are enabled by default for DELETE with a subquery, on Databricks Runtime 10.4 LTS, on the assumption the data will be shuffled. In situations where one side is small enough to be broadcast, this does not happen and you may see a performance hit. Solution If you encounter this issue, and you do not want to upgrade to a newer Databricks Runtime, you should disable auto optimize in your Delta table by setting delta.autoOptimize.optimizeWrite = false in the table properties. You should also set this value in your cluster's Spark config (AWS | Azure | GCP): spark.databricks.delta.delete.forceOptimizedWrites = false Info\nDatabricks Runtime 11.2 and above disables auto optimized writes for DELETE with subqueries by default.", "format": "html", "updated_at": "2023-04-21T16:48:28.676Z"}, "author": {"id": 886595, "email": "sergios.lalas@databricks.com", "name": "sergios.lalas ", "first_name": "sergios.lalas", "last_name": "", "role_id": "draft_writer", "created_at": "2022-06-17T15:36:18.243Z", "updated_at": "2023-04-21T15:02:56.121Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3062531, "name": "autooptimize"}, {"id": 3086173, "name": "auto optimize"}, {"id": 3086169, "name": "aws"}, {"id": 3086170, "name": "azure"}, {"id": 3062532, "name": "dbr 10.4"}, {"id": 3086174, "name": "delete"}, {"id": 3086171, "name": "gcp"}, {"id": 3086175, "name": "optimize"}, {"id": 3086172, "name": "optimized writes"}, {"id": 3086176, "name": "subquery"}], "url": "https://kb.databricks.com/sql/decreased-performance-when-using-delete-with-a-subquery-on-databricks-runtime-104-lts"}, {"id": 1516640, "name": "Copy installed libraries from one cluster to another", "views": 4217, "accessibility": 1, "description": "Copy libraries from a source cluster to a target cluster with a custom Python script.", "codename": "copy-installed-libraries-from-one-cluster-to-another", "created_at": "2022-09-19T06:38:30.376Z", "updated_at": "2023-01-06T01:33:49.532Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSs1Y1RQTjRzRXBpaWJ1MjZCK1Ura1BIWk9OQ3F3WHdtaFVjVFNiTERuTWFadFB3K1FyCnlLaXJwZDNyRHl1L25SWldVVzRmdm5OVSsvWUlwVUZWWkJpOUg5cngrNWFNMUtFeFhYU3BqeEVFNkxNUApZajY2anhRaUl5UDR1RVFsVFgvbitFR0ZKdXhScjZDeWVjbkpoNG1jc1haWFg5KzlENjJybUhCZzFmVVkKMWlkZlhPRFhWSTgxTFQ3aHhZWTBaRGk1d3ZBb1ljN3YxaDNYUC9ZS3hFU2wvRTNFSngyMVQ3L3cvVXZKCmkyY3QwUDRucDhhNVZjQUhudllEN1FnUjZ3YWhLQTQxSmc1TVlheWFhQjhmWDh3YWNCQVpFMDF2aXpuSgpFUVo3WXBDQlJSd0N6QThDSkJ4SFhjR1k5ZkRyMEhkOWVDK2hCZ1pyZnNUR1JXOU9JbkFhTm05TFNRWXUKNUhkd21pbmVhcGU2ZjJiTnJJOGRuMXFFNmhNQ08ra3U2MVo5cDV5QUN3VVoxSDdlODcrV3h6VEM0eVB5Cit2TmpHMUtmVUsyRVV6UU4rSkZ5NjdkVVBWUlZlLzJXQXZIVS9vZmZ0WW1qdUxyd2V3OVdmcWtMWE8rWQo4V0FjUmp3bEJPWVA3QXczdTgyUjY5Zi85Z3A2QjdQT1N2cTdVWnRFTU9IcFoycWszd0tSRGpOKzdYZUoKUzRwODhjYnpXcmFGSzllYndRQXplZ3hJOEdjUEtTSytHZkdlV0NJSWxiamh4cm05b1MzVDJzenJHMXZjCmVObnhxaUxzWHJGVENZMitnbjR0bXhXYVljMVExc0xzMkJaV1FLd204NDRjdHpCbldLU2xIMUpoOGxFTQo5d1JlRC9rRzZVQkxISDlmRkdKSDNDY1Y3NldxL28rdS9rWmxyQ1QveWN0UWdQZUZFdlZOVVhFOE1VcHYKNzE1Skp2UGFPajliUkpLMzhmT2tGa0RCZ1I5UldXa21kUk1ONnp1YnRYNEttREgwd09qWVk1UmxyR0dtCnl5TGpWdWJFRnZTQllmQU5WY1lYaC9WcGt3aEo5OUIxcWhLeE1DRmhwU084WjhTdjJaRFh0eEY3TGc1OApnQkd0UlE4OEJBQzZyY0l2ZWc0bWdhQ3p5bHlPcnczTm4yR2dJSlVrOHMva0F5V0djM3F1SDNoRTNCMXgKYnc2a29Rd1hXTG5nZVlOZGZTcE9jYmlnRXZ4K0c3aG9CYXYyR3p1b240aDBRdDRodHVVREZKaDhrMXVECkNaeGJFdUxheEp3RTg4Ylg0N1NXSnZSYzRhbFFKa1UrZUMyWDREMnNQWGFYUmFVODZXNGJKdHVFQ1ZZUQp6UTFTR2NBNGpHMFNtYlIvQWpqaHMrSDZFdEdRV28wUGJhOTM0cEtYV2RuQWFZWXEyQVowSGlQL29qWlgKS0N4bmhROGhhcmJxbWxRYlJrSXp1cmhtWVFVbDI1ZkYxVUI2dzlHYWxlYVFWZGxHUmV1VzdjRlJDOXRxCkNKZ045emMxUnI3dXNnSnkzR29JL1YzNFZ0Qm5GMmRHNUd2T0RmUlIySEtQRnZUYXdCNVhHU0Y2SzBJQwpRVlRtUGx1d0djcVhjZlU2SldoNTlmcGt4SEhIQ1BxNkRNSkJGTy9MMHk2QW5pQWVXdFUxVWRNa3ZGb0kKVGpMbUpUcXlYazVKUVFsWnpJeHhHaFo5YzNEeVZwU2YrRkNkdVFJek92Mlo1MFJ2OWlHZytvbUdNSGhNCnE0RkErUjhkZkVnVTB5ZjV3Z2VrZnFsRDdTN1ZBemlaeHlQOGRBTUdmNlowQ1dwbmI3MFlhUmcxeXBmMwpLY3hiNnBaaEdNVFcrQlB0M1Q3UlJpUkY1V1hsCg==.3ff0f186d3c61b9dc9dacef88a6cd90c\"></div><p>If you have a highly customized Databricks cluster, you may want to duplicate it and use it for other projects. When you clone a cluster, only the Apache Spark configuration and other cluster configuration information is copied. Installed libraries are not copies by default.</p><p>To copy the installed libraries, you can run a Python script after cloning the cluster.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><h2 data-toc=\"true\" id=\"identify-source-and-target-1\">Identify source and target</h2><p>The source cluster is the cluster you want to copy from.</p><p>The target cluster is the cluster you want to copy to.</p><p>You can find the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;source-cluster-id&gt;</span> and the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;target-cluster-id&gt;</span> by selecting the cluster in the workspace, and then looking for the cluster ID in the URL.</p><pre>https://&lt;databricks-instance&gt;/#/setting/clusters/&lt;cluster-id&gt;</pre><p data-toc=\"true\" id=\"-2\">In the following screenshot, the cluster ID is <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">0801-112947-n650q4k</span>.</p><h3 data-toc=\"true\" id=\"-2\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1670470255490-Screenshot%202022-12-08%20at%209.00.41%20AM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\"></h3><h2 data-toc=\"true\" id=\"create-a-databricks-personal-access-token-3\">Create a Databricks personal access token</h2><p>Follow the <strong>Personal access tokens for users</strong> (<a href=\"https://docs.databricks.com/dev-tools/auth.html#personal-access-tokens-for-users\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Personal access tokens for users\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/auth#--azure-databricks-personal-access-tokens\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Azure Databricks personal access tokens\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/auth.html#personal-access-tokens-for-users\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Personal access tokens for users\">GCP</a>) documentation to create a personal access token.</p><h2 data-toc=\"true\" id=\"create-a-secret-scope-4\">Create a secret scope</h2><p>Follow the <strong>Create a Databricks-backed secret scope</strong> (<a href=\"https://docs.databricks.com/security/secrets/secret-scopes.html#create-a-databricks-backed-secret-scope\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a Databricks-backed secret scope\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/security/secrets/secret-scopes#create-a-databricks-backed-secret-scope\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a Databricks-backed secret scope\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/security/secrets/secret-scopes.html#create-a-databricks-backed-secret-scope\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a Databricks-backed secret scope\">GCP</a>) documentation to create a secret scope.</p><h3 data-toc=\"true\" id=\"store-your-personal-access-token-and-your-databricks-instance-in-the-secret-scope-5\">Store your personal access token and your Databricks instance in the secret scope</h3><p>Follow the <strong>Create a secret in a Databricks-backed scope</strong> (<a href=\"https://docs.databricks.com/security/secrets/secrets.html#create-a-secret-in-a-databricks-backed-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a secret in a Databricks-backed scope\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/security/secrets/secrets#create-a-secret-in-a-databricks-backed-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a secret in a Databricks-backed scope\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/security/secrets/secrets.html#create-a-secret-in-a-databricks-backed-scope\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Create a secret in a Databricks-backed scope\">GCP</a>) documentation to store the personal access token you created and your Databricks instance as new secrets within your secret scope.</p><p data-toc=\"true\" id=\"databricks_instance-when-viewing-a-databricks-workspace-after-you-have-logged-into-your-databricks-workspace-look-at-the-url-displayed-in-your-browsers-address-bar-that-contained-within-the-web-url-called-databricks_instance-6\">Your Databricks instance is the hostname for your workspace, for example, xxxxx.cloud.databricks.com.</p><h2 data-toc=\"true\" id=\"use-a-python-script-to-clone-the-installed-libraries-6\">Use a Python script to clone the installed libraries</h2><p>You can use this example Python script to copy the installed libraries from a source cluster to a target cluster.</p><p>You need to replace the following values in the script before running:</p><ul>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;scope-name&gt;</span> - The name of your scope that holds the secrets.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;secret-name-1&gt;</span> - The name of the secret that holds your Databricks instance.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;secret-name-2&gt;</span> - The name of the secret that holds your personal access token.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;source-cluster-id&gt;</span> - The cluster ID of the cluster you want to copy FROM.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;target-cluster-id&gt;</span> - The cluster ID of the cluster you want to copy TO.</li>\n</ul><p>Copy the example script into a notebook that is attached to a running cluster in your workspace.</p><pre id=\"isPasted\">%python\r\n\r\nimport requests\r\nimport json\r\nimport time\r\nfrom pyspark.sql.types import (StructField, StringType, StructType, IntegerType)\r\n\r\nAPI_URL = dbutils.secrets.get(scope = \"&lt;scope-name&gt;\", key = \"&lt;secret-name1&gt;\") \u00a0 \u00a0# <a href=\"https://xxxxx.cloud.databricks.com/\" id=\"isPasted\">https://xxxxx.cloud.databricks.com/</a>\r\nTOKEN = dbutils.secrets.get(scope = \"&lt;scope-name&gt;\", key = \"&lt;secret-name2&gt;\") \u00a0 \u00a0# Databricks PAT token\r\nsource_cluster_id = \"&lt;source-cluster-id&gt;\"\r\ntarget_cluster_id = \"&lt;target-cluster-id&gt;\"\r\nsource_cluster_api_url = API_URL+\"/api/2.0/libraries/cluster-status?cluster_id=\" + &lt;source-cluster-id&gt;\r\nresponse = requests.get(source_cluster_api_url,headers={'Authorization': \"Bearer \" + &lt;TOKEN&gt;})\r\nlibraries = []\r\nfor library_info in \u00a0response.json()['library_statuses']:\r\n\u00a0 lib_type = library_info['library']\r\n\u00a0 status = library_info['status']\r\n\u00a0 libraries.append(lib_type)\r\n\u00a0 \u00a0\u00a0\r\nprint(\"libraries from source cluster (\"+source_cluster_id+\") : \"+str(libraries)+\"\\n\")\r\ntarget_cluster_api_url = API_URL +\"/api/2.0/libraries/install\"\r\n\u00a0\r\ntarget_lib_install_payload = json.dumps({'cluster_id': target_cluster_id, 'libraries': libraries})\r\nprint(\"Installing libraries in target cluster (\"+source_cluster_id+\") with payload: \"+str(target_lib_install_payload)+\"\\n\")\r\nresponse = requests.post(target_cluster_api_url, headers={'Authorization': \"Bearer \" + TOKEN}, data = target_lib_install_payload)\r\nif response.status_code ==200:\r\n\u00a0 print(\"Installation request is successful.Response code :\"+str(response.status_code))\r\nelse:\r\n\u00a0 print(\"Installation failed.Response code :\"+str(response.status_code))</pre><h2 data-toc=\"true\" id=\"test-target-cluster-7\">Test target cluster</h2><p>After the script finishes running, start the target cluster and verify that the libraries have been copied over.</p><p><br></p>", "body_txt": "If you have a highly customized Databricks cluster, you may want to duplicate it and use it for other projects. When you clone a cluster, only the Apache Spark configuration and other cluster configuration information is copied. Installed libraries are not copies by default. To copy the installed libraries, you can run a Python script after cloning the cluster. Instructions Identify source and target The source cluster is the cluster you want to copy from. The target cluster is the cluster you want to copy to. You can find the &lt;source-cluster-id&gt; and the &lt;target-cluster-id&gt; by selecting the cluster in the workspace, and then looking for the cluster ID in the URL. https://&lt;databricks-instance&gt;/#/setting/clusters/&lt;cluster-id&gt; In the following screenshot, the cluster ID is 0801-112947-n650q4k. Create a Databricks personal access token Follow the Personal access tokens for users (AWS | Azure | GCP) documentation to create a personal access token. Create a secret scope Follow the Create a Databricks-backed secret scope (AWS | Azure | GCP) documentation to create a secret scope. Store your personal access token and your Databricks instance in the secret scope Follow the Create a secret in a Databricks-backed scope (AWS | Azure | GCP) documentation to store the personal access token you created and your Databricks instance as new secrets within your secret scope. Your Databricks instance is the hostname for your workspace, for example, xxxxx.cloud.databricks.com. Use a Python script to clone the installed libraries You can use this example Python script to copy the installed libraries from a source cluster to a target cluster. You need to replace the following values in the script before running: &lt;scope-name&gt; - The name of your scope that holds the secrets. &lt;secret-name-1&gt; - The name of the secret that holds your Databricks instance. &lt;secret-name-2&gt; - The name of the secret that holds your personal access token. &lt;source-cluster-id&gt; - The cluster ID of the cluster you want to copy FROM. &lt;target-cluster-id&gt; - The cluster ID of the cluster you want to copy TO. Copy the example script into a notebook that is attached to a running cluster in your workspace. %python import requests import json import time from pyspark.sql.types import (StructField, StringType, StructType, IntegerType) API_URL = dbutils.secrets.get(scope = \"&lt;scope-name&gt;\", key = \"&lt;secret-name1&gt;\") \u00a0 \u00a0# https://xxxxx.cloud.databricks.com/ TOKEN = dbutils.secrets.get(scope = \"&lt;scope-name&gt;\", key = \"&lt;secret-name2&gt;\") \u00a0 \u00a0# Databricks PAT token source_cluster_id = \"&lt;source-cluster-id&gt;\" target_cluster_id = \"&lt;target-cluster-id&gt;\" source_cluster_api_url = API_URL+\"/api/2.0/libraries/cluster-status?cluster_id=\" + &lt;source-cluster-id&gt; response = requests.get(source_cluster_api_url,headers={'Authorization': \"Bearer \" + &lt;TOKEN&gt;}) libraries = [] for library_info in \u00a0response.json()['library_statuses']: \u00a0 lib_type = library_info['library'] \u00a0 status = library_info['status'] \u00a0 libraries.append(lib_type) \u00a0 \u00a0\u00a0 print(\"libraries from source cluster (\"+source_cluster_id+\") : \"+str(libraries)+\"\\n\") target_cluster_api_url = API_URL +\"/api/2.0/libraries/install\" \u00a0 target_lib_install_payload = json.dumps({'cluster_id': target_cluster_id, 'libraries': libraries}) print(\"Installing libraries in target cluster (\"+source_cluster_id+\") with payload: \"+str(target_lib_install_payload)+\"\\n\") response = requests.post(target_cluster_api_url, headers={'Authorization': \"Bearer \" + TOKEN}, data = target_lib_install_payload) if response.status_code ==200: \u00a0 print(\"Installation request is successful.Response code :\"+str(response.status_code)) else: \u00a0 print(\"Installation failed.Response code :\"+str(response.status_code)) Test target cluster After the script finishes running, start the target cluster and verify that the libraries have been copied over.", "format": "html", "updated_at": "2023-01-06T01:33:49.498Z"}, "author": {"id": 831829, "email": "manoj.hegde@databricks.com", "name": "manoj.hegde ", "first_name": "manoj.hegde", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T15:23:57.894Z", "updated_at": "2023-04-13T05:12:40.616Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256852, "name": "Libraries", "codename": "libraries", "accessibility": 1, "description": "These articles can help you manage libraries in Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2971673, "name": "aws"}, {"id": 2971674, "name": "azure"}, {"id": 2971676, "name": "clone"}, {"id": 2971677, "name": "cluster"}, {"id": 2971678, "name": "compute"}, {"id": 2978843, "name": "copy"}, {"id": 2971675, "name": "gcp"}, {"id": 2971679, "name": "libraries"}], "url": "https://kb.databricks.com/libraries/copy-installed-libraries-from-one-cluster-to-another"}, {"id": 1515519, "name": "Failed to add user error due to email or username already existing with a different case", "views": 5449, "accessibility": 1, "description": "You should ensure casing for usernames is consistent across all accounts and providers in your system.", "codename": "failed-to-add-user-error-due-to-email-or-username-already-existing-with-a-different-case", "created_at": "2022-09-16T17:21:03.598Z", "updated_at": "2023-01-20T13:41:46.288Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSt3bUFMZEZBQ1VNS1R3bmIxbE9kajFpZ2JybU9kb3UxYUhHdEFHVnVkbVdHRGdRNEF6ClFnVkxSaEM5RlR1MTB2dVJ2SERFaklUeWxYR0FCTFRnVGw5RWJ3MWpnY0pOeXVrd3liaHRzUzI1S0VXMApOOU9PNVV2YnByQ0c4MHl6QjNQc0NDanptbW0xSWRNOENsN0tqRTlaM2ZKMzB4M05RS0VweldrYVV6WjAKelp5QUdtTDdSVUNDUm80dy9mZk1hQ2FyTk96OFRpSHQyQUpoRU9mSVo1TjBubmtTTSttaFA5TGM2TTRRCkdUZDVKbFVIMnYzNlkrM3ovdFcybHJ6TFlDREQxNE5Hc3VBWVFwWFFNV29ITjVQb1hWanU4LzBRMEFKcwpYeHNiNmtlNUlRMTc2d21oYzg4RkYxNDRYMWtFWFA3MElBdGtvYWMxV0xqTCtISGFtNXFiQ3VNa05kajMKZUhPbnQrR3pqQ1VvZTVRZ1BOMmpSNW0zdXVLTG94Zkd6Z3VQbjF0eStRc1V4RDZvMWY5OURjOERRMU1iClJMaGIzTTltZzZPZTJCejJjV1ZKTnZud2tWTm4wZVRBSlRHQ1lGQWxIZUhnNFZtNk9UYW14czU4ZmJBNwo0QWZkb2lWcWtoRi9QbnoxdGdMbTg0MUR3L2g0RG9lRTl4RXlBaGNoSXJ6L2ZIKzlFc2RHdWJpSVo4dTcKL0I2U1ROdSsrT2NzUkNETU0zK0dyRXdOYmZaTnpzazdCbEFERnM4ZUYvVjhXWHg2YmNXdG9vcGlQUVAxCjJURzRENGVaSFFuV0dzTnRmZHdZZWZTTHBlNkNRZk91RkV1QUpMcHh1dnB0VEJjSWVRM0J0M3BNUzFjaQpqd25UMUY4anVZQlVOLzRFRkZKU0g1aXRGZDF4WHMyVUtVK1Q3R1dRdGVFRTZXWGQxYUtsd2lwTkZwaEoKbkRtTWlTWDg1aDRUUzdhNFF5OEhLU1FyVFllc2hnY2twaTN2UmFHSitrY1haeUh2QTQwRGwwblJjdW90Cjd5VUhkL2N0N3dzSWZJWXB2ZEJpN05OSStDMTl4NXlTRVpGOFBtZm9XSE1PYUQwaWQ5YzhUZldVOGhBdgp6QjZHNjloQzVqMnFWc2pKVEdlZzFjeDViY1pjQUZHYnBoVnJTdC9Mc3VQRzVucHpxcjk2MWhZeDhxNDUKV09BYjN6dWxIQjF0ZGZrbmxiak5Tbm9RVytGdzJVdWJobzZ0ckluVEduNzdCdThRNFdTNSszRm1ad0RsCmhwUmV4aTVUTU1URXJvY3dNd3lJeUJnWEJHdVpJdVZ5S0o0MmZ5cHliUFdUMVZVYUpibTNQOXg1ODNhTwpaYVZMeGxndnprbVZpTXpBd3BkME80Rlh1VVFMOWp6bnl2ZlVUSjBWQzV3Q0RZM0ZpTmUwSTlnZ2RqaXIKTW1MQnd4TkhEc0VkSTE2NEdNZk1nek9GVUxLTDA5Y1dEOGdzaExRTUpON1ZXT09aTWpiSGFyWmJZOXMxClhERjJ1ckhrVnpYL3h1dUJpdGEySkhQclJlRjV5c1N0ZFpnbVFhYXQzdXk2SXZnZnZGRDR5S2hCWnQ4Tgp2SzV6V0JDMDVLUFdoL3FVZnpSQ0NZRzl6enA3SG52MWpJd0xlZnVEWDU0UmZtOXpYaktmUGxlVU5jSUwKOXV6TTlQNnhwYWdtS3haRm9leXp1Q0tSZlliY1YvU05HWkVxYkVjVFNrc0x3MVRJaklTRFczOERiREJkCm84U3BEVzEvejlHcllDbmpTMWhUcGx6OUxFVWxIWElFMk5KNG5kY2hkcldxL1E9PQo=.a4c7ff1aa81c3b17250f6f1b5a574f59\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to add a new user to your Databricks workspace via the <strong>Admin Console</strong>, but it fails with an error saying that the email or username already exists with a different case.</p><pre>Failed to add user: A user with email &lt;email-address&gt; and username &lt;username&gt; in difference cases already exists in the account.</pre><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1667849862714-ss1.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><p>You may also get this error when a newly added user tries to log into the workspace via SSO.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This happens when you try to use a username that is identical to an existing name with the exception of case (lowercase or uppercase).</p><ul>\n<li>The user already exists in the account with different case formatting. For example, you are trying to add the username <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;firstname.lastname&gt;</span>, but the user <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;Firstname.Lastname&gt;</span> already exists in your account.</li>\n<li>The email format used by your Single Sign-On (SSO) provider uses different casing than an existing account in your workspace. For example, the username was added to the account in lowercase <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;firstname.lastname&gt;</span>, but the email used by the SSO provider is in uppercase <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;Firstname.Lastname@yourcompany.com&gt;</span>.</li>\n</ul><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Ensure that casing for usernames is consistent across all accounts and providers in your system.</p><p>For example, if you use the lowercase convention for your Databricks workspace, make sure that you also use the lowercase convention for your SSO provider.</p>", "body_txt": "Problem You are trying to add a new user to your Databricks workspace via the Admin Console, but it fails with an error saying that the email or username already exists with a different case. Failed to add user: A user with email &lt;email-address&gt; and username &lt;username&gt; in difference cases already exists in the account. You may also get this error when a newly added user tries to log into the workspace via SSO. Cause This happens when you try to use a username that is identical to an existing name with the exception of case (lowercase or uppercase). The user already exists in the account with different case formatting. For example, you are trying to add the username &lt;firstname.lastname&gt;, but the user &lt;Firstname.Lastname&gt; already exists in your account.\nThe email format used by your Single Sign-On (SSO) provider uses different casing than an existing account in your workspace. For example, the username was added to the account in lowercase &lt;firstname.lastname&gt;, but the email used by the SSO provider is in uppercase &lt;Firstname.Lastname@yourcompany.com&gt;. Solution Ensure that casing for usernames is consistent across all accounts and providers in your system. For example, if you use the lowercase convention for your Databricks workspace, make sure that you also use the lowercase convention for your SSO provider.", "format": "html", "updated_at": "2023-01-20T13:41:46.258Z"}, "author": {"id": 976862, "email": "harrison.schueler@databricks.com", "name": "harrison.schueler ", "first_name": "harrison.schueler", "last_name": "", "role_id": "draft_writer", "created_at": "2022-08-24T17:13:08.035Z", "updated_at": "2023-04-13T19:13:55.472Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 254612, "name": "Databricks administration", "codename": "administration", "accessibility": 1, "description": "These articles can help you administer your Databricks workspace, including user and group management, access control, and workspace storage.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2955864, "name": "add"}, {"id": 2955862, "name": "add user"}, {"id": 2955857, "name": "admin console"}, {"id": 2955853, "name": "aws"}, {"id": 2955854, "name": "azure"}, {"id": 2955866, "name": "email"}, {"id": 2955855, "name": "gcp"}, {"id": 2955861, "name": "new"}, {"id": 2955860, "name": "sso"}, {"id": 2955859, "name": "user"}, {"id": 2955865, "name": "username"}], "url": "https://kb.databricks.com/administration/failed-to-add-user-error-due-to-email-or-username-already-existing-with-a-different-case"}, {"id": 1513427, "name": "Delta writing empty files when source is empty", "views": 4809, "accessibility": 1, "description": "Delta can write empty files under Databricks Runtime 7.3 LTS. You should upgrade to Databricks Runtime 9.1 LTS or above to resolve the issue.", "codename": "delta-writing-empty-files-when-source-is-empty", "created_at": "2022-09-14T21:42:27.162Z", "updated_at": "2022-12-02T21:07:32.340Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStoVTlZWURTU01rV0tKdGdEVEpaN29KNGNKUEJ0SSs1dWpsS0g5NktlZTlKaWd6RGE1CnhzdEJoK1ZIa0d5TkNKZjdBVGdhQm1VZEJaSU8vNk1CTFlxWUJ4RTkrYnhNRkFaS2ZrSkx3azV1WFNLQwo5alZOZU1wcEo1S0J2TUcyaHlyNmdKa21mdTIyRnY4T1ZQYnNyaFdwSFdMNUVvbjExUHlHc1RIZUhXaTAKNDIxb3Ftamp1ODhUR2x1Smo2VnBZTENPeFlZenFET3hUcStlYmtHcFRMY2FDeURDWmRIV3Y1eUFqcEFzCnR1MjVsT0dNOHVWT3puL3Yvb2M5d2VHSG1LaVRmbUdXditOOTkvUllEclVTR3NkZUlmWUY2U3JSTHpMSApaR0prVHpYMXpVSkhrRlFxc0FWdnhEd00yRnl1WmZ3NnJDL0FUb3NmZEFEWkVLS3BQSVQxV3B4RG9EVzkKRzduMldCZ0V5RTNJWEorU0kxRTBJQkdoZVRoK2dDN1hBSXNXNi9ZdWsxQVBnVWpLeXpMaVlhK3R2UUt0CkY3aFFjaXZ6YnRJL3ozVnNkUWFXcjR0d1U5OGd4cUVwYjQvczZQZ1ByNE5tUTBNMjRhb1Q5Q2VyWis2MApmS0JhVDFXQlh4eDlFclNnNG9GQys0NU9rWERzczZtZjFmNXNiQ3JTVjVHN0Z4YS9RNzFGcDA1OWR0bncKRFN3VWpvWHBHWVNyTnV6RW5TYXYyTktQbmhoYXhVd1FRUWtiMHB1a2d1ZEtYU1dnbVZtUjQ3aTM4OE9yCkJQL3pBR0s4WHBpVHRWSDNQUHpkeEk2YU1VYUdETWlJcTRPbi9uckdWR29LVnJMZ0greWU4TDUyUTFmOApoNTJRUlg3c1lqUm02K3NCWk5TUGZUbTlLa0hpWlQxNnVkeG1LdEtTbkttVk1uS1FGSFpaV2FBRE9say8KMjZjZW1Tb1gwcUpOaHlWRzN1RW9RL09aWW0waFo0UnBTNGZ5T3UzbzlVaGZvdkhuV3J3VFQ2WGJubW9OCm5PcGZkenBKV29PWitrUmF4b2h2Ylc4RDdKcnlYZkk3RGxqSExtdUhETERXS0lGWXAxSmo5Y1E2UGlYSwo1ankzZTUzN1JiR1NIb1VVamh6dWVWU1pXUDJjdVBxODkxVWIrd1VIUmxLRHp2cHZaLzYyS0Fkcms1QzUKblN5RDJGNmxNM2p3Y0ZxVWFrWjR1UG1VRDhrZFRrVlVmNFJ5Z0xrZnNVQWhLcUNoQ0xvMVhOUkRXZXRLCnNPYUZuWlIxVUtiSTYvQVZDTlhSczBOcGxGTkp5eldOUFYxR2FrUTRPcVdIVDdGZ2RPQlRRZ2UwTDc2MwpuU2ZMajZRR2tSM0prbC9MR3NVQnFuRmNFRllpRE9NeldKSkVseGxvbmpHUVBZek0zakVZWjdDaERTa0QKRW9BSm10bUlSN0NvZW9jeW8zSGJ5Y2h6UHA1Rmg0QUpYNll3b3l4b2tGZlhqVlNtWUlSc1NOUkdyRXpFClBiQU5vL0pHY2lNendVK1RKRjBicTFsOXl4ZUZOMUVTbC83S0V1RXdkSzF2NUpOMkRYSVBxSm1Wei81YQo3ZnlvZkhtcTFpUk1rZTRIOXQvNmlPNm8wRDNNRGpMRUVoRHU5S2dNR3JTRXhrVW9ZeUI1T05PTkUwU3QKc1pWYTZOWUcwdWZBRDdlVUFVUnFGNmwzR2ZSakFaNDNjMURhN3VBaE92ODU5NjZzQXJscE15WGxpNngyClo5eit6cXprNWhtbVJpd3pBMUdKTlZwL1ZaK24xaXVzV0drREg2OFU4K0JCOFhkQjd5SEY2Q3BIR1ErQwpZV3JXaWdvTGJzMG9FTzhzc3R6cFVZZnhFcVo5eDhkKzZtTzVNVGFMN05wVElEa3E0Zz09Cg==.4bf1a0a29032dd44f0abd46e0791c965\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Delta writes can result in the creation of empty files if the source is empty. This can happen with a regular Delta write or a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">MERGE INTO</span> (<a href=\"https://docs.databricks.com/sql/language-manual/delta-merge-into.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"MERGE INTO\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/delta-merge-into\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"MERGE INTO\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/sql/language-manual/delta-merge-into.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"MERGE INTO\">GCP</a>) operation.</p><p>If your streaming application is writing to a target Delta table and your source data is empty on certain micro batches, it can result in writing empty files to your target Delta table.\u00a0</p><p>Writing empty files to a Delta table should be avoided as they can cause performance issues (ex. too many small files, multiple unnecessary commits, etc.). If there were too many commits happening at a high frequency (either due to a very large inflow of high frequency events and/or due to a low streaming trigger frequency configuration), then it could result in too many small files on the target delta table. This too many small empty files can increase the overall listing time and thereby could hamper the subsequent read performance.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">The writing of empty files is a known issue in Databricks Runtime 7.3 LTS. Empty writes create additional files as well as new versions in Delta.</p><p>If there are 1000 empty writes in a day you see 1000 empty files created which accumulate over time. Even a table with just three records can result in several thousand empty files, depending on how frequently writes are performed.</p><p>For example, in this sample Delta commit, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">numOutputRows</span> is <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">0</span>, however <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">numTargetFilesAdded</span> is <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">1</span>. This means it has added one file, even though there are no output rows.</p><pre>Operation - Write\r\n\u00a0{\"numFiles\":\"1\",\"numOutputBytes\":\"2675\",\"numOutputRows\":\"0\"} OperationParameters{\"mode\":\"Append\",\"partitionBy\":\"[]\"}\r\n\r\nOperation - Merge\r\n{\"numOutputRows\":\"0\",\"numSourceRows\":\"0\",\"numTargetFilesAdded\":\"1\",\"numTargetFilesRemoved\":\"0\",\"numTargetRowsCopied\":\"0\",\"numTargetRowsDeleted\":\"0\",\"numTargetRowsInserted\":\"0\",\"numTargetRowsUpdated\":\"0\"}</pre><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You should upgrade your clusters to Databricks Runtime 9.1 LTS or above.</p><p>Databricks Runtime 9.1 LTS and above contains a fix for the issue and no longer creates empty files for empty writes.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">If you cannot upgrade to Databricks Runtime 9.1 LTS or above, you should periodically run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> (<a href=\"https://docs.databricks.com/sql/language-manual/delta-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/delta-optimize\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/sql/language-manual/delta-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">GCP</a>) on the affected table to clean up the empty files. This is not a permanent fix and should be considered a workaround until you can upgrade to a newer runtime.</p>\n</div>\n</div><p><br></p>", "body_txt": "Problem Delta writes can result in the creation of empty files if the source is empty. This can happen with a regular Delta write or a MERGE INTO (AWS | Azure | GCP) operation. If your streaming application is writing to a target Delta table and your source data is empty on certain micro batches, it can result in writing empty files to your target Delta table.\u00a0 Writing empty files to a Delta table should be avoided as they can cause performance issues (ex. too many small files, multiple unnecessary commits, etc.). If there were too many commits happening at a high frequency (either due to a very large inflow of high frequency events and/or due to a low streaming trigger frequency configuration), then it could result in too many small files on the target delta table. This too many small empty files can increase the overall listing time and thereby could hamper the subsequent read performance. Cause The writing of empty files is a known issue in Databricks Runtime 7.3 LTS. Empty writes create additional files as well as new versions in Delta. If there are 1000 empty writes in a day you see 1000 empty files created which accumulate over time. Even a table with just three records can result in several thousand empty files, depending on how frequently writes are performed. For example, in this sample Delta commit, numOutputRows is 0, however numTargetFilesAdded is 1. This means it has added one file, even though there are no output rows. Operation - Write \u00a0{\"numFiles\":\"1\",\"numOutputBytes\":\"2675\",\"numOutputRows\":\"0\"} OperationParameters{\"mode\":\"Append\",\"partitionBy\":\"[]\"} Operation - Merge {\"numOutputRows\":\"0\",\"numSourceRows\":\"0\",\"numTargetFilesAdded\":\"1\",\"numTargetFilesRemoved\":\"0\",\"numTargetRowsCopied\":\"0\",\"numTargetRowsDeleted\":\"0\",\"numTargetRowsInserted\":\"0\",\"numTargetRowsUpdated\":\"0\"} Solution You should upgrade your clusters to Databricks Runtime 9.1 LTS or above. Databricks Runtime 9.1 LTS and above contains a fix for the issue and no longer creates empty files for empty writes. Info\nIf you cannot upgrade to Databricks Runtime 9.1 LTS or above, you should periodically run OPTIMIZE (AWS | Azure | GCP) on the affected table to clean up the empty files. This is not a permanent fix and should be considered a workaround until you can upgrade to a newer runtime.", "format": "html", "updated_at": "2022-12-02T21:07:32.334Z"}, "author": {"id": 965161, "email": "rajeevkannan.thangaiah@databricks.com", "name": "Rajeev kannan Thangaiah", "first_name": "Rajeev kannan", "last_name": "Thangaiah", "role_id": "draft_writer", "created_at": "2022-08-03T00:44:50.300Z", "updated_at": "2023-04-20T21:52:06.303Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256849, "name": "Delta Lake", "codename": "delta", "accessibility": 1, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2957542, "name": "aws"}, {"id": 2957546, "name": "azure"}, {"id": 2957549, "name": "cluster"}, {"id": 2957547, "name": "dbr"}, {"id": 2957545, "name": "gcp"}, {"id": 2957550, "name": "runtime"}, {"id": 2957548, "name": "upgrade"}], "url": "https://kb.databricks.com/delta/delta-writing-empty-files-when-source-is-empty"}, {"id": 1507060, "name": "Get workspace configuration details", "views": 3293, "accessibility": 1, "description": "Display the complete configuration details for your Databricks workspace.", "codename": "get-workspace-configuration-details", "created_at": "2022-09-12T10:17:12.193Z", "updated_at": "2022-10-28T10:28:04.115Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlkMWQwaTIzNXhsRjBaS1hHellNQWRBandWcDZrR2h1MEJEZ3FvK2Z5R1hPRzVhWXJ1Cm40UkUrcFlzb1g4ZlZPU1RhWTY5dDMrU0Q3alJRT01YWmFySVhITk9GNVFLdUw4RllWRjNlV1dzb3hxTQpZU25qYjRVcnduSVpxbVkyd2VoR0YyUlpHNjBiV2NtbkE5S1hzQXAwUmF1N1hUQzd4b2Zkd09DdTJxemQKY1BzU1h6ekE2SlkvWW1iMFMzUWxyeVRGbmpibVkvUEkzRWNMK0VTSytTWUpoaTNDYzN6LzFSaUZwcHdxCmp4UDRBKzlDWktuUUFzQjJvdUFwS1ZsVzVETGNFUXRTYWsvMVpJL0VqRWRqRnBKaWNFWkVOMEZXbFkxUApvM0JabE5uellqdkUyc2p3M0QzYnNCL1VUU3M4aFBnK2J6Q0ZXT1FoRWpDODhxSXZLVnZBSDU0ekR2OWgKcnhZVGV1b1cxSjd2WUd1QXlsbG1UQmNIODJYQmNBMDBxN0dWTTRVOFpBTU42UWd0YXpGemJMYWxwOE9DClI5YlB5TEIrK0g0NWZLNi9mVHZ6YUhvSHNnTzQvTmVnK25DK1FlUEhZRzhXdkx6QXJBUDJiZENudktXWQppU0ZqY2tNcEtONWwwN2piY2FUYmp4azl0V3dBS1l2NzVDWFZSWW9VOElxNGRBSVV5elNRUkNVRjFNNzIKNkNkQ1YvNDJXRXJlejlVSzU5NS82WkNTL1Y5dkl1d0NKbFRxZjJodUdJd0RmUGxndlcrRGJ2UG4rdU1mCmc1Q3FEaHkvSndEeHhDOE50NkUrZjdYZXRSeTlOWnU1LzhCWHF1akphT1ZnQVRXK3ZVMi9BUEZXRW5kdAp2VDBFQU1LYkh3VHZ1OG1EQjFUcWw3blZuYy8vMXhvaVY4cWJpL2NZRVN0RDhVTHJrNy8zS2VqcjdnVjUKc0tCMVZuY1lFUjQ0a2IvRmx5dDBoRDArOU5jVWRMcm03Q2Rkb2FEM3NidWVyTFFUNUkvaGI3bkRYRlRICkdHNzFhK1dkS01HY213Vlp4dk8xN1NERXQ3L2JyZ3h4cjYyTE4yZTNSYlBXM3Fka21ZcVhTL2Jjbk5IQwpFT0dQOGhab0JPRlFjczhhemNjdVZISE9pVG5qbWVzRXJVN2ZFSEdrd2kwWUhkeVNPTGFoQjE4NjRkb08KeTJCUDZDaXpGb2V4VWFlK1hPV2I3cnZOVmJndmZNOXRBeWhwTGp1eTYwVEZ0ZFhqZkUyUGVwMmZFSFRKCm9TQnBIbEVjMWhtcjVGUjZWYkFxbGwxNkR0eHBrdk4zamcwTXpCT0U3WURmc2JxVlZlenpNUHBkR0k4aAp1VzZaY2ZibDBIN3NSNFI1ajh0MTFEMXpyTmZsSU04Si9Hc2dRMHRqYSs0LzhrMXFINnRodldLQmN0WnoKN1lxZHA3cnFUbVhRM2lVNWtpU3BlZDVOYkllYXh2ZkZEWVkyM2xWU3RFbHFjbmVGV0VIQWJUNUthQTdjCmdQa1pZZWhhQTlRdGs1dzBvekx4NnZlNWM0R3J5d0VSN3c9PQo=.efc81b7f7ef39771504c21baf2482d13\"></div><p>This article explains how to display the complete configuration details for your Databricks workspace.</p><p>This can be useful if you want to review the configuration settings and services that are enabled in your workspace. For example, you can use the workspace configuration details to quickly see if Unity Catalog or Identity Federation is enabled on your workspace.</p><p>Additionally, the workspace configuration contains cluster configuration information for the clusters in your workspace.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><ol>\n<li>Login to your Databricks workspace.</li>\n<li>Look at the URL displayed in your browser's address bar.</li>\n<li>Delete your <a href=\"find-your-workspace-id\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"workspace ID\">workspace ID</a> from the workspace URL.</li>\n<li>Append <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/config\u00a0</span>to the workspace URL, immediately after the instance name.<pre>https://&lt;databricks-instance&gt;/config</pre>\n</li>\n<li>Load the new URL to display the workspace configuration details.</li>\n</ol><p><br>The workspace configuration is displayed as plain text. It can be used to review all of the configuration settings and for the workspace.</p><h2 data-toc=\"true\" id=\"example-configuration-display-1\">Example configuration display</h2><div class=\"f-tab ui-sortable\" data-controller=\"tab\">\n<h2>AWS</h2>\n<p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1666952499834-AWS%20workspace%20config.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p>\n<i class=\"far fa-arrows-alt\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-tab fa-trash-alt\" contenteditable=\"false\" data-action=\"tab#delete\" href=\"#\">Delete</a>\n</div><p><br></p><div class=\"f-tab ui-sortable\" data-controller=\"tab\">\n<h2>Azure</h2>\n<p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1666952515247-Azure%20workspace%20config.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p>\n<i class=\"far fa-arrows-alt\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-tab fa-trash-alt\" contenteditable=\"false\" data-action=\"tab#delete\" href=\"#\">Delete</a>\n</div><p><br></p><div class=\"f-tab ui-sortable\" data-controller=\"tab\">\n<h2>GCP</h2>\n<p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1666952543268-GCP%20workspace%20config.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p>\n<i class=\"far fa-arrows-alt\" title=\"Move\" contenteditable=\"false\"></i><a class=\"delete-tab fa-trash-alt\" contenteditable=\"false\" data-action=\"tab#delete\" href=\"#\">Delete</a>\n</div><p><br></p><p>A few example properties are listed in the table below. This is a not a comprehensive list of all configuration settings.</p><table style=\"width: 100%;\"><tbody>\n<tr>\n<td><strong>Property</strong></td>\n<td><strong>Flag</strong></td>\n</tr>\n<tr>\n<td>List Account ID</td>\n<td>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">accountId</span><br>\n</td>\n</tr>\n<tr>\n<td>Check if Identity Federation is enabled</td>\n<td><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">identityFederationEnabled</span></td>\n</tr>\n<tr>\n<td>Check if Unity Catalog is enabled</td>\n<td><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">unityCatalogServiceEnabled</span></td>\n</tr>\n<tr>\n<td>Check if the Azure 'Manage Account' Tab is enabled</td>\n<td><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">enableAzureManageAccountTab</span></td>\n</tr>\n</tbody></table><p><br></p>", "body_txt": "This article explains how to display the complete configuration details for your Databricks workspace. This can be useful if you want to review the configuration settings and services that are enabled in your workspace. For example, you can use the workspace configuration details to quickly see if Unity Catalog or Identity Federation is enabled on your workspace. Additionally, the workspace configuration contains cluster configuration information for the clusters in your workspace. Instructions Login to your Databricks workspace.\nLook at the URL displayed in your browser's address bar.\nDelete your workspace ID from the workspace URL.\nAppend /config\u00a0to the workspace URL, immediately after the instance name.https://&lt;databricks-instance&gt;/config Load the new URL to display the workspace configuration details. The workspace configuration is displayed as plain text. It can be used to review all of the configuration settings and for the workspace. Example configuration display AWS Delete Azure Delete GCP Delete A few example properties are listed in the table below. This is a not a comprehensive list of all configuration settings. Property Flag List Account ID accountId Check if Identity Federation is enabled identityFederationEnabled Check if Unity Catalog is enabled unityCatalogServiceEnabled Check if the Azure 'Manage Account' Tab is enabled enableAzureManageAccountTab", "format": "html", "updated_at": "2022-10-28T10:28:04.112Z"}, "author": {"id": 789487, "email": "kavya.parag@databricks.com", "name": "kavya.parag ", "first_name": "kavya.parag", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T06:35:45.179Z", "updated_at": "2023-03-29T14:10:28.525Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256864, "name": "Notebooks", "codename": "notebooks", "accessibility": 1, "description": "These articles can help you with your Databricks notebooks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2921709, "name": "aws"}, {"id": 2921710, "name": "azure"}, {"id": 2921711, "name": "gcp"}], "url": "https://kb.databricks.com/notebooks/get-workspace-configuration-details"}, {"id": 1505799, "name": "Cluster init script fails with mirror sync in progress error", "views": 3995, "accessibility": 1, "description": "If the mirror you are using is not in sync with the main repository, apt-get update returns a Mirror sync in progress error.", "codename": "cluster-init-script-fails-with-mirror-sync-in-progress-error", "created_at": "2022-09-09T17:48:52.303Z", "updated_at": "2022-10-31T22:49:36.487Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"eb20103deb3a4\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStrUmpkWUh6clNsN1BzeDc2NFB2MkFwa0xmRDRYUTFNaCtOUC9jUm5sZnpua3ZjeXN4Ckp5eDJoUGI4c0tlOUxFQWVNbG5DVW5BSFh5ZEI2K01IcGRxV3VSeGFqTXM4TDkzNno3Zm5Sdm5ndHE0OAo5NTZ3SGs4UGw2R3lzS3VMeTA5eTRmSm9mRUoyRWRZNzlqaU9mRFdRdVdUYWJ1d0tCRGRIb2pSY3BFL3AKcDZEMy95TEtUUXNtdE52Sm1Nd0xLMGlidDl2eVFyc080Z00xMlBLOVVDT0ZVaGdXU3JVZFdVWVBPQ1VECmhGWGdVNU93ejlJSG5XNnhwbThXRk5PQTRtTm9aQmw4OGx5MlE1dlNZMzVNQ2VFazRoYXF2aGJQV0FZOQpLbW1CU0lwTlZDUDcvNEpvamVPOHIvVE1QVTlyeXBOWno3eVpheFdzcDZ6b2EvY1k2a1d6WUVUUjVNaFIKd0s0TjZjbElwaHlpeVB6Yi9HKzFkcGhGaXRQamRvbStKTmhuU1lSeWhLa3NWZ3czblpQZXIxZXprT1RQCnhYUE5HdnhvNENLTW1FckNIRU0wZEZVeXhlWTJRak1IMkZnRjF2WWRVRzJZb3V5bE1kQ2cxdjNZWWdqQQpKeWdDNjMyaGo0azA3Vy9rTWpRU2tzQnd4d21pL1lxT3VUczdpQTZLYzlXbWhpakRGY3FPVGJiRzV2THAKM2lIL0lwZ05teEJIQ2FRSCtzMEFUWktRdEJpVkM1T2hheUFFODRCVHhOcUlKbFhQcWpJa1BWTkZpQS9wClN1QzN5S0tIdXhTN0h6Y0FPa0lUNUdZK0dNU3MyZC9iUjNnM0xYenp0QjJGdTRlU2FXWkZMREVBUGVtbApyOFpqQ2Juckkwa240cHJMd2V6TEtFbUllbEdUV3F1akF1N3VPOUFodGtYZVZmZnJ2bGpJaHZRSFdFQ3kKdnhYNmw0TThDZHdMbFRpY21pVzdYR1k4emIxZktHd1pYTVlBZ0tqcDNsU3VZbVVYcGFDTWIvVHNpaEthCjhWc2VobVdDNENWcTVoZmxua3dSdUowRmhkZUxVdkdqV3RUZEM3UFJNd1JoZXJoVTZaOVhlbU0vUHp1cQo0dHoyWU1TZUdSZFpLU3hEaTM0UDVJV2VyTktFS0JEYldqaEc3NFI4WEdFRlpYSFNTUGU4RUpBWWhITlYKekswYXJRaU5xeXdURDE4UXdscVBVWFV2cFVXS3lOVTRQdGdCbUh1U2VQdStKSXUzR1Q3bmdKYUs5MjFuClRJdEtldmZJdENoUmxZNlNOeDMxZ1BpUnRIbGR3cDlGMWpLOStFTXN4RzRQem9HeDR0d2thRHZ1ZkRMeQppNzZtSlExdGFkZjBwdzRkZmlVNElPdVhBMG9wUEJJTndsR2d6bVBkZXFZZ3B5ekVQaGNSREF3T25sMVIKM0lEN3JLd014T2xHUWVqc3BjVmhNbjhwSzlvRXdrQWFpZVhZa0w3ZG9vT0Nsc2FuS2R5UElCSElJdWVHCmx5cGZhZ3oxTGdFblAvUUxRLzVTeFZDekRWZjh4WWZVa3lkZFZnNWJ1aUw3dDg4anpOZE9YOHFlTVp0NwpOWnB1ZXkrRHpmemN3TzU1VG9JaVBwTEtyV1JyRGtlWWF0Z2N2cnBhZEJKd2xjRDl0ODNQT3hSRENXZk8Kb3JyU2pzSVIyUHI3WnU2cHZFN2gzMTRxSDBsbVZpbjkxVzJRbEFyU2psUGhEZVZyWlBTdWlKS3dYemE1Ci96a3llS2gwMkFEcEwxSysrblk1Z3c4Qk84QXpCR1R2cXkyckNvRmFBMDdFZHV1TDlvaXVHL2MxRWN6eApjVGR2ZmlHcmhTSFZSdXQ5ajVCVy9PYlV4MnpPWUNkZW1tNjVHM21PTzFUVW9Lb1I2RmJYRDY0Njl5dXoKMGVka0V4dWRHYko4Q0FqOUx3UkFsK3RIK0RZbWRCQ2cK.6f0958504dffb1ce4275423270383177\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are using a custom init script running at cluster start to install a custom library. It works most of the time, but you encounter intermittent failures when <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">apt-get update</span> runs in the init script.</p><p>The failures return a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Mirror sync in process</span> error message.</p><pre>Failed to fetch https://repos.&lt;site&gt;.com/zulu/deb/dists/stable/main/binary-amd64/by-hash/SHA512/&lt;filename&gt;\u00a0 File has unexpected size (228870 != 201863). Mirror sync in progress? File has unexpected size (228870 != 201863). Mirror sync in progress? [IP: 123.45.67.89 443]</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This can happen if you are trying to download from a mirror that is not in sync with the main repository. Official repositories usually resolve the issue within 30 minutes, however in rare cases it can take much longer.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Wait for the mirror to finishing synchronizing with the repository before attempting to start your cluster.</p><p>Alternatively, if the library causing the failure is no longer needed, you can edit your init script to remove the reference to the problematic repo. You should only take this step if you are positive the library is not used directly or as a dependency.</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-3\">Warning</h3>\n<p class=\"hj-alert-text\">If the repo you are using has multiple mirrors, you can edit <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/etc/apt/sources.list</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/etc/apt/sources.list.d</span> to remove the problematic mirror and point to another mirror instead. This allows the init script to complete.</p>\n<p class=\"hj-alert-text\">This is not recommended as a long term solution.</p>\n</div>\n</div><p><br></p>", "body_txt": "Problem You are using a custom init script running at cluster start to install a custom library. It works most of the time, but you encounter intermittent failures when apt-get update runs in the init script. The failures return a Mirror sync in process error message. Failed to fetch https://repos.&lt;site&gt;.com/zulu/deb/dists/stable/main/binary-amd64/by-hash/SHA512/&lt;filename&gt;\u00a0 File has unexpected size (228870 != 201863). Mirror sync in progress? File has unexpected size (228870 != 201863). Mirror sync in progress? [IP: 123.45.67.89 443] Cause This can happen if you are trying to download from a mirror that is not in sync with the main repository. Official repositories usually resolve the issue within 30 minutes, however in rare cases it can take much longer. Solution Wait for the mirror to finishing synchronizing with the repository before attempting to start your cluster. Alternatively, if the library causing the failure is no longer needed, you can edit your init script to remove the reference to the problematic repo. You should only take this step if you are positive the library is not used directly or as a dependency. Warning\nIf the repo you are using has multiple mirrors, you can edit /etc/apt/sources.list or /etc/apt/sources.list.d to remove the problematic mirror and point to another mirror instead. This allows the init script to complete.\nThis is not recommended as a long term solution.", "format": "html", "updated_at": "2022-10-31T22:49:36.455Z"}, "author": {"id": 976862, "email": "harrison.schueler@databricks.com", "name": "harrison.schueler ", "first_name": "harrison.schueler", "last_name": "", "role_id": "draft_writer", "created_at": "2022-08-24T17:13:08.035Z", "updated_at": "2023-04-13T19:13:55.472Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2921608, "name": "aws"}, {"id": 2921609, "name": "azure"}, {"id": 2921610, "name": "gcp"}, {"id": 2921614, "name": "init"}, {"id": 2921613, "name": "libraries"}, {"id": 2921611, "name": "mirror"}, {"id": 2921612, "name": "sync"}], "url": "https://kb.databricks.com/clusters/cluster-init-script-fails-with-mirror-sync-in-progress-error"}, {"id": 1501547, "name": "Pin cluster configurations using the API", "views": 3899, "accessibility": 1, "description": "Pin up to 100 compute cluster configurations using the API.", "codename": "pin-cluster-configurations-using-the-api", "created_at": "2022-09-07T12:21:45.091Z", "updated_at": "2022-12-21T09:36:50.838Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStldFBQNGFmbUNHTEFhbGdCMkxJNWJaUHJrcGo5MFBsYVpkcXU0ekp6R1p3V1c4b0c2Cnh5YWZWN0JvUkV1WWRhVks0SjJRTjc2dGdRUVEvRjlHRk9ZWXJnR2xSY3R6TTdLSUhWRjEzVXlQbVllNApyQkM5TnhPUzRFdmh1LzgzZDNZOEdhd05IVWJHckxQTE5HbFYxbXFaRjNtbS9CMXB5bFRVVUlvSmZHcDgKNGg2QzhJQWhSdG8wWERBYmJZWjVSWGxNTWRBK2tGN0VGeDZDUE53NDZtNGw2eEhaVTN4WUcvbGZsUEt2CldiaXlEcUgzMy96NDlxc3FIcWFIbm9Qd0s3RG9BYlF6b2krMW9Cb0trNFM2OHB3ZU9Ld3NaMlloaGpiRwpsOFNDZFcyYWRTang4U1RzWDlVV0hDcmJyazNTeDJVV2hUVE1oSzdHWGlHR2NVV2E5YU1HbEVxMkdQc00KRUw3RUFVTzRGV3JxUFlZMEhabk1GZkdiUERRTjVvK0xPa3h3dk83SitBblRRTEFUK1kweGZOS0poTTRsCm1ZTEE4blBmNUUwcHlUd2l4OVJ0UnpNblM2U280OE16VUF5ZHBNOEFPVDB0cmhHZXk4SHFGczIwVVZFUApGVmpMVXlORlZUQ25xK3ZzMmxWanc3cytPeHBvVXVkK0l1ZDVqMTNnR25VZHkzaDFJdW0xcmpiREJ2bjkKQ25VVVNkVnQxYU95VXBiYjdWTytNam5KenZSTDNzamQ3TzJSVVExYi84YmcvZVlnSi8wNTBhc2tjZDVjCmVVRHoxMGVxWXU2K0VwckE0cjRSRmF3ZWd2YWJaVVBUeENBc3Y1UGxWQVcwcHhBTlkwRWFYNWFleS8ycAo1NUxFbE4vNnlTajN5UEl1Ti8xZ3NIelJCbEVWVVZENzh5eVlTcWQ3eDZsOTZaeGlYL1hiQzBIUTFJbjIKelpEMGQyUDJkanN2ZE43R3cyNDlqTDZzQlV6TzkvWFc4QVdJSWdIZDhjQjlwZ0xXeTdpbHo5TUFnbkR6CkNKMWhnSzh1bkNRMXk5ZEN6d0FUeFJZUVJqVjNBQ3QyUEJEWU1qWk1kZWxuTnR3VTl2a0ZuZ2pPaUpNcApaQUI1R3Y5c3dpMnUva3o4RW1OOW1WRzdlMjcyZnMvVTdIQ2VvdlN3THl0M2RhQUt0U0NIYUV6bXQza3AKaHBaVjFDS3lmWGNESHJKaytEd3ZBcE9zeEtidjNsYnZVMkxpR24rSXB1ZHRYQUJ1djBVK2p1Y1JWSmNOCmFKdHUzazJBbW5EOUpOTjZaVjdNOWpzZ1NvaHJMYWMzcm1ZYTFVOFZyZG1xZ3plYnZ0YW8rYXFnSnRVNApnZkZIN1NSbEZsNGtlb1NYcXVlK3RGZjdGZCtTV3VyU3VKYWxwalMyWWhXTlo4cS9UamM0aEVERG1ObTAKT0NsU0pjQ0h5YkM2RXJoSGJUc285dElhVUhLWjRyNmJHNHNFU3o5YWdZeURNV2hsb01rNUV0TGpoZ1FoCldISWtZRDhWT3ZNbAo=.05f94b47b2c5a1d12b337a49db4d9de8\"></div><p>Normally, cluster configurations are automatically deleted 30 days after the cluster was last terminated.</p><p>If you want to keep specific cluster configurations, you can pin them. Up to 100 clusters can be pinned.</p><p>Pinned clusters are not automatically deleted, however they can be manually deleted.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-0\">Info</h3>\n<p class=\"hj-alert-text\">You must be a Databricks administrator to pin a cluster.</p>\n</div>\n</div><p>You can easily pin a cluster (<a href=\"https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pin a cluster\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/clusters-manage#--pin-a-cluster\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pin a cluster\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/clusters-manage.html#pin-a-cluster\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pin a cluster\">GCP</a>) via the workspace UI, but if you are managing your clusters via the API, you can also use the Pin endpoint (<a href=\"https://docs.databricks.com/dev-tools/api/latest/clusters.html#pin\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Pin endpoint\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/clusters#--pin\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Pin endpoint\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/api/latest/clusters.html#pin\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Pin endpoint\">GCP</a>) in the Clusters API.</p><h1 data-toc=\"true\" id=\"instructions-1\">Instructions</h1><h2 data-toc=\"true\" id=\"pin-all-unpinned-clusters-2\">Pin all unpinned clusters</h2><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">A maximum of 100 clusters can be pinned. If you attempt to \"pin all\" via the API, the clusters are pinned based on the order in which they are listed in the response. Once a total of 100 clusters are pinned, no more can be pinned until some are removed.</p>\n</div>\n</div><p>Use the following sample code to pin all unpinned clusters in your workspace.</p><p>Before running the sample code, you will need a personal access token (<a href=\"https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"personal access token\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/authentication#--generate-a-personal-access-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"personal access token\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"personal access token\">GCP</a>) and your workspace domain. The workspace domain is just the domain name.</p><ol>\n<li>Copy and paste the sample code into a notebook cell.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;workspace-domain-without-trailing-backlash&gt;</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;personal-access-token&gt;</span> values.</li>\n<li>Run the cell to pin all unpinned clusters in your workspace.</li>\n</ol><pre data-stringify-type=\"pre\" id=\"isPasted\">%python\r\n\r\nimport requests\r\nworkspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\"\r\naccess_token = \"&lt;personal-access-token&gt;\"\r\n\r\nurl = workspace_url + \"/api/2.0/clusters/list\"\r\n\r\nheaders = {\r\n 'Authorization': 'Bearer ' + access_token\r\n}\r\n\r\ncluster = requests.request(\"GET\", url, headers=headers).json()\r\nfor unpinned in cluster[\"clusters\"]:\r\n    if not 'pinned_by_user_name' in unpinned :\r\n        print(\"Pinning\"+\" , \"+ unpinned[\"default_tags\"]['ClusterName'])\r\n        url = workspace_url + \"/api/2.0/clusters/pin\"\r\n        requests.post(url,json={\"cluster_id\" : unpinned[\"cluster_id\"]},headers=headers)</pre><h2 data-toc=\"true\" id=\"pin-a-cluster-by-name-4\">Pin a cluster by name</h2><p id=\"isPasted\">Use the following sample code to pin a specific cluster in your workspace.</p><p>Before running the sample code, you will need a personal access token and your workspace domain. The workspace domain is just the domain name.</p><ol>\n<li>Copy and paste the sample code into a notebook cell.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;workspace-domain-without-trailing-backlash&gt;</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;personal-access-token&gt;</span> values.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;cluster-name-to-pin&gt;</span> value with the name of the cluster you want to pin.</li>\n<li>Run the cell to pin the selected cluster in your workspace.</li>\n</ol><pre data-stringify-type=\"pre\" id=\"isPasted\">%python\r\n\r\nimport requests\r\nworkspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\"\r\naccess_token = \"&lt;personal-access-token&gt;\"\r\n\r\nurl = workspace_url + \"/api/2.0/clusters/list\"\r\n\r\nheaders = {\r\n 'Authorization': 'Bearer ' + access_token\r\n}\r\n\r\ncluster = requests.request(\"GET\", url, headers=headers).json()\r\n\r\nfor unpinned in cluster[\"clusters\"]:\r\n    if not 'pinned_by_user_name' in unpinned :\r\n        if unpinned[\"default_tags\"]['ClusterName'] == \"&lt;cluster-name-to-pin&gt;\" :\r\n        \u00a0\u00a0\u00a0\u00a0print(\"Pinning\"+\" , \"+ unpinned[\"default_tags\"]['ClusterName'])\r\n        \u00a0\u00a0\u00a0\u00a0url = workspace_url + \"/api/2.0/clusters/pin\"\r\n        \u00a0\u00a0\u00a0\u00a0requests.post(url,json={\"cluster_id\" : unpinned[\"cluster_id\"]},headers=headers)</pre><h2 data-toc=\"true\" id=\"pin-all-clusters-by-a-specific-user-5\">Pin all clusters by a specific user</h2><p id=\"isPasted\">Use the following sample code to pin a specific cluster in your workspace.</p><p>Before running the sample code, you will need a personal access token and your workspace domain. The workspace domain is just the domain name.</p><ol>\n<li>Copy and paste the sample code into a notebook cell.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;workspace-domain-without-trailing-backalsh&gt;</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;personal-access-token&gt;</span> values.</li>\n<li>Update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;cluster-creator-username&gt;</span> value with the name of the user whose clusters you want to pin.</li>\n<li>Run the cell to pin the selected clusters in your workspace.</li>\n</ol><pre id=\"isPasted\">%python\r\n\r\nimport requests\r\nworkspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\"\r\naccess_token = \"&lt;personal-access-token&gt;\" \u00a0\r\n\r\nurl = workspace_url + \"/api/2.0/clusters/list\"\r\n\r\nheaders={\r\n\u00a0'Authorization': 'Bearer ' + access_token\r\n}\r\n\r\ncluster = requests.request(\"GET\", url, headers=headers).json()\r\n\r\nfor unpinned in cluster[\"clusters\"]:\r\n\u00a0 \u00a0 \u00a0if not 'pinned_by_user_name' in unpinned :\r\n\u00a0 \u00a0 \u00a0 \u00a0 if unpinned[\"creator_user_name\"] == \"&lt;cluster-creator-username&gt;\" :\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 url = shard_url + \"/api/2.0/clusters/pin\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests.post(url,json={\"cluster_id\" : unpinned[\"cluster_id\"]},headers=headers)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0print(\"Pinning\"+\" , \"+ unpinned[\"default_tags\"]['ClusterName'])</pre><p><br></p>", "body_txt": "Normally, cluster configurations are automatically deleted 30 days after the cluster was last terminated. If you want to keep specific cluster configurations, you can pin them. Up to 100 clusters can be pinned. Pinned clusters are not automatically deleted, however they can be manually deleted. Info\nYou must be a Databricks administrator to pin a cluster. You can easily pin a cluster (AWS | Azure | GCP) via the workspace UI, but if you are managing your clusters via the API, you can also use the Pin endpoint (AWS | Azure | GCP) in the Clusters API. Instructions Pin all unpinned clusters Info\nA maximum of 100 clusters can be pinned. If you attempt to \"pin all\" via the API, the clusters are pinned based on the order in which they are listed in the response. Once a total of 100 clusters are pinned, no more can be pinned until some are removed. Use the following sample code to pin all unpinned clusters in your workspace. Before running the sample code, you will need a personal access token (AWS | Azure | GCP) and your workspace domain. The workspace domain is just the domain name. Copy and paste the sample code into a notebook cell.\nUpdate the &lt;workspace-domain-without-trailing-backlash&gt; and &lt;personal-access-token&gt; values.\nRun the cell to pin all unpinned clusters in your workspace. %python import requests workspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\" access_token = \"&lt;personal-access-token&gt;\" url = workspace_url + \"/api/2.0/clusters/list\" headers = { 'Authorization': 'Bearer ' + access_token } cluster = requests.request(\"GET\", url, headers=headers).json() for unpinned in cluster[\"clusters\"]: if not 'pinned_by_user_name' in unpinned : print(\"Pinning\"+\" , \"+ unpinned[\"default_tags\"]['ClusterName']) url = workspace_url + \"/api/2.0/clusters/pin\" requests.post(url,json={\"cluster_id\" : unpinned[\"cluster_id\"]},headers=headers) Pin a cluster by name Use the following sample code to pin a specific cluster in your workspace. Before running the sample code, you will need a personal access token and your workspace domain. The workspace domain is just the domain name. Copy and paste the sample code into a notebook cell.\nUpdate the &lt;workspace-domain-without-trailing-backlash&gt; and &lt;personal-access-token&gt; values.\nUpdate the &lt;cluster-name-to-pin&gt; value with the name of the cluster you want to pin.\nRun the cell to pin the selected cluster in your workspace. %python import requests workspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\" access_token = \"&lt;personal-access-token&gt;\" url = workspace_url + \"/api/2.0/clusters/list\" headers = { 'Authorization': 'Bearer ' + access_token } cluster = requests.request(\"GET\", url, headers=headers).json() for unpinned in cluster[\"clusters\"]: if not 'pinned_by_user_name' in unpinned : if unpinned[\"default_tags\"]['ClusterName'] == \"&lt;cluster-name-to-pin&gt;\" : \u00a0\u00a0\u00a0\u00a0print(\"Pinning\"+\" , \"+ unpinned[\"default_tags\"]['ClusterName']) \u00a0\u00a0\u00a0\u00a0url = workspace_url + \"/api/2.0/clusters/pin\" \u00a0\u00a0\u00a0\u00a0requests.post(url,json={\"cluster_id\" : unpinned[\"cluster_id\"]},headers=headers) Pin all clusters by a specific user Use the following sample code to pin a specific cluster in your workspace. Before running the sample code, you will need a personal access token and your workspace domain. The workspace domain is just the domain name. Copy and paste the sample code into a notebook cell.\nUpdate the &lt;workspace-domain-without-trailing-backalsh&gt; and &lt;personal-access-token&gt; values.\nUpdate the &lt;cluster-creator-username&gt; value with the name of the user whose clusters you want to pin.\nRun the cell to pin the selected clusters in your workspace. %python import requests workspace_url = \"&lt;workspace-domain-without-trailing-backlash&gt;\" access_token = \"&lt;personal-access-token&gt;\" \u00a0 url = workspace_url + \"/api/2.0/clusters/list\" headers={ \u00a0'Authorization': 'Bearer ' + access_token } cluster = requests.request(\"GET\", url, headers=headers).json() for unpinned in cluster[\"clusters\"]: \u00a0 \u00a0 \u00a0if not 'pinned_by_user_name' in unpinned : \u00a0 \u00a0 \u00a0 \u00a0 if unpinned[\"creator_user_name\"] == \"&lt;cluster-creator-username&gt;\" : \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 url = shard_url + \"/api/2.0/clusters/pin\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests.post(url,json={\"cluster_id\" : unpinned[\"cluster_id\"]},headers=headers) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0print(\"Pinning\"+\" , \"+ unpinned[\"default_tags\"]['ClusterName'])", "format": "html", "updated_at": "2022-12-21T09:36:50.830Z"}, "author": {"id": 790705, "email": "simran.arora@databricks.com", "name": "simran.arora ", "first_name": "simran.arora", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T08:29:23.302Z", "updated_at": "2023-04-25T08:04:15.648Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2978859, "name": "100"}, {"id": 2978851, "name": "aws"}, {"id": 2978852, "name": "azure"}, {"id": 2978856, "name": "cluster"}, {"id": 2978857, "name": "compute"}, {"id": 2978853, "name": "gcp"}, {"id": 2978858, "name": "limit"}, {"id": 2978854, "name": "pin"}, {"id": 2978855, "name": "unpin"}], "url": "https://kb.databricks.com/clusters/pin-cluster-configurations-using-the-api"}, {"id": 1498572, "name": "ANSI compliant DECIMAL precision and scale", "views": 4268, "accessibility": 1, "description": "Learn how to enable ANSI compliant error messages when incorrect values are used for DECIMAL precision and scale.", "codename": "ansi-compliant-decimal-precision-and-scale", "created_at": "2022-09-05T13:04:47.930Z", "updated_at": "2022-10-29T21:53:37.135Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.b036af4a8750148b935b3d4a27e5d863\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to cast a value of one or greater as a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DECIMAL</span> using equal values for both precision and scale. A null value is returned instead of the expected value.</p><p>This sample code:</p><pre>%sql\r\n\r\nSELECT CAST (5.345 AS DECIMAL(20,20))</pre><p>Returns:</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1665746312648-Screen%20Shot%202022-10-14%20at%204.18.04%20AM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Image showing a null value is returned.\"></p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DECIMAL</span> type (<a href=\"https://docs.databricks.com/sql/language-manual/data-types/decimal-type.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"DECIMAL type\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/data-types/decimal-type\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"DECIMAL type\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/sql/language-manual/data-types/decimal-type.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"DECIMAL type\">GCP</a>) is declared as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DECIMAL(precision, scale)</span>, where precision and scale are optional.</p><p>Precision represents the total number of digits in the value of the variable. This includes the whole number part and the fractional part.</p><p>The scale represents the number of fractional digits in the value of the variable. Put simply, this is the number of digits to the right of the decimal point.<br data-aura-rendered-by=\"421:769;a\"><br data-aura-rendered-by=\"421:769;a\">For example, the number 123.45 has a <strong>precision</strong> of five (as there are five total digits) and a <strong>scale</strong> of two (as only two digits are on the\u00a0right-hand\u00a0side of the decimal point).</p><p>When the precision and scale are equal, it means the value is less than one, as all digits are used for the fractional part of the number. For example, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DECIMAL(20, 20)</span> defines a value with 20 digits and 20 digits to the right of the decimal point. All 20 digits are used to represent the fractional part of the number, with no digits used for the whole number.</p><p>If the precision in the value overflows the precision defined in the datatype declaration, null is returned instead of the fractional decimal value.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.ansi.enabled</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span> in your cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>).\u00a0</p><p>This enables Spark SQL ANSI compliance.</p><p>For more information, review the <a href=\"https://spark.apache.org/docs/3.0.0/sql-ref-ansi-compliance.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">ANSI compliance</a> documentation.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">You can also set this value at the notebook level using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.conf.set(\"spark.sql.ansi.enabled\", \"True\")</span> in a Python cell if you don't have the ability to edit the cluster's <strong>Spark config</strong>.</p>\n</div>\n</div><p>Once ANSI compliance is enabled, passing incorrect precision and scale values returns an error indicating the correct value.</p><p>For example, this sample code:</p><pre>%sql\r\n\r\nSELECT CAST (5.345 AS DECIMAL(20,20))</pre><p>Returns this error message:</p><pre>Error in SQL statement: SparkArithmeticException: [CANNOT_CHANGE_DECIMAL_PRECISION] Decimal(expanded, 5.345, 4, 3) cannot be represented as Decimal(20, 20).</pre><p><br></p><p>However, this sample code:</p><pre>SELECT CAST (5.345 AS DECIMAL(4,3))</pre><p>Returns the expected result:</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1665746662666-Screen%20Shot%202022-10-14%20at%204.24.06%20AM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Image showing the expected value is returned.\"></p><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"048182c637a15\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlRYnZKUUY0cmFIbVdreXI1ekJ6aG11d3phRlZESkczUllRSm9idzVKSmx4MExhQ3QrClAwZzVXcEd6a2FYZi9vb3FYM1NNNnZ1eUFTSmFQUG1iSGorbUptYTR4Y2o1dnNndUZRdm9IczBDaGwvTgptZlE0YVZLOXlUM0RIbXhJSTBVN2hYRjdtSkdiV1FNbjVMY2FHV1Q4bGlUWkVxT1BmdC9ZQ2kzKzBESkQKTmtaNmVKZ3ptQTRIVmRaNHluSXJLQWRZa2NJaG5SY3oyQjBzNlQ1S3lYNkFCS3R1eFVKOEdrSCs0UGVqCkE5enM5N1drMXpDS1QyQVYrc0xHTzhjbWRGaGRiNjl1dVlLU1pyN282cUNIaHdVUmJjTzhWZDBiTDVEUgpEVzlDdDZsK1hoU3BIZ01KOTJ2Y1JyTlNqSTdtSE1rVzMxeFc4QVlhdlI1WUhSR3ZTdlZ1c0dVOWV3OVIKUjdvZkRmYmVzNG12R0prSEJoaGFWYnV4Z2NjbnlSWDRNYmoxa295NE5ub2VMVW9Va3RDcFVQbTV5S0RuCmplR3NYNTdhUEI4SXNENlFkSzFaQWlLTjBqR2R2NFJONGY4SUIzZjA4em94SGEvamZieENaRW03M1BjRwpENUdpcDFla0NMR0dZdDNKSkhwTXdqOTE2eE5BVWVvOWRoNnZod3lOK1h1ejkxajkrdmNDQkM3ZGk0cTQKNUxtNGdVbWFHZ3E3K3lqdzJVd3NJY3ppdFZxeGo0YzNKWUYrZHNBSGFBaFF3NXF4QmhMaldEcG5LUGtaCmZGOEI1Mk9MRjF1Wk1BNzYrZ2pPbytFNjUzeU9mSC9BYlJZd3AycmhCaUlvR281L0lkRGdnS1owZGJuWQpOUXA1VDJZcnAzZWMxbEVjTlZTcmpEL2dSdXlWNENMdG83OFhaYjNDVUlIZGdwcE1yVFg3VG0zRnZwZ3gKSGVGaWZzbENuUUtYREtXKyt6U004aWZveFNxdU5DSXYyamZsRUhjdk44ek9iS1cvMWEzVGJTT1d3cTlGClBLSFcwZDFVVlVyRXJOdWV5V25xUEdOaDlheTd2OUVvUzR6ZW4wWm9MUk9hOUwxTmRmWWttelRrS3VNZgpxeDNzM3RCVVVnaS9lWTFHQ1A1YXljV0lhejdkWllpRWhSWXNMeEdCcmoxL1U4bjBPMlRLK0hjTFZCdXkKdUg1VS9LYjMwMVhWbmpUNVE1YVE2empWaDA2VlZlZmM1N3VFUTRBMGFLbm1jbTVtNHBqQk9FWk5ETWpNClp5U0NwTHRSSXBoTEsvYlFaYThseXBXRlJ3d0FscU95NkZodXpoT1ZOTW89Cg==.de5151751f0948bf7ad8ebccd0b9d1a2\"></div>", "body_txt": "Problem You are trying to cast a value of one or greater as a DECIMAL using equal values for both precision and scale. A null value is returned instead of the expected value. This sample code: %sql SELECT CAST (5.345 AS DECIMAL(20,20)) Returns: Cause The DECIMAL type (AWS | Azure | GCP) is declared as DECIMAL(precision, scale), where precision and scale are optional. Precision represents the total number of digits in the value of the variable. This includes the whole number part and the fractional part. The scale represents the number of fractional digits in the value of the variable. Put simply, this is the number of digits to the right of the decimal point. For example, the number 123.45 has a precision of five (as there are five total digits) and a scale of two (as only two digits are on the\u00a0right-hand\u00a0side of the decimal point). When the precision and scale are equal, it means the value is less than one, as all digits are used for the fractional part of the number. For example, DECIMAL(20, 20) defines a value with 20 digits and 20 digits to the right of the decimal point. All 20 digits are used to represent the fractional part of the number, with no digits used for the whole number. If the precision in the value overflows the precision defined in the datatype declaration, null is returned instead of the fractional decimal value. Solution Set spark.sql.ansi.enabled to true in your cluster's Spark config (AWS | Azure | GCP).\u00a0 This enables Spark SQL ANSI compliance. For more information, review the ANSI compliance documentation. Info\nYou can also set this value at the notebook level using spark.conf.set(\"spark.sql.ansi.enabled\", \"True\") in a Python cell if you don't have the ability to edit the cluster's Spark config. Once ANSI compliance is enabled, passing incorrect precision and scale values returns an error indicating the correct value. For example, this sample code: %sql SELECT CAST (5.345 AS DECIMAL(20,20)) Returns this error message: Error in SQL statement: SparkArithmeticException: [CANNOT_CHANGE_DECIMAL_PRECISION] Decimal(expanded, 5.345, 4, 3) cannot be represented as Decimal(20, 20). However, this sample code: SELECT CAST (5.345 AS DECIMAL(4,3)) Returns the expected result:", "format": "html", "updated_at": "2022-10-29T21:53:37.130Z"}, "author": {"id": 789805, "email": "saritha.shivakumar@databricks.com", "name": "saritha.shivakumar ", "first_name": "saritha.shivakumar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T14:42:15.447Z", "updated_at": "2023-04-17T04:09:51.795Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2913890, "name": "aws"}, {"id": 2913891, "name": "azure"}, {"id": 2913895, "name": "cast"}, {"id": 2913893, "name": "decimal"}, {"id": 2913892, "name": "gcp"}, {"id": 2913894, "name": "null"}], "url": "https://kb.databricks.com/sql/ansi-compliant-decimal-precision-and-scale"}, {"id": 1496369, "name": "Use audit logs to identify who deleted a cluster", "views": 4406, "accessibility": 1, "description": "You can use audit logs to identify who deleted a cluster configuration.", "codename": "use-audit-logs-to-identify-who-deleted-a-cluster", "created_at": "2022-09-02T18:45:03.134Z", "updated_at": "2022-10-31T23:49:56.947Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStZQjdDYVF6VEFhSGM1cTdVSlFVUGxyYko4a0pjSzZxY2hrSmhYSDRJN1c3NEc5alg1CnNWMFJ3cUxRa2lMNjdmN0xpU29TM1U4WDZpNGdxbUVIUVlqL1N0ODV3MDdqb2tLbVZIM2pZVFlXSDVEcQp5ajZZaHRrak1qMjV5YWlad0U4ejU5a0xRK2xVbjVTVURQelpCRWpVLzVxVDVKYnhOenNyMjg4a3BWYlIKKzFQV3ErZkE5ZVVJTDBndDMxcktUWmtNSWxlMXpNZm1yQlA4RTFYVnk4NCtab2RVam5RYitEdU1KTEZECisrenFsVGFEYnRrdVd5ZUw4QjNjZEZreFMyZFlva2dxZ1NzUXBJZEhrdUF5cjRUNlQ5M1JIZWRlcWFwcApjZmhiamFRNkIvQ0w3RjJmUWdRT2Y3MlRoWHUyeUNIb1Vib045dCtzYVQvbnhIaU5ldk8rZmR0NTFNUDIKNjV5TWU1cExsTGxNY3U4bi9Qd1lmL1M3eDVWajFwYis2dGxHUXA2R08xSGl5V1diY0k4NFJYUlgvcXM1Cnd1YW83RmRMdEdabmdWYnRyRXI2a2R4d3pOYnczaEJ4Z0IvVVVseWVhb252ODlCNmNheStkV1N6TWJkVwpsamR1VDM0cW1mL1VMS3NqR3FBRUZSaHQ0cTZrY0pXYUZ2Sk9BelF5d1NXdmpNUGhPMmJoTXVHU2R6cDkKazNhVFB4L3V5MzNjUjQyZEpnUVZ6VTN0MFNmRFdVZWRYRHJtejg5WFJCRWhvUDM4YWllbWxzYmxGN3l6CktCK1FodUhRL1JIWnJHcEorZS9Banp3Z0V2dDJWK1ZhOE5LNG9GKzFSRjcxb2JUTWcwRzlWTGNTVnhkLwp1YlVWb29ESVIySzZxd04rQ3JZRGoxTnVvc0sxSmQ0ZmNGTWVzWFpjUENZdFZRT1RkTVkvMC9JNmhOQmgKajQzUXhIa3VPTndjbXBDRmVrMjBDbmZ4cW1RdldSVEprYy9aYzVwWEhNQVRhNlJzNUxESEd3R001MitRCldWRG1FYklWTnpJSzZvNFdZL1NReTFjZWZublVueFlocjJRUGdWUmg3WXBVNW40L3U4UWVjMHNNQVl2VAprbEdXOGtJWHptYTg2ZkVnTW51OG1FYllrZHhIemUyS1N1SGxWY081d3JUVnBzVTR2SDRDanJJaDBLRjMKYzFqYkYxOFNRM1QzTTdRc0FRY2M5a0FhSDRsY1JLV2JFRXNKMUdOVkhsY2hQQ1lCWWRIeHEzOS9qUUNnCkVmZWxJZ1lmQ1FsQlhKRlNJTjUrRm9sVUxXS29mY0tObStlczJleVExR3B2T0FsOFFKbk9UQnZzWWtaegoweUp0Znp3VHM4RmtCcXE5MG92ZU5UTmFiSmhUSXZyakVHeFN0SXZGN2JuM0FndW8vd2pDeUxwOW5tRkMKRTlDemUyVUtjNzZ2VHhJcHJsUjIxMTA2Z1lSZ3JhcWI3UEJyQVRVVjdleUFYQzF0M01Kbm1CUG5FQkNpCmFLMkpmMWVSUkl5VGRENUROUktpSVQrTFRYc04xV0JWYzBWa3Y4SnJaNjhtNkQ5dlQvWjNIOXpYTWFRSgo2T2t4UGplOVBPcmxBTWtmZUUwMnB4d29yc1htQ3Ewb2l0cTZOekdZQ0t1cmQydm9XZC82RTRpTG04am4KQlhiVG5UczFnU3FuNUpaeC9ZQjFWL0NqYXg3Q3RYZUVhQS90Z1crMzhHbTh4d1p3bnRpdUM5Nk5jek8zCnF5SHdDZ2lYdzVTN0hWb1V0bjlIWGF5Wgo=.4f31e3faa70dcf274a6e821670224733\"></div><p>By default, all-purpose cluster configurations are deleted 30 days after the cluster was last terminated. It is possible to keep a cluster configuration for longer than 30 days if an administrator <a href=\"https://docs.databricks.com/clusters/clusters-manage.html#cluster-pin\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">pins the cluster</a>.</p><p>In either situation, it is possible for an administrator to manually delete a cluster configuration at any time.</p><p>If you try to run a job on a cluster that has had its configuration deleted, the run fails with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">cluster does not exist</span> error message. \u00a0</p><pre>Run executed on existing cluster ID &lt;cluster_id&gt; failed since the cluster does not exist.</pre><p id=\"isPasted\"><br>Databricks audit logs can be used to record the activities in your workspace, allowing you to monitor detailed Databricks usage patterns.\u00a0</p><p>Audit logging is NOT enabled by default and requires a few API calls to initialize the feature.\u00a0</p><p>Please review the <a href=\"https://docs.databricks.com/administration-guide/account-settings/audit-logs.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Configure audit logging\">Configure audit logging</a> documentation for instructions on how to setup audit logging in your Databricks workspace.</p><p data-toc=\"true\">If a cluster configuration is deleted unexpectedly, you can use the audit logs to identify who deleted the cluster configuration and when it was deleted.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><p>Once audit logging is enabled on your workspace, you can use it to find information on who deleted a specific cluster configuration.</p><h2 data-toc=\"true\" id=\"load-audit-logs-1\">Load audit logs</h2><p>Before you can search through the audit logs, you must load them as a DataFrame and register the DataFrame as a temp table.</p><p>You will need to provide the S3 bucket name, the full path to the audit logs, and a name for the table.</p><p>Please review the <a href=\"https://docs.databricks.com/data/data-sources/aws/amazon-s3.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Working with data in Amazon S3\">Working with data in Amazon S3</a> documentation for more information.</p><pre id=\"isPasted\">%scala\r\n\r\nval df = spark.read.format(\"json\").load(\"s3a://&lt;s3-bucket-name&gt;/&lt;path-to-audit-logs&gt;\")\r\ndf.createOrReplaceTempView(\"&lt;audit-logs&gt;\")</pre><h2 data-toc=\"true\" id=\"query-audit-log-table-2\">Query audit log table</h2><p>Once you have the audit logs in a table, you can use SQL to query them.</p><p>This article contains two example queries, showing how to find information on a specific cluster, as well as how to view all clusters that were deleted within a specific date range.</p><p>You can use these examples to build your own custom queries.</p><h3 data-toc=\"true\" id=\"display-information-on-a-specific-cluster-3\">Display information on a specific cluster</h3><p>This example query returns details on the cluster deletion event such as who deleted, when the cluster it was deleted.</p><p>You need to provide the name of the audit log table and the cluster ID of the deleted cluster.</p><pre id=\"isPasted\">%sql\r\n\r\nselect\r\n\u00a0 workspaceId,\r\n\u00a0 userIdentity.email,\r\n\u00a0 sourceIPAddress,\r\n\u00a0 to_timestamp(timestamp / 1000) as evenTimeStamp,\r\n\u00a0 ServiceName,\r\n\u00a0 actionName,\r\n\u00a0 requestParams.cluster_id as clusterId\r\nfrom\r\n\u00a0 &lt;audit-logs&gt;\r\nwhere\r\n\u00a0 serviceName = \"clusters\"\r\n\u00a0 AND actionName = \"permanentDelete\"\r\n\u00a0\u00a0AND requestParams.cluster_id = \"&lt;cluster-id&gt;\"</pre><h3 data-toc=\"true\" id=\"display-clusters-deleted-within-a-specific-range-4\">Display clusters deleted within a specific range</h3><p id=\"isPasted\">This example query returns a list of all clusters that were deleted during a specific date range.</p><p>You need to provide the name of the audit log table as well as the start date and the end date of the search period.</p><pre id=\"isPasted\">%sql\r\n\r\nselect\r\n\u00a0 workspaceId,\r\n\u00a0 userIdentity.email,\r\n\u00a0 sourceIPAddress,\r\n\u00a0 to_timestamp(timestamp / 1000) as evenTimeStamp,\r\n\u00a0 ServiceName,\r\n\u00a0 actionName,\r\n\u00a0 requestParams.cluster_id as clusterId\r\nfrom\r\n\u00a0 &lt;audit-logs&gt;\r\nwhere\r\n\u00a0 serviceName = \"clusters\"\r\n\u00a0 AND actionName = \"permanentDelete\"\r\n\u00a0 AND date &gt;= \"&lt;start-date&gt;\"  #Date is in yyyy-MM-dd format\r\n\u00a0 AND date &lt;=\"&lt;end-date&gt;\"  #Date is in yyyy-MM-dd format</pre><p><br></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-5\">Info</h3>\n<p class=\"hj-alert-text\">If your queries do not returns any results for a cluster, it means that the cluster configuration was unpinned and it was automatically deleted more than 30 days ago.</p>\n</div>\n</div><p><br></p>", "body_txt": "By default, all-purpose cluster configurations are deleted 30 days after the cluster was last terminated. It is possible to keep a cluster configuration for longer than 30 days if an administrator pins the cluster. In either situation, it is possible for an administrator to manually delete a cluster configuration at any time. If you try to run a job on a cluster that has had its configuration deleted, the run fails with a cluster does not exist error message. \u00a0 Run executed on existing cluster ID &lt;cluster_id&gt; failed since the cluster does not exist. Databricks audit logs can be used to record the activities in your workspace, allowing you to monitor detailed Databricks usage patterns.\u00a0 Audit logging is NOT enabled by default and requires a few API calls to initialize the feature.\u00a0 Please review the Configure audit logging documentation for instructions on how to setup audit logging in your Databricks workspace. If a cluster configuration is deleted unexpectedly, you can use the audit logs to identify who deleted the cluster configuration and when it was deleted. Instructions Once audit logging is enabled on your workspace, you can use it to find information on who deleted a specific cluster configuration. Load audit logs Before you can search through the audit logs, you must load them as a DataFrame and register the DataFrame as a temp table. You will need to provide the S3 bucket name, the full path to the audit logs, and a name for the table. Please review the Working with data in Amazon S3 documentation for more information. %scala val df = spark.read.format(\"json\").load(\"s3a://&lt;s3-bucket-name&gt;/&lt;path-to-audit-logs&gt;\") df.createOrReplaceTempView(\"&lt;audit-logs&gt;\") Query audit log table Once you have the audit logs in a table, you can use SQL to query them. This article contains two example queries, showing how to find information on a specific cluster, as well as how to view all clusters that were deleted within a specific date range. You can use these examples to build your own custom queries. Display information on a specific cluster This example query returns details on the cluster deletion event such as who deleted, when the cluster it was deleted. You need to provide the name of the audit log table and the cluster ID of the deleted cluster. %sql select \u00a0 workspaceId, \u00a0 userIdentity.email, \u00a0 sourceIPAddress, \u00a0 to_timestamp(timestamp / 1000) as evenTimeStamp, \u00a0 ServiceName, \u00a0 actionName, \u00a0 requestParams.cluster_id as clusterId from \u00a0 &lt;audit-logs&gt; where \u00a0 serviceName = \"clusters\" \u00a0 AND actionName = \"permanentDelete\" \u00a0\u00a0AND requestParams.cluster_id = \"&lt;cluster-id&gt;\" Display clusters deleted within a specific range This example query returns a list of all clusters that were deleted during a specific date range. You need to provide the name of the audit log table as well as the start date and the end date of the search period. %sql select \u00a0 workspaceId, \u00a0 userIdentity.email, \u00a0 sourceIPAddress, \u00a0 to_timestamp(timestamp / 1000) as evenTimeStamp, \u00a0 ServiceName, \u00a0 actionName, \u00a0 requestParams.cluster_id as clusterId from \u00a0 &lt;audit-logs&gt; where \u00a0 serviceName = \"clusters\" \u00a0 AND actionName = \"permanentDelete\" \u00a0 AND date &gt;= \"&lt;start-date&gt;\" #Date is in yyyy-MM-dd format \u00a0 AND date &lt;=\"&lt;end-date&gt;\" #Date is in yyyy-MM-dd format Info\nIf your queries do not returns any results for a cluster, it means that the cluster configuration was unpinned and it was automatically deleted more than 30 days ago.", "format": "html", "updated_at": "2022-10-31T23:49:56.940Z"}, "author": {"id": 791327, "email": "john.lourdu@databricks.com", "name": "John.Lourdu ", "first_name": "John.Lourdu", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-27T21:44:58.925Z", "updated_at": "2023-04-25T22:02:09.299Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2928464, "name": "audit"}, {"id": 2928459, "name": "aws"}, {"id": 2928460, "name": "cluster"}, {"id": 2928461, "name": "config"}, {"id": 2928462, "name": "configuration"}, {"id": 2928463, "name": "deletion"}, {"id": 2928465, "name": "log"}, {"id": 2928466, "name": "logging"}], "url": "https://kb.databricks.com/clusters/use-audit-logs-to-identify-who-deleted-a-cluster"}, {"id": 1494482, "name": "Use an Azure AD service principal as compute ACL", "views": 2139, "accessibility": 1, "description": "Create an Azure AD service principal and use it for access control.", "codename": "use-an-azure-ad-service-principal-as-compute-acl", "created_at": "2022-09-01T13:32:18.985Z", "updated_at": "2022-12-21T09:34:37.414Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlNbzRzUVJ3UFl6Uy84SndvSWlYdHo3c0FrOGJDVXRPWUt4dVhFdlFyWWR1YTJ1ZVBsCm9zR3haUmYwci8zUFBJNGJMOU9JdWhGTmlnSkduWDRFc1AzRDZNRkNjVzZwei9oWE5zOVlvT0xuYVFPMQpoaTM2WGo1V0hEQmRjWTZZUVNPRHlpSFF3OWRNQkxJL1NBZGRRTkgyVzlIU3ludlJ3SE5OUnN3YkNjQjUKZ3pyeWRrUzdRMndxK25GV3N6d21tR2h6aWhkdVZYK3p4MHl6TWtCNmIrUXRnWDR0V3RHU0RlWEpaYzRqCnJWaVhQY0pKMDJYTUFlU1BZRkdCZlN0OWdzZ1FuQWlJQWh1MTlydjJ0amsySEJLbE42d25sLzk1SXpMeApwS1RWMDJaa3ViWXJnLzJmdUEzamNBam9mZE5ld1ZaWVFYQjhzUGFRb3U2aHhnWjRxYzZocU5ldnExNmQKRTJJT3ZFaGRYVHZYRGExYWY4cXhONVIrY3g3TU9IUmRLTEZaQUgvcTEwMFo4a2U3d2lDT0tteTJYNjhCCkpkeEdhcDROaE1JcjJOamxGeWxZeW5VNDh2NDFIKzV6M1FDd2FYZUduS0FJcTFsWE9raDljS1RpL29oMQpxekxqUXNvUXdJVklkdVlUYkJxdDJTK2F1YlNLYy93SlZpWk5HVHN5L3k0T0EvZDJVTXBpWVZCUUpqUGQKMEcvdFlFcmozREg5VFAvVVJjN0J3a21oOWMyOENxaUZvMHJjNFdPcTVrbjdrY3dBK0ZIdlBIVDd3Rk1ECm9aR3Fuby96Q2t2VzZLOXRBTmVPUkkwZHpGeFMreU5mVGlNREh6dGVEeGpKZDlaY0M0MGZTcmxZMFZ3OApBMlZ0VWxwdVM2TDFQaXBORnJPc1BFaVREZklob0lYOHBlV1dZSzlXK3lDVkJEOFlZOUM2Q3dUcno5WG8KSnhvYjhReGxUSlhjTm4yaEliOWNvMzF5VEF1RlE1WUFrOUw4LzFET3ZBdkNyaWFzWmtiemdNNVpuWm0yClZ2UUFmSytObzZ1NzVraVdYZThsK1d2Tjkwd3V5RTJxaTNnRWZRNXZkN0lFbFd1V1ZKS2dHN1IyTllPWAplSjQvTHo0SkZmdWJuMWxxcVZPS0dyWUd5c25Dc1NmSTI1NVVWNUwxK0ROdWNCMEQ0dGJRWHNBSGVKVTMKM2N3V1pESFJCdnVEc2lRYmdPUXBMRVlMRnUrZTF1UXFNQnZGZWlNRWVYa0p4Y3YwZG1aU1RaWFhZN2t5Cm1RSVJnWmNpNkJuRXZEQkx1U1UxUmxRSFdtUkV1V3FLLzdSNm1NaVplbFNlWlFBT1JqWHMzakJZSTZicAphOUlBZXpyaFE3d1YzVWZVVkk0VXV2Z1lqUTJUdGdya0RRMkRZd0gvRVkwQjQrWWxmdnJjMmh3bS9sTmgKVVNDY1FWMk5ZRFl5a1RIdXB3TDIzQTZkT3psZ2t1dGVta3VlZlNoaEw3WkxXU3g2UEVvQnErZlhBbnVWCnRWT2hvMUxla1JZcjJNU0ZibXp2QkMyUnliZFpGVDBEUG44NGdPb3A3a3dqL2tWRVVYTFcxWGZlaHRRVAo3bDZkejN3Y2ozRTN6ZFNEQkVuWERUMkNWeHRadjVhT0xDN2NlY3l1Mzk4N1dLMWd4RGhqZnRtZDlpVkwKMGhWNFUyR2l1S3NweVBXTFpqM0tUdmRmRElxM0lHY1hoQkNHRFIwdkszQXllSGN2TEo2OXJMZkJxV1Y4CkFlMD0K.79a74b4b4fc567f088f42b33da6d9e44\"></div><p data-aura-rendered-by=\"372:778;a\" id=\"isPasted\">When granting permissions to a compute\u00a0cluster (compute access control), it is possible to grant permission to the following entities:</p><ul data-aura-rendered-by=\"372:778;a\">\n<li>Users</li>\n<li>Groups</li>\n<li>Service principals (Azure only)\u00a0</li>\n</ul><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-0\">Warning</h3>\n<p class=\"hj-alert-text\">Before you can use compute access control, an administrator must enable it for the workspace. Review <a href=\"https://learn.microsoft.com/azure/databricks/administration-guide/access-control/cluster-acl\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Enable cluster access control for your workspace</a> for more information. You should also ensure you meet the requirements to use <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/scim/scim-sp#requirements\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">SCIM API 2.0 (ServicePrincipals)</a>.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"instructions-1\">Instructions</h1><h2 data-toc=\"true\" id=\"create-a-service-principal-and-add-it-to-your-workspace-2\">Create a service principal and add it to your workspace</h2><h3 data-toc=\"true\" id=\"option-1-3\">Option 1:</h3><p>Follow the <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/scim/scim-sp#add-service-principal\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Add service principal</a> API documentation to create a service principal and add it to your workspace.</p><h3 data-toc=\"true\" id=\"option-2-4\">Option 2:\u00a0</h3><p>Run this example code in a notebook.</p><pre data-aura-rendered-by=\"424:778;a\">%sh\r\n\r\ncurl --location --request POST 'https://&lt;databricks-instance&gt;<a href=\"https://adb-6872698692919570.10.azuredatabricks.net/api/2.0/preview/scim/v2/ServicePrincipals'\" rel=\"noopener\" target=\"_blank\">/api/2.0/preview/scim/v2/ServicePrincipals'</a>; \\\r\n--header 'Authorization: Bearer &lt;access-token&gt;' \\\r\n--header 'Content-Type: application/json' \\\r\n--data-raw '{\r\n\u00a0\"schemas\":[\r\n\u00a0\u00a0\"urn:ietf:params:scim:schemas:core:2.0:ServicePrincipal\"\r\n\u00a0],\r\n\u00a0\"applicationId\":\"&lt;azure-application-id&gt;\",\r\n\u00a0\"displayName\":\"&lt;display-name&gt;\u201d\r\n}'</pre><p>Replace the following values before running the example code:</p><ul>\n<li>\n<span id=\"isPasted\" style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;access-token&gt;</span> - Your <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/auth#pat\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure Databricks personal access token</a>. If you do not have an access token, you will have to create one.</li>\n<li>\n<span id=\"isPasted\" style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;azure-application-id&gt;</span> - The Azure application ID of the service principal, for example <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">12345a67-xxx-0d1e-23fa-4567b89cde01</span>.</li>\n<li id=\"isPasted\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;databricks-instance&gt;</span> - The workspace instance name, for example <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">adb-1234567890123456.7.azuredatabricks.net</span>.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;display-name&gt;</span> - The display name of the service principal, for example <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">service-principal-dbuser@azure.com</span>.</li>\n</ul><h2 data-toc=\"true\" id=\"add-the-service-principal-to-your-compute-acl-5\">Add the service principal to your compute ACL</h2><p>After the service principal has been added to your workspace, you have to add it to your compute.</p><ol>\n<li>Click <strong>Compute</strong> in the left menu bar.</li>\n<li>Click the name of your compute cluster.</li>\n<li>Click\u00a0<strong>More</strong>.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1671162109870-compute%20more%20button%20expand.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"More button expanded with Permissions highlighted.\">\n</li>\n<li>Click <strong>Permissions</strong>.</li>\n<li>Click the <strong>Select User, Group or Service Principal</strong> drop-down.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1671162817036-Screenshot%202022-12-16%20at%209.23.13%20AM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Permission settings menu listing users and permissions.\">\n</li>\n<li>Select the service principal you created in the previous step.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1671160980444-Screenshot%202022-12-16%20at%208.52.29%20AM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Selecting a service principal from the user list.\">\n</li>\n<li>Select the permission to assign to the service principal (ex. <strong>Can Read</strong>, <strong>Can Manage</strong>).</li>\n<li>Click <strong>+Add</strong>.</li>\n<li>Click <strong>Save</strong>.</li>\n<li>Click\u00a0<strong>More</strong>.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1671598753041-compute-more-button-expand-restart.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"More button expanded with Restart highlighted.\">\n</li>\n<li>Click <strong>Restart.</strong>\n</li>\n<li>Click <strong>Confirm</strong> to restart the compute cluster.</li>\n</ol><p><br></p>", "body_txt": "When granting permissions to a compute\u00a0cluster (compute access control), it is possible to grant permission to the following entities: Users\nGroups\nService principals (Azure only)\u00a0 Warning\nBefore you can use compute access control, an administrator must enable it for the workspace. Review Enable cluster access control for your workspace for more information. You should also ensure you meet the requirements to use SCIM API 2.0 (ServicePrincipals). Instructions Create a service principal and add it to your workspace Option 1: Follow the Add service principal API documentation to create a service principal and add it to your workspace. Option 2:\u00a0 Run this example code in a notebook. %sh curl --location --request POST 'https://&lt;databricks-instance&gt;/api/2.0/preview/scim/v2/ServicePrincipals'; \\ --header 'Authorization: Bearer &lt;access-token&gt;' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \u00a0\"schemas\":[ \u00a0\u00a0\"urn:ietf:params:scim:schemas:core:2.0:ServicePrincipal\" \u00a0], \u00a0\"applicationId\":\"&lt;azure-application-id&gt;\", \u00a0\"displayName\":\"&lt;display-name&gt;\u201d }' Replace the following values before running the example code: &lt;access-token&gt; - Your Azure Databricks personal access token. If you do not have an access token, you will have to create one. &lt;azure-application-id&gt; - The Azure application ID of the service principal, for example 12345a67-xxx-0d1e-23fa-4567b89cde01. &lt;databricks-instance&gt; - The workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. &lt;display-name&gt; - The display name of the service principal, for example service-principal-dbuser@azure.com. Add the service principal to your compute ACL After the service principal has been added to your workspace, you have to add it to your compute. Click Compute in the left menu bar.\nClick the name of your compute cluster.\nClick\u00a0More. Click Permissions.\nClick the Select User, Group or Service Principal drop-down. Select the service principal you created in the previous step. Select the permission to assign to the service principal (ex. Can Read, Can Manage).\nClick +Add.\nClick Save.\nClick\u00a0More. Click Restart. Click Confirm to restart the compute cluster.", "format": "html", "updated_at": "2022-12-21T09:34:37.410Z"}, "author": {"id": 789492, "email": "venkatasai.vanaparthi@databricks.com", "name": "venkatasai.vanaparthi ", "first_name": "venkatasai.vanaparthi", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:40:11.432Z", "updated_at": "2022-12-21T12:31:50.595Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256864, "name": "Notebooks", "codename": "notebooks", "accessibility": 1, "description": "These articles can help you with your Databricks notebooks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2978875, "name": "access"}, {"id": 2978874, "name": "acl"}, {"id": 2978871, "name": "azure"}, {"id": 2978872, "name": "permissions"}, {"id": 2978873, "name": "service principal"}], "url": "https://kb.databricks.com/notebooks/use-an-azure-ad-service-principal-as-compute-acl"}, {"id": 1494442, "name": "Cannot start Databricks workspace from Google Cloud console", "views": 5069, "accessibility": 1, "description": "Check to make sure your Databricks subscription is active and that you are using the correct Google account.", "codename": "cannot-start-databricks-workspace-from-google-cloud-console", "created_at": "2022-09-01T12:54:20.311Z", "updated_at": "2022-12-21T08:18:21.679Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9qclV1VXp2S0grSE9tbDBUdldqNjVtMU1Bb1cxQnZ0cjlQbEdkdC8xM0ZmR1NXNEZxCldmU0d5d0xnOW12RVBkdVI2bGk4Y2h0T3dWcmF3bk9YSW5DN25Xb0plNFZLWTc5REhTbitCVkpDT05KdApQYURwcWQweUwyU1QxSXFzeTlHZG9mRVhTRTVUcnNNcUpNMUo4b01LYlRGM0V3ZXBFRVVKYVh5YkQ3MzkKR2ZZcStrckgvOEZTZm8zZUxyc3BHTVJEZnMyZmwyTDlXcUIyMTJBL1QxU0FtNmpUZkhXWkNkMCtUbmpGCklhbjZSZ0p3akZETFFGZTJ2a1grTllTMkp5MDJoK0p4Qy9RVUtWelU2UHBRT3pqRUlQOGJXZTlMdHp2VApFMWhHYi9XYll3dnVobzNvbG1zb1JTSHBUZ1h4YmRwSVVvOGtvYmhiejkxMVVDWi9wVUVFVmZmeUpJV3cKdnhqRmxncDNmN2djZDllZm5vYjFvcVpGVjhvMmtCZndaa29Yb3BmMlBuMjlkZjBaOWZMMVo5TkZSS3RzCmVhYi9BWmMzUW9Nb2wxNlpRUkFFTFdXYUNZT3IyOUZyOXVMb0pTQ2JVZkJtNzYrU3dFTWRDdGtLeTdHSAo1bXFGTWpERlhrVjZjR2EyM2JEaEhaMXhuL3h0YkJSaW0zWm5hRHdKT3FJelJQWGRyUDJyWEJvNUM4dzIKQXczaHMzZDdjTmlMeU41Z3RTWnM5SVNXeUZpeVZXbnloN1RYK0hERGJiNGl6c3VLMFFUN1FDTmRRS2h2CkhoOUF3eFQ0L0tGTlhObVQyVDU2dW9rTVNreGVZc2M2MUd5NnJjUU12TzRrdDBwakIvOUN1amdPL0MvNgpybC9ENHBNdHVoRHdjOGRtWU5aYVBiTTJSdFgwUjRJc3Nta3Rpa0ozQTg5eVhlQVdnVjFqb3NxZ3VsV1YKT0thY21XNElaa0loZDhNVWQ0Sm1JaUFleDI3ZDZ4NzRxS1ZYZnNoSHM4Z2FoaXRsQlo1eEp1R0VSUHBZClVzSmlDdWljUzFzL2dteG5VcHVBQUNOdG9DMzRJUDljYjBXNVJsWjdXY3RZc2FZRktUUDQ2OU1hTmVwYgoxa0xhdlJUVldTZVc5YUtBMU5nWERWWU1wZkZUdFNuMmhOZ3A3bnNJejJ3WW9nRVVBcVNOYzB1ZUliQVgKVlE5bmpTV0V3OUl5S1B2aHFLQy9lMmdHRFVPcXBIVE53YmsvMnQzaGF2ZnVVS1ZYMkdLUjVscGtLMHFPCnJjdUY3bUtRUUNTZHZ1a09QVWFOT0tSY1o2cmprRTllVVhUWmlQTzMwNCtMRGNyWCswYnBqS3RiWXV3UwpXUTIyYnloc2FJOUUzejcwSFc0cmZCMm1rZTUrVXQwVnk1TnliNVVpeFdXRXlsLzI1anlxbEdvaGhCWFcKZGxqaUFlL1I1OUN4ckt5SXBYT3lWTktVbEY4b3BvSkdkenNRbCtuekY3ZVNJOGRjc1Q3OGs0SWlwWEZCCmhpY3JPS28vL1BIQ2N4VXVJUFk4U2JhNTZKTkJhb3NnTXhaTW1jbGUwcXkxKzBVQTc5bFdBTEp0cmhIWApJWkhFSzhQS21yby9uSUNDZ3dTb3lzcURsSkFWU3NFM3hlbEkyMnI4QThjWnRQY2JwSDF0ZkU2OFFsS20KZDBwZ25DczNTUVN2NjRudE5LNktKdGFzYitHWG9aUXJRZjNKenJka2tIN3I4ZW8wZUxCLzNiN3c2MTdECkJ2NU5pTEtRWFdvdFQ5Zmpoa21NVXVQM3A4eUVKdm1jbGlRT2NqZU5Tbm0weDZJcXV0Z0V6VnQ5S2pVVgp4Um9RVEx4dU0yYlZPSS9zSytqUGNmVklxdU0ycEtkNFRLVzIrNUN5TWhnMXR1TUZRSkFaMTlsN2hRb3MKcFVUb2pyaHJYdXc0VDNCaks4TzJud2c0ZFRMZm1CTElDMVRMVU9vdUdjOUtEQnlaSVpkSEREK2gxTkdpCm9lNEF1OXBCTVVKYUVjNkU4MFdVYXE0YWMzdDZETG1WMU0zcVF5Y2hHV0FlNHlRWDBTQnh5Q2FndTBpawpqSlh4d0NTSUNHTVVweGs4UnlZPQo=.e78f3ecca85355e24e6d3af35d19d923\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are not able to launch Databricks workspaces from the <a href=\"https://cloud.google.com/cloud-console\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Google Cloud console</a>. The button to launch workspaces is grayed out with the message <strong>Your subscription is not active</strong>.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>You do not have an active Databricks subscription on this Google account.</p><p>The most likely cause is the creation of two (or more) trial accounts, with different billing accounts, and with most users in the second account. When the subscription was extended, it was done in the account without the majority of users. For example, your Databricks subscription is on Google account A, while the majority of users are in Google account B.</p><p>As a result, the users are directed to the Google account with the inactive Databricks account when they login to the Databricks accounts console.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>The Google account owner should remove the users from the inactive Databricks account and add them to the Databricks subscription in the primary Google account. This can be done in the Databricks account console.</p>", "body_txt": "Problem You are not able to launch Databricks workspaces from the Google Cloud console. The button to launch workspaces is grayed out with the message Your subscription is not active. Cause You do not have an active Databricks subscription on this Google account. The most likely cause is the creation of two (or more) trial accounts, with different billing accounts, and with most users in the second account. When the subscription was extended, it was done in the account without the majority of users. For example, your Databricks subscription is on Google account A, while the majority of users are in Google account B. As a result, the users are directed to the Google account with the inactive Databricks account when they login to the Databricks accounts console. Solution The Google account owner should remove the users from the inactive Databricks account and add them to the Databricks subscription in the primary Google account. This can be done in the Databricks account console.", "format": "html", "updated_at": "2022-12-21T08:18:21.659Z"}, "author": {"id": 790745, "email": "vivian.wilfred@databricks.com", "name": "vivian.wilfred ", "first_name": "vivian.wilfred", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T10:42:50.853Z", "updated_at": "2023-03-28T11:16:09.635Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 254612, "name": "Databricks administration", "codename": "administration", "accessibility": 1, "description": "These articles can help you administer your Databricks workspace, including user and group management, access control, and workspace storage.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2977286, "name": "access"}, {"id": 2977284, "name": "gcp"}, {"id": 2977287, "name": "inactive"}, {"id": 2977289, "name": "permissions"}, {"id": 2977285, "name": "trial"}, {"id": 2977288, "name": "users"}], "url": "https://kb.databricks.com/administration/cannot-start-databricks-workspace-from-google-cloud-console"}, {"id": 1488679, "name": "List all mount points on a cluster with configuration info", "views": 368, "accessibility": 4, "description": "Display all cluster mount points and the associated configuration information in a notebook.", "codename": "list-all-mount-points-on-a-cluster-with-configuration-info", "created_at": "2022-08-30T06:51:15.587Z", "updated_at": "2023-04-04T18:12:40.773Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlZalQ3VWZydDRlK3pJbDNlRHNwczdPYXZtRVFxTTZrdW9walBhYU1kb2REK2J5VkpuCkJSUXRKZ3E3RzRvQTFJQS9BM2FYVnI1LytLdWJSQnQ2ekZpYndqMkx3cVdrbnU0aEt0TFJObkhNSUwwSgpiT1FEVGpBeHVHSXpLZWEvMUVLMWk2Q3BVS1cxWGdkWnVHWFcvWUNoNGEvYlRpRmdqTGpwRUFxNkd4V0sKQmwzZWVIYk1obVpoV1RFTW8wUHdTN2NOM24xODlzNWF6cHNjZSttSURYWUltYzMraUxkVUVPcHpKczV3Clp1SEtRZVNTWHIySTcxNVRNc3Y1N0c5WEthcnBvQ3FIU3hVcTNkTEVBbWV2ekR6dmlEUHY5dkxUMDFJaQpvK0lXT3d6akJaQzlad3k2Yi9iRXRBZUtSYkM4cUJyK2dFR21WL2VGQTdseEdRNGJzVjBJbjB3ZVBoVDEKd1hTV01BT251UUhxT1NqQWdOMEV0MEJtbHBIQTA4SkNpdUJ4RTRKNmZ0VnZSOU1sbWUveS9FRFZ1bEhRClpHYjM1OTFhK3IyNVQ2b21FVE5Ld2tRaDZ3YkZpckdIUEx1enJZeWh6Z0VxcWd4dVk4ZERFWjhoSXZFYgpBRDhzaXJNSkgwQmJuMnB0LzRlKzJ4TFc5V3ZUQmZCVU1HeFEzQ0NIV29sK1FmTjFuSTBhU0tXUC9OcEcKcGpETndoWTdKbUdLSFZndnVzWWZOcWhSMzVOaHVRN1dtbGVSdnlqV0NLTnk3UWY5SGxhejl5TGozQjBKCnk1Tm83ZENmeDNYZ25ZK3BscVpVaEFWWG9SQlNWRlRYYThwQjliZEVhY3MxSmZBbXpUMTlRbkcrQTA3VwpBMmxMZmpESGptbGpSNENST3RMaDdyMVB6Y3c2UGRtamh3RjNRK1FwaUgwaUV5UUZTai9PUVNqdk9qclQKTlNLQWs2Yk1wdWVzeUpJeEpXK2NETzBSbUEzbytmclc0RllLVTRkbzBSSmdVRVVZcTQwZEhLVDFwc2xMCmRERnF1Zk1zMUVEMDY2SXZaU2ltOE9iYi81N0NZbFhOemlYWkNBdllHRU1aaUhQMFIrTVJYL1VEa1lSZApvVk8xclU0L2xnbks0eEIzLzduQTlUdExrQnJVZ29wZEN4R0Q5L1ZyZ1pQUk55OW1lMzZIVWJJZG5rNGIKRHVsZG5OWTgrSXhWUlVVQUNDbTJ0Q1MzSjN1dmdpQlMzcXNHZDluemI3QWZDbkQvaFByMEN6ejh6eG55CmQ5aU5SZ3dSMjNlamJaMEIrRzVjM1UvTEI3VWlnZThDRUdaQ0VNck9idXdoNUJFOStMYURJdlZjK3o5KwpwTU1PTUFLTVRrcjJhQWl1T3dTZXdTNjJyWVZ4Q3haeTRQblFJNHAwRkdKOC9pSUg3dS8rZU8vTnNHNUMKLzl6WFBOZkFQOTJGZloraDZiMitVYXNlemVOTVExSE1kMEVRWjJXZ0hoVjJ2Qm16bDdxNjlRVGh3Q2pNClM5UUFCVXl4ZWJVVmJWN0FQZFNJZnRPMS82bnEwWDY0UlgwcW5EaWJJSUQ3SDMyOUtDZHJ2WXE4TDhVWApoekNsWVArQ1pZOXRWUFVPYWVlbzgzTklObWxPVFhCeXpmTnV6QW54Uk8xeHY2QkpldC9EUm5UQ0JDWUsKejZrd2w3b1lWNmthdVhCSVgxVmhseFhGR0FOSnRSTnhmUHlmUG1CL2xnVmZZNWRkYzZ6NW55aGN4T2VsCk1wWEFoTWZPUG5qVmRHMkxHMW03NENWWUF1VnQ0SUxUaGtiT2ppcE82ZkZvbDZ6djQ1UWlRQ2dzQTdacQpyVk5QUDlPZGEzb2wreisramUwNytSNXg5UnBtSmtnL21xdk5uOUYvdjRRVUJtYnNXUHpQWnd2a095VEgKU1c1SHdxOGt6cE09Cg==.0fd4362c4c7d9a820f9aa4c06dce92a3\"></div><h1 data-toc=\"true\" id=\"what-are-mount-points-0\">What are mount points?</h1><p>The mount point is a directory that is linked to an external storage blob. Once mounted, the external storage is browsable as part of the local file system. Any user of a cluster that has the required permissions can access the external storage via the mount point.</p><h1 data-toc=\"true\" id=\"list-all-mount-points-on-a-cluster-1\">List all mount points on a cluster</h1><p>When you are troubleshooting an issue, you may need to know who performed a specific operation and what configuration was used to mount a storage blob.\u00a0</p><p>You can review the cluster <strong>audit logs</strong> (<a href=\"https://docs.databricks.com/administration-guide/account-settings/audit-logs.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS</a> | <a href=\"https://learn.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/audit-logs\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/administration-guide/account-settings-gcp/log-delivery.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>) to identify the owner of the mount operation by looking for the <em>dbfs/mount</em> audit event. The audit logs also contain some basic mount configuration information. However, to obtain detailed mount configuration information, you must call the information in a notebook.</p><h2 data-toc=\"true\" id=\"example-code-2\">Example code</h2><p>This sample code returns the storage <strong>sourceString</strong> (the external storage source), the <strong>mountPointString</strong> (the local directory containing the mount point), and <strong>configurations</strong> (configuration data required to access the external storage source).</p><pre id=\"isPasted\">%scala\r\n\r\nimport com.databricks.backend.daemon.data.common.DataMessages._\r\nimport scala.concurrent.duration._\r\nimport com.databricks.backend.daemon.data.server.util._\r\nimport com.databricks.common.util._\r\nval dbfs = dbutils.fs.dbfs.asInstanceOf[com.databricks.backend.daemon.data.client.DBFS]\r\ndisplay(dbfs.client.sendIdempotent(GetMountsV2(), 1.minutes, Some(5.minutes)).toSeq.toDF)</pre><p><br></p><ol>\n<li>Copy and paste the sample code into a notebook cell.</li>\n<li>Run the sample code.</li>\n<li>The notebook displays mount point information for the current cluster.</li>\n</ol>", "body_txt": "What are mount points? The mount point is a directory that is linked to an external storage blob. Once mounted, the external storage is browsable as part of the local file system. Any user of a cluster that has the required permissions can access the external storage via the mount point. List all mount points on a cluster When you are troubleshooting an issue, you may need to know who performed a specific operation and what configuration was used to mount a storage blob.\u00a0 You can review the cluster audit logs (AWS | Azure | GCP) to identify the owner of the mount operation by looking for the dbfs/mount audit event. The audit logs also contain some basic mount configuration information. However, to obtain detailed mount configuration information, you must call the information in a notebook. Example code This sample code returns the storage sourceString (the external storage source), the mountPointString (the local directory containing the mount point), and configurations (configuration data required to access the external storage source). %scala import com.databricks.backend.daemon.data.common.DataMessages._ import scala.concurrent.duration._ import com.databricks.backend.daemon.data.server.util._ import com.databricks.common.util._ val dbfs = dbutils.fs.dbfs.asInstanceOf[com.databricks.backend.daemon.data.client.DBFS] display(dbfs.client.sendIdempotent(GetMountsV2(), 1.minutes, Some(5.minutes)).toSeq.toDF) Copy and paste the sample code into a notebook cell.\nRun the sample code.\nThe notebook displays mount point information for the current cluster.", "format": "html", "updated_at": "2023-03-17T15:04:15.230Z"}, "author": {"id": 790705, "email": "simran.arora@databricks.com", "name": "simran.arora ", "first_name": "simran.arora", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T08:29:23.302Z", "updated_at": "2023-04-25T08:04:15.648Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256846, "name": "Databricks File System (DBFS)", "codename": "dbfs", "accessibility": 1, "description": "These articles can help you with the Databricks File System (DBFS).", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3127666, "name": "aws"}, {"id": 3127667, "name": "azure"}, {"id": 3108935, "name": "configure mounts"}, {"id": 3108936, "name": "dump mounts"}, {"id": 3127668, "name": "gcp"}], "url": "https://kb.databricks.com/dbfs/list-all-mount-points-on-a-cluster-with-configuration-info"}, {"id": 1479486, "name": "Allow spaces and special characters in nested column names with Delta tables", "views": 6162, "accessibility": 1, "description": "Upgrade to Databricks Runtime 10.2 or later and use column mapping mode to allow spaces and special characters in column names.", "codename": "allow-spaces-and-special-characters-in-nested-column-names-with-delta-tables", "created_at": "2022-08-17T21:08:13.592Z", "updated_at": "2022-10-26T20:55:31.651Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlKOTdlVWFlZWlXbXlEZmVOeXdham95UUVFVmhFQkE1YStySVhTT1ArZGh1cEw1UnQ2CnhsUURRV1p0WnQ3cVZmaWlBaEdDMEhJNTNMS2prdHZwUE84aHV4ZUpZclhGNHVDQzMwQjhqOEF6RTF3SAo4SjRyRnBvM296cDF5UFQ3T1hIT2xQMytiWCtVdS9YcTdJN0p6WTllL2dLWC80SFY4Rms2SjFSM1RIOEEKOG5ndXhtREdXMGFsMDl1TG96Tzg4YTc1ejVJZ0Jlbm91YjJid1dmZkhpZlNIa0o3TEsyYkFOQWFReW5sCkwyTnkvZnlHcnpEek9Kd3lCSHBLa3NRU3NXQS9xMmI0L2VzQjJ0MXB1b2Jlb1ZlNHFZVGZxNDlCWHFaZAowTUQ5dVlqYWZxUU9mWEtBTkhuUGpzSEl2amxNaEFlZDN5VlZBMGlVODR1b0dPcDNDQTVhN2UzeTNJQUkKd2tNcnF5RVhobkFValp5ZWo0UzFVL2VNWGo0WUk4VVFXSEdSNkcrUVVxcm50aXRhQ01ZbytHQW5xSVpSClYrejUybm92L0Y2dnBwSG1WZUVTb2l1TEsrV0VPbk9ncVYxd3BsdkVYUjg3NjY5amxCZk9PSUNydXBLTwo5Q0lENHpNZ3VCNEdSeTRPOFNZM2xMQTdxTGhZUUdiYnVrOXRweVNPM0JSUWwvdEVGM2xYcGtCMytLcVAKZHhVR2tESEdJM0FlZ0hTMTg1eVpvRVN6UmFsaHR5QnJPOUdvY1JPbkFEaDBEN2hOVy80Wi9qM3Z5TGxTCkpPVjFaMlRmTUFBM3lDWHVvSzdYOTRmMWFDYlRTbklib0paZEJjMXhPL3hqcWtST1dtZHNtbDhCRjJVaApPQjR6cUcxQVY5Z1QvWEJaWGhJYWVhUjVJNEo3anpPWmtYUzJLVUJzZ25Oc0R3dmV1aXZJbmpRR0Ura3cKQk9qamM4NklmaVNaUGpwZ2xXU0lpaUUzT2dXRTVNV1hncFpzQlRpNzBBT2UxRDZ6RWhKc0VQbmV2bldhCkJ1OVljOGFMdXd3cDVIdm42OVlNVUdpcHo5MlVyR3RmYjJIMUo4T3RrYXdqL040ZVBycHB5YVhGODNveAp5cDNLRlAxQ1dzanUzRGM5TTRyWnFNRm9CSGswdHIzYVlBMjlud0EvR1B0dXZjK081TDExZzl5TFJOSEQKeXpjVVJRbmRvWG9DWGw0SCtvU0FSeEhKK0Vka25UN3lUTTFjTXFQTndrL3ZRWGVWZTlUVlI4MU1kR1FSCm02RGY4WFQ2Sk03clJxNkozTlhVVkc1enZzcUxwQzdYUVZjRGtRbWcxTUd6MzVxZ0FEM3BHZU5Ea2xDawpIOFVCOHVMMU12cXBqazg0MVhOZzY0eXNhNi9XVWcvUWhiUUJ6NVFIem9UV3NqekViajFMb2o2akd6ZkMKcEgxYTA5bE4wcEl4amZnUEdKWWlhVW9IK2Nzb0tTRld6OWYyZU91a3JQOHJvdlpUa3ZTeEhDeXJ0UzFWCkpyZHlxVjlkUDlkakpsWTIyQXJ5TzkraEs4SEJ5SjFDSVlsTm1vMVl0UkVHcHZ3RE8wd1lpdklWYjd0aQpnVHVNVkpCMkdCVUpZeEtmV1R0YnkyZEViWG8wMkQ1RVpUNEV5OE5BV011NTB2VzJVeUlGbitvMXZTc0sKL05WSmpNOVYreDBCQ0ZReldGZ0VOQ09oNkxyTmdreU83amZIK1ljU3VHZ0pyNDhGbXJxRkNZTW5VeEpJCitWbjJ4am81cFVoSFY2S0NITkJxUUxuWDRRUTY5cHBmS3VtK2daRHpseldPQnZYeFI5Tk9GVk5LazRSbwovN1ZSZElhS3EwL0QrVk5ValN5elI3RkZvVUwzd2ozbXk2L2ExTnd2eGwwVGJpNVpZdz09Cg==.2a0850cf2d4fb056ceb4110b27b9c2b9\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>It is common for JSON files to contain nested\u00a0struct columns. Nested column names in a JSON file can have spaces between the names.</p><p>When you use Apache Spark to read or write JSON files with spaces in the nested column names, you get an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">AnalysisException</span> error message.</p><p>For example, if you try to read a JSON file, evaluate the DataFrame, and then write it out to a Delta table on DBR 10.2 or below it returns an error.</p><pre class=\"language-plain\">%scala\r\n\r\nval df = spark.read.json(\"&lt;path-to-JSON-file&gt;\") \r\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"&lt;path-to-delta-table&gt;\")</pre><p>The expected error message is visible in the stack trace.</p><pre class=\"language-plain\">AnalysisException: Attribute name \"stage_info.Accumulables.Count Failed Values\" contains invalid character(s) among \" ,;{}()\\n\\t=\". Please use alias to rename it.</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>One of the nested column names in the DataFrame contains spaces, which is preventing you from writing the output to the Delta table.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">If your source files are straightforward, you can use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">withColumnRenamed</span> to rename multiple columns and remove spaces. However, this can quickly get complicated with a nested schema.\u00a0</p><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">withColumn</span> can be used to flatten nested columns and rename the existing column (with spaces) to a new column name (without spaces). In case of a large schema, flattening all of the nested columns in the DataFrame can be a tedious task.</p><p>If your clusters are using Databricks Runtime 10.2 or above you can avoid the issue entirely by enabling column mapping mode. Column mapping mode allows the use of spaces as well as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">, ; { } ( ) \\n \\t =</span> characters in table column names.\u00a0</p><p>Set the Delta table property <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">delta.columnMapping.mode</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">name</span> to enable column mapping mode.</p><p>This sample code sets up a Delta table that can support nested column names with spaces, however it does require a cluster running Databricks Runtime 10.2 or above.</p><pre class=\"language-plain\">%scala\r\n\r\nimport io.delta.tables.DeltaTable\r\n\r\nval df = spark.read.json(\"&lt;path-to-JSON-file&gt;\") \r\nDeltaTable.create()\r\n    .addColumns(df.schema)\r\n    .property(\"delta.minReaderVersion\", \"2\")\r\n    .property(\"delta.minWriterVersion\", \"5\")\r\n    .property(\"delta.columnMapping.mode\", \"name\")\r\n    .location(\"&lt;path-to-delta-table&gt;\")\r\n    .execute()\r\n\r\ndf.write.format(\"delta\").mode(\"append\").save(\"&lt;path-to-delta-table&gt;\")</pre>", "body_txt": "Problem It is common for JSON files to contain nested\u00a0struct columns. Nested column names in a JSON file can have spaces between the names. When you use Apache Spark to read or write JSON files with spaces in the nested column names, you get an AnalysisException error message. For example, if you try to read a JSON file, evaluate the DataFrame, and then write it out to a Delta table on DBR 10.2 or below it returns an error. %scala val df = spark.read.json(\"&lt;path-to-JSON-file&gt;\") df.write.format(\"delta\").mode(\"overwrite\").save(\"&lt;path-to-delta-table&gt;\") The expected error message is visible in the stack trace. AnalysisException: Attribute name \"stage_info.Accumulables.Count Failed Values\" contains invalid character(s) among \" ,;{}()\\n\\t=\". Please use alias to rename it. Cause One of the nested column names in the DataFrame contains spaces, which is preventing you from writing the output to the Delta table. Solution If your source files are straightforward, you can use withColumnRenamed to rename multiple columns and remove spaces. However, this can quickly get complicated with a nested schema.\u00a0 withColumn can be used to flatten nested columns and rename the existing column (with spaces) to a new column name (without spaces). In case of a large schema, flattening all of the nested columns in the DataFrame can be a tedious task. If your clusters are using Databricks Runtime 10.2 or above you can avoid the issue entirely by enabling column mapping mode. Column mapping mode allows the use of spaces as well as , ; { } ( ) \\n \\t = characters in table column names.\u00a0 Set the Delta table property delta.columnMapping.mode to name to enable column mapping mode. This sample code sets up a Delta table that can support nested column names with spaces, however it does require a cluster running Databricks Runtime 10.2 or above. %scala import io.delta.tables.DeltaTable val df = spark.read.json(\"&lt;path-to-JSON-file&gt;\") DeltaTable.create() .addColumns(df.schema) .property(\"delta.minReaderVersion\", \"2\") .property(\"delta.minWriterVersion\", \"5\") .property(\"delta.columnMapping.mode\", \"name\") .location(\"&lt;path-to-delta-table&gt;\") .execute() df.write.format(\"delta\").mode(\"append\").save(\"&lt;path-to-delta-table&gt;\")", "format": "html", "updated_at": "2022-10-26T20:55:31.616Z"}, "author": {"id": 790360, "email": "shanmugavel.chandrakasu@databricks.com", "name": "shanmugavel.chandrakasu ", "first_name": "shanmugavel.chandrakasu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T21:14:15.541Z", "updated_at": "2023-04-20T21:51:08.608Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256849, "name": "Delta Lake", "codename": "delta", "accessibility": 1, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2917921, "name": "aws"}, {"id": 2917922, "name": "azure"}, {"id": 2917936, "name": "columnmapping"}, {"id": 2917935, "name": "delta"}, {"id": 2917923, "name": "gcp"}], "url": "https://kb.databricks.com/delta/allow-spaces-and-special-characters-in-nested-column-names-with-delta-tables"}, {"id": 1477230, "name": "Different tables with same data generate different plans when used in same query", "views": 3479, "accessibility": 1, "description": "Ensure that tables with the same data generate the same physical plans with Spark SQL.", "codename": "different-tables-with-same-data-generate-different-plans-when-used-in-same-query", "created_at": "2022-08-16T12:28:51.338Z", "updated_at": "2022-10-14T13:48:16.816Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTl2Qmdtbng5aGw0VUFxOXNsdTJhSjdNMGxicElSSTI0REtONjN3REdwMzlhNjR4YldICnRiQkZ6TXpmNFh1WStRWmVLTzN0TDlSUk5zYTBiMlNGd3p3WCtMWTE3aldyTlI4VENocnlvMlFzdlA0WgplaVh2RWNCd0Y0Q3dKTTFrZEkyWXA4YWcyczBna1dKdEtTb2VPcGhka25KMWk3QnhlZ3drczlPN3lyY2wKM0I4NndaNW4ybG1RTFgrTFVubzRLZXVCZ29xM3JPclViSVg3MVQ5ZFJDNTlUQzAzMUozYllBdGozZFZQCnVwazdzM29xeDYzMUlVQ0doNkZrR1Q3aXN6dVZqVHhUWDdTbmJxY2krc3dQajl4WWNUYmlwRWp1cVMzbQpGN2xYQWtZdjl2bWFURm95QTRWM2VoMUdaN3h2VERHaU1EK2xqMFF4dkhvbXFwTUpsWThVc01ldFd0MVYKQ2JNTkVOeGVHY2dFZVBhRnVFbVQxTjVyUFBSMU5wczlLVlI0cjdFM0dUcDdoc204YzV1YmRTVDZIVmlpCk54dm5GQnEvcm9tY2Mxc0lQQVF3OHhpazJJZ1Z4NzVKSFZqSTFBUm9uK0Y2TGdJVFVZbUl5eWxiTE43WAo3czE0bTNTamlxdnQvODRUWllXYmlLVDRYd2hqdndZUHIrejNNK0x1dy84MHBFVThFMkkvWTdJcG1uaWoKVmMxYWwrSGNpeC9VQi9meTV6cjhUY3N1NFJsQUplSm5SMHVFc2NyTmVOSUY2djI4OERHcjBOMVdJVVVOCk9WbXpieVRrS3NBNWRHNTh4Tk1kM1U5UitEQTN0bll2OEc5QmdpK2NsQlRGdkU4a09zdUZTZXZtYVpmZwpsOVZJM2xHWUU2ZzMwYmpiODFUc3BYN3gzclB3ME5ZbjdpNWxrTVlHcXVOaWlVdnZTV08yc2tsYlk1d3AKeFNYeU1UMlE3OTZMV0c5U1NMR21mNUpXc1VpK1ZPdTVFaFEwTVlSWWxmejZkYzZaTVNsZmVwVXZjT3JECllsT2F2Mmh2dTRzckZWZ1V5ZExQRjlKQXN5dk9UcVIxZ3dHcTZQRFdQTzVsdWtST2tMRmIwOTZuZnNsawo5WkU3Ynk0THROcHdHY3M3U3dScG5wRDFRL0Z0VHEwaUJMak1RNUp3OU9ZOFV2bjF0di9pR1M4NzNCUWMKY3pOSndVOGVNUE5INFNCRHVNQzF5UVg5UDloZVAwbUJMTjcyWVJ1cndoS2NZMnFNckFSanFadUFKRXNPCm9sR1pIN2VrVm41QWRYK2J6YWlza3IxaE5YVUxLMURrOFF3U25peDIzOHBJVzNLWVE2czA2WWQzT0Y3YQpXblp0UFB6V0UrRTJUNGYvMHlyR21yRyswY05uSG9HSzQwSWdXT3lRZ1F6dkdtTVFjazhLRVVyLzQ3SFUKbkE0M1VpSzhPTXc4RDhTckh1c2h0WWQ4eDUxOEpPMWs1Z2V1ZDZZL3doelIyRGduaTltd2pJYUUxWkYyCkRwV3lFakdNUFlLNEQ4SjhjaTV2RW45TDgrMzZoblRucDVJZU81ZU5TcUIxbTBjcVdzRHpSbElzenNMVgp6eVVHK1I5TUk0dHhUYnJpSEg1aUg0SjBRdHZIYSsrVGpySUVrVjNEU2d4ckpyVDBrZTNRMm5jUiszRlMKTU5DSFBrMG9TK25ZNXBMYjIySGlZTEY3Q29vUWpvdEUyaFJmd1NTUFZhYnJOWFREWjZkd1lCS0JFbndnCmVFajR3dk81UlJDaVp1SXhkTDhNSGlrbldkdExlNDl2cHpvSzZyeWdBdDM1WGFrN3hVU3hoZXNQeDBKMwpVZVdFd204dndOdEJETXgrcEp1TUR3aExTWndrNC9sazAxV0dTQVM5Wlg1c095ZWRJTjdpMlpySHUzcTEKV21XNW5CZ1hyNjVRaUNoM0NEMlAzNE94ei84UTE3L0xveHlFMTZGZmdRYjNNSW5TT2M5VVNvRk0yMUJrCk9WQmRBdjk1UXZ5cHRlNE42dktrM1ZuSmpJS3g4dklBZjR2Two=.bf0851365a7940fcb0b6a86952fc0e9b\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Assume you have two Delta tables <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">test_table_1</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">test_table_2</span>. Both tables have the same schema, same data volume, same partitions, and contain the same number of files. You are doing a join transformation with another Delta table, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">test_table_join</span>, which has a million records.</p><p>When you run the below join queries using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">test_table_1</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">test_table_2</span>, different physical plans are generated, even if both tables are identical.</p><p><strong>Query 1:</strong></p><pre data-aura-rendered-by=\"371:770;a\">%sql\r\n\r\nSELECT COUNT(A.*) FROM TEST_TABLE_1 A INNER JOIN TEST_TABLE_JOIN B ON \r\nA.ID=B.ID</pre><p><strong>Query 2:</strong></p><pre data-aura-rendered-by=\"371:770;a\">%sql\r\n\r\nSELECT COUNT(A.*) FROM TEST_TABLE_2 A INNER JOIN TEST_TABLE_JOIN B ON \r\nA.ID=B.ID</pre><p>When different physical plans are generated for identical tables it can result in a delay as compared to other queries.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Apache Spark generates the physical plan for the transformation based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution you choose depends on the specific Databricks Runtime version you are running on your cluster.</p><h2 data-toc=\"true\" id=\"databricks-runtime-73-lts-3\">Databricks Runtime 7.3 LTS</h2><p>Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics.</p><pre data-aura-rendered-by=\"471:770;a\">%sql\r\n\r\nSELECT COUNT(A.*) FROM delta.`path` A INNER JOIN TEST_TABLE_JOIN B ON \r\nA.ID=B.ID</pre><h2 data-toc=\"true\" id=\"databricks-runtime-91-lts-and-above-4\">Databricks Runtime 9.1 LTS and above</h2><p>Run the ANALYZE TABLE (<a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-aux-analyze-table.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"ANALYZE TABLE\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/spark/latest/spark-sql/language-manual/sql-ref-syntax-aux-analyze-table\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"ANALYZE TABLE\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-aux-analyze-table.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"ANALYZE TABLE\">GCP</a>) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata.</p><pre data-aura-rendered-by=\"471:770;a\">%sql\r\n\r\nANALYZE TABLE test_table_1 COMPUTE STATISTICS;</pre><p><br></p><p>After implementing the solution, re-run both queries and\u00a0use a diff checker to compare the resulting physical plans for the two Delta tables.</p><p>The plans are now identical.</p><p><br></p>", "body_txt": "Problem Assume you have two Delta tables test_table_1 and test_table_2. Both tables have the same schema, same data volume, same partitions, and contain the same number of files. You are doing a join transformation with another Delta table, test_table_join, which has a million records. When you run the below join queries using test_table_1 and test_table_2, different physical plans are generated, even if both tables are identical. Query 1: %sql SELECT COUNT(A.*) FROM TEST_TABLE_1 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Query 2: %sql SELECT COUNT(A.*) FROM TEST_TABLE_2 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID When different physical plans are generated for identical tables it can result in a delay as compared to other queries. Cause Apache Spark generates the physical plan for the transformation based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution you choose depends on the specific Databricks Runtime version you are running on your cluster. Databricks Runtime 7.3 LTS Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics. %sql SELECT COUNT(A.*) FROM delta.`path` A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and\u00a0use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "format": "html", "updated_at": "2022-10-14T13:48:16.536Z"}, "author": {"id": 840937, "email": "deepak.bhutada@databricks.com", "name": "deepak.bhutada ", "first_name": "deepak.bhutada", "last_name": "", "role_id": "draft_writer", "created_at": "2022-04-06T11:01:04.397Z", "updated_at": "2023-04-21T14:02:10.764Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256849, "name": "Delta Lake", "codename": "delta", "accessibility": 1, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2861118, "name": "aws"}, {"id": 2861121, "name": " azure"}, {"id": 2861119, "name": " gcp"}, {"id": 2907828, "name": "sparksql"}, {"id": 2913987, "name": "sql"}], "url": "https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query"}, {"id": 1477179, "name": "Using datetime values in Spark 3.0 and above", "views": 8696, "accessibility": 1, "description": "How to correctly use datetime functions in Spark SQL with Databricks runtime 7.3 LTS and above.", "codename": "using-datetime-values-in-spark-30-and-above", "created_at": "2022-08-16T11:33:43.574Z", "updated_at": "2022-10-26T08:05:38.871Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.1b0bf8848166ad0838eaef9c6869ebd8\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are migrating jobs from unsupported clusters running Databricks Runtime 6.6 and below with Apache Spark 2.4.5 and below to clusters running a current version of the Databricks Runtime.</p><p>If your jobs and/or notebooks process date conversions, they may fail with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SparkUpgradeException</span> error message after running them on upgraded clusters.</p><pre>Error in SQL statement: SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'YYYY-MM-DD' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from <a data-fr-linked=\"true\" href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\">https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html</a></pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Prior to Spark 3.0, Spark used a combination of the Julian and Gregorian calendars. For dates before 1582, Spark used the Julian calendar. For dates after 1582, Spark used the Gregorian calendar.</p><p>In Spark 3.0 and above, Spark uses the Proleptic Gregorian calendar. This calendar is also used by other systems such as Apache Arrow, Pandas, and R.</p><p>The calendar usage is inherited from the legacy <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.sql.Date</span> API, which was superseded in Java 8 by <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.time.LocalDate</span> and uses the Proleptic Gregorian calendar.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You should update your DateTime references so they are compatible with Spark 3.0 and above.</p><p>For example, if you try to parse a date in the format <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">YYYY-MM-DD</span>, it returns an error in Spark 3.0 and above.\u00a0</p><pre>select TO_DATE('2017-01-01', 'YYYY-MM-DD') as date</pre><p>Using the format <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">yyyy-MM-dd</span> works correctly in Spark 3.0 and above.</p><pre>select TO_DATE('2017-01-01', 'yyyy-MM-dd') as date</pre><p>The difference in capitalization may appear minor, but to Spark,\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">D</span> references the day-of-year, while <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">d</span> references the day-of-month when used in a DateTime function.</p><p>Review all of the defined Spark <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">DateTime patterns for formatting and parsing</a> for more details.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">If you want to temporarily revert to Spark 2.x DateTime formatting, you can set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.legacy.timeParserPolicy</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">LEGACY</span> in a notebook. You can also set this value in the cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>).</p>\n<p class=\"hj-alert-text\">While this option works, it is only recommended as a temporary workaround.</p>\n</div>\n</div><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"06a285c7e10e2\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSsyeVlpUVpJd1kzS0QyOSthU1p4Yy9Za1E5TUFBdjRuUUdxVVViVkcvcHRKVUFOUldTCkZTYlE2M0hwY1dSaEE4anNzNmduRnhXV05BYktWWCsrb0lGNUxkeE9WcHNGQVZLZWlSdTN1ZWxRTUVLMgppeXF5UGhJOFpSTzhhVSszYTQzY1RVTDhuY1hpZktZOWE2SzNmRHI5M3RNR0pXc01CTFc0NEJHWGdxRmwKZ1BmSGRUMFZPVDU3ZXRYaE9QQmhlejB3SnhwRnFmSVlnaG96dDJBVEJkWE1uY3YzdnNNRFZRckU0aDZkCnh5VC96SjNMK2dNVHpLK3o2ekp5WjhxZmo3RWpFakFrZ3Qwbnk5aFM2UGxkU1pMRDVYdDZvcHRoV1FqSgpBWWNZNnVvbHliaVhibE1ESktHUHJ1aGZTMVJjQ01YT05FWW1sYlAxQTJ2azVRZTlQNGVUVUNudDYwbXIKbHpUd216RnJlY0FVNnV6ZkdXeW92WmJLbGN0V3ZHUnJJMkhKWHlVbzlSQUZqU05GdHJFYUtTR3JrclF2CnZML3FWUDJlNjJoVElIV0cvNmprRHlsK1BqdW01K3hQUWRVYUtQVkJZd0tpRGlPY25CMllKNEVFSkRHRQo0ck5IV0M1cDBYUmk1K3Mxa3FYQnJ0M2pXQ3BtaElrNWdYUTlyamxWcHIyU0ZRRHpGRmg0UWFQandoR2YKSXRYcWtLdGJiZXY1UlhrTzdKT090TkF3eGRDOEx1RjNFSWU5T2d1MExKSTBCczVWVnhVN29WUVhpQkNNCmtnYndoT3pRaFZZeWM4aFlxemZ4d2N1dGVBOEJyTnFFNzBXTzE5YTloYlExSTNUNWxzb2NmSlIwY3hxdApqRVBOSWxTNm9zemFXUk1QOE1PTVMwZy9ZUG0vQ3VBdWVWZDB3dk84OFMyaWtkWjZTRHNKUHJ1YWtuTFcKTityMm1yS0lPNzVtT2RuNFJtWGdHM1RpUm9MR2J1RUx4V0pNVktnVUIvVXJqN1ZteVl0eXh3REIyUHpNClptL0Zqa3VqL092cC9VQ3VWcWVsR21KNG9LbDJMWWFkZVp3eCtoMVY4a1JscWhHS0lKL3RBSXdzQnVycQorZWVTREtDUExHaFY2V293QnJXYzI5N3ZIVVNoRDcvKzEvUDU1MWZiU3ZwRmZuYmU5K3c2ZklsK21RSFIKQXdxSkJTS2haMEJZWldtbGR6WW52QVRRM0dvNUMxUisydmVjQmVmZ3UzV1AzZ3NVaVJmNkQrUGN4dkc5CnhhSUFqTWNreThUMDh5WGRxVmhsdjFDSTZlTlF0NmdsRUQ2NjhERDBjNFI5VTIyZXJjbFJNMkZFY01UZQpBSGpQbSs1aS9tdExMditnOU42ZVhmcFU3ZWRoREdYcXBaNU1DMzZOaGpTRlM2WDNIR0QvSmliKzR0TzUKRVZQejVCZGFRVDY5RWUrQ2M0c0FpUFJac0FwZzF0UkluL1NwaTFWMHdWVy9XQ3Z2K3NmY2t1NmxwYXBkCkdmZm5OWVFiQ1VTVTUxS21rdXM0cHZXUU5OSHQrOFdjUXVpNEVFaHczQUtMd2lNbXJpcUYySGdyQzJRVQptVEFCNitSdUxHa2xjd2NKCg==.83180a84ec2308f837966c808842d6a6\"></div><p><br></p>", "body_txt": "Problem You are migrating jobs from unsupported clusters running Databricks Runtime 6.6 and below with Apache Spark 2.4.5 and below to clusters running a current version of the Databricks Runtime. If your jobs and/or notebooks process date conversions, they may fail with a SparkUpgradeException error message after running them on upgraded clusters. Error in SQL statement: SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'YYYY-MM-DD' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html Cause Prior to Spark 3.0, Spark used a combination of the Julian and Gregorian calendars. For dates before 1582, Spark used the Julian calendar. For dates after 1582, Spark used the Gregorian calendar. In Spark 3.0 and above, Spark uses the Proleptic Gregorian calendar. This calendar is also used by other systems such as Apache Arrow, Pandas, and R. The calendar usage is inherited from the legacy java.sql.Date API, which was superseded in Java 8 by java.time.LocalDate and uses the Proleptic Gregorian calendar. Solution You should update your DateTime references so they are compatible with Spark 3.0 and above. For example, if you try to parse a date in the format YYYY-MM-DD, it returns an error in Spark 3.0 and above.\u00a0 select TO_DATE('2017-01-01', 'YYYY-MM-DD') as date Using the format yyyy-MM-dd works correctly in Spark 3.0 and above. select TO_DATE('2017-01-01', 'yyyy-MM-dd') as date The difference in capitalization may appear minor, but to Spark,\u00a0D references the day-of-year, while d references the day-of-month when used in a DateTime function. Review all of the defined Spark DateTime patterns for formatting and parsing for more details. Info\nIf you want to temporarily revert to Spark 2.x DateTime formatting, you can set spark.sql.legacy.timeParserPolicy to LEGACY in a notebook. You can also set this value in the cluster's Spark config (AWS | Azure | GCP).\nWhile this option works, it is only recommended as a temporary workaround.", "format": "html", "updated_at": "2022-10-26T08:05:38.856Z"}, "author": {"id": 840937, "email": "deepak.bhutada@databricks.com", "name": "deepak.bhutada ", "first_name": "deepak.bhutada", "last_name": "", "role_id": "draft_writer", "created_at": "2022-04-06T11:01:04.397Z", "updated_at": "2023-04-21T14:02:10.764Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2861067, "name": "aws"}, {"id": 2861068, "name": "azure"}, {"id": 2861069, "name": "gcp"}, {"id": 2907825, "name": "sparksql"}], "url": "https://kb.databricks.com/sql/using-datetime-values-in-spark-30-and-above"}, {"id": 1474098, "name": "VACUUM best practices on Delta Lake", "views": 2457, "accessibility": 1, "description": "Learn best practices for using, and troubleshooting, VACUUM on Delta Lake.", "codename": "vacuum-best-practices-on-delta-lake", "created_at": "2022-08-11T21:27:33.349Z", "updated_at": "2023-02-03T01:34:19.370Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.b3b8071802e6eb1c7bf360ca61473161\"></div><h1 data-toc=\"true\" id=\"why-use-vacuum-on-delta-lake-0\">Why use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> on Delta Lake?</h1><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> is used to clean up unused and stale data files that are taking up unnecessary storage space. Removing these files can help reduce storage costs.\u00a0</p><p>When you run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> on a Delta table it removes the following files from the underlying file system:</p><ul>\n<li>Any data files that are not maintained by Delta Lake</li>\n<li>Removes stale data files (files that are no longer referenced by a Delta table) and are older than 7 days</li>\n</ul><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-1\">Info</h3>\n<p class=\"hj-alert-text\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> does NOT remove directories that begin with an underscore, such as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">_delta_log</span>.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"when-should-you-run-vacuum-2\">When should you run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>?</h1><p>When you run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> it removes stale data files. This does not impact regular work, but it can limit your ability to <strong>time travel</strong> (<a href=\"https://docs.databricks.com/delta/history.html#configure-data-retention-for-time-travel\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Configure data retention for time travel\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/delta/history#--configure-data-retention-for-time-travel\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Configure data retention for time travel\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/delta/history.html#configure-data-retention-for-time-travel\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Configure data retention for time travel\">GCP</a>).</p><p>The default configuration for a Delta table allows you to time travel 30 days into the past. However, to do this, the underlying data files must be present.</p><p>The default configuration for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> deletes stale data files that are older than seven days. As a result, if you run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> with the default settings, you will only be able to time travel seven days into the past, from the time you run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>.</p><p>If you do not need to time travel more than seven days into the past, you can <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> on a daily basis.</p><p>Running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> daily helps keep storage costs in check, especially for larger tables. You can also run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> on-demand if you notice a sudden surge in the storage costs for a specific Delta table.\u00a0</p><h1 data-toc=\"true\" id=\"issues-you-may-face-with-vacuum-3\">Issues you may face with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>\n</h1><ul>\n<li>\n<strong>No progress update</strong>: You may not know how far the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> has completed, especially when <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> has run for a long time. You may not know how many files have been successfully removed and how many files remain.\u00a0</li>\n<li>\n<strong>Poor run performance</strong>: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> runs for a long time, especially when tables are huge and/or when tables are a source for high frequency input streams.</li>\n</ul><h1 data-toc=\"true\" id=\"mitigate-issues-with-vacuum-4\">Mitigate issues with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>\n</h1><h2 data-toc=\"true\" id=\"no-progress-update-5\">No progress update</h2><p>If <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> completes within an hour or two, there is no need to troubleshoot. However, if <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> runs for longer than two hours (this can happen on large tables when <span id=\"isPasted\" style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> hasn\u2019t been run recently), you may want to check the progress. In this case you can run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DRY RUN</span> option before and after the actual <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> run to monitor the performance of a specific <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> run and to identify the number of files deleted.</p><ol>\n<li>Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM DRY RUN</span> to determine the number of files eligible for deletion. Replace <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;table-path&gt;\u00a0</span>with the actual table path location.<pre>%python\r\n\r\nspark.sql(\"VACUUM delta.`&lt;table-path&gt;` DRY RUN\")</pre>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DRY RUN</span> option tells <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> it should not delete any files. Instead,\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DRY RUN</span> prints the number of files and directories that are safe to be deleted. The intention in this step is not to delete the files, but know the number of files eligible for deletion.<br><br>The example <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DRY RUN\u00a0</span>command returns an output which tells us that there are <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">x\u00a0</span>files and directories that are safe to be deleted.<pre>Found x files and directories in a total of y directories that are safe to delete.</pre>You should record the number of files identified as safe to delete.<br><br>\n</li>\n<li>Run\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>.<br><br>\n</li>\n<li>Cancel\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> after one hour.<br><br>\n</li>\n<li>Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> with\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DRY\u00a0RUN</span> again.<br><br>\n</li>\n<li>The second\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DRY RUN</span> command identifies the number of outstanding files that can be safely deleted.<br><br>\n</li>\n<li>Subtract the outstanding number of files (second <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DRY RUN</span>) from the original number of files to get the number of files that were deleted.</li>\n</ol><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-6\">Info</h3>\n<p class=\"hj-alert-text\">You can also review your storage bucket information in your cloud portal to identify the remaining number of files existing in the bucket, or the number of deletion requests issued, to determine how far the deletion has progressed.</p>\n</div>\n</div><h2 data-toc=\"true\" id=\"poor-run-performance-7\">Poor run performance\u00a0</h2><p>This can be mitigated by following <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> best practices.</p><h3 data-toc=\"true\" id=\"avoid-actions-that-hamper-performance-8\">Avoid actions that hamper performance</h3><h4 data-toc=\"true\" id=\"avoid-over-partitioned-data-folders-9\">Avoid over-partitioned data folders</h4><ul>\n<li>Over-partitioned data can result in a lot of small files. You should avoid partitioning on a high cardinality column. When you over-partition data, even running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> can have issues compacting small files, as compaction does not happen across partition directories.</li>\n<li>File deletion speed is directly dependent on the number of files. Over-partitioning data can hamper the performance of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>.</li>\n</ul><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-10\">Info</h3>\n<p class=\"hj-alert-text\">You should partition on a low cardinality column and z-order on a high cardinality column.</p>\n</div>\n</div><h4 data-toc=\"true\" id=\"avoid-concurrent-runs-11\">Avoid concurrent runs</h4><ul>\n<li>When running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> on a large table, avoid concurrent runs (including dry runs).</li>\n<li>Avoid running other operations on the same location to avoid file system level throttling. Other operations can compete for the same bandwidth.</li>\n</ul><h4 data-toc=\"true\" id=\"avoid-cloud-versioning-12\">Avoid cloud versioning</h4><ul>\n<li>Since Delta Lake maintains version history, you should avoid using cloud version control mechanisms, like <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/manage-versioning-examples.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">S3 versioning on AWS</a>.</li>\n<li>Using cloud version controls in addition to Delta Lake can result in additional storage costs and performance degradation.</li>\n</ul><h3 data-toc=\"true\" id=\"actions-to-improve-performance-13\">Actions to improve performance</h3><h4 data-toc=\"true\" id=\"enable-autooptimizeautocompaction-14\">Enable <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">autoOptimize</span>/<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">autoCompaction</span>\n</h4><ul>\n<li>Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> to eliminate small files. When you combine <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> with regular <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> runs you ensure the number of stale data files (and the associated storage cost) is minimized.</li>\n<li>Review the documentation on <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\"><strong>autoOptimize</strong></span><strong>\u00a0and\u00a0</strong><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\"><strong>autoCompaction</strong></span> (<a href=\"https://docs.databricks.com/optimizations/auto-optimize.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/optimizations/auto-optimize\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/optimizations/auto-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>) for more information.</li>\n<li>Review the documentation on <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\"><strong>OPTIMIZE</strong></span> (<a href=\"https://docs.databricks.com/sql/language-manual/delta-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/delta-optimize\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/sql/language-manual/delta-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>) for more information.</li>\n</ul><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-15\">Info</h3>\n<p class=\"hj-alert-text\">Before you modify table properties, you must ensure there are no active writes happening on the table.</p>\n</div>\n</div><h4 data-toc=\"true\" id=\"use-databricks-runtime-104-lts-or-above-and-additional-driver-cores-azure-and-gcp-only-16\">Use Databricks Runtime 10.4 LTS or above and additional driver cores (Azure and GCP only)</h4><ul><li>On Azure and GCP <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> performs the deletion in parallel on the driver, when using Databricks Runtime 10.4 \u00a0LTS or above. The higher the number of driver cores, the more the operation can be parallelized.\u00a0</li></ul><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-17\">Info</h3>\n<p class=\"hj-alert-text\">On AWS deletes happen in batches and the process is single threaded. AWS uses a bulk delete API and deletes in batches of 1000, but it doesn\u2019t use parallel threads. As a result, using a multi-core driver may not help on AWS.</p>\n</div>\n</div><h4 data-toc=\"true\" id=\"use-databricks-runtime-111-or-above-on-aws-18\">Use Databricks Runtime 11.1 or above on AWS</h4><ul>\n<li>Databricks Runtime 11.1 and above set the checkpoint creation interval to 100, instead of 10. As a result, fewer checkpoint files are created. With less checkpoint files to index, the faster the listing time in the transaction log directory. This reduces the delta log size and improves the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> listing time. It also decreases the checkpoint storage size.</li>\n<li>If you are using Databricks Runtime 10.4 LTS on AWS and cannot update to a newer runtime, you can manually set the table property with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">delta.checkpointInterval=100</span>. This creates checkpoint files for every 100 commits, instead of every 10 commits.<pre>%sql\r\n\r\nalter table &lt;delta-table-name&gt; set tblproperties ('delta.checkpointInterval' = 100)</pre>\n</li>\n</ul><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-19\">Info</h3>\n<p class=\"hj-alert-text\">Reducing the number of checkpoints on Databricks Runtime 10.4 LTS may degrade table query/read performance, though in most cases the difference should be negligible. Before you modify table properties, you must ensure there are no active writes happening on the table.</p>\n</div>\n</div><h4 data-toc=\"true\" id=\"use-compute-optimized-instances-20\">Use compute optimized instances</h4><ul><li>Since <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>is compute intensive, you should use compute optimized instances.<ul>\n<li>On AWS use <strong>C5 series</strong> worker types.</li>\n<li>On Azure use <strong>F series</strong> worker types.</li>\n<li>On GCP use <strong>C2 series</strong> worker types.</li>\n</ul>\n</li></ul><h4 data-toc=\"true\" id=\"use-auto-scaling-clusters-21\">Use auto-scaling clusters</h4><ul>\n<li>Before performing file deletion, VACUUM command lists the files. File listing happens in parallel by leveraging the workers in the cluster. Having more workers in the cluster can help with the initial listing of files. The higher the number of workers, the faster the file listing process.</li>\n<li>Additional workers are NOT needed for file deletion. This is why you should use an auto-scaling cluster with multiple workers. Once the file listing completes, the cluster can scale down and use the driver for the file deletion. This saves cluster costs.</li>\n</ul><p>Review the documentation on how to <strong>enable and configuring autoscaling</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#enable-and-configure-autoscaling\" rel=\"noopener noreferrer\" target=\"_blank\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#enable-and-configure-autoscaling\" rel=\"noopener noreferrer\" target=\"_blank\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#enable-and-configure-autoscaling\" rel=\"noopener noreferrer\" target=\"_blank\">GCP</a>) for more information.</p><h4 data-toc=\"true\" id=\"set-a-higher-trigger-frequency-for-streaming-jobs-22\">Set a higher trigger frequency for streaming jobs</h4><ul>\n<li>Use a trigger frequency of 120 seconds or more for streaming jobs that write to Delta tables. You can adjust this based on your needs.<pre>// ProcessingTime trigger with 120 seconds micro-batch interval\r\n\r\ndf.writeStream\r\n\u00a0 .format(\"console\")\r\n\u00a0 .trigger(Trigger.ProcessingTime(\"120 seconds\"))\r\n\u00a0 .start()</pre>\n</li>\n<li>The higher the trigger frequency, the bigger the data files. The bigger the data files, the lesser the number of total files. The lesser the number of total files, the less time it takes to delete files. As a result, future <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> attempts run faster.</li>\n</ul><h4 data-toc=\"true\" id=\"reduce-log-retention-23\">Reduce log retention</h4><ul>\n<li>If you do not need to time travel far into the past, you can reduce log retention to seven days. This reduces the number of JSON files and thereby reduces the listing time. This also reduces the delta log size.</li>\n<li>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">delta.logRetentionDuration</span> property configures how long you can go back in time. The default value is 30 days. You need to use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ALTER TABLE</span>to modify existing property values.<pre>%sql\r\n\r\nALTER TABLE &lt;table-name&gt;\r\nSET TBLPROPERTIES ('delta.logRetentionDuration'='7 days')</pre>\n</li>\n</ul><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-24\">Info</h3>\n<p class=\"hj-alert-text\">Before you modify table properties, you must ensure there are no active writes happening on the table.</p>\n</div>\n</div><h4 data-toc=\"true\" id=\"run-vacuum-daily-25\">Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> daily</h4><ul>\n<li>If you reduce log retention to seven days (thereby limiting time travel to seven days) you can run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> on a daily basis.</li>\n<li>This deletes stale data files that are older than sever days, every day. This is a good way to avoid stale data files and reduce you storage costs.</li>\n</ul><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-26\">Warning</h3>\n<p class=\"hj-alert-text\">If the Delta table is the source for a streaming query, and if the streaming query falls behind by more than seven days, then the streaming query will not be able to correctly read the table as it will be looking for data that has already been deleted. You should only run a daily <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> if you know that all queries will never ask for data that is more than seven days old.</p>\n</div>\n</div><ul>\n<li>After testing and verification on a small table, you can schedule <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> to run everyday via a job.</li>\n<li>Schedule <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> to run using a job cluster, instead of running it manually on all-purpose clusters, which may cost more.</li>\n<li>Use auto-scaling cluster when configuring the job to save costs.</li>\n</ul><h2 data-toc=\"true\" id=\"summary-27\">Summary</h2><p>To improve <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> performance:</p><ul>\n<li>Avoid over-partitioned directories</li>\n<li>Avoid concurrent runs (during <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>)</li>\n<li>Avoid enabling cloud storage file versioning</li>\n<li>If you run a periodic <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> command, \u00a0enable <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">autoCompaction</span>/<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">autoOptimize</span> on the delta table</li>\n<li>Use a current Databricks Runtime</li>\n<li>Use auto-scaling clusters with compute optimized worker types</li>\n</ul><p>In addition, if your application allows for it:</p><ul>\n<li>Increase the trigger frequency of any streaming jobs that write to your Delta table</li>\n<li>Reduce the log retention duration of the Delta table</li>\n<li>Perform a periodic <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span>\n</li>\n</ul><p>These additional steps further increase <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> performance and can also help reduce storage costs.</p>", "body_txt": "Why use VACUUM on Delta Lake? VACUUM is used to clean up unused and stale data files that are taking up unnecessary storage space. Removing these files can help reduce storage costs.\u00a0 When you run VACUUM on a Delta table it removes the following files from the underlying file system: Any data files that are not maintained by Delta Lake\nRemoves stale data files (files that are no longer referenced by a Delta table) and are older than 7 days Info VACUUM does NOT remove directories that begin with an underscore, such as _delta_log. When should you run VACUUM? When you run VACUUM it removes stale data files. This does not impact regular work, but it can limit your ability to time travel (AWS | Azure | GCP). The default configuration for a Delta table allows you to time travel 30 days into the past. However, to do this, the underlying data files must be present. The default configuration for VACUUM deletes stale data files that are older than seven days. As a result, if you run VACUUM with the default settings, you will only be able to time travel seven days into the past, from the time you run VACUUM. If you do not need to time travel more than seven days into the past, you can VACUUM on a daily basis. Running VACUUM daily helps keep storage costs in check, especially for larger tables. You can also run VACUUM on-demand if you notice a sudden surge in the storage costs for a specific Delta table.\u00a0 Issues you may face with VACUUM No progress update: You may not know how far the VACUUM has completed, especially when VACUUM has run for a long time. You may not know how many files have been successfully removed and how many files remain.\u00a0 Poor run performance: VACUUM runs for a long time, especially when tables are huge and/or when tables are a source for high frequency input streams. Mitigate issues with VACUUM No progress update If VACUUM completes within an hour or two, there is no need to troubleshoot. However, if VACUUM runs for longer than two hours (this can happen on large tables when VACUUM hasn\u2019t been run recently), you may want to check the progress. In this case you can run VACUUM with the DRY RUN option before and after the actual VACUUM run to monitor the performance of a specific VACUUM run and to identify the number of files deleted. Run VACUUM DRY RUN to determine the number of files eligible for deletion. Replace &lt;table-path&gt;\u00a0with the actual table path location.%python spark.sql(\"VACUUM delta.`&lt;table-path&gt;` DRY RUN\")The DRY RUN option tells VACUUM it should not delete any files. Instead,\u00a0DRY RUN prints the number of files and directories that are safe to be deleted. The intention in this step is not to delete the files, but know the number of files eligible for deletion. The example DRY RUN\u00a0command returns an output which tells us that there are x\u00a0files and directories that are safe to be deleted.Found x files and directories in a total of y directories that are safe to delete.You should record the number of files identified as safe to delete. Run\u00a0VACUUM. Cancel\u00a0VACUUM after one hour. Run VACUUM with\u00a0DRY\u00a0RUN again. The second\u00a0DRY RUN command identifies the number of outstanding files that can be safely deleted. Subtract the outstanding number of files (second DRY RUN) from the original number of files to get the number of files that were deleted. Info\nYou can also review your storage bucket information in your cloud portal to identify the remaining number of files existing in the bucket, or the number of deletion requests issued, to determine how far the deletion has progressed. Poor run performance\u00a0 This can be mitigated by following VACUUM best practices. Avoid actions that hamper performance Avoid over-partitioned data folders Over-partitioned data can result in a lot of small files. You should avoid partitioning on a high cardinality column. When you over-partition data, even running OPTIMIZE can have issues compacting small files, as compaction does not happen across partition directories.\nFile deletion speed is directly dependent on the number of files. Over-partitioning data can hamper the performance of VACUUM. Info\nYou should partition on a low cardinality column and z-order on a high cardinality column. Avoid concurrent runs When running VACUUM on a large table, avoid concurrent runs (including dry runs).\nAvoid running other operations on the same location to avoid file system level throttling. Other operations can compete for the same bandwidth. Avoid cloud versioning Since Delta Lake maintains version history, you should avoid using cloud version control mechanisms, like S3 versioning on AWS.\nUsing cloud version controls in addition to Delta Lake can result in additional storage costs and performance degradation. Actions to improve performance Enable autoOptimize/autoCompaction Run OPTIMIZE to eliminate small files. When you combine OPTIMIZE with regular VACUUM runs you ensure the number of stale data files (and the associated storage cost) is minimized.\nReview the documentation on autoOptimize \u00a0and\u00a0 autoCompaction (AWS | Azure | GCP) for more information.\nReview the documentation on OPTIMIZE (AWS | Azure | GCP) for more information. Info\nBefore you modify table properties, you must ensure there are no active writes happening on the table. Use Databricks Runtime 10.4 LTS or above and additional driver cores (Azure and GCP only) On Azure and GCP VACUUM performs the deletion in parallel on the driver, when using Databricks Runtime 10.4 \u00a0LTS or above. The higher the number of driver cores, the more the operation can be parallelized.\u00a0 Info\nOn AWS deletes happen in batches and the process is single threaded. AWS uses a bulk delete API and deletes in batches of 1000, but it doesn\u2019t use parallel threads. As a result, using a multi-core driver may not help on AWS. Use Databricks Runtime 11.1 or above on AWS Databricks Runtime 11.1 and above set the checkpoint creation interval to 100, instead of 10. As a result, fewer checkpoint files are created. With less checkpoint files to index, the faster the listing time in the transaction log directory. This reduces the delta log size and improves the VACUUM listing time. It also decreases the checkpoint storage size.\nIf you are using Databricks Runtime 10.4 LTS on AWS and cannot update to a newer runtime, you can manually set the table property with delta.checkpointInterval=100. This creates checkpoint files for every 100 commits, instead of every 10 commits.%sql alter table &lt;delta-table-name&gt; set tblproperties ('delta.checkpointInterval' = 100) Info\nReducing the number of checkpoints on Databricks Runtime 10.4 LTS may degrade table query/read performance, though in most cases the difference should be negligible. Before you modify table properties, you must ensure there are no active writes happening on the table. Use compute optimized instances Since VACUUMis compute intensive, you should use compute optimized instances.\nOn AWS use C5 series worker types.\nOn Azure use F series worker types.\nOn GCP use C2 series worker types. Use auto-scaling clusters Before performing file deletion, VACUUM command lists the files. File listing happens in parallel by leveraging the workers in the cluster. Having more workers in the cluster can help with the initial listing of files. The higher the number of workers, the faster the file listing process.\nAdditional workers are NOT needed for file deletion. This is why you should use an auto-scaling cluster with multiple workers. Once the file listing completes, the cluster can scale down and use the driver for the file deletion. This saves cluster costs. Review the documentation on how to enable and configuring autoscaling (AWS | Azure | GCP) for more information. Set a higher trigger frequency for streaming jobs Use a trigger frequency of 120 seconds or more for streaming jobs that write to Delta tables. You can adjust this based on your needs.// ProcessingTime trigger with 120 seconds micro-batch interval df.writeStream \u00a0 .format(\"console\") \u00a0 .trigger(Trigger.ProcessingTime(\"120 seconds\")) \u00a0 .start() The higher the trigger frequency, the bigger the data files. The bigger the data files, the lesser the number of total files. The lesser the number of total files, the less time it takes to delete files. As a result, future VACUUM attempts run faster. Reduce log retention If you do not need to time travel far into the past, you can reduce log retention to seven days. This reduces the number of JSON files and thereby reduces the listing time. This also reduces the delta log size.\nThe delta.logRetentionDuration property configures how long you can go back in time. The default value is 30 days. You need to use ALTER TABLEto modify existing property values.%sql ALTER TABLE &lt;table-name&gt; SET TBLPROPERTIES ('delta.logRetentionDuration'='7 days') Info\nBefore you modify table properties, you must ensure there are no active writes happening on the table. Run VACUUM daily If you reduce log retention to seven days (thereby limiting time travel to seven days) you can run VACUUM on a daily basis.\nThis deletes stale data files that are older than sever days, every day. This is a good way to avoid stale data files and reduce you storage costs. Warning\nIf the Delta table is the source for a streaming query, and if the streaming query falls behind by more than seven days, then the streaming query will not be able to correctly read the table as it will be looking for data that has already been deleted. You should only run a daily VACUUM if you know that all queries will never ask for data that is more than seven days old. After testing and verification on a small table, you can schedule VACUUM to run everyday via a job.\nSchedule VACUUM to run using a job cluster, instead of running it manually on all-purpose clusters, which may cost more.\nUse auto-scaling cluster when configuring the job to save costs. Summary To improve VACUUM performance: Avoid over-partitioned directories\nAvoid concurrent runs (during VACUUM)\nAvoid enabling cloud storage file versioning\nIf you run a periodic OPTIMIZE command, \u00a0enable autoCompaction/autoOptimize on the delta table\nUse a current Databricks Runtime\nUse auto-scaling clusters with compute optimized worker types In addition, if your application allows for it: Increase the trigger frequency of any streaming jobs that write to your Delta table\nReduce the log retention duration of the Delta table\nPerform a periodic VACUUM These additional steps further increase VACUUM performance and can also help reduce storage costs.", "format": "html", "updated_at": "2023-02-03T01:34:19.351Z"}, "author": {"id": 790229, "email": "mathan.pillai@databricks.com", "name": "mathan.pillai ", "first_name": "mathan.pillai", "last_name": "", "role_id": "admin", "created_at": "2022-01-26T19:28:01.464Z", "updated_at": "2023-04-28T22:28:56.701Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256849, "name": "Delta Lake", "codename": "delta", "accessibility": 1, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3009307, "name": "aws"}, {"id": 3009308, "name": "azure"}, {"id": 3009310, "name": "best practices"}, {"id": 3009309, "name": "gcp"}, {"id": 3009312, "name": "performance"}, {"id": 3009314, "name": "slow"}, {"id": 3009313, "name": "speed"}, {"id": 3009311, "name": "vacuum"}], "url": "https://kb.databricks.com/delta/vacuum-best-practices-on-delta-lake"}, {"id": 1456580, "name": "Pass arguments to a notebook as a list", "views": 4014, "accessibility": 1, "description": "Use a JSON file to temporarily store arguments that you want to use in your notebook.", "codename": "pass-arguments-to-a-notebook-as-a-list", "created_at": "2022-08-03T06:58:53.887Z", "updated_at": "2022-10-29T21:49:49.476Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThrTElvWmxkRDBYOCtZMXNTWHNoRjFJSWkxb2MrU1lQT3E3ZUtjeW03b3RwYXNKVnlyCnVwVDJtM1NhOTlMd1VRMFZpcEl2ckhqREN3VlgzMTY5eE83U1gxeDRFT2Z3bWxNM2poZXlkd0lPVG43bApvYVVabUpuRSszaS8yNnFvOXk1ajRrOGtZVEt4VGlHVm5jYjA3b3RhbEZZbFVKNGNMT2VZbHJKZm9zaTAKK1NFUXNEQUpwLzNrbnpaTjg1RGQxZVFDRjdjV1pOenQ1bWlyVXpOVXFQZjNhTGxVRHB6dGtpc3V5dk9hCi8yR3A5aE8wREZ5SlFKQ2tyV3lhUXkvRGJXZnZWUEYvUXErU0Z0UmgzNnJVejBwelozcUJxZ2xFK3MreAplVlRCZkh2cHdtamdGcHBrT0t2VG1tNnJTalR3S1ptU3lCYm9aeWxlRkI0bmpLQnRjQzdUTXArWVY2NnIKTSt1dWlubTlTVktFZ3IrMVZrRUZmNDg4WDBNMlBBYVAyTkg4S0lXTFlvRVpxQ3lRQllVc2dhenNoa2JWCmxVK21tZmJWL3VpQXdVNmhXcVh5c25zVWRxejJlbXdmbzJ5SkMwOStEMy8rN3pwNXZ4bnFZT1ZZQUNzUgpSZVpMeWszTi9zVmhreEhueEJUTWlIU3VtclpTanRFZ3l3UkY1RFBtcElTSGRjTWV6MWdvYUJrYUdxWHEKR05kTFJoSDRyYmVlL2Q4OGZ2NUxIUWtPMWJ2K1d2ak1wYWRVYzdaWHZXSVJIYlBYUDZpS1ZoWUszc0VWClpIZVlhZ1hZRUdjWkNjemFkQkJQMHBGOERyaFZXWEhzQUt1a3U4a3o3TFFadGgxc2FlRWFRcXBnYkI4NwpxNkVRNDhCRll3ZU1wT2NRZnd6RzVNdlZXOGhWTDhDOWdzNDROYkx0cFY4K21yWlFNZll0eTI3aDVYRXkKV2lxOWx2R3FvVCtxcGY2ZkFTdlRGbk4vbzBjYmJOcW1XaXhoa2FTQWw5UHlJVUcxWS9vdE5TL3RmbUNNClZLOHBYWFlEcGEvVlkwSndyRlQyMjdyTXVhZ1R3K1ppTGdnemdKVFdCR3NMSytNV2RtNkdtWGRTZXh3MAprZnhqWkVxNDN0K1RjN3dtUFBLSkl5dm40YVFQdmlmSHlaYjFwdHZzclpobzlTQXBxdGNyVlVLUTErdlUKOXNpL2g1dm9FR0ZXeXZQVDNmdXo2amVoZlljQitvRFg0SGpJeStwWWlwcTE0L2k1amZtMHFHdmV1Z21tCkpudDQzMk1LeVhOR0lKMjAwYnpJZzdnQnZsdFBXcSttVTI0bEh0N2dhQ2Z3Kzd0YTZNcXRlQ1JCblQycApwQ0E4VXo0NE5GRzR2NDI5c094ZlhZVHl6WWsrckcyeXRTVS96WGJlVHg1WkhFNXQ3ZmRvZy9jOFlJbUsKR1VEQlkwdEo0TjcrQk1tOTJJd0Y5SGxtQm9Hekx6QXFjaTBYM05OaGpERGJCVUNIdjVibDF3UjVsR2JTCkozNFVPeUFnam82ZE4rUlhlUXl3RG10eC9Ka1VkcEp3cnl1ZXdYN1AralNHMG80bGdYT2ZqT2lkclAvVQpLTlVPSzFheTA0ckNUQ25JMXFuZzJwdUl2OWVjWGg2YUxmTHFGeU5rWHdtb2RPS2hTNnhsUnEybDk3b0kKZHFrUzBFbUwwRkdZV3kxUGlQUUFSNWo3WTJHR1A3b0hZbVcxZGNCVmZZQitHcS9xSGZxampMOFYrRDFjCmhKNW5ENUttZHc1WTBJTDRyVWxlblZ3OWorSU85SWhpUXEvNTVHSFZ3TnpaWnNnM0RKQkMxc2daVUYyagpaMEFIdzhOUjdlUWFLTytjekNFT3NzT2dsWWUvRXAvSkdJVDBtamhON0J2UUlsRW8rNUV1KzVVTFhoTnUKZVNxdmE2bThiUVk9Cg==.c1a6d8e9e105115d0db6d999ca31f858\"></div><p>There is no direct way to pass arguments to a notebook as a\u00a0dictionary or list.</p><p>You can work around this limitation by\u00a0serializing your\u00a0list as a JSON file and then passing it as one argument.</p><p>After passing the JSON file to the notebook, you can parse it with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">json.loads()</span>.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><p>Define the argument list and convert it to a JSON file.</p><ol>\n<li>Start by defining your argument list.<pre>args = {'arg_1': 'a', 'arg_2': 'b', 'arg_3': 'c', 'arg_4': 'd', 'arg_5': 'e'}</pre>\n</li>\n<li>Declare the arguments as a dictionary.\u00a0</li>\n<li>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">json.dumps()</span> to convert the dictionary to JSON.</li>\n<li>Write the converted JSON to a file.<pre>%python\r\n\r\nimport json\r\n\r\nargs = {\"arg_1\": \"a\", \"arg_2\": \"b\", \"arg_3\": \"c\", \"arg_4\": \"d\", \"arg_5\": \"e\"}\r\n\r\nfile = open(\"arguments.txt\", \"w\")\r\nfile.write(json.dumps(args)) \u00a0\r\nfile.close()\u00a0\r\n\r\nprint(\"Argument file is created.\")</pre>\n</li>\n</ol><p>\u00a0<br>Once the argument file is created, you can open it inside a notebook and use the arguments.</p><ol>\n<li>Open the JSON file that contains the arguments.</li>\n<li>Read the contents.</li>\n<li>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">json.loads()</span> to convert the JSON back into a dictionary.\u00a0</li>\n<li>Now the arguments are available for use inside the notebook.\u00a0<pre>%python\r\n\r\nimport json\r\n\r\nfile = open(\"arguments.txt\", \"r\")\r\ndata = file.read()\r\nfile.close()\r\n\r\nargs = json.loads(data)\r\nprint(args)</pre>\n</li>\n</ol><p><br>You can use this example code as a base for your own argument lists.</p><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"e762c1d643e56\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThZakNZTmJsTG1BY21kSy9lNi9UNTlVZTVudTRMTEFwQlRrSEEvN1phUVQvNk92aWNWCklMdnd0VlFqVlB4UHVRNEV5YmZkTE1KTnZQeGZOWFZzUnRBNFJlY09zYkpEWmZ6YU1UZXBGWUprYVBqbwpNbVowdVV3cUNGU3RIbisrN1RFR0VMSXRDNEtreFRsUEJjR2lTVUJFQWx6WC9BSE9tOVQyaysydnVlQWQKRjZlcnQvTGx6bEJyUTZVeHhJVUhKODVwcmdOWTBQcVJkV0xOTHFoOVJjMDBRc2dJK2JRR2htdDMzMHVrCmxMbEdac3FOWC84Q0FKdXlyMW5DVlJjSFljcmJ1aDBjZmZnQTR4TndTak5FV09mMWVNWFJGOEZoeXRtVgo2RkZ5VkxwSWJKZVNyMHdEcDBlelpaY0hsUXNXc1Z5MTJZekVlMnB4bnBNTWlDNGJLMEN0UmVMWHNGVkkKbTRCczFYeGUvTWJOdmtzdzZzbGNOY25RamN5MER6bmNRWnV3SmlzVzBvU1hHNXVFUkovU1ZuOUVwSnBaCkhUMnJoam9rMUVtRGhWaXBZRmRYUzlKd3BzeXBKMkNJNENGMkdzeG1tTjQ5Tkxva1lRSUR4aHMvS09TcgpLYmR6NWZyVWpldFNjV21qM0NYZ1dBeGZLYUFaTkQvOHdVQWkrUjJFUGtwdDJxbWhRaVlweCtyazV0ZjAKZVdIOTVvWDVxd2trajEzV0dFSSt3YjlJTkdTaHRCczQ1YWJZUmdVUjRyUEE4aVZuNlExY1ozY0VCaXJ4CnpiVlRvTXpOa3NQdEZRL3RiQW5aalZ1UmkxNjRCU0tWcUhPc3RFQStlTDJmWllWbXlaemwrTFpXa2wyagpTbE5kbjhQYldPdnQwUVA4NmUvSTBGOURRUkV4c3pPeElZR0RvMnJGOEtTQlFFcVRyMHB1OTBya2ZRenYKNnJJWDJOL1JaWEFGeDY2dEdoSnZLTnZqMHRQQzM5ZmRTdUpMQWNucERnYlJNYklaQjMxR2FHRXBnRWQyCmNuQnJDbVAvbFR1UlZia0d1M0FMSVcvV2pYSEVNTzVyWCtabjhMTExNWW5MenpRNW9iUEU2WVRSOWFIWApaRVRTSkNVUHl1WXNzTVNYamtzdWtrWUdiZmJiMWIzYUxsZjR4eXBnQU5oL1pUSGlsanJBQUZ3MWtHQUQKV3hwdHVJZ2p0VlZVVmhKZ1pHNGl6NFNNRlFuZ3g5aEUxcE5jUlNzbTZSRGdhUnFKaEo0NWRwNWVLWWc3CkszNXhhaDJDcDJHL3U4U0FIQ040bFE9PQo=.b8a732bf898aa515134a65aeabe02b1f\"></div><p><br></p><p><br></p>", "body_txt": "There is no direct way to pass arguments to a notebook as a\u00a0dictionary or list. You can work around this limitation by\u00a0serializing your\u00a0list as a JSON file and then passing it as one argument. After passing the JSON file to the notebook, you can parse it with json.loads(). Instructions Define the argument list and convert it to a JSON file. Start by defining your argument list.args = {'arg_1': 'a', 'arg_2': 'b', 'arg_3': 'c', 'arg_4': 'd', 'arg_5': 'e'} Declare the arguments as a dictionary.\u00a0\nUse json.dumps() to convert the dictionary to JSON.\nWrite the converted JSON to a file.%python import json args = {\"arg_1\": \"a\", \"arg_2\": \"b\", \"arg_3\": \"c\", \"arg_4\": \"d\", \"arg_5\": \"e\"} file = open(\"arguments.txt\", \"w\") file.write(json.dumps(args)) \u00a0 file.close()\u00a0 print(\"Argument file is created.\") \u00a0Once the argument file is created, you can open it inside a notebook and use the arguments. Open the JSON file that contains the arguments.\nRead the contents.\nUse json.loads() to convert the JSON back into a dictionary.\u00a0\nNow the arguments are available for use inside the notebook.\u00a0%python import json file = open(\"arguments.txt\", \"r\") data = file.read() file.close() args = json.loads(data) print(args) You can use this example code as a base for your own argument lists.", "format": "html", "updated_at": "2022-10-29T21:49:49.471Z"}, "author": {"id": 798433, "email": "pallavi.gowdar@databricks.com", "name": "pallavi.gowdar ", "first_name": "pallavi.gowdar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-04T12:27:36.050Z", "updated_at": "2023-02-06T13:31:44.617Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256850, "name": "Jobs", "codename": "jobs", "accessibility": 1, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2907843, "name": "args"}, {"id": 2922522, "name": "aws"}, {"id": 2922523, "name": "azure"}, {"id": 2922524, "name": "gcp"}, {"id": 2907842, "name": "json"}], "url": "https://kb.databricks.com/jobs/pass-arguments-to-a-notebook-as-a-list"}, {"id": 1452230, "name": "Structured streaming jobs slow down on every 10th batch", "views": 5258, "accessibility": 1, "description": "Automatic compaction of the metadata folder can slow down structured streaming jobs.", "codename": "structured-streaming-jobs-slow-down-on-every-10th-batch", "created_at": "2022-07-29T02:31:29.267Z", "updated_at": "2022-10-28T12:42:26.965Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"7953,7942\" data-permitted-users=\"\" data-encrypted-content=\"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.f960c0bdda4ec1d5ec03d0163bd351a6\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are running a series of structured streaming jobs and writing to a file sink. Every 10th run appears to run slower than the previous jobs.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The file sink creates a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">_spark_metadata</span> folder in the target path. This metadata folder stores information about each batch, including which files are part of the batch. This is required to provide an exactly-once guarantee for file sink streaming. By default, on every 10th batch, the previous nine batch data files are compacted into a single file at <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/&lt;target-folder&gt;/data/_spark_metadata/9.compact</span>.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>There are three possible solutions. Choose the one that is most appropriate for your situation.</p><ul>\n<li>Option 1: Mitigates the issue in a production environment, with minimal code changes, but retains less metadata.</li>\n<li>Option 2: Recommended if you can switch to using Delta tables. This is a good long-term solution.</li>\n<li>Option 3: Recommended if the pipeline doesn't require exactly-once semantics or downstream can handle duplicates.\u00a0</li>\n</ul><h2 data-toc=\"true\" id=\"option-1-shorten-metadata-retention-time-3\">Option 1: Shorten metadata retention time</h2><p>The metadata folder grows larger over time by default. To mitigate this, you can set a maximum retention time for the output files. Files older than the retention period are automatically excluded, which limits the number of files in the metadata folder. Fewer files in the metadata folder means compaction takes less time.</p><p>Set the retention period when you write the streaming DataFrame to your file sink:</p><pre data-stringify-type=\"pre\" id=\"isPasted\" style='box-sizing: inherit; margin: 4px 0px; padding: 8px; --saf-0:rgba(var(--sk_foreground_low,29,28,29),0.13); overflow-wrap: break-word; font-size: 12px; font-variant-ligatures: none; line-height: 1.50001; tab-size: 4; white-space: pre-wrap; word-break: normal; font-family: Monaco, Menlo, Consolas, \"Courier New\", monospace !important; background: rgba(var(--sk_foreground_min,29,28,29),0.04); border: 1px solid var(--saf-0); border-radius: 4px; counter-reset: list-0 0 list-1 0 list-2 0 list-3 0 list-4 0 list-5 0 list-6 0 list-7 0 list-8 0 list-9 0; color: rgb(29, 28, 29); font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>%python\r\n\r\ncheck_point = '&lt;checkpoint-folder-path&gt;'\r\ntarget_path = '&lt;target-path&gt;'\r\nretention = '&lt;retention-time&gt;' # You can provide the value as string format of the time in hours or days. For example, \"12h\", \"7d\", etc. This value is disabled by default\r\n\r\ndf.writeStream.format('json').mode('append').option('checkPointLocation', check_point).option('path', target-path).option('retention', retention).start() </pre><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-4\">Info</h3>\n<p class=\"hj-alert-text\">Retention defines the time to live (TTL) for output files. Output files committed before the TTL range are excluded from the metadata log. Attempts to read the sink's output directory will not process any files older than the TTL range.</p>\n</div>\n</div><h2 data-toc=\"true\" id=\"option-2-use-a-delta-table-as-the-sink-5\">Option 2: Use a Delta table as the sink</h2><p>Delta tables do not use a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark_metadata</span> folder and they provide exactly-once semantics.</p><p>For more information, please review the documentation on using a Delta table as a sink (<a href=\"https://docs.databricks.com/delta/delta-streaming.html#delta-table-as-a-sink\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Delta table as a sink\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/delta/delta-streaming#--delta-table-as-a-sink\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Delta table as a sink\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/delta/delta-streaming.html#delta-table-as-a-sink\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Delta table as a sink\">GCP</a>).\u00a0</p><h2 data-toc=\"true\" id=\"option-3-use-foreachbatch-6\">Option 3: Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">foreachBatch</span>\n</h2><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">foreachBatch</span> does not create a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark_metadata</span> folder when writing to the sink.\u00a0</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-7\">Warning</h3>\n<p class=\"hj-alert-text\">Exactly-once semantics are not supported with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">foreachBatch</span>. Only use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">foreachBatch</span> if you are certain that your application does not require exactly-once semantics.</p>\n<p class=\"hj-alert-text\">This warning can be disregarded if you are writing to a Delta table.</p>\n</div>\n</div>", "body_txt": "Problem You are running a series of structured streaming jobs and writing to a file sink. Every 10th run appears to run slower than the previous jobs. Cause The file sink creates a _spark_metadata folder in the target path. This metadata folder stores information about each batch, including which files are part of the batch. This is required to provide an exactly-once guarantee for file sink streaming. By default, on every 10th batch, the previous nine batch data files are compacted into a single file at /&lt;target-folder&gt;/data/_spark_metadata/9.compact. Solution There are three possible solutions. Choose the one that is most appropriate for your situation. Option 1: Mitigates the issue in a production environment, with minimal code changes, but retains less metadata.\nOption 2: Recommended if you can switch to using Delta tables. This is a good long-term solution.\nOption 3: Recommended if the pipeline doesn't require exactly-once semantics or downstream can handle duplicates.\u00a0 Option 1: Shorten metadata retention time The metadata folder grows larger over time by default. To mitigate this, you can set a maximum retention time for the output files. Files older than the retention period are automatically excluded, which limits the number of files in the metadata folder. Fewer files in the metadata folder means compaction takes less time. Set the retention period when you write the streaming DataFrame to your file sink: %python check_point = '&lt;checkpoint-folder-path&gt;' target_path = '&lt;target-path&gt;' retention = '&lt;retention-time&gt;' # You can provide the value as string format of the time in hours or days. For example, \"12h\", \"7d\", etc. This value is disabled by default df.writeStream.format('json').mode('append').option('checkPointLocation', check_point).option('path', target-path).option('retention', retention).start() Info\nRetention defines the time to live (TTL) for output files. Output files committed before the TTL range are excluded from the metadata log. Attempts to read the sink's output directory will not process any files older than the TTL range. Option 2: Use a Delta table as the sink Delta tables do not use a spark_metadata folder and they provide exactly-once semantics. For more information, please review the documentation on using a Delta table as a sink (AWS | Azure | GCP).\u00a0 Option 3: Use foreachBatch foreachBatch does not create a spark_metadata folder when writing to the sink.\u00a0 Warning\nExactly-once semantics are not supported with foreachBatch. Only use foreachBatch if you are certain that your application does not require exactly-once semantics.\nThis warning can be disregarded if you are writing to a Delta table.", "format": "html", "updated_at": "2022-10-28T12:42:26.961Z"}, "author": {"id": 791529, "email": "gopinath.chandrasekaran@databricks.com", "name": "gopinath.chandrasekaran ", "first_name": "gopinath.chandrasekaran", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T02:33:30.987Z", "updated_at": "2023-02-17T09:49:59.033Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256866, "name": "Streaming", "codename": "streaming", "accessibility": 1, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2922515, "name": "addbatch"}, {"id": 2922512, "name": "aws"}, {"id": 2922513, "name": "azure"}, {"id": 2924712, "name": "compaction"}, {"id": 2922517, "name": "delta"}, {"id": 2922516, "name": "foreachbatch"}, {"id": 2922514, "name": "gcp"}, {"id": 2924714, "name": "metadata"}], "url": "https://kb.databricks.com/streaming/structured-streaming-jobs-slow-down-on-every-10th-batch"}, {"id": 1446532, "name": "Uncommitted files causing data duplication", "views": 3427, "accessibility": 1, "description": "Partially uncommitted files from a failed write can result in apparent data duplication. Adjust VACUUM settings to resolve the issue.", "codename": "uncommitted-files-causing-data-duplication", "created_at": "2022-07-26T02:29:17.969Z", "updated_at": "2022-11-08T06:31:30.592Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThpSWxLRUVWSVZUdWdmTnBoUjd2NXBMNzkyOVI4bkZhVU5SbU1uY2NXRy92UGQ3Zkk2ClJpS3NNWFVTRlBJcnQ0UW0wTUcrZHNHSUJUOXlKdXBnNnpRa2VlQjRubUxZMFUvbFQ5MkhPWGZnZlZhKwpCRlMwM2ZOMWZoWHRzNWZlV3BQSlkwcTJGOG1uL2R4eUcrbXpZcTVBeWZUU2l2emorbms2VGh1ZCtTT1cKVkJKQjRTVXJIdmRvNjhWNWFrUG1DK1UwTFY5bXpkZldQTDk5d1piTUZSVS82S1BxdXNvY0RIQk9qdDZkCncyejdUQmQ3YXIvWlhkSjZoVUpvbEI1Mk1sRkl3a1BQRkNVVHFyNmpocEw0dE1FWldwc0FncnNWVXd0dwpNZ2dtaE5HaGpXTmhYU2dBTWtlT2tHVTF2TkVzcGJ4MjF1YWVUbUYwcE9aNjBxdXRnQmdZbmlIQnF1Vm4KM0pvWjNSbGQ0cHRvWFJGZExZdDVyT2wwUFdUTW13c0JiT09GaDZ4dXJUQldzM0g1WHViRXZCcXpIY2IrCjl5RGxHQWdMc3lqNW5HbkFneTBPam5sbDdKMGhxdjdSbEJQbFJBdDhyUjl0NEZhdGdZTEhoSDNmNlorWgpiYmh6U3NqbW12WTl1R2NCbmpkS3VWQUpKZWxxZkt6L2JUQ1ViSmgrMENSemZXeEZwalczZ1A5VTlTS2IKNzRaSW1ia3prajhESzVjcE5VT2FsTmJZKys4Mm5jT1g3R2t4WDR2MSsyN3Ura1lUNmsxY04zT3BiZ2h6CmdNZUFqZ3hSNGl4ek9rWE0vY2RRaHJZQTdHZVZmd3c5ZWd3Znc5VmQvcXlwT210TW5Yajk1anJTRU5nNgpweFpjK0VPY01ka1cycmxsUWFRR0tQZHpFQkxta01TamNLZkZyTVNBRS9HMkNwV2xjYVg0d1lTaVVrTmMKMS9vd25FTkVCTnliUldwbmxIa1MyY3JidmY5TGxFSEw4bTI4OTY5Ti9YSGgwVWc5bkVtaFlOVkJOVW1sCmEybjFjTEVPVGZiQXowenJxWkkvQlMvM3NPcElPM2lKdUppdjJTOTNoaVY2QUU3QVYzN2NSOHBVRE1zNgpObmdMbE1vbTNMZFI1cVRoanhleG1kRkJRcGZzOWdncUtUZk9vNnZmcFJ5cE80ZXNhd250aXI1dEdsRDAKUkI1WDNXc1FuOXNkYlU2SEZJMDM4SU1XVEVURUNaQXUxSmV2WDRNUDIrbEFvSE0vK1h0bVdJQzNxMVFrCndFUm5UUEhGZzNqMGVVNzlpNzdLL0dQaEE2M3cxemVQZXh1K2c2aUkxaW53bEEwTm45bkE1Y2o5UVUzSAp3NlhNeEFPRzBYL1pHaTRoOWFGRllhaVg1YWhmNTdPeW9JdzQxTnJibzJZcVJmN05FbEphNll4M2ZHMWEKeEZsN2JpelhhczR4NmxwUWRsWTMxbWVLNEMyS0VUaWlsRWE5ci9vQ2Rya00vR3VwNWNVUjJWblFRdHJhCjd1NmRYNXhIMEtTSDluS0w2QVlXWlJ5bEdTS3lRMUJXbTFIa3pncHF4OFB2SXVKbjVDekUvVzlnREs5SwpSeWFFRUU1bzZKUmRhcXJYYzZJeEtna3VTNVFCbmx4WFdPWlhPMndSeWlvWEhyWFo5R09qVnY2a1BrWDMKTDZZR3pNWThLazlGbi9odDVkdXp1MWZyRnFmaWEzUHBSbElJUGtmb1cvUmNqQlVCMW1nSFJHNWg2cVVSCi9YMDBPWnZxblJtTlkrdGhFd2dObEM1Nkw5QUdoMW5rSytKcGlKVUMra2xBQjhrZEZ2dGpnNjNMRHFiRQpFN1BzT2ZRVHJBMXRDZlZRUExMRnJ6V2taUVRmMFNqVXpNUVdESG9LblFtNTlUcS8xdz09Cg==.431c8f7df63130e418bd29115e1ef765\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You had a network issue (or similar) while a write operation was in progress. You are rerunning the job, but partially uncommitted files during the failed run are causing unwanted data duplication.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">How Databricks commit protocol works:</p><ul>\n<li>The DBIO commit protocol (<a href=\"https://docs.databricks.com/spark/latest/spark-sql/dbio-commit.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/spark/latest/spark-sql/dbio-commit\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/spark/latest/spark-sql/dbio-commit.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>) is transactional. Files are only committed after a transaction successfully completes. If the job fails in the middle of a transaction, only <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">_started_&lt;id&gt;</span> and other partially written data files are stored.\u00a0</li>\n<li>When the job is rerun, a new <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">_started_&lt;id&gt;</span> file is created. Once the transaction is successfully completed a new <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">_committed_&lt;id&gt;</span> file is generated. This <span id=\"isPasted\" style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">_committed_&lt;id&gt;</span> file is a JSON file that contains all the parquet file names to be read by the upstream.</li>\n<li>If you read the folder using Apache Spark there are no duplicates as it only reads the files which are inside <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">_committed_&lt;id&gt;</span>.</li>\n<li>To delete the uncommitted data files from the target path, DBIO runs <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> at the end of every job. By default, uncommitted files older than 48 hours (2 days) are removed.</li>\n</ul><p>When the issue occurs:</p><ul><li>If you read the folder within two days of the failed job, using another tool (which does not use DBIO or Spark) or read the folder with a wildcard (<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.read.load('/path/*')</span>), all the files are read, including the uncommitted files. This results in data duplication.</li></ul><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>The ideal solution is to only use Spark or DBIO to access file storage.</p><p>If you must preserve access for other tools, you should update the value of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.databricks.io.directoryCommit.vacuum.dataHorizonHours</span> in your cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>).</p><p>You can also update this property in a notebook:</p><pre>spark.conf.set(\"spark.databricks.io.directoryCommit.vacuum.dataHorizonHours\",\"&lt;number-of-hours&gt;\")</pre><p>This property determines which files are deleted when the automatic <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACCUM</span> runs at the end of every job. Any file older than the time specified is removed.</p><p>The default value is 48 hours (2 days). You can reduce this to as little as one hour, depending on your specific needs. If you set the value to one hour, the automatic <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACCUM</span> removes any uncommitted files older than one hour at the end of every job.</p><p>Alternatively, you can run VACUUM manually after rerunning a failed job with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RETAIN HOURS</span> value low enough to remove the partially uncommitted files..</p><p>Please review the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> (<a href=\"https://docs.databricks.com/sql/language-manual/delta-vacuum.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"VACUUM\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/delta-vacuum\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"VACUUM\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/sql/language-manual/delta-vacuum.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"VACUUM\">GCP</a>) documentation for more information.</p><p>See vacuum documentation: <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-vacuum.html\">https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-vacuum.html</a></p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-3\">Warning</h3>\n<p class=\"hj-alert-text\">Running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RETAIN HOURS</span> set to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">0</span> can cause data consistency issues. If any other Spark jobs are writing files to this folder, running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RETAIN 0 HOURS</span> deletes those files. In general, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> should not have a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RETAIN HOURS</span> value smaller than <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">1</span>.</p>\n</div>\n</div><p><br></p><p><br></p>", "body_txt": "Problem You had a network issue (or similar) while a write operation was in progress. You are rerunning the job, but partially uncommitted files during the failed run are causing unwanted data duplication. Cause How Databricks commit protocol works: The DBIO commit protocol (AWS | Azure | GCP) is transactional. Files are only committed after a transaction successfully completes. If the job fails in the middle of a transaction, only _started_&lt;id&gt; and other partially written data files are stored.\u00a0\nWhen the job is rerun, a new _started_&lt;id&gt; file is created. Once the transaction is successfully completed a new _committed_&lt;id&gt; file is generated. This _committed_&lt;id&gt; file is a JSON file that contains all the parquet file names to be read by the upstream.\nIf you read the folder using Apache Spark there are no duplicates as it only reads the files which are inside _committed_&lt;id&gt;.\nTo delete the uncommitted data files from the target path, DBIO runs VACUUM at the end of every job. By default, uncommitted files older than 48 hours (2 days) are removed. When the issue occurs: If you read the folder within two days of the failed job, using another tool (which does not use DBIO or Spark) or read the folder with a wildcard (spark.read.load('/path/*')), all the files are read, including the uncommitted files. This results in data duplication. Solution The ideal solution is to only use Spark or DBIO to access file storage. If you must preserve access for other tools, you should update the value of spark.databricks.io.directoryCommit.vacuum.dataHorizonHours in your cluster's Spark config (AWS | Azure | GCP). You can also update this property in a notebook: spark.conf.set(\"spark.databricks.io.directoryCommit.vacuum.dataHorizonHours\",\"&lt;number-of-hours&gt;\") This property determines which files are deleted when the automatic VACCUM runs at the end of every job. Any file older than the time specified is removed. The default value is 48 hours (2 days). You can reduce this to as little as one hour, depending on your specific needs. If you set the value to one hour, the automatic VACCUM removes any uncommitted files older than one hour at the end of every job. Alternatively, you can run VACUUM manually after rerunning a failed job with a RETAIN HOURS value low enough to remove the partially uncommitted files.. Please review the VACUUM (AWS | Azure | GCP) documentation for more information. See vacuum documentation: https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-vacuum.html Warning\nRunning VACUUM with RETAIN HOURS set to 0 can cause data consistency issues. If any other Spark jobs are writing files to this folder, running VACUUM with RETAIN 0 HOURS deletes those files. In general, VACUUM should not have a RETAIN HOURS value smaller than 1.", "format": "html", "updated_at": "2022-11-08T06:31:30.589Z"}, "author": {"id": 791529, "email": "gopinath.chandrasekaran@databricks.com", "name": "gopinath.chandrasekaran ", "first_name": "gopinath.chandrasekaran", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T02:33:30.987Z", "updated_at": "2023-02-17T09:49:59.033Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256850, "name": "Jobs", "codename": "jobs", "accessibility": 1, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2898104, "name": "aws"}, {"id": 2898105, "name": "azure"}, {"id": 2898103, "name": "dbio"}, {"id": 2898107, "name": "gcp"}, {"id": 2898106, "name": "parquet"}], "url": "https://kb.databricks.com/jobs/uncommitted-files-causing-data-duplication"}, {"id": 1444419, "name": "Cannot read audit logs due to duplicate columns", "views": 6007, "accessibility": 1, "description": "Case-sensitive parameter names cause a duplicate column error when reading audit logs.", "codename": "cannot-read-audit-logs-due-to-duplicate-columns", "created_at": "2022-07-22T12:01:13.136Z", "updated_at": "2022-07-22T12:11:32.141Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTkyWk9xK29GOXBab3VlUWEvVE9NaGIwNzAzZWVESkNTS3RUS1g4Ymhic2IrQlZjSXBHClFMV1FwSUNuNmd4ZFowbWUxMjViaTBhNThEMGo3d05OdVYrNXNkOEVWb0w1MTFodi94cDlzSmFONTEyOQpENjFhUHNEc3Q0a05WbSt3YTdiTFcxc0Q4SlJsSS9uTXQzUU1DYXpzT0NXQVU4cGdHZ2c4R0VqcmNncncKRVp1NVRpTmE5REFRTjVuMnRHVEY3aGJHNCtjTFhLc2ZqdDJEOExmeHlwQ01ZV3RJR3F0RmdudEt4bVVMCm90TjdaSkR5RjVjY0Zpd0IrY0M1M1VQK2JnSnRJbDRETG5rN3JBby9JTmJLVzZOd291Vml0R3VpcEhzRwo2d3YyTE5WRW5nc2V0QkdrZnEzbHNuR2hwNGUwTEZwN2Y2WDJOU1l1OWxpV2RBNGJFUWpnM3VMUDJtSDIKWml0UEg5V2M1ak9rV0pZbGVuNlNoeXF6N3U2RGp1SS9hSUJoY3VZNERTeTg3L0dlVVl3cHc5NW1HV0F6CjN5Wko3SEFYVVNwcC9UOHVFQXdiK2lJZCtsUnpWRXBQcmFybTVwSk0vbHducW9OVThTS2xPMkRNS1hnUgpJM2g1bzdtaXVONnhMVFBoaFgvbjRWNWFYcWhCVlVNOGJra1dDNnpEenhzeGtYdm5jS0sycHlyVTdXaUUKYWN6bkRJWDd4L2I4REhaK0VLSTU0c0Z5ZXgzb1JHU0hRaXhMWERKb3JoUkF3MUJRL0dMemhTZDdQQUNSCnp4VnV0eTFLZnl3QVN4eEdaZTdoMnlSSDlrQ1NaZUhSUEVySnlEVVBvNHU2cmgvL2dOekwzZXBFZ2lWRwprWm9CcjVObnowdERzR0orTVFKbmloYWI4ek1oQlZJMWVVSDhKdzIzNWpoKzgzZnhMUkwxUDBkamdEMm8KUHErV25tUjBSakRxZmt6eDlGVDMrdjVGcEhvVHU2RTRBUVhVYnJMS0lsK3paeWozWG9kN2RRYmh1WnZ3ClEwSmJWRGM3RUFleGdGMjN0MnVhSHRkQjJBTm1mM0ZpVk1oNWYyVmFZZzUxWVE4eWVuUW5PTlVOUjE4ZQpJWnVzUEQ1b3RNVnRyUTJDaXpTNWpBOVNpK01JTHRKcmJtaTNOUmhkT2ZpL0pXRS9ZMml5YkZXbTN6c2YKYlgrTnl0MXA4UW1COEJPb2RYOWpoOW8reXA5bzNTeloyVE9FcHNUd05NQlRncmUrNEk5cHE0TzhnajA4CmxSZEh1cDJ0ck9MZmk1MHBVOGxsYjB4QVdxT3B6U1JrRzluTE9uc2Fsc3B0OXpSNW9Xdkx1cWNTQWlMVQoxbGlnOExjNWxSeUJCTXdSbzFzQ0w5N1psVUhPL0ZZQU5rN0JmKzNITy81eFRsVWc0UzBhUWFnSDZxOFUKSU5MUEE3NXpkaUU5WkE0bFN3S0xLak40VXlqL3dDbjc2NGZ1K0hKMzZHRmFVWkpDeHg5M3RFVk05Z0c5CkFCZlhlL2U1by9ncnl1WjhMazE0UFFKa2UxSThBRTBuS2tJSjhmNkRsTkc2Y1ZFdDViU3ROazJKUVVzVwpvSDQ4cno2aXlseHlWODB5NEh0dVdGZWI0MWZpZ2NiaktKL3N2QmJ5akpoRzNaVFhwMWFHdHNqcFl5UE4Kanp0Q0Y0bGNpVGRlZjZSbTdUcFVaMkJENHA3dTNBS1UyNWVOZGFSTExiSHpMdkhUL2kwS0J1Q25wRU8yCmpUbVVkTm9tb2FKK0w0SGVoeDUvWTEreQo=.3dd529d8eb703c8067ecf7c62515b740\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p id=\"isPasted\">You are trying to read <a href=\"https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#workspace-level-audit-log-events\" title=\"\" id=\"\" target=\"_blank\" rel=\"noopener noreferrer\"></a><a href=\"https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#workspace-level-audit-log-events\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">a</a><a href=\"https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#workspace-level-audit-log-events\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">udit logs</a> and get an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">AnalysisException: Found duplicate column(s)</span> error.</p><pre>spark.read.format(\"json\").load(\"dbfs://mnt/logs/&lt;path-to-logs&gt;/date=2021-12-07\")\r\n// \u00a0AnalysisException: Found duplicate column(s) in the data schema: `&lt;some_column&gt;`</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">From November 2021 to December 2021, a limited number of Databricks SQL audit logs were published with duplicate case-sensitive parameter names. This can break the schema inference and generate an error when you try to read audit logs generated during this time.</p><p>The following parameter names were duplicated:</p><pre>| Correct name | Duplicate name |\r\n| ------------ | -------------- |\r\n| dataSourceId | DataSourceId \u00a0 |\r\n| alertId \u00a0 \u00a0 \u00a0| AlertId \u00a0 \u00a0 \u00a0 \u00a0|\r\n| dashboardId \u00a0| DashboardId \u00a0 \u00a0|</pre><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">Create a new cluster to read and repair the audit log files.</p><p>Turn case sensitivity on so that schema inference can properly load the data.</p><pre>%scala\r\n\r\nspark.conf.set(\"spark.sql.caseSensitive\", true)\r\nspark.read.format(\"json\").load(\"dbfs://mnt/logs/pathToMyLogs/date=2021-12-07\").write...\r\nspark.conf.set(\"spark.sql.caseSensitive\", false)</pre><p>After the logs have been processed, turn case sensitivity off.</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-3\">Warning</h3>\n<p class=\"hj-alert-text\">Enabling schema inference on shared clusters and/or clusters that perform other workloads could cause issues with other workloads.</p>\n</div>\n</div><p><br></p>", "body_txt": "Problem You are trying to read a udit logs and get an AnalysisException: Found duplicate column(s) error. spark.read.format(\"json\").load(\"dbfs://mnt/logs/&lt;path-to-logs&gt;/date=2021-12-07\") // \u00a0AnalysisException: Found duplicate column(s) in the data schema: `&lt;some_column&gt;` Cause From November 2021 to December 2021, a limited number of Databricks SQL audit logs were published with duplicate case-sensitive parameter names. This can break the schema inference and generate an error when you try to read audit logs generated during this time. The following parameter names were duplicated: | Correct name | Duplicate name | | ------------ | -------------- | | dataSourceId | DataSourceId \u00a0 | | alertId \u00a0 \u00a0 \u00a0| AlertId \u00a0 \u00a0 \u00a0 \u00a0| | dashboardId \u00a0| DashboardId \u00a0 \u00a0| Solution Create a new cluster to read and repair the audit log files. Turn case sensitivity on so that schema inference can properly load the data. %scala spark.conf.set(\"spark.sql.caseSensitive\", true) spark.read.format(\"json\").load(\"dbfs://mnt/logs/pathToMyLogs/date=2021-12-07\").write... spark.conf.set(\"spark.sql.caseSensitive\", false) After the logs have been processed, turn case sensitivity off. Warning\nEnabling schema inference on shared clusters and/or clusters that perform other workloads could cause issues with other workloads.", "format": "html", "updated_at": "2022-07-22T12:11:32.135Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:27.886Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2817270, "name": "audit"}, {"id": 2817269, "name": "aws"}, {"id": 2817272, "name": "duplicate"}, {"id": 2817271, "name": "logs"}], "url": "https://kb.databricks.com/clusters/cannot-read-audit-logs-due-to-duplicate-columns"}, {"id": 1437778, "name": "Set nullability when using SaveAsTable with Delta tables", "views": 5141, "accessibility": 1, "description": "Learn how to create a Delta table with the nullability of columns set to false.", "codename": "set-nullability-when-using-saveastable-with-delta-tables", "created_at": "2022-07-15T10:31:56.290Z", "updated_at": "2022-10-14T04:19:42.036Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.f230270b6f908f4b1e0b239f91c3bf9c\"></div><p>When creating a Delta table with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">saveAsTable</span>, the nullability of columns defaults to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span> (columns can contain null values). This is expected behavior.</p><p>In some cases, you may want to create a Delta table with the nullability of columns set to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span> (columns cannot contain null values).</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><p>Use the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">CREATE TABLE</span> command to create the table and define the columns that cannot contain null values by using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">NOT NULL</span>.</p><p>For example, this sample code creates a Delta table with two integer columns. The column named <strong>null</strong> can contain null values, but the column named <strong>null1</strong> cannot contain null values because it was created with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">NOT NULL</span>.</p><pre data-aura-rendered-by=\"423:810;a\">%sql \r\n\r\nCREATE TABLE &lt;table-name&gt; (\r\n  num Int,\r\n  num1 Int NOT NULL\r\n  )\r\nUSING DELTA</pre><p>Now that we have the Delta table defined we can create a sample DataFrame and use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">saveAsTable</span> to write to the Delta table.</p><p>This sample code generates sample data and configures the schema with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">isNullable</span> property set to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span> for the field <strong>num</strong> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span> for field <strong>num1</strong>. This sample data is stored in a newly created DataFrame.</p><p>For the final step, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">saveAsTable</span> is used to write the data to the table we previously created.</p><pre data-aura-rendered-by=\"423:810;a\">import org.apache.spark.sql.types._\r\nval data = Seq(\r\n  Row(1, 3),\r\n  Row(5, 7)\r\n)\r\n\r\nval schema = StructType(\r\n  List(\r\n    StructField(\"num\", IntegerType, true),\r\n    StructField(\"num1\", IntegerType, false)\r\n  )\r\n)\r\n\r\nval df = spark.createDataFrame(\r\n  spark.sparkContext.parallelize(data),\r\n  schema\r\n)\r\n\r\n\r\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"&lt;table-name&gt;\")</pre><p>If you read the table schema, <strong>num</strong> allows for null values while <strong>num1</strong> does not allow null values.</p><p><img src=\"https://s3.amazonaws.com/helpjuice-static/helpjuice_production%2Fuploads%2Fupload%2Fimage%2F10723%2Fdirect%2F1664513084239-1664513084239.png\" class=\"fr-fic fr-dib\"></p><pre data-aura-rendered-by=\"423:810;a\">root\r\n |-- num: integer (nullable = true)\r\n |-- num1: integer (nullable = false)\r\n\r\n</pre><p><br></p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-1\">Warning</h3>\n<p class=\"hj-alert-text\">If you do not configure the nullability of your columns by creating a table in advance and instead try to write data to an undefined table, the nullability of all columns defaults to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span>. The DataFrame scheme is ignored in this case.</p>\n</div>\n</div><p>For example, if you skip table creation and just try to write the data to a table with saveAsTable, and then read the schema, all columns are defined as being nullable.</p><p><img src=\"https://s3.amazonaws.com/helpjuice-static/helpjuice_production%2Fuploads%2Fupload%2Fimage%2F10723%2Fdirect%2F1664514117327-1664514117327.png\" class=\"fr-fic fr-dib\"></p><p><br></p>", "body_txt": "When creating a Delta table with saveAsTable, the nullability of columns defaults to true (columns can contain null values). This is expected behavior. In some cases, you may want to create a Delta table with the nullability of columns set to false (columns cannot contain null values). Instructions Use the CREATE TABLE command to create the table and define the columns that cannot contain null values by using NOT NULL. For example, this sample code creates a Delta table with two integer columns. The column named null can contain null values, but the column named null1 cannot contain null values because it was created with NOT NULL. %sql CREATE TABLE &lt;table-name&gt; ( num Int, num1 Int NOT NULL ) USING DELTA Now that we have the Delta table defined we can create a sample DataFrame and use saveAsTable to write to the Delta table. This sample code generates sample data and configures the schema with the isNullable property set to true for the field num and false for field num1. This sample data is stored in a newly created DataFrame. For the final step, saveAsTable is used to write the data to the table we previously created. import org.apache.spark.sql.types._ val data = Seq( Row(1, 3), Row(5, 7) ) val schema = StructType( List( StructField(\"num\", IntegerType, true), StructField(\"num1\", IntegerType, false) ) ) val df = spark.createDataFrame( spark.sparkContext.parallelize(data), schema ) df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"&lt;table-name&gt;\") If you read the table schema, num allows for null values while num1 does not allow null values. root |-- num: integer (nullable = true) |-- num1: integer (nullable = false) Warning\nIf you do not configure the nullability of your columns by creating a table in advance and instead try to write data to an undefined table, the nullability of all columns defaults to true. The DataFrame scheme is ignored in this case. For example, if you skip table creation and just try to write the data to a table with saveAsTable, and then read the schema, all columns are defined as being nullable.", "format": "html", "updated_at": "2022-10-14T04:19:42.033Z"}, "author": {"id": 987347, "email": "anshuman.sahu@databricks.com", "name": "anshuman.sahu ", "first_name": "anshuman.sahu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-09-16T14:49:03.717Z", "updated_at": "2022-10-14T07:18:04.547Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2907877, "name": "aws"}, {"id": 2907878, "name": "azure"}, {"id": 2907840, "name": "delta"}, {"id": 2907879, "name": "gcp"}, {"id": 2907841, "name": "sparksql"}], "url": "https://kb.databricks.com/sql/set-nullability-when-using-saveastable-with-delta-tables"}, {"id": 1437584, "name": "Job fails with Spark Shuffle FetchFailedException error", "views": 4874, "accessibility": 1, "description": "Disable the default Spark Shuffle service to work around a FetchFailedException error.", "codename": "job-fails-with-spark-shuffle-fetchfailedexception-error", "created_at": "2022-07-15T00:15:43.052Z", "updated_at": "2022-12-05T18:42:53.083Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.27d9b08c93cac8c3ae284598cba1423a\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>If your application contains any aggregation or join stages, the execution will require a Spark Shuffle stage. Depending on the specific configuration used, if you are running multiple streaming queries on an interactive cluster you may get a shuffle <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">FetchFailedException</span> error.</p><pre class=\"language-plain\">ShuffleMapStage has failed the maximum allowable number of times\r\nDAGScheduler: ShuffleMapStage 499453 (start at command-39573728:13) failed in 468.820 s due to\r\norg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 228703\r\norg.apache.spark.shuffle.FetchFailedException: Connection reset by peer\r\nat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:747)\r\nCaused by: java.io.IOException: Connection reset by peer</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Shuffle fetch failures usually occur during scenarios such as cluster downscaling events, executor loss, or worker decommission. In certain cases, shuffle files from the executor are lost. When a subsequent task tries to fetch the shuffle files, it fails.</p><p>The shuffle service is enabled by default in Databricks. This service enables an external shuffle service that preserves the shuffle files written by executors so the executors can be safely removed.</p><p>Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.conf.get(\"spark.shuffle.service.enabled\")</span> in a Python or Scala notebook cell to return the current value of the shuffle service. If it returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span> the service is enabled.</p><pre class=\"language-plain\">spark.conf.get(\"spark.shuffle.service.enabled\") </pre><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Disable the default Spark Shuffle service.</p><p>Disabling the shuffle service does not prevent the shuffle, it just changes the way it is performed. When the service is disabled, the shuffle is performed by the executor.</p><p>You can disable the shuffle service by adding <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.shuffle.service.enabled false</span> to the cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>).</p><pre class=\"language-plain\">spark.shuffle.service.enabled false</pre><p>\u00a0Restart the cluster after updating the <strong>Spark config</strong>.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">There is a slight performance impact on when the shuffle service is disabled.</p>\n</div>\n</div><p><br></p>", "body_txt": "Problem If your application contains any aggregation or join stages, the execution will require a Spark Shuffle stage. Depending on the specific configuration used, if you are running multiple streaming queries on an interactive cluster you may get a shuffle FetchFailedException error. ShuffleMapStage has failed the maximum allowable number of times DAGScheduler: ShuffleMapStage 499453 (start at command-39573728:13) failed in 468.820 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 228703 org.apache.spark.shuffle.FetchFailedException: Connection reset by peer at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:747) Caused by: java.io.IOException: Connection reset by peer Cause Shuffle fetch failures usually occur during scenarios such as cluster downscaling events, executor loss, or worker decommission. In certain cases, shuffle files from the executor are lost. When a subsequent task tries to fetch the shuffle files, it fails. The shuffle service is enabled by default in Databricks. This service enables an external shuffle service that preserves the shuffle files written by executors so the executors can be safely removed. Run spark.conf.get(\"spark.shuffle.service.enabled\") in a Python or Scala notebook cell to return the current value of the shuffle service. If it returns true the service is enabled. spark.conf.get(\"spark.shuffle.service.enabled\") Solution Disable the default Spark Shuffle service. Disabling the shuffle service does not prevent the shuffle, it just changes the way it is performed. When the service is disabled, the shuffle is performed by the executor. You can disable the shuffle service by adding spark.shuffle.service.enabled false to the cluster's Spark config (AWS | Azure | GCP). spark.shuffle.service.enabled false \u00a0Restart the cluster after updating the Spark config. Info\nThere is a slight performance impact on when the shuffle service is disabled.", "format": "html", "updated_at": "2022-12-05T18:42:53.066Z"}, "author": {"id": 790360, "email": "shanmugavel.chandrakasu@databricks.com", "name": "shanmugavel.chandrakasu ", "first_name": "shanmugavel.chandrakasu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T21:14:15.541Z", "updated_at": "2023-04-20T21:51:08.608Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256850, "name": "Jobs", "codename": "jobs", "accessibility": 1, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2957609, "name": "aws"}, {"id": 2957610, "name": "azure"}, {"id": 2957614, "name": "fetch"}, {"id": 2957611, "name": "gcp"}, {"id": 2957613, "name": "shuffle"}, {"id": 2957612, "name": "spark shuffle"}], "url": "https://kb.databricks.com/jobs/job-fails-with-spark-shuffle-fetchfailedexception-error"}, {"id": 1437067, "name": "Run a custom Databricks runtime on your cluster", "views": 4654, "accessibility": 1, "description": "Configure your cluster to run a custom Databricks runtime image via the UI or API.", "codename": "run-a-custom-databricks-runtime-on-your-cluster", "created_at": "2022-07-14T16:37:36.056Z", "updated_at": "2022-10-26T17:24:59.100Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.31a46f2e6da16379397ebcf5425d4c30\"></div><p>The majority of Databricks customers use production Databricks runtime releases (<a href=\"https://docs.databricks.com/release-notes/runtime/releases.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks runtime releases\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/release-notes/runtime/releases\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks runtime releases\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/release-notes/runtime/releases.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks runtime releases\">GCP</a>) for their clusters. However, there may be certain times when you are asked to run a custom Databricks runtime after raising a support ticket.</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-0\">Warning</h3>\n<p class=\"hj-alert-text\">Custom Databricks runtime images are created for specific, short-term fixes and edge cases. If a custom image is appropriate, it will be provided by Databricks Support during case resolution.</p>\n<p class=\"hj-alert-text\">Databricks Support cannot provide a custom image on demand. You should NOT open a ticket just to request a custom Databricks runtime.</p>\n</div>\n</div><p>This article explains how to start a cluster using a custom Databricks runtime image after you have been given the runtime image name by support.</p><h1 data-toc=\"true\" id=\"instructions-1\">Instructions</h1><h2 data-toc=\"true\" id=\"use-the-workspace-ui-2\">Use the workspace UI</h2><p>Follow the steps for your specific browser to add the <strong>Custom Spark Version</strong> field to the <strong>New Cluster</strong> menu.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1666694744990-Custom%20Spark%20Version%20cluster%20option.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\"></p><p>After you have enabled the <strong>Custom Spark Version</strong> field, you can use it to start a new cluster using the custom Databricks runtime image you were given by support.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">When following the steps in this article, you will see a warning in your browser's Javascript console that says:</p>\n<pre class=\"hj-alert-text\">Do not copy-paste anything here. This can be used to compromise your account.</pre>\n<p class=\"hj-alert-text\">It is OK to enter the commands listed in this article.</p>\n</div>\n</div><h3 data-toc=\"true\" id=\"chrome-edge-4\">Chrome / Edge</h3><ol>\n<li>Login to your Databricks workspace.</li>\n<li>Click <strong>Compute</strong>.</li>\n<li>Click <strong>All-purpose clusters</strong>.</li>\n<li>Click <strong>Create Cluster</strong>.</li>\n<li>Press Command+Option+J (Mac) or Control+Shift+J (Windows, Linux, ChromeOS) to open the Javascript console.</li>\n<li>Enter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">window.prefs.set(\"enableCustomSparkVersions\",true)</span> in the Javascript console and run the command.</li>\n<li>Reload the page.</li>\n<li>\n<strong>Custom Spark Version</strong> now appears in the New Cluster menu.</li>\n<li>Enter the custom Databricks runtime image name that you got from Databricks support in the <strong>Custom Spark Version</strong> field.</li>\n<li>Continue creating your cluster as normal.</li>\n</ol><h3 data-toc=\"true\" id=\"firefox-5\">Firefox</h3><ol>\n<li id=\"isPasted\">Login to your Databricks workspace.</li>\n<li>Click <strong>Compute</strong>.</li>\n<li>Click <strong>All-purpose clusters</strong>.</li>\n<li>Click <strong>Create Cluster</strong>.</li>\n<li>Press Command+Option+K (Mac) or Control+Shift+K (Windows, Linux) to open the Javascript console.</li>\n<li>Enter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">window.prefs.set(\"enableCustomSparkVersions\",true)</span> in the Javascript console and run the command.</li>\n<li>Reload the page.</li>\n<li>\n<strong>Custom Spark Version</strong> now appears in the New Cluster menu.</li>\n<li>Enter the custom Databricks runtime image name that you got from Databricks support in the <strong>Custom Spark Version</strong> field.</li>\n<li>Continue creating your cluster as normal.</li>\n</ol><h3 data-toc=\"true\" id=\"safari-6\">Safari</h3><ol>\n<li id=\"isPasted\">Login to your Databricks workspace.</li>\n<li>Click <strong>Compute</strong>.</li>\n<li>Click <strong>All-purpose clusters</strong>.</li>\n<li>Click <strong>Create Cluster</strong>.</li>\n<li>Press Command+Option+C (Mac) to open the Javascript console.</li>\n<li>Enter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">window.prefs.set(\"enableCustomSparkVersions\",true)</span> in the Javascript console and run the command.</li>\n<li>Reload the page.</li>\n<li>\n<strong>Custom Spark Version</strong> now appears in the New Cluster menu.</li>\n<li>Enter the custom Databricks runtime image name that you got from Databricks support in the <strong>Custom Spark Version</strong> field.</li>\n<li>Continue creating your cluster as normal.</li>\n</ol><h2 data-toc=\"true\" id=\"use-the-api-7\">Use the API</h2><p>You need to set the custom image with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark_version</span> attribute when starting a cluster via the API.</p><p>You can use the API to create both interactive clusters and job clusters with a custom Databricks runtime image.</p><pre data-renderer-start-pos=\"646\">\"spark_version\": \"custom:&lt;custom-runtime-version-name&gt;</pre><h3 data-toc=\"true\" id=\"example-code-8\">Example code</h3><p>This sample code shows the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark_version</span> attribute used within the context of starting a cluster via the API.</p><pre id=\"isPasted\">%sh\r\n\r\ncurl -H \"Authorization: Bearer &lt;token-id&gt;\" -X POST \u00a0https://&lt;databricks-instance&gt;/api/2.0/clusters/create -d '{\r\n\u00a0 \"cluster_name\": \"heap\",\r\n\u00a0 \"spark_version\": \"custom:&lt;custom-runtime-version-name&gt;\",\r\n\u00a0 \"node_type_id\": \"r3.xlarge\",\r\n\u00a0 \"spark_conf\": {\r\n\u00a0 \u00a0 \"spark.speculation\": true\r\n\u00a0 },\r\n\u00a0 \"aws_attributes\": {\r\n\u00a0 \u00a0 \"availability\": \"SPOT\",\r\n\u00a0 \u00a0 \"zone_id\": \"us-west-2a\"\r\n\u00a0 },\r\n\u00a0 \"num_workers\": 1,\r\n\u00a0 \"spark_env_vars\": {\r\n\u00a0 \u00a0 \"SPARK_DRIVER_MEMORY\": \"25g\"\r\n\u00a0 }\r\n}'</pre><p><br>For more information please review the create Clusters API 2.0 (<a href=\"https://docs.databricks.com/dev-tools/api/latest/clusters.html#create\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"create Clusters API 2.0\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/clusters#--create\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"create Clusters API 2.0\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/api/latest/clusters.html#create\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"create Clusters API 2.0\">GCP</a>) documentation.</p>", "body_txt": "The majority of Databricks customers use production Databricks runtime releases (AWS | Azure | GCP) for their clusters. However, there may be certain times when you are asked to run a custom Databricks runtime after raising a support ticket. Warning\nCustom Databricks runtime images are created for specific, short-term fixes and edge cases. If a custom image is appropriate, it will be provided by Databricks Support during case resolution.\nDatabricks Support cannot provide a custom image on demand. You should NOT open a ticket just to request a custom Databricks runtime. This article explains how to start a cluster using a custom Databricks runtime image after you have been given the runtime image name by support. Instructions Use the workspace UI Follow the steps for your specific browser to add the Custom Spark Version field to the New Cluster menu. After you have enabled the Custom Spark Version field, you can use it to start a new cluster using the custom Databricks runtime image you were given by support. Info\nWhen following the steps in this article, you will see a warning in your browser's Javascript console that says:\nDo not copy-paste anything here. This can be used to compromise your account.\nIt is OK to enter the commands listed in this article. Chrome / Edge Login to your Databricks workspace.\nClick Compute.\nClick All-purpose clusters.\nClick Create Cluster.\nPress Command+Option+J (Mac) or Control+Shift+J (Windows, Linux, ChromeOS) to open the Javascript console.\nEnter window.prefs.set(\"enableCustomSparkVersions\",true) in the Javascript console and run the command.\nReload the page. Custom Spark Version now appears in the New Cluster menu.\nEnter the custom Databricks runtime image name that you got from Databricks support in the Custom Spark Version field.\nContinue creating your cluster as normal. Firefox Login to your Databricks workspace.\nClick Compute.\nClick All-purpose clusters.\nClick Create Cluster.\nPress Command+Option+K (Mac) or Control+Shift+K (Windows, Linux) to open the Javascript console.\nEnter window.prefs.set(\"enableCustomSparkVersions\",true) in the Javascript console and run the command.\nReload the page. Custom Spark Version now appears in the New Cluster menu.\nEnter the custom Databricks runtime image name that you got from Databricks support in the Custom Spark Version field.\nContinue creating your cluster as normal. Safari Login to your Databricks workspace.\nClick Compute.\nClick All-purpose clusters.\nClick Create Cluster.\nPress Command+Option+C (Mac) to open the Javascript console.\nEnter window.prefs.set(\"enableCustomSparkVersions\",true) in the Javascript console and run the command.\nReload the page. Custom Spark Version now appears in the New Cluster menu.\nEnter the custom Databricks runtime image name that you got from Databricks support in the Custom Spark Version field.\nContinue creating your cluster as normal. Use the API You need to set the custom image with the spark_version attribute when starting a cluster via the API. You can use the API to create both interactive clusters and job clusters with a custom Databricks runtime image. \"spark_version\": \"custom:&lt;custom-runtime-version-name&gt; Example code This sample code shows the spark_version attribute used within the context of starting a cluster via the API. %sh curl -H \"Authorization: Bearer &lt;token-id&gt;\" -X POST \u00a0https://&lt;databricks-instance&gt;/api/2.0/clusters/create -d '{ \u00a0 \"cluster_name\": \"heap\", \u00a0 \"spark_version\": \"custom:&lt;custom-runtime-version-name&gt;\", \u00a0 \"node_type_id\": \"r3.xlarge\", \u00a0 \"spark_conf\": { \u00a0 \u00a0 \"spark.speculation\": true \u00a0 }, \u00a0 \"aws_attributes\": { \u00a0 \u00a0 \"availability\": \"SPOT\", \u00a0 \u00a0 \"zone_id\": \"us-west-2a\" \u00a0 }, \u00a0 \"num_workers\": 1, \u00a0 \"spark_env_vars\": { \u00a0 \u00a0 \"SPARK_DRIVER_MEMORY\": \"25g\" \u00a0 } }' For more information please review the create Clusters API 2.0 (AWS | Azure | GCP) documentation.", "format": "html", "updated_at": "2022-10-26T17:24:59.091Z"}, "author": {"id": 488150, "email": "rakesh.parija@databricks.com", "name": "rakesh.parija ", "first_name": "rakesh.parija", "last_name": "", "role_id": "admin", "created_at": "2021-10-07T02:59:41.577Z", "updated_at": "2023-04-21T14:21:18.111Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2921865, "name": "aws"}, {"id": 2921866, "name": "azure"}, {"id": 2922587, "name": "custom"}, {"id": 2921867, "name": "gcp"}, {"id": 2922588, "name": "runtime"}], "url": "https://kb.databricks.com/clusters/run-a-custom-databricks-runtime-on-your-cluster"}, {"id": 1437064, "name": "Cannot set a custom PYTHONPATH", "views": 5739, "accessibility": 1, "description": "Setting a custom PYTHONPATH in an init script or in DCS is not supported.", "codename": "cannot-set-a-custom-pythonpath", "created_at": "2022-07-14T16:34:24.317Z", "updated_at": "2022-09-13T07:25:56.397Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.8bbc8a8f5045aaa92005c85ef15d1d57\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>When you are trying to set a custom <span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard;'>PYTHONPATH</span> environment variable in a cluster-scoped init script, but the values are overridden at driver startup.\u00a0</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Setting a custom <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PYTHONPATH</span> in an init scripts does not work and is not supported.</p><p>Additionally, you cannot set a custom\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PYTHONPATH</span> when using Databricks Container Services.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You should not try to set a custom <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PYTHONPATH</span>.</p><p>If you need to use custom Python libraries or modules, install the required files to pre-existing directories that are included in the cluster's <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PYTHONPATH</span>.</p><p>This sample code lists all directories in the cluster's PYTHONPATH.</p><pre>%python\r\n\r\nimport sys\r\nprint(sys.path)</pre><p><br></p>", "body_txt": "Problem When you are trying to set a custom PYTHONPATH environment variable in a cluster-scoped init script, but the values are overridden at driver startup.\u00a0 Cause Setting a custom PYTHONPATH in an init scripts does not work and is not supported. Additionally, you cannot set a custom\u00a0PYTHONPATH when using Databricks Container Services. Solution You should not try to set a custom PYTHONPATH. If you need to use custom Python libraries or modules, install the required files to pre-existing directories that are included in the cluster's PYTHONPATH. This sample code lists all directories in the cluster's PYTHONPATH. %python import sys print(sys.path)", "format": "html", "updated_at": "2022-09-13T07:25:56.395Z"}, "author": {"id": 791590, "email": "prakash.jha@databricks.com", "name": "prakash.jha ", "first_name": "prakash.jha", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T04:43:14.706Z", "updated_at": "2023-03-10T13:32:57.702Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2889668, "name": "aws"}, {"id": 2889669, "name": "azure"}, {"id": 2889671, "name": "custom"}, {"id": 2889673, "name": "dcs"}, {"id": 2889670, "name": "gcp"}, {"id": 2889672, "name": "library"}], "url": "https://kb.databricks.com/clusters/cannot-set-a-custom-pythonpath"}, {"id": 1434499, "name": "User does not have permission SELECT on ANY File", "views": 5819, "accessibility": 1, "description": "Regular users cannot create tables without permission when access control is enabled.", "codename": "user-does-not-have-permission-select-on-any-file", "created_at": "2022-07-13T16:02:54.231Z", "updated_at": "2022-12-21T10:45:22.907Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.6ae565d0c02af688e59517665eda044a\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to create an external hive table, but keep getting a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">User does not have permission SELECT on any file</span> error message.</p><pre>java.lang.SecurityException: User does not have permission SELECT on any file.</pre><p><br>Table access control (<a href=\"https://docs.databricks.com/security/access-control/table-acls/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Table access control\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/security/access-control/table-acls/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Table access control\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/security/access-control/table-acls/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Table access control\">GCP</a>) is enabled your cluster and you are not an admin.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The Databricks SQL query analyzer enforces access control policies at runtime on Databricks clusters with table access control enabled as well as all SQL warehouses.</p><p>When table access control is enabled on a cluster, the user must have specific permission to access a table in order to be able to read the table.</p><p>The only users who can bypass table access control are Databricks admins.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>An admin must grant <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SELECT</span> permission on files so the selected user can create a table.</p><p>Review the <strong>Data object privileges</strong> (<a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Data object privileges\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/security/access-control/table-acls/object-privileges\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Data object privileges\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/security/access-control/table-acls/object-privileges.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Data object privileges\">GCP</a>) documentation for more information.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">The following steps must be run as an Admin.</p>\n<p class=\"hj-alert-text\">Admins can also grant permissions to groups instead of users.</p>\n</div>\n</div><ol>\n<li>Start the cluster.</li>\n<li>Open a notebook.</li>\n<li>Run the following to grant\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SELECT</span> permission on any file to the specified user.\u00a0<br><pre>%sql\r\nGRANT SELECT ON ANY FILE TO `&lt;user@domain-name&gt;`</pre>\n</li>\n</ol><p><br></p>", "body_txt": "Problem You are trying to create an external hive table, but keep getting a User does not have permission SELECT on any file error message. java.lang.SecurityException: User does not have permission SELECT on any file. Table access control (AWS | Azure | GCP) is enabled your cluster and you are not an admin. Cause The Databricks SQL query analyzer enforces access control policies at runtime on Databricks clusters with table access control enabled as well as all SQL warehouses. When table access control is enabled on a cluster, the user must have specific permission to access a table in order to be able to read the table. The only users who can bypass table access control are Databricks admins. Solution An admin must grant SELECT permission on files so the selected user can create a table. Review the Data object privileges (AWS | Azure | GCP) documentation for more information. Info\nThe following steps must be run as an Admin.\nAdmins can also grant permissions to groups instead of users. Start the cluster.\nOpen a notebook.\nRun the following to grant\u00a0SELECT permission on any file to the specified user.\u00a0 %sql GRANT SELECT ON ANY FILE TO `&lt;user@domain-name&gt;`", "format": "html", "updated_at": "2022-12-21T10:45:22.887Z"}, "author": {"id": 791192, "email": "sivaprasad.cs@databricks.com", "name": "sivaprasad.cs ", "first_name": "sivaprasad.cs", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T19:34:47.857Z", "updated_at": "2023-03-20T13:02:42.882Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2978920, "name": "acl"}, {"id": 2978923, "name": "admin"}, {"id": 2978889, "name": "aws"}, {"id": 2978890, "name": "azure"}, {"id": 2978891, "name": "gcp"}, {"id": 2978922, "name": "permissions"}, {"id": 2978921, "name": "table access control"}], "url": "https://kb.databricks.com/data/user-does-not-have-permission-select-on-any-file"}, {"id": 1434495, "name": "Apache Spark UI task logs intermittently return HTTP 500 error", "views": 2246, "accessibility": 1, "description": "If the Spark property spark.databricks.ui.logViewingEnabled is set to false, you cannot view task logs in the Spark UI.", "codename": "apache-spark-ui-task-logs-intermittently-return-http-500-error", "created_at": "2022-07-13T15:58:09.800Z", "updated_at": "2023-03-17T15:08:44.003Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSt4dTF2c2VlQnY4ZHliOGpoZUFPN0Z2bDRFd2haa215QmZweFBLQmJzanFrcytqdjZNCmF5bFA0NUllZmFnNklqZHBjVWttL0Nvemh5WHZrV2I2bDkxTFFCSXRXeWNuQXlBM2hZbVFrTjZGd25zQwpiTlNxZTRINkVmV0NTNGZiY1FjTXJTeW02ZWRNNE83eUVGcmJhRkhjNE1JSVNpalBEMVUyL3hHbWsvUngKUmVOam1JaXJ3L2w1ZFlRdXJYWkpQMWlNdDdUZlo2QUN0Q0o3TURkNXpKcWk3OHljQ1NlNzNRY1NXQWxjCmxHd0FuaGxJUkRSeDB0UW11Y2N0V2JnMnZmM3dVa1pVNmx0cERGTkRSd1FpYXFwZ0MzTTA4RlFMT2xCUApvQmtuTFY3QTl3RlU5Y1I0SFhRV2tVSWdsMkZYR29OTU90cGZlT3NDaHg5NWVyZHBBaTZadGNTNUJMdSsKS1Y1OWZvUWRrbWQ5Z1ZnR2RYdFN0T2drbXVlZ2lDVFNUVWpSYll1d3JwSElid0tPL0ltK2FzeXlubStQCnpacEtpR0x6Zm0zd09iakpQVFY3Tis1dFY4NTc1UWhsdkZGNFEzTy90YW4zcFFMRXNYWk10ZXROcklXdwppZ0I1NGxJbFVhU1R4Qk1BOGtnYnVOaC82aEhhWEUwQkVLTjNWdlJWcmcvQmJSOHlCUG0zTDVVazRlSjQKV0ZRWEp0RGFocXdtNzZBSXdHVG92UVhDZDhBTHlDRDZIWlFwQlNIZnpIZ2VuWDUzbFJESWppV2ZrUWlUCm5nWERKQjdPUllJVS9mMXJaOCtiZk9kSlBlRVplNG81OUJuN2c4aTFkQTgzQ2ZoU1JWZXVqK3FTVFlRRQp1WU5iVTFUSEpxTnFMZDJrWXRzSXdjWVBHNFVBZ2pmWnQwZU91K3FaMWxaUlpweXN4MXpMdmtLRUVyTWsKWHhpaEZtaDJqRDc4aTdQd0VCS3RETkhycnROdDN5czN3b284L3ViRHNIUEhyNFRTKzJaSHYza0Fjb1FyCitwQ2pZTy9uOFp1N0ZwcjBzaWIwdlB0QWZyWXowVmN5c2gyemlOdnNkT2thcUlHYmplUnJXVm9ZNkNDOQpwUTZTcmU2MFZXQ25qeE9zRy9pSy9Gdm5ybDI1R2EwYzR6RDZnK291S1F6aUpEY1JRRDBiaFh1c29mKzIKT2tRRUdacTkvYURwRmFBc0tmRjMvMXJSaW1CWU1qU1daSVdlWDl4S2V1SU16dEp1Q2hTOEtpU1BKZjY3CnliWlBXUnVYbjNEOVd0ZTNEVFFHNEpsY0NkYVFVd25QSzZMNUZqbDRpbnBRL3I4T3pCY2owRE55UEg1VwpJRHdSVjF4YmVLdmtnMlZMS3JReEhjRytLVEMzRVVidEUvQ2ljb3RZVHEzNEJEYVZRSnVxdUQraTZKZmQKODhRQVNYeVJENjlqbmxsK0Y3ekRrdWs3bHljbUY3OGRZUEpQbzM3YTRjQ1ZMRmQ1NUZjYmtQQmZpZHZrCkhHTXYxN1l1eU12ZEVzTi9mOWlBOHhjS0VQankyUzNaM2ZQSTJpdmkrclhBem5ROXloZWtWTk80bDd1SgpqSjlFT3RPcEJZbWNTbU5RTXdnUzBHZ3I1VENqZVcvUkZIVFBMakxaSEpqQkpHQ1ZrdXRlcHhqVytRU1AKL1haQ3hpd3RTUCt6OUtnc0d5emF2aXpkbTltNlh4WGs4clBzbXZpQjlnPT0K.ffe48e7820fd6d23497bebe1da6134cf\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Users of Shared access mode clusters experience intermittent HTTP 500 errors when trying to view task logs in the Apache Spark UI. This also applies to admins.</p><pre>ErrorCaused by:java.lang.Exception: Log viewing is disabled on this cluster\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.spark.deploy.worker.ui.LogPage.render(LogPage.scala:65)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.spark.ui.WebUI$$anonfun$3.apply(WebUI.scala:100)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.spark.ui.WebUI$$anonfun$3.apply(WebUI.scala:100)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)\r\n\u00a0\u00a0\u00a0\u00a0at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\r\n\u00a0\u00a0\u00a0\u00a0at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n\u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)</pre><p><br></p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1669149915951-1669149915951.png\" class=\"fr-fic fr-dib\"></p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This specific exception is controlled by the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.databricks.ui.logViewingEnabled</span> Spark property. When this value is set to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span>, log viewing is disabled. When Spark log viewing is disabled on the cluster, the Spark UI generates an error when you attempt to view the logs.</p><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.databricks.ui.logViewingEnabled</span> property defaults to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span>, however sometimes other Spark configurations (such as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.databricks.acl.dfAclsEnabled</span>) can alter its value and set it to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span>.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.databricks.ui.logViewingEnabled</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span> in the cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>).</p><pre>spark.databricks.ui.logViewingEnabled true</pre><p>This restores the default configuration in case it is accidentally overwritten.</p>", "body_txt": "Problem Users of Shared access mode clusters experience intermittent HTTP 500 errors when trying to view task logs in the Apache Spark UI. This also applies to admins. ErrorCaused by:java.lang.Exception: Log viewing is disabled on this cluster \u00a0\u00a0\u00a0\u00a0at org.apache.spark.deploy.worker.ui.LogPage.render(LogPage.scala:65) \u00a0\u00a0\u00a0\u00a0at org.apache.spark.ui.WebUI$$anonfun$3.apply(WebUI.scala:100) \u00a0\u00a0\u00a0\u00a0at org.apache.spark.ui.WebUI$$anonfun$3.apply(WebUI.scala:100) \u00a0\u00a0\u00a0\u00a0at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90) \u00a0\u00a0\u00a0\u00a0at javax.servlet.http.HttpServlet.service(HttpServlet.java:687) \u00a0\u00a0\u00a0\u00a0at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.Server.handle(Server.java:534) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) \u00a0\u00a0\u00a0\u00a0at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) Cause This specific exception is controlled by the spark.databricks.ui.logViewingEnabled Spark property. When this value is set to false, log viewing is disabled. When Spark log viewing is disabled on the cluster, the Spark UI generates an error when you attempt to view the logs. The spark.databricks.ui.logViewingEnabled property defaults to true, however sometimes other Spark configurations (such as spark.databricks.acl.dfAclsEnabled) can alter its value and set it to false. Solution Set spark.databricks.ui.logViewingEnabled to true in the cluster's Spark config (AWS | Azure | GCP). spark.databricks.ui.logViewingEnabled true This restores the default configuration in case it is accidentally overwritten.", "format": "html", "updated_at": "2023-03-17T15:08:44.000Z"}, "author": {"id": 790745, "email": "vivian.wilfred@databricks.com", "name": "vivian.wilfred ", "first_name": "vivian.wilfred", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T10:42:50.853Z", "updated_at": "2023-03-28T11:16:09.635Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3127722, "name": "aws"}, {"id": 3127723, "name": "azure"}, {"id": 3127724, "name": "gcp"}, {"id": 3092477, "name": "http 500"}, {"id": 3127725, "name": "logviewingenabled"}, {"id": 3092478, "name": "spark ui"}], "url": "https://kb.databricks.com/clusters/apache-spark-ui-task-logs-intermittently-return-http-500-error"}, {"id": 1434487, "name": "Object ownership is getting changed on dropping and recreating tables", "views": 2873, "accessibility": 1, "description": "Use TRUNCATE or REPLACE for tables and ALTER VIEW for views instead of dropping and recreating them.", "codename": "object-ownership-is-getting-changed-on-dropping-and-recreating-tables", "created_at": "2022-07-13T15:49:31.217Z", "updated_at": "2022-12-15T23:54:07.977Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.cff55ab84ad5947838fda9724f0f6982\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Ownership of SQL objects changes after dropping and recreating them. This can result in job failures due to permission issues.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>In Databricks Runtime 7.3 LTS, when jobs are run with table ACLs turned off, any action that drops and recreates tables or views preserves the table ACLs that was set the last time the job was run with table ACLs turned on.</p><p>In Databricks Runtime 9.1 LTS and above this behavior changed. Any action that drops a table or view clears the table ACL state.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You should use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TRUNCATE</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">REPLACE</span> for tables and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ALTER VIEW</span> for views instead of dropping and recreating them.</p><p>To replace a view, you should be the owner of the view or an administrator.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">If you want to restore the behavior from Databricks Runtime 7.3 LTS, you can add <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.databricks.acl.enforceTableOwnerAssignment false</span> to the cluster's <strong>Spark config</strong>.</p>\n</div>\n</div><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.databricks.acl.enforceTableOwnerAssignment</span> was introduced in Databricks Runtime 9.1 LTS.\u00a0</p><p>Previously, when objects were created outside of a table ACL enabled cluster the ACL system had no knowledge of them. A Databricks administrator would have to set ownership permissions for new objects and clean up dangling permissions for deleted objects.</p><p>Now, objects created outside of Databricks SQL or table ACL enabled clusters create representations in the ACL system, assigning ownership automatically or dropping permissions as needed.</p>", "body_txt": "Problem Ownership of SQL objects changes after dropping and recreating them. This can result in job failures due to permission issues. Cause In Databricks Runtime 7.3 LTS, when jobs are run with table ACLs turned off, any action that drops and recreates tables or views preserves the table ACLs that was set the last time the job was run with table ACLs turned on. In Databricks Runtime 9.1 LTS and above this behavior changed. Any action that drops a table or view clears the table ACL state. Solution You should use TRUNCATE or REPLACE for tables and ALTER VIEW for views instead of dropping and recreating them. To replace a view, you should be the owner of the view or an administrator. Info\nIf you want to restore the behavior from Databricks Runtime 7.3 LTS, you can add spark.databricks.acl.enforceTableOwnerAssignment false to the cluster's Spark config. spark.databricks.acl.enforceTableOwnerAssignment was introduced in Databricks Runtime 9.1 LTS.\u00a0 Previously, when objects were created outside of a table ACL enabled cluster the ACL system had no knowledge of them. A Databricks administrator would have to set ownership permissions for new objects and clean up dangling permissions for deleted objects. Now, objects created outside of Databricks SQL or table ACL enabled clusters create representations in the ACL system, assigning ownership automatically or dropping permissions as needed.", "format": "html", "updated_at": "2022-12-15T23:54:07.974Z"}, "author": {"id": 790739, "email": "akash.bhat@databricks.com", "name": "akash.bhat ", "first_name": "akash.bhat", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T10:22:42.406Z", "updated_at": "2023-04-24T05:43:50.678Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2956096, "name": "access"}, {"id": 2956092, "name": "aws"}, {"id": 2956093, "name": "azure"}, {"id": 2956095, "name": "ownership"}, {"id": 2956094, "name": "permission"}], "url": "https://kb.databricks.com/data/object-ownership-is-getting-changed-on-dropping-and-recreating-tables"}, {"id": 1431699, "name": "Jobs fails with a TimeoutException error", "views": 1730, "accessibility": 1, "description": "This error is usually caused by a Broadcast join that takes excessively long to complete.", "codename": "jobs-fails-with-a-timeoutexception-error", "created_at": "2022-07-11T22:59:29.287Z", "updated_at": "2023-03-03T14:25:35.644Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.4a6d998d363b50d1b51718abc4436d7d\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are running Apache Spark SQL queries that perform join operations DataFrames, but the queries keep failing with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TimeoutException</span> error message.</p><h2 data-toc=\"true\" id=\"example-stack-trace-1\">Example stack trace</h2><pre data-stringify-type=\"pre\" data-toc=\"true\" id=\"caused-by-javautilconcurrenttimeoutexception-futures-timed-out-after-300-seconds-at-scalaconcurrentimplpromisedefaultpromisereadypromisescala219-at-scalaconcurrentimplpromisedefaultpromiseresultpromisescala223-at-scalaconcurrentawaitanonfunresult1applypackagescala190-at-scalaconcurrentblockcontextdefaultblockcontextblockonblockcontextscala53-1\">Caused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\r\n <wbr>\u00a0 <wbr>\u00a0 <wbr>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0<wbr>at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\r\n <wbr>\u00a0 <wbr>\u00a0 <wbr>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0<wbr>at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\r\n <wbr>\u00a0 <wbr>\u00a0 <wbr>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0<wbr>at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)\r\n <wbr>\u00a0 <wbr>\u00a0 <wbr>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <wbr>at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)</wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></wbr></pre><h1 data-toc=\"true\" id=\"cause-2\">Cause</h1><p>This problem usually stems from Spark trying to perform Broadcast join and when the fetching of blocks from different executors consumes an excessive amount of time.</p><p>Spark performs Broadcast join using the BitTorrent protocol. The driver splits the data to be broadcasted into small chunks and stores the chunks in the block manager of the driver. The driver also sends the chunks of data to the executors. Each executors keeps copies of the chunks of data in its own block manager.\u00a0</p><p>When a specific executor is not able to fetch the chunks of data from its local block manager (say the executor died and re-launched) that executor tries to fetch the broadcast data from the driver as well as other executors. This avoids driver being the bottleneck in serving the remote requests.\u00a0</p><p>Even with this distributed approach, there are some scenarios where the broadcast can take an excessive amount of time, resulting in a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TimeoutException</span> error.</p><ul>\n<li>\n<strong>Busy driver or busy executor</strong>: If the Spark driver and executors are extremely busy, it can introduce delay in the broadcast process. If the broadcast process exceeds the threshold limits, it can result in a broadcast timeout.</li>\n<li>\n<strong>Large broadcast data size</strong>: Trying to broadcast a large amount of data can also result in a broadcast timeout. Spark has a default limit of 8GB for broadcast data.\u00a0</li>\n</ul><h1 data-toc=\"true\" id=\"solution-3\">Solution</h1><p>You need to identify the query that is causing resource bottleneck on the cluster. Open the <strong>Spark UI</strong> (<a href=\"https://docs.databricks.com/clusters/debugging-spark-ui.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/debugging-spark-ui\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/debugging-spark-ui.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>) and review any failed stages to locate the SQL query causing the failure. Review the Spark SQL plan to see if it uses <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastNestedLoopJoin</span>.</p><ul>\n<li>If the Spark SQL plan uses <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastNestedLoopJoin</span>, you need to follow the instructions in the <a href=\"/_questions/1383834\" rel=\"noopener noreferrer\" target=\"_blank\">Disable broadcast when query plan has BroadcastNestedLoopJoin</a> article.</li>\n<li>If the Spark SQL plan does not use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastNestedLoopJoin</span>, you can disable the Broadcast join by setting Spark config values right before the problematic query. You can then revert these changes after the problematic query. Making the change query specific allows other queries, which can benefit from the Broadcast join, to still leverage the benefits.<ul>\n<li>\n<pre>SET spark.sql.autoBroadcastJoinThreshold=-1 \u00a0</pre>This disables Broadcast join.</li>\n<li>\n<pre>SET spark.databricks.adaptive.autoBroadcastJoinThreshold=-1\u00a0</pre>This particular configuration disables adaptive Broadcast join.</li>\n</ul>\n</li>\n</ul><p>Another option is to increase <span id=\"isPasted\" style='color: rgb(56, 76, 96); font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;'>spark.sql.broadcastTimeout</span> to a value above 300 seconds, which is the default value. Increasing <span id=\"isPasted\" style='color: rgb(56, 76, 96); font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;'>spark.sql.broadcastTimeout</span> allows more time for the broadcasting process to finish before it generates a failure. The downside to this approach, is that it may result in longer query times.</p><p>For example, setting the value to 600 doubles the amount of time for the Broadcast join to complete.</p><pre>SET spark.sql.broadcastTimeout=600</pre><p>This value can be set at the cluster level or the notebook level.</p>", "body_txt": "Problem You are running Apache Spark SQL queries that perform join operations DataFrames, but the queries keep failing with a TimeoutException error message. Example stack trace Caused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds] \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) Cause This problem usually stems from Spark trying to perform Broadcast join and when the fetching of blocks from different executors consumes an excessive amount of time. Spark performs Broadcast join using the BitTorrent protocol. The driver splits the data to be broadcasted into small chunks and stores the chunks in the block manager of the driver. The driver also sends the chunks of data to the executors. Each executors keeps copies of the chunks of data in its own block manager.\u00a0 When a specific executor is not able to fetch the chunks of data from its local block manager (say the executor died and re-launched) that executor tries to fetch the broadcast data from the driver as well as other executors. This avoids driver being the bottleneck in serving the remote requests.\u00a0 Even with this distributed approach, there are some scenarios where the broadcast can take an excessive amount of time, resulting in a TimeoutException error. Busy driver or busy executor: If the Spark driver and executors are extremely busy, it can introduce delay in the broadcast process. If the broadcast process exceeds the threshold limits, it can result in a broadcast timeout. Large broadcast data size: Trying to broadcast a large amount of data can also result in a broadcast timeout. Spark has a default limit of 8GB for broadcast data.\u00a0 Solution You need to identify the query that is causing resource bottleneck on the cluster. Open the Spark UI (AWS | Azure | GCP) and review any failed stages to locate the SQL query causing the failure. Review the Spark SQL plan to see if it uses BroadcastNestedLoopJoin. If the Spark SQL plan uses BroadcastNestedLoopJoin, you need to follow the instructions in the Disable broadcast when query plan has BroadcastNestedLoopJoin article.\nIf the Spark SQL plan does not use BroadcastNestedLoopJoin, you can disable the Broadcast join by setting Spark config values right before the problematic query. You can then revert these changes after the problematic query. Making the change query specific allows other queries, which can benefit from the Broadcast join, to still leverage the benefits. SET spark.sql.autoBroadcastJoinThreshold=-1 \u00a0This disables Broadcast join. SET spark.databricks.adaptive.autoBroadcastJoinThreshold=-1\u00a0This particular configuration disables adaptive Broadcast join. Another option is to increase spark.sql.broadcastTimeout to a value above 300 seconds, which is the default value. Increasing spark.sql.broadcastTimeout allows more time for the broadcasting process to finish before it generates a failure. The downside to this approach, is that it may result in longer query times. For example, setting the value to 600 doubles the amount of time for the Broadcast join to complete. SET spark.sql.broadcastTimeout=600 This value can be set at the cluster level or the notebook level.", "format": "html", "updated_at": "2023-03-03T14:25:35.459Z"}, "author": {"id": 973283, "email": "swetha.nandajan@databricks.com", "name": "swetha.nandajan ", "first_name": "swetha.nandajan", "last_name": "", "role_id": "draft_writer", "created_at": "2022-08-16T20:22:25.595Z", "updated_at": "2023-04-24T17:36:19.596Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3043807, "name": "aws"}, {"id": 3043808, "name": "azure"}, {"id": 3043812, "name": "broadcast"}, {"id": 3043813, "name": "broadcastjoin"}, {"id": 3043815, "name": "broadcast timeout"}, {"id": 3043809, "name": "gcp"}, {"id": 3043814, "name": "join"}], "url": "https://kb.databricks.com/scala/jobs-fails-with-a-timeoutexception-error"}, {"id": 1429996, "name": "Use custom classes and objects in a schema", "views": 3481, "accessibility": 1, "description": "You must define custom classes and objects inside a package if you want to use them in a notebook. ", "codename": "use-custom-classes-and-objects-in-a-schema", "created_at": "2022-07-08T11:25:26.791Z", "updated_at": "2022-11-08T08:42:30.399Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.27f13ea20818b024aba80269ebb643a4\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to create a dataset using a schema that contains Scala enumeration fields (classes and objects). When you run your code in a notebook cell, you get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ClassNotFoundException</span> error.</p><h2 data-toc=\"true\" id=\"sample-code-1\">Sample code</h2><pre data-aura-rendered-by=\"371:767;a\">%scala\r\n\r\nobject TestEnum extends Enumeration {\r\n  type TestEnum = Value\r\n  val E1, E2, E3 = Value\r\n}\r\n\r\nimport spark.implicits._\r\nimport TestEnum._\r\n\r\ncase class TestClass(i: Int,  e: TestEnum) {}\r\nval ds = Seq(TestClass(1, TestEnum.E1)).toDS</pre><h2 data-toc=\"true\" id=\"error-message-2\">Error message</h2><pre>ClassNotFoundException: lineb3e041f628634740961b78d5621550d929.$read$$iw$$iw$$iw$$iw$$iw$$iw$TestEnum</pre><h1 data-toc=\"true\" id=\"cause-3\">Cause</h1><p>The\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ClassNotFoundException</span> error occurred because the sample code does not define the class and object in a package cell.</p><h1 data-toc=\"true\" id=\"solution-4\">Solution</h1><p>If you want to use custom Scala classes and objects defined within notebooks (in Apache Spark and across notebook sessions) you must define the class and object inside a package and import the package into your notebook.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-5\">Info</h3>\n<p class=\"hj-alert-text\">Only class and object definitions can go in a package cell. Package cells cannot contain any function definitions, values, or variables.</p>\n</div>\n</div><h2 data-toc=\"true\" id=\"define-the-class-and-object-6\">Define the class and object</h2><p>This sample code starts off by creating the package <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">com.databricks.example</span>. It then defines the object <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TestEnum</span> and assigns values, before defining the class <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TestClass</span>.</p><pre data-aura-rendered-by=\"471:767;a\">%scala\r\n\r\npackage com.databricks.example   // Create a package.\r\nobject TestEnum extends Enumeration {  // Define an object called TestEnum.\r\n\u00a0 type TestEnum = Value \r\n\u00a0 val E1, E2, E3 = Value  // Enum values \r\n}\r\ncase class TestClass(i: Int, other:TestEnum.Value)   // Define a class called TestClass.</pre><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-7\">Info</h3>\n<p class=\"hj-alert-text\">Classes defined within packages cannot be redefined without a cluster restart.</p>\n</div>\n</div><h2 data-toc=\"true\" id=\"import-the-package-8\">Import the package</h2><p>After the class and object have been defined, you can import the package you created into a notebook and use both the class and the object.</p><p>This sample code starts by importing the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">com.databricks.example</span> package that we just defined.</p><p>It then evaluates a DataFrame using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TestClass</span> class and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TestEnum</span> object. Both are defined in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">com.databricks.example</span> package.</p><pre data-aura-rendered-by=\"471:767;a\">%scala\r\n\r\nimport com.databricks.example \r\nval df = sc.parallelize(Array(example.TestClass(1,(example.TestEnum.E1)))).toDS().show()</pre><p>The DataFrame successfully displays after the sample code is run.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1667884923988-Result%20of%20a%20custom%20class%20and%20object%20in%20a%20package.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Shows the result of running the sample code using the custom class and object defined in the Scala package.\"></p><p>Please review the package cells (<a href=\"https://docs.databricks.com/notebooks/package-cells.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"package cells\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/notebooks/package-cells\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"package cells\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/notebooks/package-cells.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"package cells\">GCP</a>) documentation for more information.</p><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"2f9bc4026ae6\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9FQXJLSzJldXk1bEhjY0lUMDMyQ2swakVCYjYybElWRTg0UVRzZ1QyZjBBZVQxZVBLCkhiMGZEd2czNFdBV1pkOGErb0pyTXpxMTZpLzgycFhlanFXdXRLTEtQc2R5NEU2Y25LRWF6cDc4S0I4bAp2Z29uUW1PRFJqbW1sQWVwVHJVQ0Z6M3Y3VVg4L1ZITWI2Z09OWVpEVmZ2aGU5QjNEZER2dmwyaWJWN0sKcnJmOHpjVHNUcjAvMGp2dHpFM2V5R3ZDaUluTzZVbmdyVGJNVFZ5eVZKTjZFaklNYk41TzZTWjduUktnCk9SczRSNy85cy9CdW5DZDJTMlVxcTVQYUNRU3VHMGVLMDdQRTB2YzlJL1lUc2ZZQ3l0Sjc3Vjl3T01ZSwpYUElNMnU1dElpRXRQYUxKZFl1bWdxMXo3VWRvRC84K1I3UnZvZjhtRzlzRWhFN0lOTUhGZFgxOFZvMnMKUlZjS1JlSUN2OEtjTGlrbzRjV3VXSEsyWUNOQml1bzhsNkJDL2dMRzdSamVzT1hMa2c1SWpURVdCL05KCjF0QmFpRGVQUTZ4R0Y5bGV2RGtSUnhaR0l4U2d6Rk9EOUhhUm8waENLSVVnMHV2RFNwL3ZWWXdyNDBOdQppekhtUmpJUkJrVHN3TXNKOGV4Z2loVDJKSUhqYmtZNjQ4bjdPejBxWXdwZThqS3BDbXU3RDRHUE9oL04KUXh5a2tBSFNpaTRnUHpIQnRmcWFOQ05waHhhejQvRXJnQnVwVVpQV294QzZHUlBoOTVhYVcvaXVJTjZPCi9QMmd0NzJSSldJNjYyTGFOZkFrK2t6SnM5dkkzS01ZVXNpOXBTbmJyRSsvRVFxT2lMSmlsUHBXQ3NQKwptYkNvYXJhSnVMekVxYW9zT1NHREt6WUJyR3FSRUx5RjErQjZNY21tVFY2dlRMMFZPamY1V2FGZXVrbDYKVFB6KzRWTFVqMnFjTFliSFVBRDVmcGtqS043TTc5OHJuQnRpVmdQOThmbGdFKzlqbFlJMHllWTVxUU4vCndNNFJyNjVrTmtDRE1oMWc5YmhhVGlMR1I4QjlLTkxsbEp0bVlUQ0FvRjJRQXpGRmM3RFpUOVFlbFBIcQpvNmF3OUMxSS8ySzlZVTdmcTlDU1BKaVhrZTUxMDlISmVYTmlab05ZRURwdTR4KzZ2OGdmMjhxdXlmTHgKNFZYUG8zZzhxNjV0UE9GRk4rV013Wk5LdWVTdW1WTHh4MkR2UlY5OW9zbkNNZzRQQ09ZK2pDaDFIeDkrCmFGWFYrNjBSOXdxY2U3UjFNbXNOOXNXUEtvbHRlU0dZNytjUk1ZK3poYVU9Cg==.dc1fb43c14208a9f8a6d46403939861f\"></div><p><br></p>", "body_txt": "Problem You are trying to create a dataset using a schema that contains Scala enumeration fields (classes and objects). When you run your code in a notebook cell, you get a ClassNotFoundException error. Sample code %scala object TestEnum extends Enumeration { type TestEnum = Value val E1, E2, E3 = Value } import spark.implicits._ import TestEnum._ case class TestClass(i: Int, e: TestEnum) {} val ds = Seq(TestClass(1, TestEnum.E1)).toDS Error message ClassNotFoundException: lineb3e041f628634740961b78d5621550d929.$read$$iw$$iw$$iw$$iw$$iw$$iw$TestEnum Cause The\u00a0ClassNotFoundException error occurred because the sample code does not define the class and object in a package cell. Solution If you want to use custom Scala classes and objects defined within notebooks (in Apache Spark and across notebook sessions) you must define the class and object inside a package and import the package into your notebook. Info\nOnly class and object definitions can go in a package cell. Package cells cannot contain any function definitions, values, or variables. Define the class and object This sample code starts off by creating the package com.databricks.example. It then defines the object TestEnum and assigns values, before defining the class TestClass. %scala package com.databricks.example // Create a package. object TestEnum extends Enumeration { // Define an object called TestEnum. \u00a0 type TestEnum = Value \u00a0 val E1, E2, E3 = Value // Enum values } case class TestClass(i: Int, other:TestEnum.Value) // Define a class called TestClass. Info\nClasses defined within packages cannot be redefined without a cluster restart. Import the package After the class and object have been defined, you can import the package you created into a notebook and use both the class and the object. This sample code starts by importing the com.databricks.example package that we just defined. It then evaluates a DataFrame using the TestClass class and TestEnum object. Both are defined in the com.databricks.example package. %scala import com.databricks.example val df = sc.parallelize(Array(example.TestClass(1,(example.TestEnum.E1)))).toDS().show() The DataFrame successfully displays after the sample code is run. Please review the package cells (AWS | Azure | GCP) documentation for more information.", "format": "html", "updated_at": "2022-11-08T08:42:30.336Z"}, "author": {"id": 789805, "email": "saritha.shivakumar@databricks.com", "name": "saritha.shivakumar ", "first_name": "saritha.shivakumar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T14:42:15.447Z", "updated_at": "2023-04-17T04:09:51.795Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2933074, "name": "aws"}, {"id": 2933075, "name": "azure"}, {"id": 2933076, "name": "gcp"}], "url": "https://kb.databricks.com/scala/use-custom-classes-and-objects-in-a-schema"}, {"id": 1429753, "name": "Recover from a DELTA_LOG corruption error", "views": 1956, "accessibility": 1, "description": "Learn how to repair a Delta table that reports an IllegalStateException error when queried.", "codename": "recover-from-a-delta_log-corruption-error", "created_at": "2022-07-08T02:05:45.644Z", "updated_at": "2023-02-17T11:54:54.505Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.51a2866dca1c0da7a5f1757f970e8112\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are attempting to query a Delta table when you get an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IllegalStateException</span> error saying that the metadata could not be recovered.</p><pre id=\"isPasted\">Error in SQL statement: IllegalStateException:\u00a0\r\n\r\nThe metadata of your Delta table couldn't be recovered while Reconstructing\r\n\r\nversion: 691193. Did you manually delete files in the _delta_log directory?\r\n\r\nSet spark.databricks.delta.stateReconstructionValidation.enabled\r\n\r\nto \"false\" to skip validation.</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The Delta table is not accessible due to corrupted CRC or JSON files.</p><p>For example, you disabled multicluster writes (<a href=\"https://docs.databricks.com/delta/s3-limitations.html#what-are-the-limitations-of-multi-cluster-writes-on-s3\" rel=\"noopener noreferrer\" target=\"_blank\">AWS</a> | <a href=\"https://learn.microsoft.com/en-us/azure/databricks/delta/s3-limitations#what-are-the-limitations-of-multi-cluster-writes-on-s3\" rel=\"noopener noreferrer\" target=\"_blank\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/delta/s3-limitations.html#what-are-the-limitations-of-multi-cluster-writes-on-s3\" rel=\"noopener noreferrer\" target=\"_blank\">GCP</a>) but you still have more than one cluster attempting to write to the same table, leading to corruption.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-3\">Warning</h3>\n<p class=\"hj-alert-text\">This solution is a workaround that allows you to recover the Delta table and make it readable. This solution assumes that your Delta table is append-only.</p>\n<p class=\"hj-alert-text\">If your Delta table contains an update operation <strong>DO NOT</strong> use this solution. Open a ticket with Databricks Support.</p>\n</div>\n</div><p>These steps make the table readable and allow you to safely remove the corrupted delta transaction files.</p><ol>\n<li>Disable the validation config. This allows you to bypass the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IllegalStateException\u00a0</span>error when you run a query on the Delta table. This should not be used for production but is necessary to access the corrupted version of the table. Any running cluster can be used for this process. A cluster restart is not required to make changes in this property.<pre>%sql\r\n\r\nset spark.databricks.delta.stateReconstructionValidation.enabled=False;</pre>\n</li>\n<li>Identify which version of the table is corrupted.\u00a0<ol style=\"list-style-type: lower-alpha;\">\n<li>Identify the latest Delta version of the table by checking the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">_delta_log\u00a0</span>folder.<pre>%fs\r\n\r\nls /delta-table-root-folder/_delta_log</pre>\n</li>\n<li>Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">select * from table@v(&lt;latest&gt;)</span> in a SQL notebook cell. You should see empty results as the latest version is corrupted. Keep running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">select * from table@v(&lt;latest-x&gt;)</span> until you find a version that is not corrupted.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1664360193390-1664360193390.png\" class=\"fr-fic fr-dib fr-fil\">For the purpose of this article, assume that versions 6 and 7 are corrupted. Version 5 is intact and has not been corrupted.<br><br>\n</li>\n</ol>\n</li>\n<li>Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7.<br><br>This sample code moves the Parquet files to a backup folder.<br><pre>%python\r\n\r\nimport os\r\nimport shutil\r\n\r\n\r\ndef get_parquet_files_list(corrupted_versions):\r\n\u00a0 '''\r\n\u00a0 Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list\r\n\u00a0 '''\r\n\u00a0 final_list=[]\r\n\u00a0 for file in corrupted_versions:\r\n    delta_table_location = '&lt;delta-table-location&gt;' # prefix /dbfs not needed\r\n\u00a0 \u00a0 json_path = delta_table_location+'/_delta_log/'+str(file)\r\n\u00a0 \u00a0 df = spark.read.json(&lt;json_path&gt;).select('add.path')\r\n\u00a0 \u00a0 target_list = list(df.select('path').toPandas()['path'])\r\n\u00a0 \u00a0 final_list.extend(target_list)\r\n\u00a0 return list(filter(None, final_list))\r\n\r\n\r\ndef copy_files(final_list,source_folder,backup_folder):\r\n\u00a0 '''\r\n\u00a0 copy parquet files from source to backup folder\r\n\u00a0 '''\r\n\u00a0\u00a0\r\n\u00a0 i=1\r\n\u00a0 for file_path in final_list:\r\n\u00a0 \u00a0 src_path = source_folder + file_path\r\n\u00a0 \u00a0 trg_path = backup_folder + file_path\r\n\u00a0 \u00a0 os.makedirs(os.path.dirname(trg_path), exist_ok=True)\r\n\u00a0 \u00a0 shutil.copy(src_path, trg_path)\r\n\u00a0 \u00a0 print(\"Done --&gt; \"+str(i))\r\n\u00a0 \u00a0 i = i+1\r\n\u00a0 \u00a0\u00a0\r\ndef main(): \u00a0 \u00a0\r\n  source_folder = '&lt;delta-table-location&gt;' # prefix /dbfs is needed if the table location is in dbfs folder or in mount point \r\n  backup_folder = '&lt;backup-folder-storage-path&gt;' # prefix /dbfs is needed\r\n\u00a0 corrupted_versions = ['00000000000000000006.json','00000000000000000007.json'] # enter the corrupted versions json files here\r\n\u00a0 final_list = get_parquet_files_list(corrupted_versions)\r\n\u00a0 copy_files(final_list,source_folder,backup_folder)\r\n\u00a0\u00a0\r\nif __name__ == \"__main__\":\r\n\u00a0 \u00a0 main()</pre>\n</li>\n<li>Remove the CRC\u00a0and JSON files for the corrupted versions of the table.<pre>%fs\r\n\r\nrm /delta-table-root-folder/_delta_log/00000000000000000006.json\r\nrm /delta-table-root-folder/_delta_log/00000000000000000006.crc</pre>\n<pre>%fs\r\n\r\nrm /delta-table-root-folder/_delta_log/00000000000000000007.json\r\nrm /delta-table-root-folder/_delta_log/00000000000000000007.crc</pre>\n</li>\n<li>Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RESTORE TABLE\u00a0</span>to restore the Delta table to the most recent version that is not corrupted. In our example, this is version 5.<pre>%sql\r\n\r\nRESTORE TABLE &lt;table-name&gt; TO VERSION AS OF 5</pre>\n</li>\n<li>Now that the corrupted files have been removed, the table can be queried on any version. To avoid data loss, you must append the Parquet files that you previously backed up. This ensures that any data added to the corrupted versions of the table are inserted into the restored version and unnecessary data loss can be avoided.<br><br>While appending, please verify:<ol style=\"list-style-type: lower-alpha;\">\n<li>If the target table is partitioned. If it is partitioned, include the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">partitionBy</span> option in the append statement.</li>\n<li>Confirm all data types match the target table.<pre>%python\r\n\r\nbackup_folder = '/backup-folder'\r\ntarget_table_path = '/delta-table-root-folder/'\r\n\r\nappend_df = spark.read.format('parquet').load(&lt;backup-folder&gt;)\r\n\r\nappend_df_new = append_df.withColumn('col1',col('col1').cast('string'))# casting to match the target table schema\r\n\r\nappend_df_new.write.format('delta').partitionBy('col2','col3','col4').mode('append').option(\"path\",&lt;target-table-path&gt;).saveAsTable(\"db.tableName\")</pre>\n</li>\n</ol>\n</li>\n<li>Re-enable validation checking.<pre>%sql\r\n\r\nset spark.databricks.delta.stateReconstructionValidation.enabled=True;</pre>\n</li>\n</ol><p><br></p>", "body_txt": "Problem You are attempting to query a Delta table when you get an IllegalStateException error saying that the metadata could not be recovered. Error in SQL statement: IllegalStateException:\u00a0 The metadata of your Delta table couldn't be recovered while Reconstructing version: 691193. Did you manually delete files in the _delta_log directory? Set spark.databricks.delta.stateReconstructionValidation.enabled to \"false\" to skip validation. Cause The Delta table is not accessible due to corrupted CRC or JSON files. For example, you disabled multicluster writes (AWS | Azure | GCP) but you still have more than one cluster attempting to write to the same table, leading to corruption. Solution Warning\nThis solution is a workaround that allows you to recover the Delta table and make it readable. This solution assumes that your Delta table is append-only.\nIf your Delta table contains an update operation DO NOT use this solution. Open a ticket with Databricks Support. These steps make the table readable and allow you to safely remove the corrupted delta transaction files. Disable the validation config. This allows you to bypass the IllegalStateException\u00a0error when you run a query on the Delta table. This should not be used for production but is necessary to access the corrupted version of the table. Any running cluster can be used for this process. A cluster restart is not required to make changes in this property.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=False; Identify which version of the table is corrupted.\u00a0\nIdentify the latest Delta version of the table by checking the _delta_log\u00a0folder.%fs ls /delta-table-root-folder/_delta_log Run select * from table@v(&lt;latest&gt;) in a SQL notebook cell. You should see empty results as the latest version is corrupted. Keep running select * from table@v(&lt;latest-x&gt;) until you find a version that is not corrupted.For the purpose of this article, assume that versions 6 and 7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions): \u00a0 ''' \u00a0 Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list \u00a0 ''' \u00a0 final_list=[] \u00a0 for file in corrupted_versions: delta_table_location = '&lt;delta-table-location&gt;' # prefix /dbfs not needed \u00a0 \u00a0 json_path = delta_table_location+'/_delta_log/'+str(file) \u00a0 \u00a0 df = spark.read.json(&lt;json_path&gt;).select('add.path') \u00a0 \u00a0 target_list = list(df.select('path').toPandas()['path']) \u00a0 \u00a0 final_list.extend(target_list) \u00a0 return list(filter(None, final_list)) def copy_files(final_list,source_folder,backup_folder): \u00a0 ''' \u00a0 copy parquet files from source to backup folder \u00a0 ''' \u00a0\u00a0 \u00a0 i=1 \u00a0 for file_path in final_list: \u00a0 \u00a0 src_path = source_folder + file_path \u00a0 \u00a0 trg_path = backup_folder + file_path \u00a0 \u00a0 os.makedirs(os.path.dirname(trg_path), exist_ok=True) \u00a0 \u00a0 shutil.copy(src_path, trg_path) \u00a0 \u00a0 print(\"Done --&gt; \"+str(i)) \u00a0 \u00a0 i = i+1 \u00a0 \u00a0\u00a0 def main(): \u00a0 \u00a0 source_folder = '&lt;delta-table-location&gt;' # prefix /dbfs is needed if the table location is in dbfs folder or in mount point backup_folder = '&lt;backup-folder-storage-path&gt;' # prefix /dbfs is needed \u00a0 corrupted_versions = ['00000000000000000006.json','00000000000000000007.json'] # enter the corrupted versions json files here \u00a0 final_list = get_parquet_files_list(corrupted_versions) \u00a0 copy_files(final_list,source_folder,backup_folder) \u00a0\u00a0 if __name__ == \"__main__\": \u00a0 \u00a0 main() Remove the CRC\u00a0and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc\n%fs rm /delta-table-root-folder/_delta_log/00000000000000000007.json rm /delta-table-root-folder/_delta_log/00000000000000000007.crc Run RESTORE TABLE\u00a0to restore the Delta table to the most recent version that is not corrupted. In our example, this is version 5.%sql RESTORE TABLE &lt;table-name&gt; TO VERSION AS OF 5 Now that the corrupted files have been removed, the table can be queried on any version. To avoid data loss, you must append the Parquet files that you previously backed up. This ensures that any data added to the corrupted versions of the table are inserted into the restored version and unnecessary data loss can be avoided. While appending, please verify:\nIf the target table is partitioned. If it is partitioned, include the partitionBy option in the append statement.\nConfirm all data types match the target table.%python backup_folder = '/backup-folder' target_table_path = '/delta-table-root-folder/' append_df = spark.read.format('parquet').load(&lt;backup-folder&gt;) append_df_new = append_df.withColumn('col1',col('col1').cast('string'))# casting to match the target table schema append_df_new.write.format('delta').partitionBy('col2','col3','col4').mode('append').option(\"path\",&lt;target-table-path&gt;).saveAsTable(\"db.tableName\") Re-enable validation checking.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=True;", "format": "html", "updated_at": "2023-02-17T11:54:54.498Z"}, "author": {"id": 791529, "email": "gopinath.chandrasekaran@databricks.com", "name": "gopinath.chandrasekaran ", "first_name": "gopinath.chandrasekaran", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T02:33:30.987Z", "updated_at": "2023-02-17T09:49:59.033Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256849, "name": "Delta Lake", "codename": "delta", "accessibility": 1, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2998833, "name": "aws"}, {"id": 2998834, "name": "azure"}, {"id": 2998838, "name": "corrupted"}, {"id": 2998839, "name": "corruption"}, {"id": 2998837, "name": "delta"}, {"id": 2998836, "name": "delta table"}, {"id": 2998835, "name": "gcp"}, {"id": 2998840, "name": "json"}], "url": "https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error"}, {"id": 1429287, "name": "Members not supported SCIM provisioning failure", "views": 5226, "accessibility": 1, "description": "You get a members or groups not supported error when trying to provision new users to your workspace via SCIM.", "codename": "members-not-supported-scim-provisioning-failure", "created_at": "2022-07-07T16:21:53.301Z", "updated_at": "2022-08-18T19:37:43.779Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThCcUhscXN0aWF4M0o1MEVRd1p1QUFKU09RZ0dNTVRMMGpwM2NPVXZJTmZZeTRZa2tlCnA2eEpsREF4L1hFdHp5TUM0U3ZFRXhKVVhxR0U5cXVUcWRWemRFQk52MjhDSFJwN1hOdmx3K3ByWk84NgpUL0plS1UzMGM2Sm5oTDZQTjhJRkl3VHJ0U093MjhQRDYwUyszRFFLUGxPRC83NHRsdGhtR0ZRdS9EeS8KY28xM1B0WTVHNEdIOVluYURhRWZqNXVmZDE1WW1WbTlMUmVtdWp6K2R1c1lSMGF6N1lXa21qdmxRSEZnCkN5T3dxN3BTc3YyeVNaTUhMSm9HM2ZyaDA3QWlJSXU5dTQ5d3FDamhRNVRXb2w4TUI2WkxJRUZYb2NZagpMRDdWZElTRm0xOTFVb1ZTL0JRVkFwRlJDdk95bXRrNHRJUTg4L0s0RXNLMElCUXYrU3ByQ2JEa3JZUUUKdkNTZzcwNHhFTUVPTkhkY3lQdGlOc0EwUk1RS28vdXhzVURkV2tWUHZnTjNyRXl6NXZUbHFEQnBYT1JFCmc2aU5JT2JWQ1N6YmpYOXVnQ2p4dEhOQUEyTExDUzFVdFJaL1dYRW1sYUFOZVpyR2l3b1d5WkVMVWxMNwpNMGsrcDY5UDM1MFRRNDduTVFJam5POFl1cmwvN0R0aVA4eFpBNk1FVktjc0RlUjNNK0U1bXBPVzh0VkcKV0tMbHd5eG50RElYTy8vVWtra3BHUGZyNWlsNmszS04zTDdpNVNvc2hlVjh2cm02cFNXZElyaDZrWHFrCmQwQ2F1emZ6T2x0U2xTc25JTUdvYTNDU3Ewd1JVQTJPd2ZLeW1TN2h0V2g4a0FZS3grbUxadTRreUpRRApwVWhjeXM2U21oL3NYNW5CTjFXbm92V2xQR3EvQys1VitveEd4ejRyam9aU24rU3lTVjdHTXpNK3B3N2EKTS85cGZOSFk5cHhZOW1PR1VNREVSSFdSNXV3aTdYSVBDNFRlTWFmRFBqb1RHeHZFcnlyTGFFSUVoVCtJCnFvWHNGcVhqdmRQTVBzbVZadjV3MDFna0dYT1paRHFvczJXV1BtMzNwK3h2a3dpbzh0bWlVeU9MVHBEMApRc000TFhVYm5UaVJrSjVVZ0dPTkJWR1JrbU4yZlpIT0wza3gvUkJaSzJEV0xhN3NaMzcrUHNOcE1NMkQKSzdqd2VobW1USVpXVjVDNUxpemxMUStGbU5mREtVbkJwUXFvK3dwZW43eEVVUWpKWTl0YXd2d2h5S1l1CjU4VjRKZm5PcUxGcWZWNkxxQXpaSVhlY3pXTmlncTNrSlhzOVlQVWgxUE1wUnpBNTI3UnZlTi9wVm1SVwpSbk1rdmZpY1Q5b2FVRlU2UXdCQTYwdGNvOXZsYXNpT1oyWXVZRVRBTTBvaDJ3NWtOVUUvQ0R2cUFTREYKcllrTDU5OW0wa2Ird2NyVy9NYnU1N0Q4d1c0aHJId1haUFpGdXZzbWtseHpRZGRvS2J6WlEzUlU1emViCnNyYjBJdk02aVhWN05Cd0RYTStORDBlbzBmUzE2NmwrVDZyK3hqZ2ZzMitHeGE5Y0M2c3JjTWprZ0I1KwpHYjhLbmJYSXNmamVOU3VoRUdzaytKR0hCVktCRG01TGs4bU5HSlRiaGxPVDcxQ1hwNmpWQ2tBRGZDTFcKS05ScFpxNjdUNmZKT3ErRVBJeUJUclljd2lxdnBUM2VXbHNJVkk4M29JT1J5dGZxYXdKd0FQeFpBTHNrCkwxanBRdlNhcjNwTk1PekFFTm1EK0lqNU5RVDdwRWJibUZmS3ZPZFNOYnRZd2c9PQo=.41a30891e4a2a4fd9fb9bc4643637a7a\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You using SCIM to provision new users on your Databricks workspace when you get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Members attribute not supported for current workspace</span> error.</p><pre>StatusCode: BadRequest\r\nMessage: Processing of the HTTP request resulted in an exception. Please see the HTTP response returned by the 'Response' property of this exception for details.\r\nWeb Response:\u00a0\r\n{\"schemas\":[\"urn:ietf:params:scim:api:messages:2.0:Error\"],\"scimType\":\"Members attribute not supported for current workspace.\",\"detail\":\"Request is unparsable, syntactically incorrect, or violates schema.\",\"status\":\"400\"}\r\n. This operation was retried 0 times. It will be retried again after this date: 2022-07-07T10:51:04.8148533Z UTC</pre><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-1\">Info</h3>\n<p class=\"hj-alert-text\">Depending on how you are provisioning users, you may get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Groups attribute not supported for current workspace</span> error message.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"cause-2\">Cause</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Members attribute not supported for current workspace</span> error message indicates that identity federation is enabled on the workspace. Accounts that have Unity Catalog enabled (Identity federation will be enabled by default) manage users and groups at the account level. You get an error if you try to manage them at the workspace level.</p><h1 data-toc=\"true\" id=\"solution-3\">Solution</h1><p>Verify your workspace settings. If Unity Catalog is enabled on the workspace you must manage users and groups at the account level.</p><p>Review the documentation on <a href=\"https://docs.databricks.com/data-governance/unity-catalog/manage-identities.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">managing identities in Unity Catalog</a> for more information.\u00a0</p><p>There are two ways to determine if Unity Catalog is enabled on the workspace or not.</p><ol>\n<li>From the accounts console as an admin user</li>\n<li>From the workspace\u00a0UI as a normal user</li>\n</ol><h2 data-toc=\"true\" id=\"from-the-account-console-admin-4\">From the Account Console (Admin)</h2><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-5\">Warning</h3>\n<p class=\"hj-alert-text\">You must have account\u00a0admin permissions to proceed.</p>\n</div>\n</div><ol>\n<li>Log in to the <strong>Account Console</strong> at <a data-fr-linked=\"true\" href=\"https://accounts.cloud.databricks.com\">https://accounts.cloud.databricks.com</a>.</li>\n<li>Review the list of workspaces in your\u00a0account.<br><img src=\"https://s3.amazonaws.com/helpjuice-static/helpjuice_production%2Fuploads%2Fupload%2Fimage%2F10723%2Fdirect%2F1659122624484-1659122624484.png\" class=\"fr-fic fr-dib\">\n</li>\n<li>Click the name of the workspace to open the workspace <strong>Configuration</strong>.</li>\n<li>On the right hand side of the screen, look for the\u00a0<strong>Identity federation</strong> value.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1660775580662-Identity%20federation%20setting.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>Review the <strong>Metastore</strong> configuration section and look for a\u00a0<strong>unity-catalog</strong> value.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1660851357888-Unity%20Catalog%20Small.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>If the <strong>Identity federation</strong> is set to <strong>Enabled</strong> or the metastore is configured with <strong>unity-catalog</strong> you must manage users and groups at the account level.</li>\n</ol><h2 data-toc=\"true\" id=\"from-the-workspace-ui-non-admin-6\">From the workspace UI (non-admin)</h2><ol>\n<li>Log in to your workspace.</li>\n<li>Click <strong>Compute</strong>.</li>\n<li>Click the <strong>Create Cluster</strong> button.</li>\n<li>Review the options under <a href=\"https://docs.databricks.com/clusters/configure.html#cluster-mode\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\"><strong>Cluster mode</strong></a>.</li>\n<li>If\u00a0<strong>High Concurrency</strong> is an option, Unity Catalog is disabled and you can manage users and groups at the workspace level.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1660016433620-Cluster%20mode%20-%20High%20Concurrency.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>If <strong id=\"isPasted\">High Concurrency</strong> mode shows as disabled, Unity Catalog is enabled and you must manage users and groups at the account level.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1660775266010-High%20Concurrency%20Disabled.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n</ol><p><br></p>", "body_txt": "Problem You using SCIM to provision new users on your Databricks workspace when you get a Members attribute not supported for current workspace error. StatusCode: BadRequest Message: Processing of the HTTP request resulted in an exception. Please see the HTTP response returned by the 'Response' property of this exception for details. Web Response:\u00a0 {\"schemas\":[\"urn:ietf:params:scim:api:messages:2.0:Error\"],\"scimType\":\"Members attribute not supported for current workspace.\",\"detail\":\"Request is unparsable, syntactically incorrect, or violates schema.\",\"status\":\"400\"} . This operation was retried 0 times. It will be retried again after this date: 2022-07-07T10:51:04.8148533Z UTC Info\nDepending on how you are provisioning users, you may get a Groups attribute not supported for current workspace error message. Cause The Members attribute not supported for current workspace error message indicates that identity federation is enabled on the workspace. Accounts that have Unity Catalog enabled (Identity federation will be enabled by default) manage users and groups at the account level. You get an error if you try to manage them at the workspace level. Solution Verify your workspace settings. If Unity Catalog is enabled on the workspace you must manage users and groups at the account level. Review the documentation on managing identities in Unity Catalog for more information.\u00a0 There are two ways to determine if Unity Catalog is enabled on the workspace or not. From the accounts console as an admin user\nFrom the workspace\u00a0UI as a normal user From the Account Console (Admin) Warning\nYou must have account\u00a0admin permissions to proceed. Log in to the Account Console at https://accounts.cloud.databricks.com.\nReview the list of workspaces in your\u00a0account. Click the name of the workspace to open the workspace Configuration.\nOn the right hand side of the screen, look for the\u00a0Identity federation value. Review the Metastore configuration section and look for a\u00a0unity-catalog value. If the Identity federation is set to Enabled or the metastore is configured with unity-catalog you must manage users and groups at the account level. From the workspace UI (non-admin) Log in to your workspace.\nClick Compute.\nClick the Create Cluster button.\nReview the options under Cluster mode .\nIf\u00a0High Concurrency is an option, Unity Catalog is disabled and you can manage users and groups at the workspace level. If High Concurrency mode shows as disabled, Unity Catalog is enabled and you must manage users and groups at the account level.", "format": "html", "updated_at": "2022-08-18T19:37:43.775Z"}, "author": {"id": 789494, "email": "prabakar.ammeappin@databricks.com", "name": "prabakar.ammeappin ", "first_name": "prabakar.ammeappin", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:54:57.352Z", "updated_at": "2023-04-05T07:45:06.002Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 313834, "name": "Unity Catalog", "codename": "unity-catalog", "accessibility": 1, "description": "These articles can help you with Unity Catalog.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2838629, "name": "aws"}, {"id": 2838628, "name": "identity"}, {"id": 2789795, "name": "scim provisioning"}, {"id": 2838627, "name": "scimtype"}, {"id": 2789794, "name": "unity catalog"}], "url": "https://kb.databricks.com/unity-catalog/members-not-supported-scim-provisioning-failure"}, {"id": 1428601, "name": " Life-cycle of a KB", "views": 25, "accessibility": 0, "description": "", "codename": "1428601-how-to-template", "created_at": "2022-07-07T06:22:05.575Z", "updated_at": "2022-07-07T07:31:43.101Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThLblNkbDRGTFI5WkhqWHZUWXU3dW9xUmtrcHZyTWxjc3J3YjFNUDZ2Wk5kQkFKbG0wCkY2RXhUOE5SUFBxazhPd1dtU29VdEZUek4rOG9kdlBsTEhDbFVJK2d4bVlic0JqZkhyakp3T3Q5L0NqawpWRGNGYUYvUHZ3ZFFCdFhiWFpJY0VGZytockdXM01idXBCZWxHWG1oVElYbFpKV041eVJ4VHBrRUxma0YKOE5lcjZMMTFFZjRMTlF2RFQ1bjhRa1ZScVpsNWZnOTlmZDZFcnNVd0tNU2NPMWJuOTFEaHFpTkdDK2RGCk1Hd2RiYUF3YlBVaW9oMUdFTUU5OThkOWJSeFA0MGNBdHRYVjhwOVRTdG1CSEMrVnRtUmxpUHdBbFJyOAovQTRtaWdHOU5tTzNxa2FtTUEraEF5M1U2Z2tpdE5qWnJ6ck1Oakp0U3NxQytsUmRhUjRmaDFOVkkyRUoKSE15QWpSc3lVOGdZaFVxMWZSSENrSmJvbEh2Q1JzYzdxaEhlL0lRbTlTR1NUVWdNbk9BNDhCeVYzTGlhCnBYOGR2MHllanZaUHpCYjdVeUpwMUljTTNrYTBUN1U0ODJhVWk1MEJQR3Y3QldYdGdMYTBrZmNpRVk0dApPODRvYXJUMFhJUHJ3ZXdGK2lpWVhrb3k1WDhXcTVLZnNNQndFYnBTWWJFVE5JVXZ3YWczdzNGaE5IWFcKcitXc0U2WHQ0bWpTbkR4SjZRZjdhN013bTJjeXo1TyttakNqV1BEWlR5N1k3bkxDUXVheldVVHUralFVCitMWkp0V0JPT1dBVXhwRWdPUmhqd2tjUnVpVFlCbUNyeUNVWHJyL2ZkYVJQSWY2ekM4KytOUHdrZEN2awoxZWNlR0k0WW03RURmNWRTYkREWjZzcGN4M255VWVKaDhGYXJGVHA3Tk93d25Vc2Fjd0ZFSXoxZk1keDEKOVhxd3AvWnBKTGxzNGlLN25Tem8wOE81b0Mzc0RFclJ2OG0wTk80aWtEQi9rMnZjeHVMUmpBMkNhNy85ClB6R2tFRG01OHk2L25oOVJKWWNPUWJiSzF6QmM0THhDTjhZTWxGeG16dzBYc3BrVFBpWEVNRmpqb3E4UQpISU1IV29FSGN5eUJ0VHpqcWF3WHFFN1ZaZWpjTHM2SHU5T1pEWkRjZms3WVdwVlpDS0VNU2tqSzlHcjIKMEliVUtCRWFZUll0VkduZi9ZUW0yWlYyRGNpbXFQNSt2eU00L0diQW5UQXJOMlR6a3ZJRCtiRDZaMnBECkdFbFNHSTFBQ2pIbmFlTlJNOUlyWWk4cytCQ3NrdzNoNVpoVElwWnRTRS82c3VvQmtvWjJrd0ZyUStrSQpSdG9NWHBmY3VZUkZhSEFPaUV1SUpyc0NiMklQYmNWUC9rRFV1R1Vkd3EyWlB5RStqeHpjSXFUZjExQTEKc0JEbzU1aDFBTXJGdkhMeU1Tc0JYd2FqNXNDMVhpYVBJR3pBTWdGc2M3eTVxMlhnQ0laNVpDNUpLdjZUCkVrMjAzaWpOM0haOHlFZHdQZkRsenc1VlRwK0JaYU9vRVU2TWpaWWtIY1RhWmpjb0NNYmVoUDZGZ0hTYgo2bjBIakpOZXdTczVBRENhS2RmaGtYdUk4WEdpRzZ6czluSWEzSXdFQUQwL1F3ZzYvdkpORTdBeno0S1QKVUtudW54YVF6Ylp3QjVjcTZ0cm9nV1V6OXNlMWpJeUsvRW1NblJIdFkzeXlBc0xsYjdQR2E0aU1oWWpiCjZtYUtYRWlPU2M2Yk9RWVArL2hxc2VUenkrSEtPUUhtVklxcmN0dW1PaENnVXJXaGpQeHFNTWE2Vm9sbgpUNkovT1JMRmVQUWVBSk1YSFJGaTVkSTdXQ05GajZZN2xMR3h4RGo0UzJNRS9yb0NRNUJmUmlDQk9wdFQKc3RhQ1hSUWEzdTR5VU1PaDVtRHkxWEpXY3dyTXpVUVQ5UHQwNFA3WVVpSjF3MU0wMUN4d05yTmJmYy9JCjVYTFlwVjZRQWgyTTdCQUg2K3V6LzZ5Z0hHeUZUdXR2Wng1THBDN29mYTM2NjhFNjRHeUROaUFUVnc9PQo=.53d360b6b8bf0858915ab5a4b199de87\"></div><h1 data-toc=\"true\" id=\"how-to-introduction-0\">How-to Introduction</h1><p>This article explains how to author a new KB article in Helpjuice.</p><p>KB Life-Cycle Chart</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657178868205-1657178868205.png\" class=\"fr-fic fr-dib\"></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"note-1\">Note</h3>\n<p class=\"hj-alert-text\">If you have any questions during the process, reach out to\u00a0<a href=\"mailto:adam.pavlacka@databricks.com\" id=\"isPasted\">adam.pavlacka@databricks.com</a> via email or adam.pavlacka on Slack.<br><br>Adam is based in San Francisco and will respond as soon as possible.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"instructions-2\">Instructions</h1><p>1. <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Navigate to the\u00a0</span><strong style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Dashboard</strong><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">\u00a0-&gt;\u00a0</span><strong style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Support Article Drafts</strong><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">\u00a0folder</span></p><p><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657175041947-Screenshot%202022-07-07%20at%2011.53.53%20AM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></span></p><p><br></p><p>2. <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Select the correct folder for your team. For example, if you are on the India Platform team, select <strong>platform</strong></span><strong>-India-drafts</strong><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><strong>.</strong></span></p><p><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><strong><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657175510135-1657175510135.png\" class=\"fr-fic fr-dib\"></strong></span><br></p><p>3. <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Select the In-Progress folder, if you want to write a new draft for the India Platform team.</span></p><p><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657175643768-1657175643768.png\" class=\"fr-fic fr-dib\"></span></p><p><span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">4.Click\u00a0</span><strong style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">New Article From Template</strong><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">.</span></p><p>\u00a0<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657175726619-1657175726619.png\" class=\"fr-fic fr-dib\"></p><p>5. Click the template you want to use. The most common KB article type is <strong>Problem Cause Solution</strong>.</p><p>6.Enter a title for the article at the top of the page. The article URL automatically fills in after you enter an article title.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654573760109-Screen%20Shot%202022-06-06%20at%208.49.00%20PM.png\" class=\"fr-fic fr-dii\"></p><p><span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">7. Complete all of the metadata in the\u00a0</span><strong style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Restricted Content</strong><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">\u00a0box. If something does not apply, enter N/A. Do NOT list \"All\" for\u00a0</span><strong style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">DBR Version</strong><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">\u00a0or\u00a0</span><strong style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Cloud Version</strong><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">. Be specific.</span></p><p><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657175840588-1657175840588.png\" class=\"fr-fic fr-dib\"></span></p><p>8. Complete the rest of the article content.</p><p>9. When the draft is finished, at-mention your lead and have them sign off on a technical review and change the Main category to <strong>Ready for Tech Review</strong></p><p><strong><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657176072677-1657176072677.png\" class=\"fr-fic fr-dib\"></strong></p><p><strong><br></strong></p><p>10. When the kb review is done by at-mention your lead or reviewer then the <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><strong>reviewer</strong> needs to move the Kb to Tech Review Approved and\u00a0</span>change the Main category to <strong>Tech Review Approved\u00a0</strong>and<strong>\u00a0<span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">at-mention to, <span>Adam.</span></span></strong></p><p><br></p><p><strong><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657176262124-Screenshot%202022-07-07%20at%2012.14.07%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></strong></p><p><strong><br></strong></p><p>11. Once Adam completes their review and completes <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">their</span> changes\u00a0</p><p>\u00a0 \u00a0 Option A. If everything is ok, then KB will move for the final review. Adam will move the kb to Ready for Final Review<span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">\u00a0and\u00a0</span>change the Main category to <strong>Ready for Final Review.</strong></p><p><strong><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657176905258-1657176905258.png\" class=\"fr-fic fr-dib\"></strong><br></p><p><strong>\u00a0 \u00a0<span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Option B: <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">If Adam needs some change on Kb from the technical side of the content side is ok, then KB will move for the Need-Work. Adam will move the kb to <strong id=\"isPasted\" style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important;\"><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important;\">Need-Work</span></span></strong></span><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; background-color: rgb(255, 255, 255); float: none; display: inline !important;\">\u00a0and\u00a0</span><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">change the Main category to\u00a0</span></span></strong><span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important;\"><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important;\"><strong>Need-Work</strong></span></span></span></span><strong><span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><strong style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">.</strong></span></strong><span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">\u00a0</span></p><p><strong>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0</strong></p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657176947842-1657176947842.png\" class=\"fr-fic fr-dib\"></p><p>12.\u00a0</p><p>If <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">\u00a0Option B- E<span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; background-color: rgb(255, 255, 255); float: none; display: inline !important;\">ngineers will start from step 8 and follow the same cycle.\u00a0</span></span></p><p>If <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Option A - Adam will be Published the KB <strong id=\"isPasted\" style=\"box-sizing: border-box; font-weight: 700; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\"><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">\u00a0then KB will move to the Real Main category and the status should be\u00a0</span></span></strong><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\"><span style=\"box-sizing: border-box; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">Published</span></span></span></p>", "body_txt": "How-to Introduction This article explains how to author a new KB article in Helpjuice. KB Life-Cycle Chart Note\nIf you have any questions during the process, reach out to\u00a0adam.pavlacka@databricks.com via email or adam.pavlacka on Slack. Adam is based in San Francisco and will respond as soon as possible. Instructions 1. Navigate to the\u00a0 Dashboard \u00a0-&gt;\u00a0 Support Article Drafts \u00a0folder 2. Select the correct folder for your team. For example, if you are on the India Platform team, select platform -India-drafts . 3. Select the In-Progress folder, if you want to write a new draft for the India Platform team. 4.Click\u00a0 New Article From Template . \u00a0 5. Click the template you want to use. The most common KB article type is Problem Cause Solution. 6.Enter a title for the article at the top of the page. The article URL automatically fills in after you enter an article title. 7. Complete all of the metadata in the\u00a0 Restricted Content \u00a0box. If something does not apply, enter N/A. Do NOT list \"All\" for\u00a0 DBR Version \u00a0or\u00a0 Cloud Version . Be specific. 8. Complete the rest of the article content. 9. When the draft is finished, at-mention your lead and have them sign off on a technical review and change the Main category to Ready for Tech Review 10. When the kb review is done by at-mention your lead or reviewer then the reviewer needs to move the Kb to Tech Review Approved and\u00a0change the Main category to Tech Review Approved\u00a0and\u00a0at-mention to, Adam. 11. Once Adam completes their review and completes their changes\u00a0 \u00a0 \u00a0 Option A. If everything is ok, then KB will move for the final review. Adam will move the kb to Ready for Final Review\u00a0and\u00a0change the Main category to Ready for Final Review. \u00a0 \u00a0Option B: If Adam needs some change on Kb from the technical side of the content side is ok, then KB will move for the Need-Work. Adam will move the kb to Need-Work \u00a0and\u00a0 change the Main category to\u00a0 Need-Work . \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 12.\u00a0 If \u00a0Option B- Engineers will start from step 8 and follow the same cycle.\u00a0 If Option A - Adam will be Published the KB \u00a0then KB will move to the Real Main category and the status should be\u00a0 Published", "format": "html", "updated_at": "2022-07-07T07:31:43.091Z"}, "author": {"id": 488150, "email": "rakesh.parija@databricks.com", "name": "rakesh.parija ", "first_name": "rakesh.parija", "last_name": "", "role_id": "admin", "created_at": "2021-10-07T02:59:41.577Z", "updated_at": "2023-04-21T14:21:18.111Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 264386, "name": "Internal Articles", "codename": "internal-articles", "accessibility": 0, "description": "All articles in this section are only viewable by Databricks employees.", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [], "url": "https://kb.databricks.com/internal-articles/1428601-how-to-template"}, {"id": 1428335, "name": "Recursive references in Avro schema are not allowed", "views": 3006, "accessibility": 1, "description": "Apache Avro data sources cannot have recursive references in the schema when used with Spark.", "codename": "recursive-references-in-avro-schema-are-not-allowed", "created_at": "2022-07-06T17:47:20.624Z", "updated_at": "2022-12-01T03:20:02.349Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.ff4d9191771b3f88abea7e779a27190b\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Apache Spark returns an error when trying to read from an <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-avro.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Apache Avro data source</a> if the Avro schema has a recursive reference.</p><pre>org.apache.spark.sql.avro.IncompatibleSchemaException:\r\nFound recursive reference in Avro schema, which can not be processed by Spark</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Spark SQL does not support recursive references in an Avro data source because it is impossible to convert the schema to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">StructType</span>.</p><p>Review the <a href=\"https://github.com/apache/spark/pull/22709\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">[SPARK-25718][SQL]Detect recursive reference in Avro schema and throw exception</a> pull request for more information.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You must avoid using recursive references in your Avro schema.</p><h2 data-toc=\"true\" id=\"test-for-recursive-references-3\">Test for recursive references</h2><p>You can test your Avro schema for recursive references with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SchemaConverters.toSqlType(&lt;avro-schema&gt;)</span>.</p><pre id=\"isPasted\">%sql\r\n\r\nimport org.apache.spark.sql.avro.SchemaConverters\r\nSchemaConverters.toSqlType(&lt;avro-schema&gt;)</pre><p>If the Avro schema contains recursive references, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SchemaConverters.toSqlType</span> returns an error.</p><h3 data-toc=\"true\" id=\"example-4\">Example</h3><ol>\n<li>Create an Avro schema with a recursive reference.<pre>%sql\r\n\r\nimport org.apache.avro.Schema\r\nval schema = new Schema.Parser().parse(\"\"\"{\r\n\u00a0 \"type\": \"record\",\r\n\u00a0 \"name\": \"LongList\",\r\n\u00a0 \"aliases\": [\"LinkedLongs\"], \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \"fields\" : [\r\n\u00a0 \u00a0 {\"name\": \"value\", \"type\": \"long\"}, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 {\"name\": \"next\", \"type\": [\"null\", \"LongList\"]}\u00a0\r\n\u00a0 ]\r\n}\"\"\")</pre>\n</li>\n<li>Test the schema with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SchemaConverters.toSqlType</span>.<pre>%sql\r\n\r\nimport org.apache.spark.sql.avro.SchemaConverters\r\nSchemaConverters.toSqlType(schema)</pre>\n</li>\n<li>It returns an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IncompatibleSchemaException</span>error.<pre>IncompatibleSchemaException: Found recursive reference in Avro schema, which can not be processed by Spark: { \u00a0\"type\" : \"record\", \u00a0\"name\" : \"LongList\", \u00a0\"fields\" : [ { \u00a0 \u00a0\"name\" : \"value\", \u00a0 \u00a0\"type\" : \"long\" \u00a0}, { \u00a0 \u00a0\"name\" : \"next\", \u00a0 \u00a0\"type\" : [ \"null\", \"LongList\" ] \u00a0} ], \u00a0\"aliases\" : [ \"LinkedLongs\" ] }</pre>\n</li>\n</ol><p><br></p><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"feb8ee0dd9144\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTkvWlFYNkF0b1hiR1VWUFV0VmVzZFQ2MzBzNTZJeDBkb0RtbmZraG1ib2Z1d1dlNDJOCnRGcGF0cFBRZW9CWFhvNzAzbUlpbXRYSGh5NnFiTEEvaXRVY2M4eTZ5UUJBYzU5WVFiVDBVRlAyUFVFQQpkRlJLUkY1a2xWZzgxMDNwWGtOTkRHakNuWVBib2JsWkUxemM4bFZSelRCSnl6RldrcStIRHZ0dTVpckgKUDRSNkhIUXZ0eC9JODNOWXJwN0h4U3BDNUpIMEk3WmQwaHY4NWRiZTZOdnN0QlpMakdGMVFkc2s4ekNnClV1VWpXWExVRWEwYnltWkdGNTNlWDkrYTFVNmRpRjBTTytPdTY5L1NVTzFqOGFsQWNScU1BU3FLdFdDNAp1SmdJN2FJYkxDQ0pQTGxqMFJEZGtWWlcrSnRQZU5lRmErTkRsUm1RaWJwVlBaUDZxa2ZFN0tjZHRWZ00KekVDRzRFK3c2cjE5TGE1dG1oZ3ZRRCtkc04ydUNSaTNnU0xxaHdrL0tOWUtHbnFpNHFIYzFmT1JHOWRLClBGMkhpbHdldTllbWEvNEZ2Z29oWUcrR0c2OEluanh3ZWN5ZUY5NGxRV20xVlM3QWVuQ2IxejI3RSs5NQp0bE1ZU2krT2xoTlhhSXNxaDlycTlGemNlN2JvTVl3UFFtbHZibC96RkFmMnFVSEk1THNHRjQ3bEdST2cKTUlEZ1JnTmEvcURxZFF1dkJpMjZ5YXRuQmk1bzRTaXN6TXpZUDhxY0oyWWVtRHBtRGpIMCtIVTdBNE0vCm9HRlZCaUZnTnVzTSs2amZDQ0lGMnZzNFIrQk5zZEhEdGE3TWxoWVlNSUVWMHIwVDRoY2NiSG0zaHY5bApGV0JQZlI2OVk3eHJmTmorOUZiT0xqYjVHaE9icEUzNmc1c2pSWCs5d1JML3BIR29ZbTcwL0pybE5UYnkKRGFzajN2ZmRDM3RPVzFzc2lHOE9JWG1vYkZGKzI4VlBsemcySDVNTjFQeC9mT0JTN3JTV0xLc2JqYUdPCkxhR0c4ZzVEc1NkemlqWkJER0tFY2ZFbHJFeW1WaitGOE1ObXJoN0tTSytyRGFXa2dtelZyRW5VNTI0aAp4VlBtb3p3WTRtUWtEYW0rVmwxTThpcWFQWldTS0JJaUdLbHB2Z2VyblpZNktTMlF5UisvbjBjOFBjeEUKNE9mNVFNN3BlK3U1WFZlSUZDUWhFNmsrNGVpcDdEU0EyTGsyaTJvZXM0LzYrYlRXY29od0RqNTBGdUdsClVITnAzUVhQRWVVOWRWN1pSSWNiU2kxUi9rbVFUaytmTlJKVGplc2E3WkU9Cg==.ca4503e0aca534b024161df1cbaa127f\"></div><p><br></p><p><br></p>", "body_txt": "Problem Apache Spark returns an error when trying to read from an Apache Avro data source if the Avro schema has a recursive reference. org.apache.spark.sql.avro.IncompatibleSchemaException: Found recursive reference in Avro schema, which can not be processed by Spark Cause Spark SQL does not support recursive references in an Avro data source because it is impossible to convert the schema to StructType. Review the [SPARK-25718][SQL]Detect recursive reference in Avro schema and throw exception pull request for more information. Solution You must avoid using recursive references in your Avro schema. Test for recursive references You can test your Avro schema for recursive references with SchemaConverters.toSqlType(&lt;avro-schema&gt;). %sql import org.apache.spark.sql.avro.SchemaConverters SchemaConverters.toSqlType(&lt;avro-schema&gt;) If the Avro schema contains recursive references, SchemaConverters.toSqlType returns an error. Example Create an Avro schema with a recursive reference.%sql import org.apache.avro.Schema val schema = new Schema.Parser().parse(\"\"\"{ \u00a0 \"type\": \"record\", \u00a0 \"name\": \"LongList\", \u00a0 \"aliases\": [\"LinkedLongs\"], \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0 \"fields\" : [ \u00a0 \u00a0 {\"name\": \"value\", \"type\": \"long\"}, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0 \u00a0 {\"name\": \"next\", \"type\": [\"null\", \"LongList\"]}\u00a0 \u00a0 ] }\"\"\") Test the schema with SchemaConverters.toSqlType.%sql import org.apache.spark.sql.avro.SchemaConverters SchemaConverters.toSqlType(schema) It returns an IncompatibleSchemaExceptionerror.IncompatibleSchemaException: Found recursive reference in Avro schema, which can not be processed by Spark: { \u00a0\"type\" : \"record\", \u00a0\"name\" : \"LongList\", \u00a0\"fields\" : [ { \u00a0 \u00a0\"name\" : \"value\", \u00a0 \u00a0\"type\" : \"long\" \u00a0}, { \u00a0 \u00a0\"name\" : \"next\", \u00a0 \u00a0\"type\" : [ \"null\", \"LongList\" ] \u00a0} ], \u00a0\"aliases\" : [ \"LinkedLongs\" ] }", "format": "html", "updated_at": "2022-12-01T03:20:02.343Z"}, "author": {"id": 790278, "email": "saikrishna.pujari@databricks.com", "name": "saikrishna.pujari ", "first_name": "saikrishna.pujari", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T20:08:51.444Z", "updated_at": "2023-02-20T11:00:57.025Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2951025, "name": "avro"}, {"id": 2788879, "name": "aws"}, {"id": 2788882, "name": "azure"}, {"id": 2956012, "name": "data source"}, {"id": 2788880, "name": "gcp"}, {"id": 2956011, "name": "spark"}], "url": "https://kb.databricks.com/data-sources/recursive-references-in-avro-schema-are-not-allowed"}, {"id": 1427796, "name": "Recover deleted notebooks from the Trash", "views": 6494, "accessibility": 1, "description": "Deleted items can be recovered from the Trash for 30 days after deletion.", "codename": "recover-deleted-notebooks-from-the-trash", "created_at": "2022-07-06T07:54:14.780Z", "updated_at": "2022-09-02T13:46:06.478Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"helpjuice-internal-block\" data-permitted-groups=\"\" data-permitted-users=\"\" data-controller=\"editor--internal-block\" data-internal-block-id=\"f31b714e372f8\">\n  <div class=\"helpjuice-internal-block-settings notranslate\" data-action=\"click-&gt;editor--internal-block#toggleSettings\"></div>\n  <div class=\"helpjuice-internal-block-body\">\n    \n<p>DBR Version: This applies to the Workspace UI, so it is separate from DBR versions.</p>\n<p id=\"isPasted\">Category: Notebooks</p>\n<p>Secondary category: &lt;list secondary category, if applicable&gt;</p>\n<p>Cloud Version: AWS, Azure, GCP</p>\n<p>Author: <a aria-expanded=\"false\" aria-haspopup=\"menu\" href=\"mailto:vivian.wilfred@databricks.com\" id=\"isPasted\" rel=\"noopener noreferrer\" style=\"box-sizing: inherit; color: rgba(var(--sk_highlight_hover,11,76,140),1); text-decoration: none; --saf-0:rgba(var(--sk_highlight,18,100,163),1); border-radius: 4px; box-shadow: 0 0 0 1px var(--saf-0),0 0 0 5px #1d9bd14d; outline: none; font-family: Slack-Lato, Slack-Fractions, appleLogo, sans-serif; font-size: 15px; font-style: normal; font-variant-ligatures: common-ligatures; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);\" target=\"_blank\">vivian.wilfred@databricks.com</a></p>\n<p>Owning Team: &lt;India + Platform&gt;</p>\n<p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p>\n<p>Last reviewed date: &lt;9th Aug 2022&gt; by &lt;Rakesh&gt;</p>\n<p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, <strong>Ready for Final Review</strong>, Published&gt;</p>\n  </div>\n  <div class=\"helpjuice-internal-block-delete\"></div>\n</div>\n<h1 data-toc=\"true\" id=\"introduction-0\">Introduction</h1><p>Deleted notebooks are moved to the user's Trash folder and stored there for 30 days. After 30 days have passed, the deleted notebooks are permanently removed and cannot be recovered. You can permanently delete the items in the Trash sooner by selecting Empty Trash.</p><p>If you accidentally delete a notebook it is not permanently deleted. It is just moved to the Trash. As long as you have not manually emptied the Trash, you can restore the notebook by moving it from the Trash folder to another folder.</p><div class=\"helpjuice-callout warning\">\n    <div class=\"helpjuice-callout-body\">\n      <h3>Warning</h3>\n      \n\n<p class=\"hj-alert-text\">Objects that are deleted via APIs or the Databricks CLI do not move to the <strong>Trash</strong>. They are permanently deleted. The <strong>Trash</strong> folder is a UI-only feature.</p>\n\n    </div>\n    <div class=\"helpjuice-callout-delete\"></div>\n</div>\n<h1 data-toc=\"true\" id=\"recover-deleted-notebooks-2\">Recover deleted notebooks</h1><ol>\n<li id=\"isPasted\">Log into Databricks.</li>\n<li>Select <strong>Workspace</strong> from the sidebar.</li>\n<li>Select <strong>Users</strong>.</li>\n<li>Select your user folder.</li>\n<li>Select <strong>Trash</strong>.</li>\n<li>Select the notebook you want to restore and drag it from the <strong>Trash</strong> folder to your user folder.</li>\n</ol><div class=\"helpjuice-callout info\">\n    <div class=\"helpjuice-callout-body\">\n      <h3>Info</h3>\n      \n\n<p class=\"hj-alert-text\">A normal user can only recover items from their own <strong>Trash</strong>. Admin users can recover deleted items from any user's <strong>Trash</strong> folder.</p>\n\n    </div>\n    <div class=\"helpjuice-callout-delete\"></div>\n</div>\n<h1 data-toc=\"true\" id=\"prevention-4\">Prevention</h1><p>You can mitigate accidental deletions by backing up your notebooks frequently.</p><p>Options to backup notebooks include:</p><ol>\n<li>Git integration with Databricks Repos (<a href=\"https://docs.databricks.com/repos/\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Git integration with Databricks Repos\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/repos/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Git integration with Databricks Repos\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/repos/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Git integration with Databricks Repos\">GCP</a>)</li>\n<li>Azure DevOps Services version control (<a href=\"https://docs.microsoft.com/azure/databricks/notebooks/azure-devops-services-version-control\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Azure DevOps Services version control\">Azure</a>)</li>\n<li>Bitbucket Cloud and Bitbucket Server version control (<a href=\"https://docs.microsoft.com/en-us/azure/databricks/notebooks/bitbucket-cloud-version-control\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Bitbucket Cloud and Bitbucket Server version control\">Azure</a>)</li>\n</ol><p><br></p>", "body_txt": "DBR Version: This applies to the Workspace UI, so it is separate from DBR versions.\nCategory: Notebooks\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: AWS, Azure, GCP\nAuthor: vivian.wilfred@databricks.com Owning Team: &lt;India + Platform&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;9th Aug 2022&gt; by &lt;Rakesh&gt;\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt; Introduction Deleted notebooks are moved to the user's Trash folder and stored there for 30 days. After 30 days have passed, the deleted notebooks are permanently removed and cannot be recovered. You can permanently delete the items in the Trash sooner by selecting Empty Trash. If you accidentally delete a notebook it is not permanently deleted. It is just moved to the Trash. As long as you have not manually emptied the Trash, you can restore the notebook by moving it from the Trash folder to another folder. Warning Objects that are deleted via APIs or the Databricks CLI do not move to the Trash. They are permanently deleted. The Trash folder is a UI-only feature. Recover deleted notebooks Log into Databricks.\nSelect Workspace from the sidebar.\nSelect Users.\nSelect your user folder.\nSelect Trash.\nSelect the notebook you want to restore and drag it from the Trash folder to your user folder. Info A normal user can only recover items from their own Trash. Admin users can recover deleted items from any user's Trash folder. Prevention You can mitigate accidental deletions by backing up your notebooks frequently. Options to backup notebooks include: Git integration with Databricks Repos (AWS | Azure | GCP)\nAzure DevOps Services version control (Azure)\nBitbucket Cloud and Bitbucket Server version control (Azure)", "format": "html", "updated_at": "2022-09-02T13:46:06.471Z"}, "author": {"id": 790745, "email": "vivian.wilfred@databricks.com", "name": "vivian.wilfred ", "first_name": "vivian.wilfred", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T10:42:50.853Z", "updated_at": "2023-03-28T11:16:09.635Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256864, "name": "Notebooks", "codename": "notebooks", "accessibility": 1, "description": "These articles can help you with your Databricks notebooks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2885075, "name": "aws"}, {"id": 2885076, "name": "azure"}, {"id": 2885078, "name": "deletion"}, {"id": 2885077, "name": "gcp"}], "url": "https://kb.databricks.com/notebooks/recover-deleted-notebooks-from-the-trash"}, {"id": 1424650, "name": "Recreate LISTAGG functionality with Spark SQL", "views": 2200, "accessibility": 1, "description": "Use collect_list and concat_ws in Spark SQL to achieve the same functionality as LISTAGG on other platforms.", "codename": "recreate-listagg-functionality-with-spark-sql", "created_at": "2022-07-01T11:08:29.686Z", "updated_at": "2023-02-24T01:19:56.405Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.a944bc4c26d36aaa1401ac39d3e595bd\"></div><p dir=\"ltr\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">LISTAGG</span> is a function that aggregates a set of string elements into one string by concatenating the strings. An optional separator string can be provided which is inserted between contiguous input strings.</p><pre dir=\"ltr\">LISTAGG(&lt;expression&gt;, &lt;separator&gt;) WITHIN GROUP(ORDER BY \u2026)</pre><p dir=\"ltr\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">LISTAGG</span> is supported in many databases and data warehouses. However, it is not natively supported in Apache Spark SQL or Databricks.</p><p dir=\"ltr\">You can obtain similar functionality in Databricks by using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">collect_list</span> (<a href=\"https://docs.databricks.com/sql/language-manual/functions/collect_list.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"collect_list\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/functions/collect_list\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"collect_list\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/sql/language-manual/functions/collect_list.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"collect_list\">GCP</a>) with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">concat_ws</span> (<a href=\"https://docs.databricks.com/sql/language-manual/functions/concat_ws.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"concat_ws\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/sql/language-manual/functions/concat_ws\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"concat_ws\">Azure</a> |\u00a0<a href=\"https://docs.gcp.databricks.com/sql/language-manual/functions/concat_ws.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"concat_ws\">GCP</a>).<br><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"773bcb72ff124\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.9905bdffd264c20f356561a8bb780582\"></div><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><p>Before using the commands, we first need to create sample data to process.</p><p>This sample code creates a table (named <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">table1</span>) that contains five lines of sample employee data.</p><pre data-aura-rendered-by=\"49:2016;a\">%python\r\n\r\ndata = [[\"James\",\"A\",\"Smith\",\"2018\",\"M\",3000],\r\n    [\"Michael\",\"Rose\",\"Jones\",\"2010\",\"M\",4000],\r\n    [\"Robert\",\"K\",\"Williams\",\"2010\",\"M\",5000],\r\n    [\"Maria\",\"Anne\",\"Jones\",\"2005\",\"F\",4000],\r\n    [\"Jen\",\"Mary\",\"Brown\",\"2010\",\"F\",6000]\r\n    ]\r\n\r\ndf = spark.createDataFrame(data, [\"fname\",\"mname\",\"lname\",\"dob_year\",\"gender\",\"salary\"])\r\ndf.createOrReplaceTempView(\"table1\")</pre><p><br>The sample table looks like this when viewed:</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656674061609-1656674061609.png\" width=\"624\" height=\"225\" class=\"fr-fic fr-dii\" alt=\"Sample table results.\"></p><p style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\">If you wanted to use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">LISTAGG</span> to display a list of salaries by gender, you would use a query like this:</p><p style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><br></p><pre id=\"isPasted\">%sql\r\n\r\nSELECT gender, LISTAGG(salary, ',') WITHIN GROUP(ORDER BY salary)\r\nFROM table1\r\nGROUP BY gender</pre><p style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><br>The resulting table has two rows, with salary values separated by gender.</p><p style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1676591102125-1656674142172-1656674142172.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Salaries displayed by gender using LISTAGG.\"></p><p><br>To replicate this functionality in Databricks, you need to use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">collect_list</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">concat_ws</span>.</p><ul>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">collect_list</span> creates a list of objects for the aggregated column. In this example, it gets the list of salary values for the aggregated gender column.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">concat_ws</span> converts a list of salary objects to a single string value containing comma separated salaries.</li>\n</ul><p><br>This Spark SQL query returns the same result that you would get with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">LISTAGG</span> on a different database.</p><pre data-aura-rendered-by=\"49:2016;a\" id=\"isPasted\">%sql\r\n\r\nSELECT gender,CONCAT_WS(',', COLLECT_LIST(salary)) as concatenated_salary\r\nFROM  table1\r\nGROUP BY gender;</pre><p id=\"isPasted\"><br>The resulting table has two rows, with salary values separated by gender.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1676591135106-1656674142172-1656674142172.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Salaries displayed by gender using CONCAT_WS and COLLECT_LIST.\"></p><p><br>If you wanted to use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">LISTAGG</span> to display the salary results in a descending order, you might write a query like this:</p><pre data-aura-rendered-by=\"49:2016;a\">%sql\r\n\r\nSELECT gender, LISTAGG(salary, ',') WITHIN GROUP(ORDER BY salary DESC)\r\nFROM table1\r\nGROUP BY gender</pre><p><br>To do the same in Databricks, you would add <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">sort_array</span> to the previous Spark SQL example. <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">collect_list</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">concat_ws</span> do the job of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">LISTAGG</span>, while <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">sort_array</span> is used to output the salary results in a descending order.</p><pre data-aura-rendered-by=\"49:2016;a\">%sql\r\n\r\nSELECT gender,CONCAT_WS(',', SORT_ARRAY(COLLECT_LIST(salary), false)) as concatenated_salary\r\nFROM  table1\r\nGROUP BY gender;</pre><p><br></p><p>Both sets of sample code return the same output, with salary values separated by gender and displayed in descending order.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656674526999-1656674526999.png\" width=\"624\" height=\"187\" class=\"fr-fic fr-dii\"></p><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"3a5fe4643e213\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlOb1VKbDZmd0F3eVdTQkJhNHp6NGU3R09zQXNXdUduaTdqeG5JWFN6bzNmWFN2Z3l2ClZXcjA2akZBRzdhT21mUjZlVEVnQVhMMUtPUFNxc21xaHVoTHJSVWd2bThjZ0IxMUlIUHlMamk5ZlkvRwo4bnBsZndBSGdxc1BuRkF6STJOS3hlTVQ5V044MTRPbjRnVzk1NmhuOUV2a2IyWWRXd0ppcWFnSEl2MmQKcWFRWG45Wko5MjA1ek45TEhsc0thTGtnVG9oVHU2NnNPY0lScTRWRmZoUU9hK1UydThmMkVaWHJRRWorCmVxUXJFS09FRlhNaHFiSTJlcGk2aEE5MTFmWElPUWwyUFM1clBtMVEyWE0zeGtrR3pxMkFQc2xhMUFrUApSSkh4citUUGtQVlJUQ0txWU53OENZWkdvam9yejMyOENUUHo0S0YySzM4NGpuM2pvenRSMll1Y1B5QTYKbE5GSGxGOXNodVFWZ3pBckRiVHhMSGxLSWNOUUZFQlZFTGtNUlNXdDBKRVlWNVBTN0lWNll1ZGxHVFVjCmpnV0VLU1l4QUJUQS93SjVJYVJVZHpSYkRPUUxaOWlnS2kzWUc5SlBtUGxWbmVCa0VMZS9EZWdOU0RxaApZUC95WlBTaVhoTEl5MkRXbnFYMFFNUjBqWHg0UFNibWxjU1VKUFJiRnFWZkU2dnAvWDhWdmQ4S0JBOU8KaTNDWDlmdE9xcnFLZTJtclJZT1R3V28xY3lFbEpBN00vbEM4dlFYNjdSZlR3dzdINzJNTDVTQzlvUjdGCkdzbGtOWitBdkJvOVlTbWpXNktYSHpDOTNucFJVc3hTem5vbllzcDdrenozaHlqY1g0WXNyeFluN0ZZSQp5SzVhK2NGQWZvLzFveFNZNFlGSTR5WmdzUTJEaFp2cExqRUpJVG5DbksyQzUxeWk4SlpwUEZPeVJhYzcKNm03MVJzMlRvRnFRQWY5OUtpeUpLU0RmRDJuS3FicGQ3cEE3alk4V3JScEl2VmhHWVZzcW4xeEo0cVN3CnZiQzM4SkVvRFdvem04OXoxSGhLVlZGVVdCY1RvNDYrZ08renI1UDcwVElMenJtUTVIMWxySVVKQTgzKwpNR215aFpYd3VaK2p2YnM2c0Y0MU5sM0VPRzlPaG1HZWQ2MGllL1gzZERhN3RNalVJV21wRWg2K2lPeFoKMGwrYStzV0FwcEZ3WW5RQ3I2a0JZRkx6U3JxbE15T0kzY3pRVGJjK2dyOUxGVHhQNU03YXA3YzZ6bXQ3CmtUaUwzQ3o0aFFuUlhFem5ydGx1TlZEQ3BlTGNtdDh3eUpWbEw5M0MwbzBqaGdnTit2ZVBNMGw2RElINgpzak1NCg==.1681e9f2708770ebb2fb96a0ade510e8\"></div><p><br></p>", "body_txt": "LISTAGG is a function that aggregates a set of string elements into one string by concatenating the strings. An optional separator string can be provided which is inserted between contiguous input strings. LISTAGG(&lt;expression&gt;, &lt;separator&gt;) WITHIN GROUP(ORDER BY \u2026) LISTAGG is supported in many databases and data warehouses. However, it is not natively supported in Apache Spark SQL or Databricks. You can obtain similar functionality in Databricks by using collect_list (AWS | Azure | GCP) with concat_ws (AWS | Azure |\u00a0GCP). Instructions Before using the commands, we first need to create sample data to process. This sample code creates a table (named table1) that contains five lines of sample employee data. %python data = [[\"James\",\"A\",\"Smith\",\"2018\",\"M\",3000], [\"Michael\",\"Rose\",\"Jones\",\"2010\",\"M\",4000], [\"Robert\",\"K\",\"Williams\",\"2010\",\"M\",5000], [\"Maria\",\"Anne\",\"Jones\",\"2005\",\"F\",4000], [\"Jen\",\"Mary\",\"Brown\",\"2010\",\"F\",6000] ] df = spark.createDataFrame(data, [\"fname\",\"mname\",\"lname\",\"dob_year\",\"gender\",\"salary\"]) df.createOrReplaceTempView(\"table1\") The sample table looks like this when viewed: If you wanted to use LISTAGG to display a list of salaries by gender, you would use a query like this: %sql SELECT gender, LISTAGG(salary, ',') WITHIN GROUP(ORDER BY salary) FROM table1 GROUP BY gender The resulting table has two rows, with salary values separated by gender. To replicate this functionality in Databricks, you need to use collect_list and concat_ws. collect_list creates a list of objects for the aggregated column. In this example, it gets the list of salary values for the aggregated gender column. concat_ws converts a list of salary objects to a single string value containing comma separated salaries. This Spark SQL query returns the same result that you would get with LISTAGG on a different database. %sql SELECT gender,CONCAT_WS(',', COLLECT_LIST(salary)) as concatenated_salary FROM table1 GROUP BY gender; The resulting table has two rows, with salary values separated by gender. If you wanted to use LISTAGG to display the salary results in a descending order, you might write a query like this: %sql SELECT gender, LISTAGG(salary, ',') WITHIN GROUP(ORDER BY salary DESC) FROM table1 GROUP BY gender To do the same in Databricks, you would add sort_array to the previous Spark SQL example. collect_list and concat_ws do the job of LISTAGG, while sort_array is used to output the salary results in a descending order. %sql SELECT gender,CONCAT_WS(',', SORT_ARRAY(COLLECT_LIST(salary), false)) as concatenated_salary FROM table1 GROUP BY gender; Both sets of sample code return the same output, with salary values separated by gender and displayed in descending order.", "format": "html", "updated_at": "2023-02-24T01:19:56.399Z"}, "author": {"id": 831506, "email": "manjunath.swamy@databricks.com", "name": "manjunath.swamy ", "first_name": "manjunath.swamy", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T05:55:43.385Z", "updated_at": "2023-03-24T10:04:17.174Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2994052, "name": "aws"}, {"id": 2994053, "name": "azure"}, {"id": 2994060, "name": "db2"}, {"id": 2994054, "name": "gcp"}, {"id": 2994059, "name": "ibm db2"}, {"id": 2994055, "name": "listagg"}, {"id": 2994058, "name": "oracle"}, {"id": 2994057, "name": "replacement"}, {"id": 2994056, "name": "snowflake"}], "url": "https://kb.databricks.com/sql/recreate-listagg-functionality-with-spark-sql"}, {"id": 1422287, "name": "MLflow 'invalid access token' error", "views": 5439, "accessibility": 1, "description": "Long running ML tasks require an access token with an extended lifetime to ensure the tasks complete before the token expires.", "codename": "mlflow-invalid-access-token-error", "created_at": "2022-06-28T21:10:04.515Z", "updated_at": "2022-07-22T11:46:45.420Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.172298cd78660970049366b01bf4df44\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You have long-running MLflow tasks in your notebook or job and the tasks are not completed. Instead, they return a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">(403) Invalid access token</span> error message.</p><pre id=\"isPasted\"><span id=\"isPasted\" style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">Error stack trace: MlflowException: API request to endpoint /api/2.0/mlflow/runs/create failed with error code\u00a0</span><span style=\"font-family: Arial, Helvetica, sans-serif;\">\r\n</span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">403 != 200. Response body: '&lt;html&gt;</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;head&gt;</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;meta data-fr-http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/&gt;</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;title&gt;Error 403 Invalid access token.&lt;/title&gt;</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;/head&gt;</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;body&gt;&lt;h2&gt;HTTP ERROR 403&lt;/h2&gt;</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;p&gt;Problem accessing /api/2.0/mlflow/runs/create. Reason:</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;pre&gt; Invalid access token.&lt;/pre&gt;&lt;/p&gt;</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;/body&gt;</span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&lt;/html&gt;</span></pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The Databricks access token that the MLflow Python client uses to communicate with the tracking server expires after several hours. If your ML tasks run for an extended period of time, the access token may expire before the task completes. This results in MLflow calls failing with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">(403) Invalid access token</span> error message in both notebooks and jobs.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>You can work around this issue by manually creating an access token with an extended lifetime and then configuring that access token in your notebook prior to running MLflow tasks.</p><ol>\n<li>Generate a personal access token (<a href=\"https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Generate a personal access token\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/authentication#--generate-a-personal-access-token\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Generate a personal access token\">Azure</a> <a href=\"https://docs.gcp.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token\" title=\"Generate a personal access token\" target=\"_blank\" rel=\"noopener noreferrer\"></a>) and configure it with an extended lifetime.</li>\n<li id=\"isPasted\">Set up the Databricks CLI (<a href=\"https://docs.databricks.com/dev-tools/cli/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks CLI\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/dev-tools/cli/\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks CLI\">Azure</a>).\u00a0<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656644354393-Screen%20Shot%202022-06-27%20at%203.57.36%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\">\n</li>\n<li>Use the Databricks CLI to create a new secret with the personal access token you just created.<pre>databricks secrets put --scope {&lt;secret-name&gt;} --key mlflow-access-token --string-value {&lt;personal-access-token&gt;}</pre>\n</li>\n<li>Insert this sample code at the beginning of your notebook. Include your secret name and your Workspace URL (<a href=\"https://docs.databricks.com/workspace/workspace-details.html#workspace-url\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Workspace URL\">AWS</a> | <a href=\"https://docs.microsoft.com/en-us/azure/databricks/workspace/workspace-details#workspace-url\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Workspace URL\">Azure</a>).<pre>%python\r\n\r\naccess_token = dbutils.secrets.get(scope=\"{&lt;secret-name&gt;}\", key=\"mlflow-access-token\")\r\n\r\nimport os\r\nos.environ[\"DATABRICKS_TOKEN\"] = access_token\r\nos.environ[\"DATABRICKS_HOST\"] = \"https://&lt;workspace-url&gt;\"\r\n\r\nfrom databricks_cli.configure import provider\r\nconfig_provider = provider.EnvironmentVariableConfigProvider()\r\nprovider.set_config_provider(config_provider)</pre>\n</li>\n<li>Run your notebook or job as normal.</li>\n</ol>", "body_txt": "Problem You have long-running MLflow tasks in your notebook or job and the tasks are not completed. Instead, they return a (403) Invalid access token error message. Error stack trace: MlflowException: API request to endpoint /api/2.0/mlflow/runs/create failed with error code\u00a0 403 != 200. Response body: '&lt;html&gt; &lt;head&gt; &lt;meta data-fr-http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/&gt; &lt;title&gt;Error 403 Invalid access token.&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;h2&gt;HTTP ERROR 403&lt;/h2&gt; &lt;p&gt;Problem accessing /api/2.0/mlflow/runs/create. Reason: &lt;pre&gt; Invalid access token.&lt;/pre&gt;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Cause The Databricks access token that the MLflow Python client uses to communicate with the tracking server expires after several hours. If your ML tasks run for an extended period of time, the access token may expire before the task completes. This results in MLflow calls failing with a (403) Invalid access token error message in both notebooks and jobs. Solution You can work around this issue by manually creating an access token with an extended lifetime and then configuring that access token in your notebook prior to running MLflow tasks. Generate a personal access token (AWS | Azure ) and configure it with an extended lifetime.\nSet up the Databricks CLI (AWS | Azure).\u00a0 Use the Databricks CLI to create a new secret with the personal access token you just created.databricks secrets put --scope {&lt;secret-name&gt;} --key mlflow-access-token --string-value {&lt;personal-access-token&gt;} Insert this sample code at the beginning of your notebook. Include your secret name and your Workspace URL (AWS | Azure).%python access_token = dbutils.secrets.get(scope=\"{&lt;secret-name&gt;}\", key=\"mlflow-access-token\") import os os.environ[\"DATABRICKS_TOKEN\"] = access_token os.environ[\"DATABRICKS_HOST\"] = \"https://&lt;workspace-url&gt;\" from databricks_cli.configure import provider config_provider = provider.EnvironmentVariableConfigProvider() provider.set_config_provider(config_provider) Run your notebook or job as normal.", "format": "html", "updated_at": "2022-07-22T11:45:21.342Z"}, "author": {"id": 790360, "email": "shanmugavel.chandrakasu@databricks.com", "name": "shanmugavel.chandrakasu ", "first_name": "shanmugavel.chandrakasu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T21:14:15.541Z", "updated_at": "2023-04-20T21:51:08.608Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256853, "name": "Machine learning", "codename": "machine-learning", "accessibility": 1, "description": "These articles can help you with your machine learning, deep learning, and other data science workflows in Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2773512, "name": "access token"}, {"id": 2773508, "name": "aws"}, {"id": 2773509, "name": "azure"}, {"id": 2773511, "name": "mlflow"}, {"id": 2773510, "name": "pat"}], "url": "https://kb.databricks.com/machine-learning/mlflow-invalid-access-token-error"}, {"id": 1420902, "name": "Deployment name prefix not defined in Terraform", "views": 5482, "accessibility": 1, "description": "You must have a deployment_name defined in Databricks, before using it to deploy workspaces with Terraform.", "codename": "deployment-name-prefix-not-defined-in-terraform", "created_at": "2022-06-28T10:16:30.204Z", "updated_at": "2022-08-16T19:39:58.947Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"U2FsdGVkX1+rmw6njekq7R/a6qy8YCONTtdur/8Ukvsbf4vGPaY0ExVWUaJW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.369bfe081d17a29c1c69fce95f7c36ae\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are using Terraform to deploy a workspace in AWS and you get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Deployment name cannot be used until a deployment name prefix is defined.</span> error message.</p><pre>\u2502 Error: MALFORMED_REQUEST: Failed parameter validation checks: Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative\r\n\u2502\r\n\u2502 \u00a0 with databricks_mws_workspaces.this,\r\n\u2502 \u00a0 on workspace.tf line 3, in resource \"databricks_mws_workspaces\" \"this\":\r\n\u2502 \u00a0 \u00a03: resource \"databricks_mws_workspaces\" \"this\" {\r\n\u2502</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">deployment_name</span> prefix is defined in Terraform, but no deployment prefix is configured in your Databricks account.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>To work around the issue, disable the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">deployment_name</span> prefix variable in your Terraform configuration.</p><p>In this example, the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">deployment_name</span> has been commented out.</p><pre id=\"isPasted\">// workspace.tf\r\n\r\nresource \"databricks_mws_workspaces\" \"this\" {\r\n\u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0= databricks.mws\r\n\u00a0 account_id \u00a0 \u00a0 \u00a0= var.databricks_account_id\r\n\u00a0 aws_region \u00a0 \u00a0 \u00a0= var.region\r\n\u00a0 workspace_name \u00a0= local.prefix\r\n\u00a0 # deployment_name = local.prefix\r\n\r\n\r\n\u00a0 credentials_id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = databricks_mws_credentials.this.credentials_id\r\n\u00a0 storage_configuration_id = databricks_mws_storage_configurations.this.storage_configuration_id\r\n\u00a0 network_id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = databricks_mws_networks.this.network_id\r\n}</pre><p>For a permanent solution, ensure that a deployment prefix is configured in your Databricks account.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">To set a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">deployment_name</span> prefix in your Databricks account, you must contact your Databricks representative. Once a new <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">deployment_name</span> prefix is added/updated, it only applies to new workspaces.</p>\n</div>\n</div><p>Review the <a href=\"https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/mws_workspaces\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Terraform\u00a0</a><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\"><a href=\"https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/mws_workspaces\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">databricks_mws_workspaces</a></span><a href=\"https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/mws_workspaces\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">\u00a0documentation</a> for more information.</p><p>You can also review the <a href=\"https://docs.databricks.com/dev-tools/terraform/index.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Databricks Terraform documentation</a>.</p>", "body_txt": "Problem You are using Terraform to deploy a workspace in AWS and you get a Deployment name cannot be used until a deployment name prefix is defined. error message. \u2502 Error: MALFORMED_REQUEST: Failed parameter validation checks: Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative \u2502 \u2502 \u00a0 with databricks_mws_workspaces.this, \u2502 \u00a0 on workspace.tf line 3, in resource \"databricks_mws_workspaces\" \"this\": \u2502 \u00a0 \u00a03: resource \"databricks_mws_workspaces\" \"this\" { \u2502 Cause The deployment_name prefix is defined in Terraform, but no deployment prefix is configured in your Databricks account. Solution To work around the issue, disable the deployment_name prefix variable in your Terraform configuration. In this example, the deployment_name has been commented out. // workspace.tf resource \"databricks_mws_workspaces\" \"this\" { \u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0= databricks.mws \u00a0 account_id \u00a0 \u00a0 \u00a0= var.databricks_account_id \u00a0 aws_region \u00a0 \u00a0 \u00a0= var.region \u00a0 workspace_name \u00a0= local.prefix \u00a0 # deployment_name = local.prefix \u00a0 credentials_id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = databricks_mws_credentials.this.credentials_id \u00a0 storage_configuration_id = databricks_mws_storage_configurations.this.storage_configuration_id \u00a0 network_id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = databricks_mws_networks.this.network_id } For a permanent solution, ensure that a deployment prefix is configured in your Databricks account. Info\nTo set a deployment_name prefix in your Databricks account, you must contact your Databricks representative. Once a new deployment_name prefix is added/updated, it only applies to new workspaces. Review the Terraform\u00a0 databricks_mws_workspaces \u00a0documentation for more information. You can also review the Databricks Terraform documentation.", "format": "html", "updated_at": "2022-08-16T19:39:58.872Z"}, "author": {"id": 888181, "email": "cedric.law@databricks.com", "name": "Cedric Law", "first_name": "Cedric", "last_name": "Law", "role_id": "draft_writer", "created_at": "2022-06-22T10:28:09.373Z", "updated_at": "2023-04-28T11:11:08.692Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 313835, "name": "Terraform", "codename": "terraform", "accessibility": 1, "description": "These articles can help you with Terraform.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2817218, "name": "aws"}, {"id": 2817219, "name": "terraform"}], "url": "https://kb.databricks.com/terraform/deployment-name-prefix-not-defined-in-terraform"}, {"id": 1420764, "name": "Upload large files using DBFS API 2.0 and PowerShell", "views": 9509, "accessibility": 1, "description": "Use PowerShell and the DBFS API to upload large files to your Databricks workspace.", "codename": "upload-large-files-using-dbfs-api-20-and-powershell", "created_at": "2022-06-28T06:29:12.811Z", "updated_at": "2022-09-27T07:17:09.250Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlYUXVhL2JhTG93eDdrcWZ0Skd6UURyRjFuejU1QXBGajB3T3Vtc09DYmFIbzRUY3RLCkFSdnBYMXVvUm1ONnB4R3l6ZlBmUXB6QWdzRldTRGVBcm80d3NDTG1jYkIrVlJNMjZiSkluSE5RaFlWdQpzcGpremlPUHFBbjR6ZEhORS9RU1h0aTdFRTlONmRpTWprS3hPQ05RQ1FQZmpad0hDTXlLVkQzQk4zR0kKUnlSSXFsdnl6cHZNYXVGTWJ0eFIzS09CKzhObjVHemJYZUJkUDdzOVJUclI2QnA5WGQvaTJFakVJQjlrCnBXYTlzTVppL3p3NWtoMHZXcWpJK0FUcFpwREFLOHdsN2hCTUFhWkhnbGpRNU8rQVovMDBCd3g4d2Y5QQpZdEY4OVl4VXNqdXNwK2pacldqaDIyY3Q1K0xzY0pxdXJ6T21tQXJjNnMyOGNka2RxZUlLdGczdGdCZUcKN1Y2MUNjUG5hSGd2Y2YzZ291U0FyVUtCSTJFNmNhSGdpVGVVNTRNSlpOcEc5YkoxcWx3ZWFCNm1sQ1RzClhBdjhlVEc1OW5Cc1RiUGNRdUJaZEZyZmx2ekl6eDNzVmZvRE1tRC9VNXZhK1krR21rTGpXWDZoRDFHRwpwYThYWjlBdUdXUDNrVUVEeFhuSkYrMXVaeXgxWXZ3Q29YVk1IMFJWZWs4U2VrNFRxRzdnWTVJM0xXMEQKSWtQOGVjdGl2Q2wrczZ6VE9sV1Fwc0lGUjJEZXZqcmVJT3JQSXUwOFZEV2VYaWZrQWtXSncvYzYrTlErCjI2K0JrMXZRa0lsTlpjMi9RMUVRTVlkcjcyMmthUUtGTGY0SGpJZndiWHdRSFJrM2pPZjQ1VlJxV3JpRwpzTXNNYktOSDI0cjhSd0pMajNxYU5jdjBQU25mbDY0THluRWtxaWRyT3JLVC95VkFvZ3NqM3B2VGJZQ1IKZFZOQnFmQ0UxNjBVSGxqUXArVzJobXlCcFg5RmdLYVpEOTBnNGJ2V1hyR1BwSW9YZjd6cVFxbitUWXVSCnhuUmdwbG5ZODFPQ25hOXpLUy9ya3FsNGlobGRndGNZT25TSnNpcnkrZXptOU8rZFJ3b3labjIyTUI0bgp5RWg4YXhzSnRuZEViL0MzSGZvQm54UFZnR2NocHBpdTBUR2VseXJPN2ZsbFBwSnVHcTdMZ081MUdCTnEKOWJrN25nb1Y0aTNIUlBzZW1abUlucERLOWUrbHNLaXdYdVB5Q0hFbjYyQmpQWHpIREJNRWdiaG83QmIvCnV0MW16YzRFNWJlY2tlU0lKQlVIUWVoQVBjanpyanNaWWkyOEhoYkwySUNyaStpTmtpblErL1ovZmEzSQpJQXo5UXhYSWFQZ2h6WDVOMDdlUmtEdVl4MEUySStaRUZwVUhDNG12ZFQ0WjNhcVZjajR2WUJZUnlyRUsKRG4wUUpJVlo1dlBLT3NiMEJTN1FNK0JqNG8zaGdaWG82WXNXSWo1TXBxQTBiWkZ0TmpFdjdtU1hzMEVYCmlORmM2bkl0dWxVOHlJeHM4b2c5UGIwUGJGVE03Q0RhT3V6a3JUKzJwR0dzUnJ5YWhSb0thaldwSld0MgpTTFlhMm53d3Y0eGpOTHBpVUdxQnI5bGdsSmRRTG9qSXI2a1VpVVlSZUk5RWgvc2djQ0Y3S21FUTNqQWEKaUhqWnZCQTk1V0wzeEdMcElJY09TK3AwdXVpcC96RXI3OXdmZVpub1RKSHRFUEtUVEdCN3FQSW1hbnJICjJJWXR2NXFjVjg2by9jWDZhMlhjTW5Hb2pPbFlycHNXWGNrRm5TRENJMVVrY01HTXg5c0dsLzh5LytENwoyaWN0dEY0ZldFVFJKUWFWYmVhYlUrc0JIY0JVQm5lUVV3aE0vb3VqNmkxdjcvREh6c3BSa3g0eHBqaHoKSTVDZm5uTFVoZTZBdmJKVkFwNFl0Ny9FbXYvQnZLSWQ3Q1YxNlpLQTlUWlo2VFZWbVNhcVNCUzh4T1VICittenBVMHZXSVRQQ2pTVlY2VXo3bzRpbUhraGRtbzZpU0JmOENtKzArUjZ2Vmpjc3g4TER2a1NDQ252QwpXNW5zM0VOZU0vYkxDeVRySDMxd2tZVWpxSzZXd2JOMkoyS3d1QTRkcXNqSGptT0JYU09mQXhYRHk1Z3YKdXRVckRFYkE4REttUU1ZOEhYa3FzYWpLeE9KQ0E1VkpjRzZhc1FGOUVhbUllYWpIclFYM3VpV09zV3RLCmMweDF0a242bjFXY1AvL0JVTENJUUdmZWlISmdqSFkxbjJvL05WZk5pcmJxWHBBVTlhMEpJb1JjVGJvSApNWSttOStJc0d5WUdMY3F0bjhiZVQvM2t0c0ZXVGtJM2NUeFpyZWp6ckQwZXZhRU5uek05ZEltNzNCckkKbmEyUjdhcVNBc0JtK1NCekJXNldNcHhJSyt1TXNVaGZaWVpLWlNPSXhmM2phYndsdE9TcTNDUkNnY2RPCmtIYXFkWWNWblAwMDBHSzJ0TVhKV245dlRpMWM5YW1wRk02bkQ1em5MOThjalVGMWNiQXZtY2JjdnlUWgpDR0ZBR0JMcDVDZEowNG9aQk9hbEJtSzR6TU5qbHRodWk1S283THVjM1BucjRyZEhSRXZYVVRySmVibisKZjhYSlFwV2R5R3hzVWpqODYrSnRJc2tOSmgyNFlsRDcrUlowRXdTUkNLVlBSVnpST2pOMi9SS1V5dE1wCmRLKzJiTEJ6MDZLZnBzMU5PTGhkQkpDYWVTRGNja2pMa0tNY1ppL3g4aWFYTkZqNEtOMlNRTmZ6VHpxTQozSTliL0l2NUJxY3J2MjI5SkV4Nmd2Uk03UFQ4L0RVcEFNeVp1eFFpTWNaSEJBRmh6STFFMVJyS0VRZ28KK2FFQ2ppNFhrUGlnNzhINzlRT0lOQmlyRy82bWVlSStTZC9hSU1BSktLd0ErRVhOSjhwYjdNajA5RWlsClhESmt2My9GSFV0VkVxeVU3R2RLQ1BPNDlzQ1grNDN5eHpUM0JVNnJRY0hVREF4OTQ2dHk4MG01R1I5ZQpid2M9Cg==.2f090250fa79de5c2ef28a1aba2e230c\"></div><p>Using the Databricks REST API to interact with your clusters programmatically can be a great way to streamline workflows with scripts.</p><p>The API can be called with various tools, including PowerShell. In this article, we are going to take a look at an example DBFS <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">put</span> command using curl and then show you how to execute that same command using PowerShell.\u00a0</p><p>The DBFS API 2.0 <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">put</span> command (<a href=\"https://docs.databricks.com/dev-tools/api/latest/dbfs.html#put\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"DBFS API 2.0 put command\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/dbfs#--put\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"DBFS API 2.0 put command\">Azure</a>) limits the amount of data that can be passed using the contents parameter to 1 MB if the data is passed as a string. The same command can pass 2 GB if the data is passed as a file. It is mainly used for streaming uploads, but can also be used as a convenient single call for data upload.</p><h1 data-toc=\"true\" id=\"curl-example-0\">Curl example</h1><p>This example uses curl to send a simple multipart form post request to the API to upload a file up to 2 GB in size.</p><p>Replace all of the values in &lt;&gt; with appropriate values for your environment.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-1\">Info</h3>\n<p class=\"hj-alert-text\">To get your workspace URL, review <strong>Workspace instance names, URLs, and IDs</strong> (<a href=\"https://docs.databricks.com/workspace/workspace-details.html#workspace-url\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Workspace instance names, URLs, and IDs\">AWS</a> |\u00a0<a href=\"https://learn.microsoft.com/azure/databricks/workspace/workspace-details\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Workspace instance names, URLs, and IDs\">Azure</a>).</p>\n<p class=\"hj-alert-text\">Review the <strong>Generate a personal access token</strong> (<a href=\"https://docs.databricks.com/sql/user/security/personal-access-tokens.html#generate-a-personal-access-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Generate a personal access token\">AWS</a> | <a href=\"https://learn.microsoft.com/en-us/azure/databricks/sql/user/security/personal-access-tokens#--generate-a-personal-access-token\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Generate a personal access token\">Azure</a>) documentation for details on how to create a personal access token for use with the REST APIs.</p>\n</div>\n</div><pre id=\"isPasted\" style=\"box-sizing: border-box; overflow: visible; font-family: monospace, monospace; font-size: 13px; white-space: pre-wrap; overflow-wrap: break-word; display: block; padding: 9.5px; margin: 0px 0px 10px; line-height: 1.42857; word-break: break-all; color: rgb(51, 51, 51); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"># Parameters\r\ndatabricks_workspace_url=\"&lt;databricks-workspace-url&gt;\"\r\npersonal_access_token=\"&lt;personal-access-token&gt;\"\r\nlocal_file_path=\"&lt;local_file_path&gt;\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# ex: /Users/foo/Desktop/file_to_upload.png\r\ndbfs_file_path=\"&lt;dbfs_file_path&gt;\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# ex: /tmp/file_to_upload.png\r\noverwrite_file=\"&lt;true|false&gt;\"\r\n\r\n\r\ncurl --location --request POST https://${databricks_workspace_url}/api/2.0/dbfs/put \\\r\n\u00a0 \u00a0 \u00a0--header \"Authorization: Bearer ${personal_access_token}\" \\\r\n\u00a0 \u00a0 \u00a0--form contents=@${local_file_path} \\\r\n\u00a0 \u00a0 \u00a0--form path=${dbfs_file_path} \\\r\n\u00a0 \u00a0 \u00a0--form overwrite=${overwrite_file}</pre><h1 data-toc=\"true\" id=\"powershell-example-2\">PowerShell example</h1><p>This PowerShell example is longer than the curl example, but it sends the same multipart form post request to the API.</p><p>The below script can be used in any environment where <a href=\"https://docs.microsoft.com/powershell/scripting/install/installing-powershell\" rel=\"noopener noreferrer\" target=\"_blank\">PowerShell is supported</a>.</p><p>To run the PowerShell script you must:</p><ol>\n<li>Replace all of the values in <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;&gt;</span> with appropriate values for your environment. Review the DBFS API 2.0 <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">put</span> documentation for more information.</li>\n<li>Save the script as a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.ps1</span> file. For example, you could call it <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">upload_large_file_to_dbfs.ps1</span>.</li>\n<li>Execute the script in PowerShell by running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">./upload_large_file_to_dbfs.ps1</span> at the prompt.</li>\n</ol><pre id=\"isPasted\">################################################## Parameters\r\n$DBX_HOST = \"&lt;databricks-workspace-url&gt;\"\r\n$DBX_TOKEN = \"&lt;personal-access-token&gt;\"\r\n$FILE_TO_UPLOAD = \"&lt;local_file_path&gt;\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# ex: /Users/foo/Desktop/file_to_upload.png\u00a0\u00a0\r\n$DBFS_PATH = \"&lt;dbfs_file_path&gt;\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# ex: /tmp/file_to_upload.png\r\n$OVERWRITE_FILE = \"&lt;true|false&gt;\"\r\n##################################################\r\n\r\n\r\n# Configure authentication\r\n$headers = New-Object \"System.Collections.Generic.Dictionary[[String],[String]]\"\r\n$headers.Add(\"Authorization\", \"Bearer \" \u00a0+ $DBX_TOKEN)\r\n\r\n$multipartContent = [System.Net.Http.MultipartFormDataContent]::new()\r\n\r\n# Local file path\r\n$FileStream = [System.IO.FileStream]::new($FILE_TO_UPLOAD, [System.IO.FileMode]::Open)\r\n$fileHeader = [System.Net.Http.Headers.ContentDispositionHeaderValue]::new(\"form-data\")\r\n$fileHeader.Name = $(Split-Path $FILE_TO_UPLOAD -leaf)\r\n$fileHeader.FileName = $(Split-Path $FILE_TO_UPLOAD -leaf)\r\n$fileContent = [System.Net.Http.StreamContent]::new($FileStream)\r\n$fileContent.Headers.ContentDisposition = $fileHeader\r\n$fileContent.Headers.ContentType = [System.Net.Http.Headers.MediaTypeHeaderValue]::Parse(\"text/plain\")\r\n$multipartContent.Add($fileContent)\r\n\r\n\r\n# DBFS path\r\n$stringHeader = [System.Net.Http.Headers.ContentDispositionHeaderValue]::new(\"form-data\")\r\n$stringHeader.Name = \"path\"\r\n$stringContent = [System.Net.Http.StringContent]::new($DBFS_PATH)\r\n$stringContent.Headers.ContentDisposition = $stringHeader\r\n$multipartContent.Add($stringContent)\r\n\r\n\r\n# File overwrite config\r\n$stringHeader = [System.Net.Http.Headers.ContentDispositionHeaderValue]::new(\"form-data\")\r\n$stringHeader.Name = \"overwrite\"\r\n$stringContent = [System.Net.Http.StringContent]::new($OVERWRITE_FILE)\r\n$stringContent.Headers.ContentDisposition = $stringHeader\r\n$multipartContent.Add($stringContent)\r\n\r\n\r\n# Call Databricks DBFS REST API\r\n$body = $multipartContent\r\n$uri = 'https://' + $DBX_HOST + '/api/2.0/dbfs/put'\r\n$response = Invoke-RestMethod $uri -Method 'POST' -Headers $headers -Body $body\r\n$response | ConvertTo-Json</pre><div data-controller=\"alert-block\"><div><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">You can use PowerShell scripts in Linux and OS X as well as Windows. The command to run a PowerShell script is slightly different in those environments. Refer to the PowerShell documentation if you are trying to run the script on a platform other than Windows.</p>\n</div>\n</div></div></div>", "body_txt": "Using the Databricks REST API to interact with your clusters programmatically can be a great way to streamline workflows with scripts. The API can be called with various tools, including PowerShell. In this article, we are going to take a look at an example DBFS put command using curl and then show you how to execute that same command using PowerShell.\u00a0 The DBFS API 2.0 put command (AWS | Azure) limits the amount of data that can be passed using the contents parameter to 1 MB if the data is passed as a string. The same command can pass 2 GB if the data is passed as a file. It is mainly used for streaming uploads, but can also be used as a convenient single call for data upload. Curl example This example uses curl to send a simple multipart form post request to the API to upload a file up to 2 GB in size. Replace all of the values in &lt;&gt; with appropriate values for your environment. Info\nTo get your workspace URL, review Workspace instance names, URLs, and IDs (AWS |\u00a0Azure).\nReview the Generate a personal access token (AWS | Azure) documentation for details on how to create a personal access token for use with the REST APIs. # Parameters databricks_workspace_url=\"&lt;databricks-workspace-url&gt;\" personal_access_token=\"&lt;personal-access-token&gt;\" local_file_path=\"&lt;local_file_path&gt;\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# ex: /Users/foo/Desktop/file_to_upload.png dbfs_file_path=\"&lt;dbfs_file_path&gt;\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# ex: /tmp/file_to_upload.png overwrite_file=\"&lt;true|false&gt;\" curl --location --request POST https://${databricks_workspace_url}/api/2.0/dbfs/put \\ \u00a0 \u00a0 \u00a0--header \"Authorization: Bearer ${personal_access_token}\" \\ \u00a0 \u00a0 \u00a0--form contents=@${local_file_path} \\ \u00a0 \u00a0 \u00a0--form path=${dbfs_file_path} \\ \u00a0 \u00a0 \u00a0--form overwrite=${overwrite_file} PowerShell example This PowerShell example is longer than the curl example, but it sends the same multipart form post request to the API. The below script can be used in any environment where PowerShell is supported. To run the PowerShell script you must: Replace all of the values in &lt;&gt; with appropriate values for your environment. Review the DBFS API 2.0 put documentation for more information.\nSave the script as a .ps1 file. For example, you could call it upload_large_file_to_dbfs.ps1.\nExecute the script in PowerShell by running ./upload_large_file_to_dbfs.ps1 at the prompt. ################################################## Parameters $DBX_HOST = \"&lt;databricks-workspace-url&gt;\" $DBX_TOKEN = \"&lt;personal-access-token&gt;\" $FILE_TO_UPLOAD = \"&lt;local_file_path&gt;\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# ex: /Users/foo/Desktop/file_to_upload.png\u00a0\u00a0 $DBFS_PATH = \"&lt;dbfs_file_path&gt;\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# ex: /tmp/file_to_upload.png $OVERWRITE_FILE = \"&lt;true|false&gt;\" ################################################## # Configure authentication $headers = New-Object \"System.Collections.Generic.Dictionary[[String],[String]]\" $headers.Add(\"Authorization\", \"Bearer \" \u00a0+ $DBX_TOKEN) $multipartContent = [System.Net.Http.MultipartFormDataContent]::new() # Local file path $FileStream = [System.IO.FileStream]::new($FILE_TO_UPLOAD, [System.IO.FileMode]::Open) $fileHeader = [System.Net.Http.Headers.ContentDispositionHeaderValue]::new(\"form-data\") $fileHeader.Name = $(Split-Path $FILE_TO_UPLOAD -leaf) $fileHeader.FileName = $(Split-Path $FILE_TO_UPLOAD -leaf) $fileContent = [System.Net.Http.StreamContent]::new($FileStream) $fileContent.Headers.ContentDisposition = $fileHeader $fileContent.Headers.ContentType = [System.Net.Http.Headers.MediaTypeHeaderValue]::Parse(\"text/plain\") $multipartContent.Add($fileContent) # DBFS path $stringHeader = [System.Net.Http.Headers.ContentDispositionHeaderValue]::new(\"form-data\") $stringHeader.Name = \"path\" $stringContent = [System.Net.Http.StringContent]::new($DBFS_PATH) $stringContent.Headers.ContentDisposition = $stringHeader $multipartContent.Add($stringContent) # File overwrite config $stringHeader = [System.Net.Http.Headers.ContentDispositionHeaderValue]::new(\"form-data\") $stringHeader.Name = \"overwrite\" $stringContent = [System.Net.Http.StringContent]::new($OVERWRITE_FILE) $stringContent.Headers.ContentDisposition = $stringHeader $multipartContent.Add($stringContent) # Call Databricks DBFS REST API $body = $multipartContent $uri = 'https://' + $DBX_HOST + '/api/2.0/dbfs/put' $response = Invoke-RestMethod $uri -Method 'POST' -Headers $headers -Body $body $response | ConvertTo-Json Info\nYou can use PowerShell scripts in Linux and OS X as well as Windows. The command to run a PowerShell script is slightly different in those environments. Refer to the PowerShell documentation if you are trying to run the script on a platform other than Windows.", "format": "html", "updated_at": "2022-09-27T07:17:09.245Z"}, "author": {"id": 789489, "email": "ravirahul.padmanabhan@databricks.com", "name": "ravirahul.padmanabhan ", "first_name": "ravirahul.padmanabhan", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:02:48.540Z", "updated_at": "2023-04-21T14:21:18.115Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 256846, "name": "Databricks File System (DBFS)", "codename": "dbfs", "accessibility": 1, "description": "These articles can help you with the Databricks File System (DBFS).", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2771468, "name": "aws"}, {"id": 2771469, "name": "azure"}, {"id": 2771470, "name": "dbfs"}], "url": "https://kb.databricks.com/dbfs/upload-large-files-using-dbfs-api-20-and-powershell"}, {"id": 1420274, "name": "Too many execution contexts are open right now", "views": 4789, "accessibility": 1, "description": "Reduce the number of notebooks used to limit the number of execution contexts required for your job.", "codename": "too-many-execution-contexts-are-open-right-now", "created_at": "2022-06-27T13:54:22.071Z", "updated_at": "2022-12-21T08:17:43.022Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.9d6f623373f3dd4035792be4efb79a4d\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You come across the below error message when you try to attach a notebook to a cluster or in a job failure.\u00a0</p><pre class=\"language-plain\">Run result unavailable: job failed with error message Too many execution contexts are open right now.(Limit set currently to 150)</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Databricks create an execution context when you attach a notebook to a cluster. The execution context contains the state for a REPL environment for each supported programming language: Python, R, Scala, and SQL.</p><p>The cluster has a maximum number of 150 execution contexts. 145 are user REPLs, while the remaining five are allocated as internal system REPLs which are reserved contexts for backend operations. Once this threshold is reached, you can no longer attach a notebook to the cluster.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt notranslate\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-2\">Info</h3>\n<p class=\"hj-alert-text\">It is not possible to view the current number of execution contexts in use.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"solution-3\">Solution</h1><p>Make sure you have not disabled auto-eviction for the cluster in your <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure\" rel=\"noopener noreferrer\" target=\"_blank\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\">GCP</a>).</p><p>If the following line is present in your Spark config, auto-eviction is disabled. Remove this line to re-enable auto-eviction:</p><pre class=\"language-plain\">spark.databricks.chauffeur.enableIdleContextTracking false</pre><h2 data-toc=\"true\" id=\"best-practices-4\">Best practices</h2><ul>\n<li>Use a job cluster instead of an interactive cluster. A job cluster for each job is the best way to avoid running out of execution contexts. Job clusters should be used for isolation and reliability.</li>\n<li>Reduce the number of separate notebooks used to reduce the number of execution contexts required.</li>\n</ul><h2 data-toc=\"true\" id=\"temporary-workaround-5\">Temporary workaround</h2><p>As a short-term solution, you can use a cluster-scoped init script to increase the execution context limit from 150 to 170.</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt notranslate\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-6\">Warning</h3>\n<p class=\"hj-alert-text\">If you increase the execution context limit, the driver memory pressure is likely to increase. You should not use this as a long-term solution.</p>\n</div>\n</div><h3 data-toc=\"true\" id=\"create-the-init-script-7\">Create the init script</h3><p>Run this sample script in a notebook to create the init script on your cluster.</p><pre class=\"language-plain\">%scala\r\n\r\nval initScriptContent = s\"\"\"\r\n|#!/bin/bash\r\n|cat &gt; /databricks/common/conf/set_exec_context_limit.conf &lt;&lt; EOL\r\n|{\r\n| rb  = 170\r\n|}\r\n|EOL\r\n\"\"\".stripMargin\r\ndbutils.fs.put(\"dbfs:/&lt;path-to-init-script&gt;/set_exec_context_limit.sh\", initScriptContent, true)</pre><p>Remember the path to the init script. You will need it when configuring your cluster.</p><h3 data-toc=\"true\" id=\"configure-the-init-script-8\">Configure the init script</h3><p id=\"isPasted\">Follow the documentation to configure a cluster-scoped init script (<a href=\"https://docs.databricks.com/clusters/init-scripts.html#configure-a-cluster-scoped-init-script\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"configure a cluster-scoped init script\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/clusters/init-scripts#configure-a-cluster-scoped-init-script\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"configure a cluster-scoped init script\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/init-scripts.html#configure-a-cluster-scoped-init-script\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"configure a cluster-scoped init script\">GCP</a>).</p><p>Set the <strong>Destination</strong> as <strong>DBFS</strong> and specify the path to the init script. Use the same path that you used in the sample script.</p><p>After configuring the init script, restart the cluster.</p><p><br></p>", "body_txt": "Problem You come across the below error message when you try to attach a notebook to a cluster or in a job failure.\u00a0 Run result unavailable: job failed with error message Too many execution contexts are open right now.(Limit set currently to 150) Cause Databricks create an execution context when you attach a notebook to a cluster. The execution context contains the state for a REPL environment for each supported programming language: Python, R, Scala, and SQL. The cluster has a maximum number of 150 execution contexts. 145 are user REPLs, while the remaining five are allocated as internal system REPLs which are reserved contexts for backend operations. Once this threshold is reached, you can no longer attach a notebook to the cluster. Info\nIt is not possible to view the current number of execution contexts in use. Solution Make sure you have not disabled auto-eviction for the cluster in your Spark config (AWS | Azure | GCP). If the following line is present in your Spark config, auto-eviction is disabled. Remove this line to re-enable auto-eviction: spark.databricks.chauffeur.enableIdleContextTracking false Best practices Use a job cluster instead of an interactive cluster. A job cluster for each job is the best way to avoid running out of execution contexts. Job clusters should be used for isolation and reliability.\nReduce the number of separate notebooks used to reduce the number of execution contexts required. Temporary workaround As a short-term solution, you can use a cluster-scoped init script to increase the execution context limit from 150 to 170. Warning\nIf you increase the execution context limit, the driver memory pressure is likely to increase. You should not use this as a long-term solution. Create the init script Run this sample script in a notebook to create the init script on your cluster. %scala val initScriptContent = s\"\"\" |#!/bin/bash |cat &gt; /databricks/common/conf/set_exec_context_limit.conf &lt;&lt; EOL |{ | rb = 170 |} |EOL \"\"\".stripMargin dbutils.fs.put(\"dbfs:/&lt;path-to-init-script&gt;/set_exec_context_limit.sh\", initScriptContent, true) Remember the path to the init script. You will need it when configuring your cluster. Configure the init script Follow the documentation to configure a cluster-scoped init script (AWS | Azure | GCP). Set the Destination as DBFS and specify the path to the init script. Use the same path that you used in the sample script. After configuring the init script, restart the cluster.", "format": "html", "updated_at": "2022-12-21T08:17:43.006Z"}, "author": {"id": 790739, "email": "akash.bhat@databricks.com", "name": "akash.bhat ", "first_name": "akash.bhat", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T10:22:42.406Z", "updated_at": "2023-04-24T05:43:50.678Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256864, "name": "Notebooks", "codename": "notebooks", "accessibility": 1, "description": "These articles can help you with your Databricks notebooks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2978824, "name": "150"}, {"id": 2976803, "name": "aws"}, {"id": 2976804, "name": "azure"}, {"id": 2976806, "name": "execution"}, {"id": 2976805, "name": "gcp"}, {"id": 2976809, "name": "limit"}, {"id": 2976807, "name": "repl"}, {"id": 2978823, "name": "temporary"}], "url": "https://kb.databricks.com/notebooks/too-many-execution-contexts-are-open-right-now"}, {"id": 1420267, "name": "Service principal cannot create access token", "views": 6414, "accessibility": 1, "description": "You cannot create a token on behalf of a service principal with the API when token usage is disabled.", "codename": "service-principal-cannot-create-access-token", "created_at": "2022-06-27T13:47:34.112Z", "updated_at": "2022-07-01T09:37:34.054Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9WdXFKL0ZrOFV4bDdrbzZwOENkWGIvcUl6cHlFSkZFd1ZnZWt5OXRVTWlZT3l4Tlp1Cm16UTNURklXbG5ieSs5TDN0MTBVQ2JYNWJvMEJXL2hsRmZjWTZDTUhUK01QdGZiUm4zbXcvUENvSjdVdwp0cDV4YS9TOVFQclRHL3pjZ3lKc3dKanc2Mi9JT0M2QmRUT0F0bTA4alZZWmZpL1lZRUsxMG9WN3NkYnUKYzVEdzFReTRNdUNXUjRqbC8vbXRIelJCWG9lakNYNzNYTDZaT2NkOFBaT0s5aTNQL2NGUjNyVStBendrCi9oUjZXVGlEQnBHamE0SkU0dHYxektLRnJPTi91N3Z3NWo2N1NRb2M1ZWVxWHVYQXQ0eGQ0K1RwWnIvaApsSlN4Zlo4ekppL3VNdWpzUDBVTFhpRlRvbFVvNjdvdmVtQXY3TldYdDk5N2dCVnpZSlc4WjZpbVVEbU8KMDNkUXZVOHlFS0xsd1FsZXhKd3prVHVYRjZZRzZscm5uaUYzd1U3SS9oTkwvQXI5WnRkOWFyeTNNTWh4CmFPMGdrRFo3a202LzRSd0RHaVd0cm50eGs2dzBVL3dvZE0zM1pQVUY2SU9vZ2ZrczhOTGNpL1ZxbC9BRwpFSEVZdkVqbm1qOVZQU0RTRFlSU2NuRlN1ZmFzcjl4bERZKzRuTFdibWloN2dDMTlZVm5ZZnBLRmxNeHUKQTRhckJFQ1V3aGVxT1h3VndXQnFtZmtkMnkramwwUEpzUEkzZndXd2M2ekZDU2I4ZkxxSDBuc3o5cmJSCkkvMytvWXRjRVZFTHpTRzdFRHRMYzgrU2NMUGV2ODRYZGtnSEJNbzVTNzBjRkhnTndseUpjZTYwaWhlcAp2cjVTc21tcmxJZGZDQkNGUlJzQnVzTTJ6eTVzcWJpMXNsMkpqQUxLYXFWMlVQNjA4SG1NS0JaeWtydmQKSGZ3aFhIUVBkdC9LckRnV083cnhCaERtdTY1Y3lRNzBRckhRK3RoTkQyZ255NjNLNVJmYXY2c3Z5dUdGCmxTYUE2RmZKSVBNd3VkdVQzTFB0eGV2VTErdVh6dWZCWEJVcWNYVmIwUVdRbmlRMDFpZ3Y0RStnMy9LRwpKaHE0S0czS1ZpbzgrRk16cmNsRzdjWTQrN2hCU0N4VW1Mbi9FNSswTFBZVGYyU1BsYTNXNW1mOGhqaTMKTU9RQUFqdkM2QkNtaHZOckFKN1VaUlRObDZZSGhMK0pWRkY2WnBGMHZSeE41QmFkUy90TkhPdXBxODBqClVEdXB0VXF5M0tIOTd2RHF1R0NHK2lvM3NmV1ovTHZERUZlNzV4bktLU0NOUmliWklnUmlidDgySWNkagpXaGFHOFBaNGZqSWV1M0loRkxtd0Zha0JYaUZWb09Lb3RMeEZ2cC9FQ0lvTGhFdDNKTzB0ZTNuVFNCb1AKUU9Hd3BraGRsb3drcGtCSmUwVEJhRmg1bkpmemlZeThJYUJIRXdZbDNxSGFKSFdsTmJlRDlCZUlPSFMvCkhMNW9HSkxwRllNVVlpTGVaWjUwajV3ZjNTejNVeWpwcm5tQUswbFVJamJNNjBLQkRrRHA0L2YrNE1WZQo5TVp2VG1aaFJ5UXY5ampGR2RwYk90WUlwZzNSR0dXamljRTVHdz09Cg==.1155f377ec39506846347e4091f403e7\"></div><h1>Problem</h1><p>You are trying to <a href=\"https://docs.databricks.com/dev-tools/api/latest/token-management.html#operation/create-obo-token\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">create a token on behalf of a service principal</a><a href=\"https://docs.databricks.com/dev-tools/api/latest/token-management.html#operation/create-obo-token\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"create a token on behalf of a service principal\"></a>, using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/2.0/token-management/on-behalf-of/tokens</span> in the REST API but are getting a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PERMISSION_DENIED</span> error.</p><pre>{\r\n\"error_code\": \"PERMISSION_DENIED\",\r\n\"message\": \"User xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx does not have permission to use tokens.\"\r\n}</pre><h1>Cause</h1><p>This happens when the service principals are assigned to a user group that has token usage disabled.</p><h1>Solution</h1><p>Your workspace admin should enable token usage for the user group that contains the service principals.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">You should create separate user groups for service principals and users who need token access and those who don't. This limits access only to those who need it and doesn't provide token access to all users in your workspace.</p>\n</div>\n</div><h2>Enable token usage via the UI</h2><ol>\n<li>Click <strong>Settings</strong> in the left hand menu.</li>\n<li>Click <strong>Admin Console</strong>.</li>\n<li>Click the <strong>Workspace settings</strong> tab.</li>\n<li>Click <strong>Permission Settings</strong> in the\u00a0<strong>Personal Access Tokens</strong> field.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656468699521-Admin%20Console%20Personal%20Access%20Token%20settings.png\" class=\"fr-fic fr-dib\" alt=\"Workspace settings tab in the Admin Console screenshot.\">\n</li>\n<li>Add the groups that need token access in the\u00a0<strong>Token Usage</strong> window.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656468875573-Token%20Usage%20permission%20settings.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Permission Settings for Token Usage pop up window screenshot.\">\n</li>\n<li>Remove any groups that should not have token access.</li>\n<li>Click <strong>Save</strong> to apply the changes and close the window.</li>\n</ol><h2>Enable token usage via the REST API</h2><ul>\n<li>Review the <a href=\"https://docs.databricks.com/dev-tools/api/latest/permissions.html#tag/Token-permissions\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">token permissions</a> <a href=\"https://docs.databricks.com/dev-tools/api/latest/permissions.html#tag/Token-permissions\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"token permissions\"></a>API settings.</li>\n<li>Use this sample code to update the token permissions.</li>\n<li>Replace the following values in the sample code before running it on your local machine:<ul>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;admin-access-token&gt;</span> - Admin personal access token.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;user-group-name&gt;</span> - The name of the user group to grant token access permission. You can add multiple group entries if needed.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;workspace-url&gt;</span> - Replace this value with your <a href=\"https://docs.databricks.com/workspace/workspace-details.html#workspace-url\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Workspace URL</a><a href=\"https://docs.databricks.com/workspace/workspace-details.html#workspace-url\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Workspace URL\"></a>.</li>\n</ul>\n</li>\n</ul><pre>curl --location --request PATCH 'https://&lt;workspace-url&gt;/api/2.0/preview/permissions/authorization/tokens'; \\\r\n--header 'Authorization: Bearer\u00a0&lt;admin-access-token&gt;' \\\r\n--header 'Content-Type: application/json' \\\r\n--data-raw '{\r\n\u00a0 \"access_control_list\": [\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"group_name\": \"&lt;user-group-name&gt;\",\r\n\u00a0 \u00a0 \u00a0 \"permission_level\": \"CAN_USE\"\r\n\u00a0 \u00a0 },\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \"group_name\": \"&lt;user-group-name&gt;\",\r\n\u00a0 \u00a0 \u00a0 \"permission_level\": \"CAN_USE\"\r\n\u00a0 \u00a0 }\r\n\u00a0 ]\r\n}'</pre><p><br></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">This sample code only allows you to add token permissions via the API. It does not allow you to delete them. To delete token permissions via the API you must replace the token permissions for the entire workspace API using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PUT</span> instead of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PATCH</span>. Review the token permissions API settings for more details.</p>\n</div>\n</div><p><br></p>", "body_txt": "Problem You are trying to create a token on behalf of a service principal , using /2.0/token-management/on-behalf-of/tokens in the REST API but are getting a PERMISSION_DENIED error. { \"error_code\": \"PERMISSION_DENIED\", \"message\": \"User xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx does not have permission to use tokens.\" } Cause This happens when the service principals are assigned to a user group that has token usage disabled. Solution Your workspace admin should enable token usage for the user group that contains the service principals. Info\nYou should create separate user groups for service principals and users who need token access and those who don't. This limits access only to those who need it and doesn't provide token access to all users in your workspace. Enable token usage via the UI Click Settings in the left hand menu.\nClick Admin Console.\nClick the Workspace settings tab.\nClick Permission Settings in the\u00a0Personal Access Tokens field. Add the groups that need token access in the\u00a0Token Usage window. Remove any groups that should not have token access.\nClick Save to apply the changes and close the window. Enable token usage via the REST API Review the token permissions API settings.\nUse this sample code to update the token permissions.\nReplace the following values in the sample code before running it on your local machine: &lt;admin-access-token&gt; - Admin personal access token. &lt;user-group-name&gt; - The name of the user group to grant token access permission. You can add multiple group entries if needed. &lt;workspace-url&gt; - Replace this value with your Workspace URL . curl --location --request PATCH 'https://&lt;workspace-url&gt;/api/2.0/preview/permissions/authorization/tokens'; \\ --header 'Authorization: Bearer\u00a0&lt;admin-access-token&gt;' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \u00a0 \"access_control_list\": [ \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"group_name\": \"&lt;user-group-name&gt;\", \u00a0 \u00a0 \u00a0 \"permission_level\": \"CAN_USE\" \u00a0 \u00a0 }, \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \"group_name\": \"&lt;user-group-name&gt;\", \u00a0 \u00a0 \u00a0 \"permission_level\": \"CAN_USE\" \u00a0 \u00a0 } \u00a0 ] }' Info\nThis sample code only allows you to add token permissions via the API. It does not allow you to delete them. To delete token permissions via the API you must replace the token permissions for the entire workspace API using PUT instead of PATCH. Review the token permissions API settings for more details.", "format": "html", "updated_at": "2022-07-01T09:37:33.862Z"}, "author": {"id": 488150, "email": "rakesh.parija@databricks.com", "name": "rakesh.parija ", "first_name": "rakesh.parija", "last_name": "", "role_id": "admin", "created_at": "2021-10-07T02:59:41.577Z", "updated_at": "2023-04-21T14:21:18.111Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 256864, "name": "Notebooks", "codename": "notebooks", "accessibility": 1, "description": "These articles can help you with your Databricks notebooks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2779936, "name": "api"}, {"id": 2779933, "name": "aws"}, {"id": 2779934, "name": "service principal"}, {"id": 2779935, "name": "token"}], "url": "https://kb.databricks.com/notebooks/service-principal-cannot-create-access-token"}, {"id": 1420262, "name": "Setup cross account bucket access in Google Cloud", "views": 2723, "accessibility": 1, "description": "Use service accounts to grant access to storage buckets in another account.", "codename": "setup-cross-account-bucket-access-in-google-cloud", "created_at": "2022-06-27T13:43:32.328Z", "updated_at": "2022-12-21T10:48:16.708Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.8acc486d7283ce530b271fa7b67ead58\"></div><h1 data-toc=\"true\" id=\"introduction-0\">Introduction</h1><p>When you setup a Databricks workspace on your Google Cloud Platform (GCP) account, you have access to the account's storage buckets by default. More complex use cases are likely to involve access to storage buckets that are owned by different GCP accounts.</p><p>This article shows you how to use the Google Cloud SDK <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">gsutil</span> commands to setup cross account read and write permission to storage buckets owned by different GCP accounts.</p><p>Before you can configure access to cross account storage buckets you must:</p><ul>\n<li>\n<a href=\"https://cloud.google.com/storage/docs/gsutil_install#sdk-install\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Install <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">gsutil</span> as part of the Google Cloud CLI</a> on your local machine.</li>\n<li>Have account credentials for the storage buckets you want to access.</li>\n</ul><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt notranslate\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"note-1\">Note</h3>\n<p class=\"hj-alert-text\">The -m option runs the command in parallel for quicker processing; the -r option runs the command recursively on resources within the bucket.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"instructions-2\">Instructions</h1><p>You should use service accounts to grant access to storage buckets owned by another account.</p><ul>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;project-number-accountA&gt;</span> - The primary account.</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;project-number-accountB&gt;</span> - The secondary account with the <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">storage buckets you want to access.</span>\n</li>\n</ul><p>The project number is an automatically generated unique identifier for your project.</p><p>Please review the Google documentation on <a href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Identifying projects</a> for more information.</p><h2 data-toc=\"true\" id=\"grant-access-to-new-objects-in-a-cross-account-bucket-3\">Grant access to new objects in a cross account bucket</h2><p data-toc=\"true\">Use <a href=\"https://cloud.google.com/storage/docs/gsutil/commands/defacl\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">defacl</span></a> to allow access to new objects created in a storage bucket that belongs to another account.</p><p data-toc=\"true\">Enter the commands in your local shell or terminal window where <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">gsutil</span> is installed.</p><p data-toc=\"true\">To grant your project's service accounts access to new objects created in a Cloud Storage bucket in another project, use the following gsutil defacl commands in your shell or terminal window.</p><pre class=\"language-plain\">gsutil defacl ch -u \\\r\n\u00a0 \u00a0 &lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com:OWNER \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil defacl ch -u \\\r\n\u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com:OWNER \\\r\ngs://&lt;bucket&gt;</pre><h2 data-toc=\"true\" id=\"grant-access-to-the-current-objects-in-a-cross-account-bucket-4\">Grant access to the current objects in a cross account bucket</h2><p>Use <a href=\"https://cloud.google.com/storage/docs/gsutil/commands/acl\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">acl</span></a> to allow access to current objects in a storage bucket that belongs to another account.</p><p>Enter the commands in your local shell or terminal window where <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">gsutil</span> is installed.</p><p>To grant your project's service accounts access to a Cloud Storage bucket and the current contents of the bucket in another project, use the following gsutil acl commands in your shell or terminal window</p><pre class=\"language-plain\">gsutil acl ch -u \\\r\n\u00a0 \u00a0 &lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com:OWNER \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil -m acl ch -r -u \\\r\n\u00a0&lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com:OWNER \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil acl ch -u \\\r\n\u00a0&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com:OWNER \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil -m acl ch -r -u \\\r\n\u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com:OWNER \\\r\ngs://&lt;bucket&gt;</pre><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-5\">Info</h3>\n<p class=\"hj-alert-text\">To grant access to new objects AND current objects in a storage bucket that belongs to another account you must run both the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">defacl</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">acl</span> commands.</p>\n</div>\n</div><h2 data-toc=\"true\" id=\"remove-account-access-to-a-storage-bucket-6\">Remove account access to a storage bucket</h2><p data-toc=\"true\" id=\"removing-account-access-of-a-bucketyou-can-run-the-following-google-cloud-sdk-gsutil-acl-commands-to-remove-your-projects-service-accounts-ownership-readwrite-permission-to-the-bucket-and-its-contents-8\">Use both <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">defacl</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">acl</span> to disconnect storage buckets that belong to other accounts from your service account.</p><p data-toc=\"true\">Enter the commands in your local shell or terminal window where <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">gsutil</span> is installed.</p><p data-toc=\"true\">If you have granted service account access to a bucket, you can run the following Google Cloud CLI gsutil acl commands to remove your project's service account ownership (read/write permission) to the bucket and its contents.</p><pre class=\"language-plain\">gsutil defacl ch -d \\\r\n\u00a0&lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com:OWNER \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil defacl ch -d \\\r\n\u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com:OWNER \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil acl ch -d \\\r\n\u00a0&lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil -m acl ch -r -d \\\r\n\u00a0&lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil acl ch -d \\\r\n\u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com \\\r\n\u00a0 \u00a0 gs://&lt;bucket&gt;\r\ngsutil -m acl ch -r -d \\\r\n\u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com \\\r\ngs://&lt;bucket&gt;</pre><p>\u00a0</p><p><br></p>", "body_txt": "Introduction When you setup a Databricks workspace on your Google Cloud Platform (GCP) account, you have access to the account's storage buckets by default. More complex use cases are likely to involve access to storage buckets that are owned by different GCP accounts. This article shows you how to use the Google Cloud SDK gsutil commands to setup cross account read and write permission to storage buckets owned by different GCP accounts. Before you can configure access to cross account storage buckets you must: Install gsutil as part of the Google Cloud CLI on your local machine.\nHave account credentials for the storage buckets you want to access. Note\nThe -m option runs the command in parallel for quicker processing; the -r option runs the command recursively on resources within the bucket. Instructions You should use service accounts to grant access to storage buckets owned by another account. &lt;project-number-accountA&gt; - The primary account. &lt;project-number-accountB&gt; - The secondary account with the storage buckets you want to access. The project number is an automatically generated unique identifier for your project. Please review the Google documentation on Identifying projects for more information. Grant access to new objects in a cross account bucket Use defacl to allow access to new objects created in a storage bucket that belongs to another account. Enter the commands in your local shell or terminal window where gsutil is installed. To grant your project's service accounts access to new objects created in a Cloud Storage bucket in another project, use the following gsutil defacl commands in your shell or terminal window. gsutil defacl ch -u \\ \u00a0 \u00a0 &lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com:OWNER \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil defacl ch -u \\ \u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com:OWNER \\ gs://&lt;bucket&gt; Grant access to the current objects in a cross account bucket Use acl to allow access to current objects in a storage bucket that belongs to another account. Enter the commands in your local shell or terminal window where gsutil is installed. To grant your project's service accounts access to a Cloud Storage bucket and the current contents of the bucket in another project, use the following gsutil acl commands in your shell or terminal window gsutil acl ch -u \\ \u00a0 \u00a0 &lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com:OWNER \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil -m acl ch -r -u \\ \u00a0&lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com:OWNER \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil acl ch -u \\ \u00a0&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com:OWNER \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil -m acl ch -r -u \\ \u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com:OWNER \\ gs://&lt;bucket&gt; Info\nTo grant access to new objects AND current objects in a storage bucket that belongs to another account you must run both the defacl and acl commands. Remove account access to a storage bucket Use both defacl and acl to disconnect storage buckets that belong to other accounts from your service account. Enter the commands in your local shell or terminal window where gsutil is installed. If you have granted service account access to a bucket, you can run the following Google Cloud CLI gsutil acl commands to remove your project's service account ownership (read/write permission) to the bucket and its contents. gsutil defacl ch -d \\ \u00a0&lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com:OWNER \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil defacl ch -d \\ \u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com:OWNER \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil acl ch -d \\ \u00a0&lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil -m acl ch -r -d \\ \u00a0&lt;project-number-accountA&gt;-compute@developer.gserviceaccount.com \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil acl ch -d \\ \u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com \\ \u00a0 \u00a0 gs://&lt;bucket&gt; gsutil -m acl ch -r -d \\ \u00a0service-&lt;project-number-accountB&gt;@trifacta-gcloud-prod.iam.gserviceaccount.com \\ gs://&lt;bucket&gt; \u00a0", "format": "html", "updated_at": "2022-12-21T10:48:16.698Z"}, "author": {"id": 790629, "email": "navya.athiraram@databricks.com", "name": "navya.athiraram ", "first_name": "navya.athiraram", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T03:23:52.605Z", "updated_at": "2022-12-21T04:51:10.484Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256028, "name": "Cloud infrastructure", "codename": "cloud", "accessibility": 1, "description": "These articles can help you manage the configuration for your Databricks workspaces.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2899548, "name": "blob"}, {"id": 2899549, "name": "bucket"}, {"id": 2978826, "name": "data"}, {"id": 2899546, "name": "gcp"}, {"id": 2978825, "name": "share"}, {"id": 2899547, "name": "storage"}], "url": "https://kb.databricks.com/cloud/setup-cross-account-bucket-access-in-google-cloud"}, {"id": 1420233, "name": "Use iptables to access the EC2 metadata server", "views": 3621, "accessibility": 1, "description": "You must setup custom iptables rules in order to access the EC2 metadata server. You cannot access it by default.", "codename": "use-iptables-to-access-the-ec2-metadata-server", "created_at": "2022-06-27T12:57:59.184Z", "updated_at": "2022-10-25T08:05:19.968Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS8ybFFQSEMxMjRycFJkcGRNQW95R2h4bS9nMkpSUm5lYnhBZmMxRVREQUxEUEFiTHhoCmJIa2RuZCtkeXU1cWZDbUU1ek9QT2VORTIrRDR6b2NqakFZWUwyWjk5azhkOVZqZTV6SGJaRE1Ld0wrawpvbHdEZi9YL1hXZUdDUU4xaGN5MDZsUWlRMy9tYU1iNk9McGY2dmhKellxRDRDdU5YK2tNMXhXRWpNeWIKOFFJb3U4eEUwcHhPSklzZ0s3VDQxbUMwRUpmSE9qY3dXUEZUczlJMWd6M0RoTlNucjR6eHRvOGtFWG85CnBybmxDaDF3ZHlzVEdxTmpkc0JPUlZkMkI5MFFtMlR3NnpPU2pXM1RQb25LSG5UT3lhdFQ5SGY5dENQdgpQSHQ4UVh6d2tiRXZLekxiZk85VFdyRmh4TDNqVUNMc3EvL2IxUTJyNXIxa2c4VUJPOVF0VHlicWdsZmEKczN5MmxDZnBCNlFCWklYYklxVG5tOUpMdTRsLzdYL1dDcU51ZWE0ZWdZSlNKcGlBeU1QcnlPQkYxbGtGCkpDU3hZWmpscjNmcUhKaldCdUYrSHQwbWhQdDg4dW1nRU54Yk9LcDZnWVB4aFdLUVhJWERydGozOFFNeQpRWkQzenZnVkRqQTBoMDl6aGVZWnIrRXpFK0kzMVlMODE2dVloeFBaSEozU1NoZFlQMlJ3eEh5c2h4OFUKbnA4bEtQVnRBZXJPOEl1OHB2dnhkMFl0SyswWUJ1SFhKODVBU1NUVEVVQU1QbE0vWDIwR2JVeTQ3dG0rCkJPMWlUaEUvS1kwUVFydnVZSXBJY3ptUUhzRDlxZEp6R0FzSUtXR2EySWxOL1lrK3FBR2ZmeUQ2NDFPdQpXVFNxemlIUVV3SncwTCtLU0Zvcm1yZHBhU3d6SjVuYnduRVNQejVCZVJnQUVaWkVzSVVCa0NmNzk5alUKMXMrdnR6VGxlcGhNZmRvQWFVWEdidXZuU0hNc3ozNlFGa3FVVkxvQ2VuT3pscmZZK1JsSlJzQjh0QlR5ClJSb0VFTGVWYUxrZlIreFhlTlBYbVo3SURNNi80MEtHakIvdlJvbjJZbE8vZStsd21VS0hXRU05WTNYNApGaDhQeDRmMjBiM0ZibGdJRXkwb25raERyWVkxY1FUbi94Wk9sTU8xeXp1Vy9OeHlVM2cwbm9nejRVZ04KM3ZqM2lwTVViYldUU3p6bVF5SjIvMDE1aks1SmVMMVcxdmtKblB4V3A4MG5RWmJxa05ESXpoY08vd1ZuCnNBNmhMU1o2N2Z0TnpEaEdacFlaOGNkN3BIUHlvaEFyb1EzZWJPUVFFcGJTYnVWLzZIQTA3NnZldHlkSwpSTUozaGhhS2lpUzhOMllqMUFaRGFzQ2ZXWG5UU3lYOTVRTnZ4NjIzTTlnK2tWM1BRMzExdSsxUzROdysKZzhDOU1vRDQwQ1lVZFBoNnRGN2dDSDBpNityUVpMY1hJOFcvZW9yM1FBT29WSDFXYjFDQjN6ZytXeExzCjREQ0xiSFAyZHBLbVFaaHkyVksrNS9RWHg2a2RzbGdKMXRPS0xNckFIWmZ2WFNCN08rVzRtaml6OUpwRgovMTlueElTVERhcW5YdkpOdDVrQTYyZ2tzdE45d2g4bVh3cklYejQwTE4zelo1YWwwUHNkLzJlbm9WYkoKRmRDWHZReFl3bkQvSE9HWDJielA2K3pxbjBRUzZ1dUJNRGUwRnhIUkRIWnM2Y3FTTGExNStxMVZqNzE5CkpxRWNNMmg2SnJHbHZPSURXUE9RUkordQo=.b3c5ec89ca525f1efea0b66edfa277ff\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to access the metadata server for your EC2 instance but cannot connect.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This is the intended, default behavior. It is functioning as designed.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Use an init script to apply a custom iptables configuration to your Databricks cluster which enables access to the metadata server.</p><ol>\n<li>Define a location to store the init script. If you do not already have a folder for your init script, you must create one. For example, using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbfs:/databricks/&lt;init-script-folder&gt;</span>:<pre>%scala\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/databricks/&lt;init-script-folder&gt;/\")</pre>\n</li>\n<li>Create the init script:<pre>%scala\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/&lt;init-script-folder&gt;/<a href=\"http://iptables.sh/\" id=\"isPasted\" rel=\"noopener\" target=\"_blank\">iptables.sh</a>\",\"\"\"\r\n#!/bin/bash\u00a0\r\nsudo iptables -A INPUT -s 169.254.169.254 -j ACCEPT\u00a0\r\nsudo iptables -A OUTPUT -d 169.254.169.254 -j ACCEPT\r\n\"\"\",True)</pre>\n</li>\n<li>Verify that the init script was created on your cluster:<pre>%scala\r\n\r\ndisplay(<a href=\"http://dbutils.fs.ls/\" id=\"isPasted\" rel=\"noopener\" target=\"_blank\">dbutils.fs.ls</a>(\"dbfs:/databricks/&lt;init-script-folder&gt;/<a href=\"http://iptables.sh/\" rel=\"noopener\" target=\"_blank\">iptables.sh</a>\"))</pre>\n</li>\n<li>Configure the init script as a <a href=\"https://docs.databricks.com/clusters/init-scripts.html#cluster-scoped-init-scripts\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">cluster-scoped init script</a> on your cluster.</li>\n<li>Restart your cluster.\u00a0</li>\n</ol><p>After the cluster restarts, the init script takes effect. You now have access to the metadata server for your EC2 instance.</p>", "body_txt": "Problem You are trying to access the metadata server for your EC2 instance but cannot connect. Cause This is the intended, default behavior. It is functioning as designed. Solution Use an init script to apply a custom iptables configuration to your Databricks cluster which enables access to the metadata server. Define a location to store the init script. If you do not already have a folder for your init script, you must create one. For example, using dbfs:/databricks/&lt;init-script-folder&gt;:%scala dbutils.fs.mkdirs(\"dbfs:/databricks/&lt;init-script-folder&gt;/\") Create the init script:%scala dbutils.fs.put(\"dbfs:/databricks/&lt;init-script-folder&gt;/iptables.sh\",\"\"\" #!/bin/bash\u00a0 sudo iptables -A INPUT -s 169.254.169.254 -j ACCEPT\u00a0 sudo iptables -A OUTPUT -d 169.254.169.254 -j ACCEPT \"\"\",True) Verify that the init script was created on your cluster:%scala display(dbutils.fs.ls(\"dbfs:/databricks/&lt;init-script-folder&gt;/iptables.sh\")) Configure the init script as a cluster-scoped init script on your cluster.\nRestart your cluster.\u00a0 After the cluster restarts, the init script takes effect. You now have access to the metadata server for your EC2 instance.", "format": "html", "updated_at": "2022-10-25T08:05:19.916Z"}, "author": {"id": 831829, "email": "manoj.hegde@databricks.com", "name": "manoj.hegde ", "first_name": "manoj.hegde", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T15:23:57.894Z", "updated_at": "2023-04-13T05:12:40.616Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256843, "name": "Clusters", "codename": "clusters", "accessibility": 1, "description": "These articles can help you manage your Apache Spark clusters.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2921672, "name": "aws"}], "url": "https://kb.databricks.com/clusters/use-iptables-to-access-the-ec2-metadata-server"}, {"id": 1420225, "name": "Generate browser HAR files", "views": 6232, "accessibility": 1, "description": "Learn how to record HAR files in your web browser. These are very useful when troubleshooting UI issues.", "codename": "generate-browser-har-files", "created_at": "2022-06-27T12:38:28.317Z", "updated_at": "2022-07-01T12:38:02.441Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStJR1NZM0duVGRnSWI1RjRvbGVtUmxJS0F3RmpjaUVkdmtmUXpmL25vVnJ6eHl4RGIyCnoyNUpyNlROL3NrbldEa1drVVB1Y2dPd2R2aTFwMlp1Z1FrVTRYZlBrUlRiNEJNNEcvZjhWTGxkdmw1cgp5WDVIY0V1SitscTFPT1BXanQ1QUNqZzl5c1RlbkZhRkhPZ0NxOWlFYkdkSW50c21pb0JsRWhJU0s4bDEKWU01KzhXQ2dXUHYzZDdHWWcvUGpTRzFDUi8wWVlWT2dXeFdRWHQ0TlhsaVpUVjgvTjdsUW5vbFo4dmVkCk5qS3RISzM2K1J5RTloT2J1R010elg2STgydlUyOFFGc0J6RXRGL05uelhEQnBGMHkyaithVkV2VW0wWApBMGFMc3BzQkUvNzBmZGNsaStTVGhNM2FjV2RzL1l5WjRmTFAzTU91VC9mY1NZRm11RGY2YjVvV09qZFEKSllmcGVrR01JRmVOd0xKMFdDN25VS1FMVXcxQlBNZzRLUTBRSXdFQzBQY3czNVhLVDNHMnI2Z3JUNkp0Cm1lTExJM05EYjl3a1ZpdEI0TU1lSkFmOVd1SHJhYzU2eVpQMU5xZXBVRUF2OUV0ZDNUbEV1WmVIUmNDUwpMWk5iTkVqMWRvRjJSV3lQNjRJWDZpS3EzUTFZT0IyOFR1cTZpRTlJVTJpbW5iakdHRm1SdHRDejMyNnIKZnNFMFd6elAyb21ISFhzeEg5bDFhMWh1ZTgwdFVZQXIzNHhSTTRUaGdPNWZiK3l1U1RQOXp2OVc2RlZzClM0dy9xUnJxUUVLQXlxcVNVMG9mb0l1SkxMNlFrVG1wdjVJcExCN0NreFpaV2lkSTRZTVpPTDNFSDdTRwpibmhhcHhkQU5UaTNvWGpocm5pbUpkMkl1SDhaVVlmajh2MW5ibTRoVmd5ZjMzL1Y0dTV5bklTa2pCdlAKa0tZeEd3alVCYm53S1RkelRGd3NxYnZJajkzNDFvOEhSdjVnYXNkbzQ3OEZzNG9xazd4WFNxV0NDUDJqCkVLUndnbHBCVEJhYWFHNHlKRmFYbGpBWVlKTXJGOXhTOXJNSitJelFIeThvamZGRnY0WVRsT1BZZ1g3YwpXT2I2Z24zODFQMDBoTm92UHg0WXF6R1BBT3dIZ2pTdGh4UU41anFkeXhVdlFTM3RqTWhHWnBESE9GYmUKcDlDMDhUWmpYdXpPYTZzSko1RXdaODRlbkJ3WHRxdmZOVVMzdlRhWG1iRnVkUkZhbitSTjRNaWNDR1MyCkNVLzd5eUNwaGRhZndMRzhDT0tTU1RVMmlmSktmOTdNcUpHeTVGM3BXcm5PWml3YTlhR21ZWFd6L3IzRwpUT0NWL0NEUVNCOWR5ZURtcnVVT0xTbm9MRlFUVUpiTmNZbVlpZVhQeGQyMkdVRnZUTWFGTVVGOENBR24KcXM5ZXBOSUNzemRCTHJZTkdzWENubUVqQmdqaS9TMFY1cnh5YzZXUFFrZmVOcHZpcGV6eUZtN3d0dldNCm53SDhsVlp3L0REdlF5MlMyaDM1ZWo1TXZOLzhmQnFwdDQ5dzdFbWtjamlGTlIwREVYUWczbCswQ2FqMAp5S2lzRlNnU0lKUGpFTHNXaUVKYkpiaGNDenJ3ZlJVN0wyakVZY3ZIVS9SU3BObWxVRkErZjlwbFdrNlYKa0p6YzBzRWpXNDVnM0c4WXVHRVEK.9e72f4b948292943f874ea8e21e7b54c\"></div><p>When troubleshooting UI issues, it is sometimes necessary to obtain additional information about the network requests that are generated in your browser. If this is needed, our support team will ask you to generate a HAR file.</p><p>This article describes how to generate a HAR file with each of the major web browsers.</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Warning</h3>\n<p class=\"hj-alert-text\">HAR files contain sensitive data, including the content of the pages you download while recording, along with your browser cookies. This is sensitive data and should not be shared publicly.</p>\n</div>\n</div><h1>Chrome</h1><ul>\n<li>Open Google Chrome and navigate to the page you want to record.</li>\n<li>Right-click on the page, and then click <strong>Inspect</strong>.</li>\n<li>Click the <strong>Network</strong> tab.</li>\n<li>Look for the <strong>Record</strong> button (<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656473456525-Screenshot%202022-06-29%20at%208.51.13%20AM.png\" class=\"fr-fic fr-dii\">) in the upper left corner of the frame. It should be red. If the <strong>Record</strong> button is grey, click it once to start recording.</li>\n<li>Check the <strong>Preserve log</strong> box.</li>\n<li>Click the <strong>Clear</strong> button (<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656473473255-Screenshot%202022-06-29%20at%208.51.18%20AM.png\" class=\"fr-fic fr-dii\">). This removes any existing logs from the tab.</li>\n<li>Reproduce the issue while the network requests are being recorded.</li>\n<li>You will see session output in the frame.</li>\n</ul><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656396765624-Screenshot%202022-06-28%20at%2011.41.17%20AM.png\" class=\"fr-fic fr-dib\"></p><ul>\n<li>Once you have reproduced the issue, click the <strong>Export HAR</strong> button (<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656678332418-Export%20HAR%20button.png\" class=\"fr-fic fr-dii\">).</li>\n<li>You are prompted to save the file on your computer.</li>\n<li>Save the HAR file.</li>\n<li>Attach the HAR file to your ticket. \u00a0</li>\n</ul><h1>Edge</h1><ul>\n<li>Open Microsoft Edge and navigate to the page you want to record.</li>\n<li>Right-click on the page, and then click <strong>Inspect</strong>.</li>\n<li>Click the <strong>Network</strong> tab.</li>\n<li>Look for the <strong>Record</strong> button (<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656473495054-Screenshot%202022-06-29%20at%208.51.13%20AM.png\" class=\"fr-fic fr-dii\">) in the upper left corner of the frame. It should be red. If the <strong>Record</strong> button is grey, click it once to start recording.</li>\n<li>Check the <strong>Preserve log</strong> box.</li>\n<li>Click the <strong>Clear</strong> button (<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656473512882-Screenshot%202022-06-29%20at%208.51.18%20AM.png\" class=\"fr-fic fr-dii\">). This removes any existing logs from the tab.</li>\n<li>Reproduce the issue while the network requests are being recorded.</li>\n<li>You will see session output in the frame.</li>\n</ul><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656397064519-Screenshot%202022-06-28%20at%2011.47.26%20AM.png\" class=\"fr-fic fr-dib\"></p><ul>\n<li>Once you have reproduced the issue, click the Export HAR button (<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656678366378-Export%20HAR%20button.png\" class=\"fr-fic fr-dii\">).</li>\n<li>You are prompted to save the file on your computer.</li>\n<li>Save the HAR file.</li>\n<li>Attach the HAR file to your ticket. \u00a0\u00a0</li>\n</ul><h1>Firefox</h1><ul>\n<li>Open Firefox and navigate to the page you want to record.</li>\n<li>Look for the the Firefox menu in the top-right.</li>\n<li>Click <strong>More Tools</strong>.</li>\n<li>Click <strong>Web Developer Tools</strong>.</li>\n<li>The <strong>Developer Network Tools</strong> panel opens.</li>\n<li>Click the <strong>Network</strong> tab.</li>\n<li>Start performing actions in the browser. Recording starts automatically.</li>\n<li>Once you have reproduced the issue, right-click on the gear (\u2699).<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656396834298-Screenshot%202022-06-28%20at%2011.43.34%20AM.png\" class=\"fr-fic fr-dib\">\n</li>\n<li>Click <strong>Save all as HAR</strong>.</li>\n<li>You are prompted to save the file on your computer.</li>\n<li>Save the HAR file.</li>\n<li>Attach the HAR file to your ticket. \u00a0</li>\n</ul><h1>Safari</h1><p>You need to enable the <strong>Develop Menu</strong> in Safari before you can access the developer console.</p><ul>\n<li>Click <strong>Safari</strong> in the menu bar</li>\n<li>Click <strong>Preferences</strong>.</li>\n<li>Click the <strong>Advanced</strong> tab.</li>\n<li>Select <strong>Show Develop menu in menu bar</strong>.</li>\n</ul><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656397015155-Screenshot%202022-06-28%20at%2011.45.00%20AM.png\" class=\"fr-fic fr-dib\"></p><ul>\n<li>Click <strong>Develop</strong> in the menu bar.</li>\n<li>Click <strong>Show Web Inspector</strong>.</li>\n<li>Click the <strong>Network</strong> tab.</li>\n<li>Start performing actions in the browser. Recording starts automatically.</li>\n</ul><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656397026444-Screenshot%202022-06-28%20at%2011.45.07%20AM.png\" class=\"fr-fic fr-dib\"></p><ul>\n<li>Once you have reproduced the issue, click <strong>Export.</strong>\n</li>\n<li>You are prompted to save the file on your computer.</li>\n<li>Save the HAR file.</li>\n<li>Attach the HAR file to your ticket. \u00a0</li>\n</ul>", "body_txt": "When troubleshooting UI issues, it is sometimes necessary to obtain additional information about the network requests that are generated in your browser. If this is needed, our support team will ask you to generate a HAR file. This article describes how to generate a HAR file with each of the major web browsers. Warning\nHAR files contain sensitive data, including the content of the pages you download while recording, along with your browser cookies. This is sensitive data and should not be shared publicly. Chrome Open Google Chrome and navigate to the page you want to record.\nRight-click on the page, and then click Inspect.\nClick the Network tab.\nLook for the Record button () in the upper left corner of the frame. It should be red. If the Record button is grey, click it once to start recording.\nCheck the Preserve log box.\nClick the Clear button (). This removes any existing logs from the tab.\nReproduce the issue while the network requests are being recorded.\nYou will see session output in the frame. Once you have reproduced the issue, click the Export HAR button ().\nYou are prompted to save the file on your computer.\nSave the HAR file.\nAttach the HAR file to your ticket. \u00a0 Edge Open Microsoft Edge and navigate to the page you want to record.\nRight-click on the page, and then click Inspect.\nClick the Network tab.\nLook for the Record button () in the upper left corner of the frame. It should be red. If the Record button is grey, click it once to start recording.\nCheck the Preserve log box.\nClick the Clear button (). This removes any existing logs from the tab.\nReproduce the issue while the network requests are being recorded.\nYou will see session output in the frame. Once you have reproduced the issue, click the Export HAR button ().\nYou are prompted to save the file on your computer.\nSave the HAR file.\nAttach the HAR file to your ticket. \u00a0\u00a0 Firefox Open Firefox and navigate to the page you want to record.\nLook for the the Firefox menu in the top-right.\nClick More Tools.\nClick Web Developer Tools.\nThe Developer Network Tools panel opens.\nClick the Network tab.\nStart performing actions in the browser. Recording starts automatically.\nOnce you have reproduced the issue, right-click on the gear (\u2699). Click Save all as HAR.\nYou are prompted to save the file on your computer.\nSave the HAR file.\nAttach the HAR file to your ticket. \u00a0 Safari You need to enable the Develop Menu in Safari before you can access the developer console. Click Safari in the menu bar\nClick Preferences.\nClick the Advanced tab.\nSelect Show Develop menu in menu bar. Click Develop in the menu bar.\nClick Show Web Inspector.\nClick the Network tab.\nStart performing actions in the browser. Recording starts automatically. Once you have reproduced the issue, click Export. You are prompted to save the file on your computer.\nSave the HAR file.\nAttach the HAR file to your ticket. \u00a0", "format": "html", "updated_at": "2022-07-01T12:38:02.437Z"}, "author": {"id": 790745, "email": "vivian.wilfred@databricks.com", "name": "vivian.wilfred ", "first_name": "vivian.wilfred", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T10:42:50.853Z", "updated_at": "2023-03-28T11:16:09.635Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256864, "name": "Notebooks", "codename": "notebooks", "accessibility": 1, "description": "These articles can help you with your Databricks notebooks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2780927, "name": "aws"}, {"id": 2780928, "name": "azure"}, {"id": 2780932, "name": "browser"}, {"id": 2780929, "name": "gcp"}, {"id": 2780930, "name": "har"}, {"id": 2780931, "name": "web"}], "url": "https://kb.databricks.com/notebooks/generate-browser-har-files"}, {"id": 1420096, "name": "Terraform registry does not have a provider error", "views": 8468, "accessibility": 1, "description": "You cannot install the Databricks Terraform provider if the required_providers block is not defined in your modules.", "codename": "terraform-registry-does-not-have-a-provider-error", "created_at": "2022-06-27T07:37:29.970Z", "updated_at": "2022-08-16T19:39:20.618Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTljNHJLODJSRHJPVDdQTFpxUGhVKzkvRzdrM3JCRTU1UHhBYzlPTEk4SlhRandDNFdRCmF0NmhzSFJ0NVRlQWx0TzNGK2IrRHBkeVV1MklpV2FRWXdGNkpYL2UxYUdFT2UrY2VzOFRnYzdobmhjagpvSEUzREtDdlMwVFI1bmpKbTlxK0Nvc3BqMHhYUFNscGR2VlFjNXQ0V0ZqVFhGU0Z3SjViSjZKdXk2SmkKM3p6V3ZFMEdXRlFMYllnU1NLU1BzK0Z2N01ZbTdkbkdzYnBUWE5CK2R3bk5LTGNINGoxMVU5eTBraG1xClFqVE9GdktMVHlFS2V5N3VQQ0RTZ0I3cFU1WGlYaFcrLzdnQ0paeUdlU0J2WjNKaG14TWRWbFdaSnY1RAphZUVlTThObTlNZU9ydHdoYm9VbHBHdUtFcHJTdW8vQnlnNUZjZzdOa0N6NHMxRXlvb0ZTUUlkcVpRVnkKMEhOVEZlWjhML3NzZnJaSkttQmNINkdjTzd2Tlc4dHp3SjlLb1M4bVhzb1BtUDdLV1dOTzZqZ3hJNXdECkNjU2FJcTZqRHlxZkY5bUJIbXRhTDBMZWlLRGcraHZrR0xJbHZzMEQ4a3JXUTRxcTJ3UlZrZmhlZTV2cAo4MlV5YldxQWRhYUdESExBOHRJeGIzMlgvNlZSUzlobWNzOVVvUU1HeDJnNGhROUQrZWtYWjlZT3dEaUQKbXd0Tng0TkVGY1pxUE9lU3cvT3ZIMFVIYmU3ZVl1Y3V6SlNzRkJQclJhN2RLRzhGT0lla3p6QzNBVHBiCnltMTNTQ3ViQmUwbnlkS2FvQllaUlB0eWR6T1pEbW14MjdjN2VDT1JmbWhjd09QaXFMb2YwT1FDcUhXOApISUZGUk1Pajd1N2FDbVFGb1FNQVloZjR5Q0RnSUZwNXh4N1Z3R04rWS9Sb0dwNXhzMnNweVVtRFNRVXgKMFU4ajlocS9HRUNsM2lwSEVzU0RGY0hSR1pUeGpXemRQTG1MaFhQdlhSSjdiMjhBc3R5RFl2clZoSitHCm1ydXZqYmxMeVpFeFg1bzZzVTlmeHhzRWFwTHV6V2E4UW92QWtwUHBFTE9RcmxiOE1xNW03ekUwa2ludwpoSGhoMUpna011WEtiVmtESUpjNnBPNGdWT0Robng3cEVjT3JWaVF0Tng5RHltQ1RpMXMrb2k2a3dkczkKK2FCUy8xc3RSb1ZPQ1NQeGRnR2x0aEs2SXk0eSs2MFZiQXgvcnpSMnMybFNObTVidmJyUk80UjRlakxsCjF5TnVDRzh5VzFnbUdsY3h4NFBUUFRwM1QvRVZtdHBEeSsxWjNTM0RLbHIzbFRjUVlSRXY0UWRhMkdpKwplV01YZGxtSlF5RVVnMWVScVZMb3o0b21UUWlBc2YrNXFaQ29MZzdKUzh0NVBmT0xoS0RYVXBmN1psdDAKRU91RUxuMS8K.d0f6fa24fb7bf85b9fc233e570a53aba\"></div><h1>Problem</h1><p>You are installing the Databricks Terraform provider (<a href=\"https://docs.databricks.com/dev-tools/terraform/index.html\" title=\"\" id=\"\" target=\"_blank\" rel=\"noopener noreferrer\"></a><a href=\"https://docs.databricks.com/dev-tools/terraform/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks Terraform provider\">AWS</a> | <a href=\"https://docs.microsoft.com/en-us/azure/databricks/dev-tools/terraform/\" title=\"\" id=\"\"></a><a href=\"https://docs.microsoft.com/azure/databricks/dev-tools/terraform/\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks Terraform provider\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/terraform/index.html\" title=\"\" id=\"\" target=\"_blank\" rel=\"noopener noreferrer\"></a><a href=\"https://docs.gcp.databricks.com/dev-tools/terraform/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks Terraform provider\">GCP</a>) and get a Databricks <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">provider registry</span> error.</p><pre id=\"isPasted\">Error while installing hashicorp/databricks: provider registry\r\nregistry.terraform.io does not have a provider named\r\nregistry.terraform.io/hashicorp/databricks</pre><h1>Cause</h1><p>This error occurs when the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">required_providers</span> block is not defined in every module that uses the Databricks Terraform provider.</p><h1>Solution</h1><p id=\"isPasted\">Create a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">versions.tf</span> file with the following contents:</p><pre># versions.tf\r\nterraform {\r\n\u00a0\u00a0required_providers {\r\n\u00a0 \u00a0\u00a0databricks = {\r\n\u00a0 \u00a0 \u00a0\u00a0source\u00a0\u00a0= \"databricks/databricks\"\r\n\u00a0 \u00a0 \u00a0\u00a0version = \"1.0.0\"\r\n\u00a0 \u00a0\u00a0}\r\n\u00a0\u00a0}\r\n}</pre><p>Save a copy of this <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">version.tf</span> file in every module in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">environments</span> level of your code base.</p><p>Remove the <strong>version</strong> field from the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">versions.tf</span> file and save a copy of the updated file in every module in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">modules</span> level of your code base.</p><p>For example:</p><pre>\u251c\u2500\u2500 environments\r\n\u2502 \u00a0 \u251c\u2500\u2500 sandbox\r\n\u2502 \u00a0 \u2502 \u00a0 \u251c\u2500\u2500 README.md\r\n\u2502 \u00a0 \u2502 \u00a0 \u251c\u2500\u2500 main.tf\r\n\u2502 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 versions.tf   // This file contains the \"version\" field.\r\n\u2502 \u00a0 \u2514\u2500\u2500 production\r\n\u2502 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 README.md\r\n\u2502 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 main.tf\r\n\u2502 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 versions.tf\u00a0 \u00a0// This file contains the \"version\" field.\r\n\u2514\u2500\u2500 modules\r\n\u00a0 \u00a0 \u251c\u2500\u2500 first-module\r\n\u00a0 \u00a0 \u2502 \u00a0 \u251c\u2500\u2500 ...\r\n\u00a0 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 versions.tf\u00a0 \u00a0// This file does NOT contain the \"version\" field.\r\n\u00a0 \u00a0 \u2514\u2500\u2500 second-module\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 ...\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 versions.tf\u00a0 \u00a0// This file does NOT contain the \"version\" field.</pre><p><br>Review the <a href=\"https://www.terraform.io/language/providers/requirements#requiring-providers\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Requiring providers</a> Terraform documentation for more information.</p>", "body_txt": "Problem You are installing the Databricks Terraform provider ( AWS | Azure | GCP) and get a Databricks provider registry error. Error while installing hashicorp/databricks: provider registry registry.terraform.io does not have a provider named registry.terraform.io/hashicorp/databricks Cause This error occurs when the required_providers block is not defined in every module that uses the Databricks Terraform provider. Solution Create a versions.tf file with the following contents: # versions.tf terraform { \u00a0\u00a0required_providers { \u00a0 \u00a0\u00a0databricks = { \u00a0 \u00a0 \u00a0\u00a0source\u00a0\u00a0= \"databricks/databricks\" \u00a0 \u00a0 \u00a0\u00a0version = \"1.0.0\" \u00a0 \u00a0\u00a0} \u00a0\u00a0} } Save a copy of this version.tf file in every module in the environments level of your code base. Remove the version field from the versions.tf file and save a copy of the updated file in every module in the modules level of your code base. For example: \u251c\u2500\u2500 environments \u2502 \u00a0 \u251c\u2500\u2500 sandbox \u2502 \u00a0 \u2502 \u00a0 \u251c\u2500\u2500 README.md \u2502 \u00a0 \u2502 \u00a0 \u251c\u2500\u2500 main.tf \u2502 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 versions.tf // This file contains the \"version\" field. \u2502 \u00a0 \u2514\u2500\u2500 production \u2502 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 README.md \u2502 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 main.tf \u2502 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 versions.tf\u00a0 \u00a0// This file contains the \"version\" field. \u2514\u2500\u2500 modules \u00a0 \u00a0 \u251c\u2500\u2500 first-module \u00a0 \u00a0 \u2502 \u00a0 \u251c\u2500\u2500 ... \u00a0 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 versions.tf\u00a0 \u00a0// This file does NOT contain the \"version\" field. \u00a0 \u00a0 \u2514\u2500\u2500 second-module \u00a0 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 ... \u00a0 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 versions.tf\u00a0 \u00a0// This file does NOT contain the \"version\" field. Review the Requiring providers Terraform documentation for more information.", "format": "html", "updated_at": "2022-08-16T19:39:20.616Z"}, "author": {"id": 789494, "email": "prabakar.ammeappin@databricks.com", "name": "prabakar.ammeappin ", "first_name": "prabakar.ammeappin", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:54:57.352Z", "updated_at": "2023-04-05T07:45:06.002Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 313835, "name": "Terraform", "codename": "terraform", "accessibility": 1, "description": "These articles can help you with Terraform.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2780169, "name": "aws"}, {"id": 2780170, "name": "modules"}, {"id": 2780241, "name": "provider"}, {"id": 2769417, "name": "terraform"}, {"id": 2780171, "name": "version"}], "url": "https://kb.databricks.com/terraform/terraform-registry-does-not-have-a-provider-error"}, {"id": 1419441, "name": "Find your workspace ID", "views": 7920, "accessibility": 1, "description": "Learn how to find your Databricks workspace ID in the web UI as well as via a notebook command.", "codename": "find-your-workspace-id", "created_at": "2022-06-24T15:32:24.755Z", "updated_at": "2022-10-25T08:53:16.972Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThRRWh6NVRlY01teUxEVGV5aVJUQ1hHNmJvc2NlekEvUWxCU3pSQnlmZ0Z1TFRjajBaCmpmUTQyL0ZqREMxWG9pMU85bzlDcUl1UzhvNDNyUzBldkpRZDZnYkZqS3A2WUR0YjQ5MWVkU3hHbHNFYgprV0RRQ3Z2ZDlSWC9qN0tvUDFueXRJQnc5S2F2bGw4MnBPSlBqWXdZZ3VROURFNmFwcG55SE5ZWW9rS0EKbFF1N1A3VXVwcGRhL2VCcnVoWmFQVHJ4T2JHQi9OMFJ0bUNLb2pucFV4RXJBTkZmYzEybmR0UU16Uld5ClIxak4yVDFLZitTbi9nODUrOFBkUUxrcEN5ODJSRlJQVjE2T2dvZ3Y5UmdTTWQ3b3RRQ2h3MnV5SmFrcQpUUGNtOUdUcUhkQk5lc2tzS3JxcDRZVWx1RXdqM3BXZUdncEdBeTdBaVRmZk9adWZnd3VPSGNGUGhqTkgKdmc3aW5WdnhVYS96VE1abkR5WFZvQ2hBZ3dhaDZ3ay95RWw3WFA3L3oyb1lwU2pJdDV4cVpYbEdMamEwClhSNkt4WjJjV24zb0tjUHMyQjB5aFM1SFk3YTVkdUZQUml1YlJlSHNhcklHSXhzWjFlRmtWYlZseTM0cwpCaytPUGdUS0NKdkhxRmJQMzljbnp3MzNKSDNwTFFxZVRhUnEyOCs2cVgrN3U1NE5UV2R1NXgxcFhZS0oKUEsyaFh6cHBoaThYTHNkSFdJMHNqQjdDQmQ5QVBtZ3NqK250ampPTmRvVVZPT3hydGhxKzVTWTF0UjlECnFiT1ZNRyt6TEZ3SXkwcE5DditQUFljNVpuWWoyT0toNzlQVWZjYUxEeW5uVUdab2RRekJkWmxNRklzWgpjaXhIS0JQQk8xdngyNHBUbFgwTUZYaThQOTdBc1N0NXlGYVdNNjZJcWc2YTF0bU55a0RET3VkMVJDVWwKc0xnd0s0YVhRU0c3aS85RDlQeU1FTm5BbWEvTUIvRVNBYTFIYzFEMlhudDlTeHg2M0ZvQ0RlWEpBb1A4CjVVMHFPaUFMcEwrWnp1djlEK3Y1aGwyNmpwOGJRTTU5ZUMwZkVSZDc2OWlPb3lPV25BTU1UMlVYai9ZQQpTVlZLZ3VpRDl0ZWRBSGF2bDVETU96WmdvNDh0clpWaXpZUjAxNk9qcnJ1M1I4ZG5IbVFLcHppMEhmUmQKSmloUDhXL0JlOFJyT0wyckQ5NkhxUEx1Wk1GWDRmZmdnR0Fqcm5kVXd5V2M3VVNhWC9hN2VxWEVxWEo2Cm9jUUFYV01URWhkdWg2dTBURllYS29tUUFlQlVSMHdFMHFGSmFyTXAyZzllMnQ3V21JblQvNDVDUE1ndwpIMEFIcjFaSkZHOW1MV2ZkWWs0bmxmTGQrcEhiZkxML0srS3Yxd3RHcXFBb0FSMVFxMEFjKzhYZ3JvalgKZmRtRTVaUjljQmpxUkw2eUU2TnlnN2NsQ0kzY01WM0RYSFprcHo3d043ZTlyZVlqTTlZK3dWbjZOS0RvCnl3TGJDMFpzRHFYZ0pTRFMrT3R3eUdkd0lhNktwbkdTTEVBNnlBK09NMDY5REdsdGsyek1NVFdSS3IzaAo5K0IxUW9QSFhuQ1lySXp4R3VjT0xWMTJkTVB6cUZXQmJyQ05XUzFMczJVUFFPQzg1eGYrbzBIK0kxMGgKd1FRVEU4YTJkcUZhRHB4dXEwQy8vOHgzQjFJSHBIMjE4emlhdDF5c3FnPT0K.cf7ae79ba9f6c39de482a43b8c3a6250\"></div><p>Everything you do in Databricks occurs within a workspace. When you use the web UI you are interacting with clusters and notebooks in the workspace. When you run automated jobs or connect to your workspace outside of the web UI you may need to know your workspace ID.</p><p>This article covers two different ways to easily find your workspace ID.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><h2 data-toc=\"true\" id=\"from-the-browser-1\">From the browser</h2><p>When viewing a Databricks workspace, the workspace ID is contained within the web URL.</p><p>After you have logged into your Databricks workspace, look at the URL displayed in your browser's address bar.</p><p>You should see a series of numbers displayed in the URL after <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">o=</span>. The numbers following the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">o=</span> make up the workspace ID.</p><p>Given <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">https://&lt;databricks-instance&gt;.com/o=XXXXX</span>, the workspace ID is the numeric value represented by <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">XXXXX</span>.</p><p>In this example URL, the workspace ID is 2281745829657864</p><pre>https://abcd-teste2-test-spcse2.cloud.databricks.com/?o=2281745829657864#</pre><p><br></p><h2 data-toc=\"true\" id=\"from-a-notebook-2\">From a notebook</h2><p>You can get the workspace ID from within a notebook by running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")</span> in a Python\u00a0or Scala cell.</p><p>It returns the workspace ID when run.</p>", "body_txt": "Everything you do in Databricks occurs within a workspace. When you use the web UI you are interacting with clusters and notebooks in the workspace. When you run automated jobs or connect to your workspace outside of the web UI you may need to know your workspace ID. This article covers two different ways to easily find your workspace ID. Instructions From the browser When viewing a Databricks workspace, the workspace ID is contained within the web URL. After you have logged into your Databricks workspace, look at the URL displayed in your browser's address bar. You should see a series of numbers displayed in the URL after o=. The numbers following the o= make up the workspace ID. Given https://&lt;databricks-instance&gt;.com/o=XXXXX, the workspace ID is the numeric value represented by XXXXX. In this example URL, the workspace ID is 2281745829657864 https://abcd-teste2-test-spcse2.cloud.databricks.com/?o=2281745829657864# From a notebook You can get the workspace ID from within a notebook by running spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\") in a Python\u00a0or Scala cell. It returns the workspace ID when run.", "format": "html", "updated_at": "2022-10-25T08:53:16.969Z"}, "author": {"id": 791192, "email": "sivaprasad.cs@databricks.com", "name": "sivaprasad.cs ", "first_name": "sivaprasad.cs", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T19:34:47.857Z", "updated_at": "2023-03-20T13:02:42.882Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 254612, "name": "Databricks administration", "codename": "administration", "accessibility": 1, "description": "These articles can help you administer your Databricks workspace, including user and group management, access control, and workspace storage.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2921698, "name": "aws"}, {"id": 2921699, "name": "azure"}, {"id": 2921700, "name": "gcp"}, {"id": 2921701, "name": "workspace"}], "url": "https://kb.databricks.com/administration/find-your-workspace-id"}, {"id": 1419411, "name": "Field name sorting changes in Apache Spark 3.x", "views": 352, "accessibility": 1, "description": "Starting with Spark 3.0.0, rows created from named arguments do not have field names sorted alphabetically.", "codename": "field-name-sorting-changes-in-apache-spark-3x", "created_at": "2022-06-24T15:08:49.379Z", "updated_at": "2023-04-21T14:53:42.891Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9tU1lvWDNWRnpSMnFsT2lwajhNUTRHNTdhTXppWDQwdDVzSElLYklFeHEvUmw1eFdaCmlnVFdRVHRxUE9iNEZrSFFFSXRsZUorbE8yeWc0djNlT21yb2d5RmluT241eENUUlpnK1VDUlVEZWpoZgpzY1FncWhDbDh0Y08zYVRyL1RRdG91TVFlRHhNeUFlaVZDbk9aWkJZNE9GWkpTQ0M5b1lVRGlDZXhleWMKUHVOSFhjK1U1TkEyc2FpU3BXNVplOEVCdUNkSEsrR0M4K1VadjZSbWF6TGl5TTdMZWxkOFkwc2hNQ1ZyCjV1dlRMQStYZERmclllS29sZER1MGVJM0VnbmRmUjlhWlloaXVYUEhYOTBhdEs2T1lHWEFJTElLM0x1UwpUS2ZEUjJOdUtYN1lsdWY5WCtqUkU0MzlPalovSEVJK04zaEcyUThjU2dYREZLeEo5TmF0S3VNN2k4TloKOGJidXU2S1c2QUhDSXVmVDJsMTE0TzFpNWROMWxka2g5TnZZclozQzNBa0ZqaG9iYkJvckJseW93bVZLCkxYQnpDTkN4eS9ZZlM4eFdpR2RCcFMwWHlYd1hXNkpkdW9KTzZnQTNPNFVqbVJVU1JOeXVUUFZyQ3ZDRwplVmlzNVBpdEZ6Z3liaUNacGlTQzYvNy8vSVBmODhYTUovYVE5dHMrbFljNmJOZUh3b1I2WXdNYkZ4VnAKM0RpQXRhQVVRNDJ0MUErZVlSN3dwVFJDM3NYTVIzeE42MlBTRnZMU2xLcUU0eXd0QjJEdHZ2Mnl3a3BlCm5kTmZWdGo4U1dtUXlyR2xmd1cyaFJlSVR4MkZ5eGtSbmh1SEc1bUNrRGFVdHgwNTc4eWxnam5TbUYyQgp5OWx6YXZ4Zng3VTVLUnpKdGFzZldsR2p6OVVYcTRta3lqYnNxa1F2c3RqSVphcm9QQmhscWMxdTBxTDYKTjdZanR5alMyQ05PTzJETW5VakZIT04ra3F3M2hzS0s4bE9qdkgyYjh4NTE2aEREaDRqMStLbWU5K2hjCkR1b0pmSU04MFN3aE1jdkVHckorNFRhR29tYWhid29pbGNRMWxibjlJWUEwb3Rid2g3N1dSanBUK2pOQQorVmV2dk12bUg0eVpRaWZ4dVRuTXo1TVJTNmR5NkdGbWo4ZWkwRGJyNno5Ukd3UmYzZlZrempRY2h5ZlYKQjhVQ2loa3VGOWRQbmpzeGZCcHBZQzRoS01SQ1AvUUw3N2NBc1doYWQ4aFo4b3V4WVRrUDJ4N0NPeHpYClBueGJpekV3b1lma3dCS2pvcVpQT2xHajNmSmNEdDgvTmtuZEtwOHVTVlZLd2FLbVVma0F2akJVYmpCOQpzRHowN3pyOHF4dDZKSzlZVThHOUdXYnUyZXI1WmpCNmlaU1ZtbnR6VzFLaFlERmxqeGtWL0R4Qk9NdDEKcDFITTV4TmlRMG1NTDA3emNJMkZyOUl3VXNraVRsSVQ5SE4ycFpVa1h3RS9CSElyVUJ6QkhWM3FxRVJCCmpBVmFrVmtHTXdUWnhUY0x2TGU3Ly8wWURGbHJuWTlITnpoYVZVVnRPZWxSbFZXSFQ3MGZrS2tuajFGeQpnaFRZVWk5TDE5N2Fxc1lBQzdZUjlod2pxQXdyV05zK2YxWHV3a0lwdjJtbktCLzcxL3VrS1NIWXhBWGMKelZmTllrTlR6ZnNham5RYzNkNStRR09MeUtDaks1MkJrbW9WbENUcjRRPT0K.6044755566cdac2564976cc4ab7ebf7c\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>When using a map transformation on a RDD using Databricks Runtime 9.1 LTS and above, the resulting schema order is different when compared to doing the same map transformation using Databricks Runtime 7.3 LTS.</p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>Databricks Runtime 9.1 LTS and above incorporate Apache Spark 3.x. Starting with Spark 3.0.0, rows created from named arguments do not have field names sorted alphabetically. Instead, they are ordered in as entered.\u00a0</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>To enable Spark 2.x style row sorting set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PYSPARK_ROW_FIELD_SORTING_ENABLED</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span> in your cluster's <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">GCP</a>).</p><pre>PYSPARK_ROW_FIELD_SORTING_ENABLED=true</pre><p>For Python versions less than 3.6, the field names can only be sorted alphabetically.</p><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"warning-3\">Warning</h3>\n<p class=\"hj-alert-text\">This workaround is deprecated and will be removed in a future version of Spark.</p>\n</div>\n</div><p><br></p>", "body_txt": "Problem When using a map transformation on a RDD using Databricks Runtime 9.1 LTS and above, the resulting schema order is different when compared to doing the same map transformation using Databricks Runtime 7.3 LTS. Cause Databricks Runtime 9.1 LTS and above incorporate Apache Spark 3.x. Starting with Spark 3.0.0, rows created from named arguments do not have field names sorted alphabetically. Instead, they are ordered in as entered.\u00a0 Solution To enable Spark 2.x style row sorting set PYSPARK_ROW_FIELD_SORTING_ENABLED to true in your cluster's Spark config (AWS | Azure | GCP). PYSPARK_ROW_FIELD_SORTING_ENABLED=true For Python versions less than 3.6, the field names can only be sorted alphabetically. Warning\nThis workaround is deprecated and will be removed in a future version of Spark.", "format": "html", "updated_at": "2023-04-21T14:53:42.855Z"}, "author": {"id": 886595, "email": "sergios.lalas@databricks.com", "name": "sergios.lalas ", "first_name": "sergios.lalas", "last_name": "", "role_id": "draft_writer", "created_at": "2022-06-17T15:36:18.243Z", "updated_at": "2023-04-21T15:02:56.121Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256868, "name": "Python with Apache Spark", "codename": "python", "accessibility": 1, "description": "These articles can help you to use Python with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3029155, "name": "aws"}, {"id": 3029156, "name": "azure"}, {"id": 3029158, "name": "gcp"}, {"id": 3029163, "name": "map"}, {"id": 3029162, "name": "rdd"}, {"id": 3029164, "name": "transformation"}], "url": "https://kb.databricks.com/python/field-name-sorting-changes-in-apache-spark-3x"}, {"id": 1419397, "name": "Change cluster config for Delta Live Table pipeline", "views": 5986, "accessibility": 1, "description": "Customize the cluster configuration when using a Delta Live Table pipeline.", "codename": "change-cluster-config-for-delta-live-table-pipeline", "created_at": "2022-06-24T15:01:12.477Z", "updated_at": "2022-07-01T08:46:45.208Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"U2FsdGVkX18Hmgq1A0DbpN6X8egXac9nWpwum6sHV/mPjhFSUpY2EirdMOcI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.cc1343c0ba78764cc333ad7c12209974\"></div><h1>Problem</h1><p>You are using Delta Live Tables and want to change the cluster configuration.</p><p>You create a pipeline, but only have options to enable or disable Photon and select the number of workers.</p><h1>Cause</h1><p>When you create a Delta Live Table pipeline, most parameters are configured with default values. These values cannot be configured before the pipeline is created.\u00a0</p><h1>Solution</h1><p>You can change the cluster configuration after the pipeline is created.</p><ol>\n<li>Click <strong>Workflows</strong> in the sidebar.</li>\n<li>Click the <strong>Delta Live Tables</strong> tab.</li>\n<li>Click the name of your pipeline.</li>\n<li>Click\u00a0<strong>Settings</strong>.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656459002508-DLT%20pipeline%20details.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Pipeline details screenshot.\">\n</li>\n<li>On the <strong>Edit Pipeline Settings</strong> pop-up, click <strong>JSON.</strong><br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656459104450-Edit%20pipeline%20settings%20JSON.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Edit pipeline settings screenshot.\">\n</li>\n<li>Edit the JSON to specify your cluster configuration. You can update all of the Delta Live Table settings (<a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-configuration.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"https://docs.gcp.databricks.com/data-engineering/delta-live-tables/delta-live-tables-configuration.html\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"https://docs.gcp.databricks.com/data-engineering/delta-live-tables/delta-live-tables-configuration.html\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/data-engineering/delta-live-tables/delta-live-tables-configuration.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Delta Live Table settings\">GCP</a>) in the JSON file.</li>\n<li>Click <strong>Save</strong>.</li>\n<li>Click <strong>Start</strong> to start your pipeline with the new cluster configuration.</li>\n</ol>", "body_txt": "Problem You are using Delta Live Tables and want to change the cluster configuration. You create a pipeline, but only have options to enable or disable Photon and select the number of workers. Cause When you create a Delta Live Table pipeline, most parameters are configured with default values. These values cannot be configured before the pipeline is created.\u00a0 Solution You can change the cluster configuration after the pipeline is created. Click Workflows in the sidebar.\nClick the Delta Live Tables tab.\nClick the name of your pipeline.\nClick\u00a0Settings. On the Edit Pipeline Settings pop-up, click JSON. Edit the JSON to specify your cluster configuration. You can update all of the Delta Live Table settings (AWS | Azure | GCP) in the JSON file.\nClick Save.\nClick Start to start your pipeline with the new cluster configuration.", "format": "html", "updated_at": "2022-07-01T08:46:45.201Z"}, "author": {"id": 791607, "email": "pratik.bhawsar@databricks.com", "name": "pratik.bhawsar ", "first_name": "pratik.bhawsar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T06:27:23.823Z", "updated_at": "2022-08-25T06:06:42.590Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256849, "name": "Delta Lake", "codename": "delta", "accessibility": 1, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2773545, "name": "aws"}, {"id": 2773546, "name": "azure"}, {"id": 2773549, "name": "delta"}, {"id": 2773548, "name": "dlt"}, {"id": 2773547, "name": "gcp"}], "url": "https://kb.databricks.com/delta/change-cluster-config-for-delta-live-table-pipeline"}, {"id": 1419284, "name": "Apache Spark jobs fail with Environment directory not found error", "views": 7546, "accessibility": 1, "description": "Spark jobs appear to time out after you install a library because security rules are preventing workers from resolving the Python executable path.", "codename": "apache-spark-jobs-fail-with-environment-directory-not-found-error", "created_at": "2022-06-24T12:41:57.752Z", "updated_at": "2022-07-01T08:35:14.409Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTl3ZVBiTVVOcFI0WVUvczVnTVRiWmxrbGlISkpQOEcyMXVpdy93d0l5cFY1dlNYSktqCmMrYWFGOVJhUjNxNll2VXFhNjNjQjhPblBVU1MvYVQ3UHFtMGVoYnQrcVU5bnFNU3QvdjFCRzJYb21JSwpTemNTRyswRlRqMVhlV2J1WEZNdkN4YTVoRjNqRkNVcG5TTkRQRW9WQ2Z3ZmtHcHY5U2t4K2p4M1pkWnEKdEUyS2k2SDV0SVdwamJRMGlyZi9jcFI2VDgvYklabDhnQUZPckthUzVZMnJnUG1vOFZHdHdnTmJUbkRFCmhiZmVBRlZ3a201SDVTM01SS01hQkpKWnJRUWQxMFptL3FMNjNBN2NpNU02VjhPTW01NGhldm9ia2tscwo2Q203WDBrYi9OQkJxMkxiYUE3bVhZK09HMHp2eXpsOURqZU42RThNemdXQnBlVXREUERaN3FZOGZraG4KdEFmNXo3RkwxLzdrREwvOUFzNmRXc1RYdmpLc0gyVzl2NGFHZGpBbE52Q2ZGa2dvSUtiakwxVFAvOWtkCnVjZ0dVSUZ4UkFwczFtbTJOMENEc2c4Uzd1dUZnRUFBcEdScEVldjVkdWFoeVROWW00Zmg2cnFTZjc5Ygp6ell5VHRNT3hCYkF0bGpnd3Y5bVorbHhHRHRnTDB0dEt5REZBclBlcjBXdzVCc1VHWWJqL2FsRm1ISFMKUmcvbU5QTnljbm9nMW1SaE5CMXl6Z1FVbkM3cTNyVlFRQzd0aWZ6NTEvaXNIVW9LS00wSko1bC81ODU1Ck9kL29pNTUzVkQzYnA5d0pUdjJUY0ZpMDUyVUhUbUE3UE1EQVNET0VSNnFLT2w5Q2FRdGw4blFBTDM3NgpLaldTdmFibFFsNVVjYjJPQUNvYk9kQ2pjWUVZMk9xNy9mSFZISlpUMUFnNzZDdndzRWY5MXVPd3Y1TDAKYXpmekVjZmNiSjlzb0R2ZDBpWUhpeVhYSG84eDFZMmVaNmpROVhVOFVVN3hyVFJFb0cwTFZ3VlAzVmNsClgrV0lIUUxRQ0UwZ3pldmlmQ01FcHZCMzVicFpVMnprM2wrZm9zZmJrMk5TWFJQR2RvdUVWK2pCcllRUwpneWdSdUZOa2NmaXJOZDByUDZsSGdhajdjdkZDNU1KeXh4L0tGbVNaakJxaldEdXdTR29RZW91Q0lzUngKKy9tRkhFdHFSUlBwYTRlRy9MdDJEdSs1TEttR1JVVUJScW0vL2ViZ2h6Mm1OYWdST0ZDTHI4MTJDNzBWCmtxZGdJcUxLcldRYkxISEZlb3gvSlNFS25VMlNIdjU5MmtDYkF1dktUTWtJU2JoUXhuem82elFxMDM2LwpKVG5ZRnBLL0NMbDdXNEhZRllJQnYvMHJuOHJGQ1VSZzRLc3k0UzRHS2duMlRhZGJFRitGeXhmZFNLNCsKU21Qa2dkSUQrNHpLdTY0N3NGL2NiVUt0WWZvWk51a0VIaGIrNE9HUHE3QyszM3FYTjJuUG44L0ZrY0Q1ClFwSFhCaDdmSkd0MTlhTnFiMytkOXVBeUpwV1JYUUxMcExRdFlZU1JPL3Q3bm1FTmJRZkJONzhZQ3ZNQwpuOUs0V1NVQ3FFY1NIYUs4cXZuRmFJQTB1bWEzTzlCUDVhTFlCZmV6WFBIVmlDeTczWUZmbWhwNW81ZlMKd0ZNQ3lYRkdrd3c3dUxTWmovVDB6MG5aUnRuTWRHclphSWJPc1NtVGFnPT0K.77db0929c87a0ec733fa35d397ca6391\"></div><h1>Problem</h1><p>After you install a Python library (via the cluster UI or by using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">pip</span>), your Apache Spark jobs fail with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Environment directory not found</span> error message.</p><pre id=\"isPasted\">org.apache.spark.SparkException: Environment directory not found at\r\n/local_disk0/.ephemeral_nfs/cluster_libraries/python</pre><h1>Cause</h1><p>Libraries are installed on a Network File System (NFS) on the cluster's driver node. If any security group rules prevent the workers from communicating with the NFS server, Spark commands cannot resolve the Python executable path.</p><h1>Solution</h1><p>You should make sure that your security groups are configured with appropriate security rules (<a href=\"https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html#security-groups\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"security rules\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject#--network-security-group-rules\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"security rules\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/customer-managed-vpc.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"security rules\">GCP</a>).</p>", "body_txt": "Problem After you install a Python library (via the cluster UI or by using pip), your Apache Spark jobs fail with an Environment directory not found error message. org.apache.spark.SparkException: Environment directory not found at /local_disk0/.ephemeral_nfs/cluster_libraries/python Cause Libraries are installed on a Network File System (NFS) on the cluster's driver node. If any security group rules prevent the workers from communicating with the NFS server, Spark commands cannot resolve the Python executable path. Solution You should make sure that your security groups are configured with appropriate security rules (AWS | Azure | GCP).", "format": "html", "updated_at": "2022-07-01T08:35:14.397Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:28.551Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256852, "name": "Libraries", "codename": "libraries", "accessibility": 1, "description": "These articles can help you manage libraries in Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2766293, "name": "aws"}, {"id": 2766294, "name": "azure"}, {"id": 2766295, "name": "gcp"}], "url": "https://kb.databricks.com/libraries/apache-spark-jobs-fail-with-environment-directory-not-found-error"}, {"id": 1419244, "name": "Iterate through all jobs in the workspace using Jobs API 2.1", "views": 3856, "accessibility": 1, "description": "Use the Jobs API 2.1 to iterate through and display a list of jobs in your workspace.", "codename": "iterate-through-all-jobs-in-the-workspace-using-jobs-api-21", "created_at": "2022-06-24T11:12:58.636Z", "updated_at": "2022-10-28T14:10:21.883Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.2e0944e38d23998db00e2c5fdcc3101e\"></div><p dir=\"ltr\">In the Databricks Jobs API 2.0 (<a href=\"https://docs.databricks.com/dev-tools/api/2.0/jobs.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Jobs API 2.0\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/dev-tools/api/2.0/jobs\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Jobs API 2.0\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/api/2.0/jobs.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Jobs API 2.0\">GCP</a>) <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">list</span> returns an unbounded number of job descriptions.</p><p dir=\"ltr\">In the Jobs API 2.1 (<a href=\"https://docs.databricks.com/dev-tools/api/latest/jobs.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Jobs API 2.1\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/dev-tools/api/latest/jobs\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Jobs API 2.1\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/api/latest/jobs.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Jobs API 2.1\">GCP</a>), this behavior has changed. The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">list</span> command now returns a maximum of 25 jobs, from newest to oldest, at a time.</p><p dir=\"ltr\">In this article we show you how to manually iterate through all of the jobs in your workspace.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><h2 data-toc=\"true\" id=\"1-determine-the-total-number-of-jobs-in-your-workspace-1\">1) Determine the total number of jobs in your workspace</h2><ol>\n<li>Click <strong>Workflows</strong> in the sidebar.</li>\n<li>Scroll to the bottom of the page.</li>\n<li>The total number of jobs in the workspace is listed in the bottom right.</li>\n</ol><h2 data-toc=\"true\" id=\"2-determine-the-values-to-use-for-offset-and-limit-2\">2) Determine the values to use for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">offset</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">limit</span>\n</h2><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">list</span> command has two modifiers, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">limit</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">offset</span>. <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">offset</span> determines the number of jobs that are skipped before the first one is displayed. <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">limit</span> determines the number of jobs (up to 25) that are displayed. By using the commands together you can display specific jobs out of the total.</p><p>For example, if there are 20 total jobs in the workspace and you specify a <span id=\"isPasted\" style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">limit</span> of 10 and an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">offset</span> of 0, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">list</span> returns jobs 1-10 (the 10 most recent jobs created, not the most recent job runs). Alternatively, if you specify a <span id=\"isPasted\" style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">limit</span> of 10 and an <span id=\"isPasted\" style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">offset</span> of 10, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">list</span> returns jobs 11-20.</p><p dir=\"ltr\" id=\"isPasted\">You should consider the total number of jobs in your workspace and choose values for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">limit</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">offset</span> that allow you to easily iterate through the total number of jobs.</p><h2 data-toc=\"true\" dir=\"ltr\" id=\"3-iterate-through-the-jobs-3\">3) Iterate through the jobs</h2><p dir=\"ltr\">You need to iterate through the total number of jobs. For this article, we are iterating through all of the jobs in a notebook, using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">curl</span> to access the API. We are assuming the list of jobs is large and are displaying the maximum of 25 at a time.</p><p dir=\"ltr\">Review the Authentication using Databricks personal access tokens (<a href=\"https://docs.databricks.com/dev-tools/api/latest/authentication.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Authentication using Databricks personal access tokens\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/authentication\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Authentication using Databricks personal access tokens\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/api/latest/authentication.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Authentication using Databricks personal access tokens\">GCP</a>)\u00a0documentation for more information on creating and using personal-access-tokens.</p><pre id=\"isPasted\">%sh\r\n\r\ncurl --location --header 'Authorization: Bearer &lt;personal-access-token&gt;' \u00a0--request GET / 'https://&lt;databricks-instance&gt;/api/2.1/jobs/list?limit=25&amp;offset=0'</pre><p><br>The first run uses <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">limit=25&amp;offset=0</span> so it returns jobs from 1-25.</p><p>If we change the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">offset</span> value to 25 and run the command again, it returns jobs 26-50.</p><p>Changing the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">offset</span> value to 50 returns jobs 51-75.</p><p>You can continue to iterate through the total number of jobs, displaying 25 at a time, until all of the jobs have been displayed.</p><h2 data-toc=\"true\" id=\"4-use-jq-to-filter-results-4\">4) Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">jq</span> to filter results</h2><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-5\">Info</h3>\n<p class=\"hj-alert-text\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">jq</span> can be described as \"<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">sed</span> for JSON data\". You can use it to slice, filter, map, and transform structured data.</p>\n</div>\n</div><p>You can use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">jq</span> to help filter for specific results. For example, if you pipe the results of your <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">list</span> request through <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">jq '.deb'</span>, it returns objects with a value for the key <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">deb</span>.</p><pre>%sh\r\ncurl --location --header 'Authorization: Bearer &lt;personal-access-token&gt;' \u00a0--request GET 'https://&lt;databricks-instance&gt;/api/2.1/jobs/list?limit=25&amp;offset=0' | jq '.deb'</pre><p><br>You can include multiple keys when using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">jq</span>. For example, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">jq '.deb, .last_updated'</span> returns jobs with values for both of the keys.</p><pre id=\"isPasted\">%sh\r\ncurl --location --header 'Authorization: Bearer &lt;personal-access-token&gt;' \u00a0--request GET 'https://&lt;databricks-instance&gt;/api/2.1/jobs/list?limit=25&amp;offset=0' | jq '.deb, .last_updated\u2019</pre><p><br></p>", "body_txt": "In the Databricks Jobs API 2.0 (AWS | Azure | GCP) list returns an unbounded number of job descriptions. In the Jobs API 2.1 (AWS | Azure | GCP), this behavior has changed. The list command now returns a maximum of 25 jobs, from newest to oldest, at a time. In this article we show you how to manually iterate through all of the jobs in your workspace. Instructions 1) Determine the total number of jobs in your workspace Click Workflows in the sidebar.\nScroll to the bottom of the page.\nThe total number of jobs in the workspace is listed in the bottom right. 2) Determine the values to use for offset and limit The list command has two modifiers, limit and offset. offset determines the number of jobs that are skipped before the first one is displayed. limit determines the number of jobs (up to 25) that are displayed. By using the commands together you can display specific jobs out of the total. For example, if there are 20 total jobs in the workspace and you specify a limit of 10 and an offset of 0, list returns jobs 1-10 (the 10 most recent jobs created, not the most recent job runs). Alternatively, if you specify a limit of 10 and an offset of 10, list returns jobs 11-20. You should consider the total number of jobs in your workspace and choose values for limit and offset that allow you to easily iterate through the total number of jobs. 3) Iterate through the jobs You need to iterate through the total number of jobs. For this article, we are iterating through all of the jobs in a notebook, using curl to access the API. We are assuming the list of jobs is large and are displaying the maximum of 25 at a time. Review the Authentication using Databricks personal access tokens (AWS | Azure | GCP)\u00a0documentation for more information on creating and using personal-access-tokens. %sh curl --location --header 'Authorization: Bearer &lt;personal-access-token&gt;' \u00a0--request GET / 'https://&lt;databricks-instance&gt;/api/2.1/jobs/list?limit=25&amp;offset=0' The first run uses limit=25&amp;offset=0 so it returns jobs from 1-25. If we change the offset value to 25 and run the command again, it returns jobs 26-50. Changing the offset value to 50 returns jobs 51-75. You can continue to iterate through the total number of jobs, displaying 25 at a time, until all of the jobs have been displayed. 4) Use jq to filter results Info jq can be described as \"sed for JSON data\". You can use it to slice, filter, map, and transform structured data. You can use jq to help filter for specific results. For example, if you pipe the results of your list request through jq '.deb', it returns objects with a value for the key deb. %sh curl --location --header 'Authorization: Bearer &lt;personal-access-token&gt;' \u00a0--request GET 'https://&lt;databricks-instance&gt;/api/2.1/jobs/list?limit=25&amp;offset=0' | jq '.deb' You can include multiple keys when using jq. For example, jq '.deb, .last_updated' returns jobs with values for both of the keys. %sh curl --location --header 'Authorization: Bearer &lt;personal-access-token&gt;' \u00a0--request GET 'https://&lt;databricks-instance&gt;/api/2.1/jobs/list?limit=25&amp;offset=0' | jq '.deb, .last_updated\u2019", "format": "html", "updated_at": "2022-10-28T14:10:21.878Z"}, "author": {"id": 827002, "email": "debayan.mukherjee@databricks.com", "name": "debayan.mukherjee ", "first_name": "debayan.mukherjee", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-16T19:16:11.808Z", "updated_at": "2023-03-19T15:06:50.313Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256864, "name": "Notebooks", "codename": "notebooks", "accessibility": 1, "description": "These articles can help you with your Databricks notebooks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2924684, "name": "aws"}, {"id": 2924685, "name": "azure"}, {"id": 2925093, "name": "display"}, {"id": 2924686, "name": "gcp"}, {"id": 2925092, "name": "jobs"}, {"id": 2925091, "name": "list"}], "url": "https://kb.databricks.com/notebooks/iterate-through-all-jobs-in-the-workspace-using-jobs-api-21"}, {"id": 1419201, "name": "Ensure consistency in statistics functions between Spark 3.0 and Spark 3.1 and above", "views": 6032, "accessibility": 1, "description": "Statistics functions in Databricks Runtime 7.3 LTS and below return NaN when a divide by zero occurs. Set a Spark config to return null instead.", "codename": "ensure-consistency-in-statistics-functions-between-spark-30-and-spark-31-and-above", "created_at": "2022-06-24T09:49:15.363Z", "updated_at": "2022-10-14T04:22:49.295Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9ValJNRkpUcXNRMFE5VzdpdmcrNE4xSFh2ZjJiWEFJaiszSVlUUkg5MkJYd01DSWhTClBpbGZYYmVsV2JKZUs1dUtOU3hOM3ZOUEZHOEVxdXRWWDN0eE5lbVJrMjNueVYwL1E1NExLWXpVYkQwcQpvSVJhTkl5eHR2S09Cc20zRTBtSWpNVkV1WmpYdTRvVjZ4UVd2UDNrMGF2U29qSU5ObmxXck1QMUlNOVEKeGtzTVVYazZQL21JeXRRYWUyamhyNC9hSEkreVVHLzc2WW50UHQzKzZKdjhPWS91c0NMa3V4akZPVzZlCmpkUS9tT1F2MGc1Wjh5UVhWRTVBUzJoT1N4aGtVYkp1algyUkZCVFZkZC9NNTFJbnNpbEtzMUdVTUR4MApkWVhkdk51Yll6UmluZmw3djB2RlQ1THZ4bVNNQTZDNWJYZy94VXFScS9TTmNoWk1TaEVDWmJ5Z0tLdlUKOXhvWG42NUo4NW5WbmhIVXBjTVJteDJRZFBVaWEraUFYRmd2RWZqOWM5UWt3eEVOZnZzQUx2b3BhQUNICkpDU3QzV09iSExaU1ZGRkZXWkM1L3hZSXg3Qy9EQW95TWVMeVAwblZQaktFWndZYmIxYnp3K3hsbE1RVgp5ZTdOc082dHg4azhKSE51NlNPOEt4VG1aYyt2UVE0OFI0OVYrQmtJVkJydVZ1VGpBOGtOMmhpSFI3eEUKeWJWa0svemswUWpEejFxWU5vb1BxTUpsNlhyWDFMRGcvc1dYRmtZN1A4dDh2QjV6Z0tWNmkrZEZaQzlBCkVGS0VuOEpiNGhpbzN4RU93YUhWeTZhNXdwNUhoOUQyZXJ6MURHTC90TzJmS2RBRWh6bkdpUmJxYXIzQgo2OW9OZXlHa216MkxVclAvd1NUdExjYjRvVnl6eUMwS0NMeCtySGsxdkZFd01aeHJuU1VzRUJNditlTXYKRFZFM0RYRHZrL3JXVW5lTWcwWC9paXhBa2l6Nm5MajZ6T3BONW4vaGlOMXRRR2s4SldmdjFhQ0lSY2J4ClFCUXZ4ektmdjJFUmRDRkUxMUoyV251bDJrbkRvSllDNlc2TjllcmhUbjM1cWN6Qnk0NGlLdjRoOXlyRwo0RVA2aXhYa3RLejcrdUlFMzg4aGdPck83SldlM2RWM2RDd2pmQU1VZk5xZHY5MlJ5NmtlOTJOcWk2blIKOXlCbnMzbDVqQnd5V1BUUnRGbDdwT2liSmxHbEU3TFdJcCtacXErcEMvMGdrRzNuV1RnWXFPcytuSG51CktVd1lVcVBaMFhNdkVWZzF6eGF6RkJ5MzFCMmdWbERMVjh0VG5rYWhPamdTd1VURzM5UUhESkoxZk8yYQpMays4QmNVZXJnQVdoeEYxTjBPN1ltb0V6WnRWaGNQNmUyVkxoMkIxYjVOb1EzVEtydDdGZXVHYjhmdGEKL1ZIWUxOU0JXVFVHQ0l4OWhmZStjZjZMNkZLbDZMT1pTdFJRQTJvZXdrWE1GM0RzQ3UxcHFRTXptcnh5CnNXeUhYSWhpbkRrSnZWM0NNODNUenpTMmZxZms1eHdYOGtrbEFTUDQ1dHJwOUVqeUs2dW8wZTlTTGtqWgpZOGY0VG1YTmpUYURkYVFlV3cyYVFrYlE2Z3VlcEJINkdoSlhZSEVBdXNpeWJITE5YcnMyTG1QN2psNFQKTVlQd3RqMnlsOWo0QndvRkJVR29rVnhoN1VEMy96QklJcU9uSFpqUHRieDVDdzA3TFBkT3psbC9pR3hZCm5PUFltWlFCL2l1M095Z3kzdnVKZmpqWXdDc21JNitZLzRVbmYvRWNXamNBSVh3bysxU0xVU0padFBkTQpLY044OWt2V3NSUzdEUlFBamthaXRJV2ZQTEhVMWE3OWRPbDMwOWpjcHMrVDRXWkFHb25xbitaU0IzVTQKRUhWdXlBMGJhdUJYMklUYkpocG9oUTZxUHJCNlpKTjYK.1ec26025235de3049cf86495f5fd5e3f\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem\u00a0</h1><p>The statistics functions <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">covar_samp</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">kurtosis</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">skewness</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">std</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">stddev</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">stddev_samp</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">variance</span>, and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">var_samp</span>, return <strong>NaN</strong> when a d<span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">ivide by zero occurs during expression evaluation in Databricks Runtime 7.3 LTS.</span> The same functions return <strong>null</strong> in Databricks Runtime 9.1 LTS and above, as well as Databricks SQL endpoints when a divide by zero occurs during expression evaluation.</p><p>This example image shows sample results when running on Databricks Runtime 7.3 LTS. In cases where divide by zero occurs, the result is returned as NaN.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1664454492069-1664454492069.png\" style=\"box-sizing: border-box; border: 0px; max-width: calc(100% - 10px); cursor: pointer; padding: 0px 1px; user-select: none; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\" id=\"isPasted\" class=\"fr-fic fr-dib\"></p><p><span id=\"isPasted\">This example image shows sample results when running on Databricks Runtime 9.1 LTS. In cases where divide by zero occurs, the result is returned as null.</span></p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1664454816123-1664454816123.png\" style=\"box-sizing: border-box; border: 0px; max-width: calc(100% - 10px); cursor: pointer; padding: 0px 1px; user-select: none; color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\" id=\"isPasted\" class=\"fr-fic fr-dib\"></p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The change in behavior is due to an underlying change in Apache Spark.</p><p>In Spark 3.0 and below, the default behavior returns NaN when divided by zero occurs while evaluating a statistics function.</p><p>In Spark 3.1 this was changed to return null when divided by zero occurs while evaluating a statistics function.</p><p>For more information on the change, please review Spark PR <a href=\"https://github.com/apache/spark/pull/29983\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">[</a><a href=\"https://github.com/apache/spark/pull/29983\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">SPARK-13860]</a>.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.legacy.statisticalAggregate</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span> in your <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>) on clusters running Databricks Runtime 7.3 LTS.</p><p>This returns null instead of NaN when a divide by zero occurs while evaluating a statistics function.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">You can also set this value at the notebook level using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.conf.set(\"spark.sql.legacy.statisticalAggregate\", \"false\")</span> if you don't have the ability to edit the cluster's <strong>Spark config</strong>.</p>\n</div>\n</div><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"7d9043b272553\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTgwaWRjOTBmenpjNzJJVDRreVRsVGVobHlpa0JHNlNvVmVqRVJLRlNLclhOQkdvaGpaCjFidi9xQVRlUGRoQm5mODZZblJ1TnIrUDlLMzhHY29raVFHdksvajkrTDdyS2M2NWVBbk1ma0IxMDdxVgpRdmxZQVA3NTMvYVk3V1lPUm5ndU5BajVQbHd3VS9SMTZLWjcveFFjeVd0c2xJaVhCcGhNSjN1ditFYksKMXErNmd0QWxNVi9DQmFoR2dnem45ZDNmT09lMTNMenc5MTkySTE1Z1ZvU0NCazRVVGoxeFcxWEMvYjVQCndnOE1jckE1c3ZXT3A0c1hZY1NHeU5tMVBSNnFpeVJCWEJjUTlTUFY4aWlRRG5lUmgwcGdoL3h4SUY0RApDbWxaNXpVeTdOYnh5UUxsTWNjVmJLU2RYVVFqYmIyRDBwRUQ5NDBGWEorczVQKytKSnpZbytoZ3ZSYmoKb2VpNDI4V04yQUdsYjVJb1V2T3Y4ZDdxMXVlbmRObFhEU1QrYVJSd3A4Z21ENkZTMmJVclF5VWV1RjJSCnNEc0QrcmUwaFZiSFBsamE4L05GcVZqSitETkNNTGNBS3hEMTBZWWNnK3lLWkZwZm8wNWUxdjYyUGNsRQphUEdWbnRQa1NpWlRJUHZqbVJ2cU5YeUhoSW5WWCs5RUo2NXN6dVZBWC96ek1RWXBKUDV5M2hHT2J5RG8KUGxIWDNxSU5vdlhweVhncmRGM0FmZHovRktlL2ZQYUtESERoZndYdi9mOU14a1hLRXBhRlNMRDBHZkl5CkxFSUdVQU82Ulk0N1poN3hVc3lxV0NJOHJwWlB5WWV5STc1MmZ1REJ6bXBvRWd4NVdNRnlpTkExaXl6aApoZE9LSUVlVmNnZnlRUVl6QWFQRkNPeFRGMkZIa29UOG1uYWE0VlhJdmdaaWZIMjJ2STFXVXU1dUZPRmsKS0lKZ3IxNU0yTm11cGZFZVZEM0hMVlZlNy9TQlNPNnorMC81Z3R4eGpBenhYdFpqSFFmcFJHZjlGOHc5Cks4R2Z1TmRXc1I1R0lKN2F3Vi9jWmg5Q0IxbTA0RjVaaGpVMDdBVE9iOGRPZ2JOMUphZDYK.9108187f9b3d25c68c6f3383944a1fb9\"></div><p><br></p>", "body_txt": "Problem\u00a0 The statistics functions covar_samp, kurtosis, skewness, std, stddev, stddev_samp, variance, and var_samp, return NaN when a divide by zero occurs during expression evaluation in Databricks Runtime 7.3 LTS. The same functions return null in Databricks Runtime 9.1 LTS and above, as well as Databricks SQL endpoints when a divide by zero occurs during expression evaluation. This example image shows sample results when running on Databricks Runtime 7.3 LTS. In cases where divide by zero occurs, the result is returned as NaN. This example image shows sample results when running on Databricks Runtime 9.1 LTS. In cases where divide by zero occurs, the result is returned as null. Cause The change in behavior is due to an underlying change in Apache Spark. In Spark 3.0 and below, the default behavior returns NaN when divided by zero occurs while evaluating a statistics function. In Spark 3.1 this was changed to return null when divided by zero occurs while evaluating a statistics function. For more information on the change, please review Spark PR [ SPARK-13860]. Solution Set spark.sql.legacy.statisticalAggregate to false in your Spark config (AWS | Azure | GCP) on clusters running Databricks Runtime 7.3 LTS. This returns null instead of NaN when a divide by zero occurs while evaluating a statistics function. Info\nYou can also set this value at the notebook level using spark.conf.set(\"spark.sql.legacy.statisticalAggregate\", \"false\") if you don't have the ability to edit the cluster's Spark config.", "format": "html", "updated_at": "2022-10-14T04:22:49.290Z"}, "author": {"id": 821786, "email": "chetan.kardekar@databricks.com", "name": "chetan.kardekar ", "first_name": "chetan.kardekar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.705Z", "updated_at": "2022-07-01T13:13:21.129Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2907852, "name": "aws"}, {"id": 2907853, "name": "azure"}, {"id": 2907856, "name": "gcp"}, {"id": 2913626, "name": "nan"}, {"id": 2913627, "name": "null"}, {"id": 2913628, "name": "zero"}], "url": "https://kb.databricks.com/sql/ensure-consistency-in-statistics-functions-between-spark-30-and-spark-31-and-above"}, {"id": 1419198, "name": "Parsing post meridiem time (PM) with to_timestamp() returns null", "views": 5005, "accessibility": 1, "description": "When converting 12-hour time to 24-hour time with to_timestamp() the hours variable must be lowercase.", "codename": "parsing-post-meridiem-time-pm-with-to_timestamp-returns-null", "created_at": "2022-06-24T09:37:33.379Z", "updated_at": "2022-07-22T11:04:06.081Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.b05d7b2b24deb8b37a1336e88f504533\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to parse a 12-hour (AM/PM) time value with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">to_timestamp()</span>, but instead of returning a 24-hour time value it returns null.</p><p>For example, this sample code:</p><pre>%sql\r\n\r\nSELECT to_timestamp('2016-12-31 10:12:00 PM', 'yyyy-MM-dd HH:mm:ss a');</pre><p>Returns null when run:<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656622067318-to%20timestamp%20null.jpeg\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">to_timestamp()</span> requires the hour format to be in lowercase.</p><p>If the hour format is in capital letters, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">to_timestamp()</span> returns null.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Make sure you specify the hour format in lowercase letters.</p><p>For example, this sample code:</p><pre>%sql\r\n\r\nSELECT to_timestamp('2016-12-31 10:12:00 PM', 'yyyy-MM-dd hh:mm:ss a');</pre><p>Returns the time as a 24-hour time value.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656679605342-timestamp%2024%20hour.jpeg\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"><br></p>", "body_txt": "Problem You are trying to parse a 12-hour (AM/PM) time value with to_timestamp(), but instead of returning a 24-hour time value it returns null. For example, this sample code: %sql SELECT to_timestamp('2016-12-31 10:12:00 PM', 'yyyy-MM-dd HH:mm:ss a'); Returns null when run: Cause to_timestamp() requires the hour format to be in lowercase. If the hour format is in capital letters, to_timestamp() returns null. Solution Make sure you specify the hour format in lowercase letters. For example, this sample code: %sql SELECT to_timestamp('2016-12-31 10:12:00 PM', 'yyyy-MM-dd hh:mm:ss a'); Returns the time as a 24-hour time value.", "format": "html", "updated_at": "2022-07-22T11:04:06.078Z"}, "author": {"id": 821786, "email": "chetan.kardekar@databricks.com", "name": "chetan.kardekar ", "first_name": "chetan.kardekar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.705Z", "updated_at": "2022-07-01T13:13:21.129Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2781439, "name": "aws"}, {"id": 2781438, "name": "azure"}, {"id": 2781440, "name": "gcp"}, {"id": 2781445, "name": "spark sql"}, {"id": 2781446, "name": "to_timestamp"}], "url": "https://kb.databricks.com/sql/parsing-post-meridiem-time-pm-with-to_timestamp-returns-null"}, {"id": 1419188, "name": "Optimize streaming transactions with .trigger", "views": 8545, "accessibility": 1, "description": "Use .trigger to define the storage update interval. A higher value reduces the number of storage transactions.", "codename": "optimize-streaming-transactions-with-trigger", "created_at": "2022-06-24T09:17:57.789Z", "updated_at": "2022-10-26T08:09:21.974Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.8a01e39e93d5629369e5024722cae83f\"></div><p>When running a structured streaming application that uses cloud storage buckets (S3, ADLS Gen2, etc.) it is easy to incur excessive transactions as you access the storage bucket.</p><p>Failing to specify a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.trigger</span> option in your streaming code is one common reason for a high number of storage transactions. When a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.trigger</span> option is not specified, the storage can be polled frequently. This happens immediately after the completion of each micro-batch by default.</p><p>The default behavior is described in the official <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Apache Spark documentation on triggers</a> as, \"If no trigger setting is explicitly specified, then by default, the query will be executed in micro-batch mode, where micro-batches will be generated as soon as the previous micro-batch has completed processing.\"</p><p>This sample code does not have a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.trigger</span> option defined. If run, it would result in excessive storage transactions.</p><pre id=\"isPasted\">%python\r\n\r\nspark.readStream.format(\"delta\").load(\"&lt;delta_table_path&gt;\")\r\n.writeStream\r\n.format(\"delta\")\r\n.outputMode(\"append\")\r\n.option(\"checkpointLocation\",\"&lt;checkpoint_path&gt;\")\r\n.options(**writeConfig)\r\n.start()</pre><p data-aura-rendered-by=\"421:771;a\"><br>You can reduce the number of storage transactions by setting the .<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">trigger</span> option in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.writeStream</span>. Setting <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.trigger</span> processing time to a few seconds prevents short polling.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><p>The default behavior is to check the source for updates every 10 ms. For most users, a longer interval between source updates will have no noticeable effect on performance, but the transaction costs are greatly reduced.</p><p>For example, let's use a processing time of 5 seconds. That is 500 times slower than 10 ms. The storage calls are reduced accordingly.</p><p>Setting a processing time of 5 seconds requires adding <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.trigger(processingTime='5 seconds')</span> to the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.writeStream</span>.</p><p>For example, modifying our existing sample code to include a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.trigger</span> processing time of 5 seconds only requires the addition of one line.</p><pre data-aura-rendered-by=\"471:771;a\">%python\r\n\r\nspark.readStream.format(\"delta\").load(\"&lt;delta_table_path&gt;\")\r\n.writeStream\r\n.format(\"delta\")\r\n.trigger(processingTime='5 seconds')  #Added line of code that defines .trigger processing time.\r\n.outputMode(\"append\")\r\n.option(\"checkpointLocation\",\"&lt;checkpoint_path&gt;\")\r\n.options(**writeConfig)\r\n.start()</pre><p><br>You should experiment with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.trigger</span> processing time to determine a value that is optimized for your application.</p>", "body_txt": "When running a structured streaming application that uses cloud storage buckets (S3, ADLS Gen2, etc.) it is easy to incur excessive transactions as you access the storage bucket. Failing to specify a .trigger option in your streaming code is one common reason for a high number of storage transactions. When a .trigger option is not specified, the storage can be polled frequently. This happens immediately after the completion of each micro-batch by default. The default behavior is described in the official Apache Spark documentation on triggers as, \"If no trigger setting is explicitly specified, then by default, the query will be executed in micro-batch mode, where micro-batches will be generated as soon as the previous micro-batch has completed processing.\" This sample code does not have a .trigger option defined. If run, it would result in excessive storage transactions. %python spark.readStream.format(\"delta\").load(\"&lt;delta_table_path&gt;\") .writeStream .format(\"delta\") .outputMode(\"append\") .option(\"checkpointLocation\",\"&lt;checkpoint_path&gt;\") .options(**writeConfig) .start() You can reduce the number of storage transactions by setting the .trigger option in the .writeStream. Setting .trigger processing time to a few seconds prevents short polling. Instructions The default behavior is to check the source for updates every 10 ms. For most users, a longer interval between source updates will have no noticeable effect on performance, but the transaction costs are greatly reduced. For example, let's use a processing time of 5 seconds. That is 500 times slower than 10 ms. The storage calls are reduced accordingly. Setting a processing time of 5 seconds requires adding .trigger(processingTime='5 seconds') to the .writeStream. For example, modifying our existing sample code to include a .trigger processing time of 5 seconds only requires the addition of one line. %python spark.readStream.format(\"delta\").load(\"&lt;delta_table_path&gt;\") .writeStream .format(\"delta\") .trigger(processingTime='5 seconds') #Added line of code that defines .trigger processing time. .outputMode(\"append\") .option(\"checkpointLocation\",\"&lt;checkpoint_path&gt;\") .options(**writeConfig) .start() You should experiment with the .trigger processing time to determine a value that is optimized for your application.", "format": "html", "updated_at": "2022-10-26T08:09:21.943Z"}, "author": {"id": 821786, "email": "chetan.kardekar@databricks.com", "name": "chetan.kardekar ", "first_name": "chetan.kardekar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.705Z", "updated_at": "2022-07-01T13:13:21.129Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256866, "name": "Streaming", "codename": "streaming", "accessibility": 1, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2922528, "name": "aws"}, {"id": 2922529, "name": "azure"}, {"id": 2922531, "name": "delta"}, {"id": 2922530, "name": "gcp"}, {"id": 2922532, "name": "streaming"}], "url": "https://kb.databricks.com/streaming/optimize-streaming-transactions-with-trigger"}, {"id": 1419185, "name": "Failed credential validation checks error with Terraform", "views": 6284, "accessibility": 1, "description": "You get a 'Failed credential validation checks' error message when using Terraform to deploy a Databricks workspace in AWS.", "codename": "failed-credential-validation-checks-error-with-terraform", "created_at": "2022-06-24T09:07:34.025Z", "updated_at": "2022-10-04T17:04:44.404Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9Zc28zSElPZXlvRHRBTDRDT0doSFJONUVnMFFDRU5VMzliK3ovZWlTNjUxK1pjNGZlCnpwMHFseHFQSGFQeTZHNytCenlwSWJHVC9DanNiZUpmUkpEanYyS21lakFEZEVFbGx5VGYvN0doOXJ2ZgpmYmh5UmNkSnRpVWtsNnJRTzFmMzlaanMycmlFeFhEQ0NtRC9hWXA5QThXc0UzdGpLME0wOHdIUUF3WTIKMEVSaUVtSEN3OS9HZEJnVnNJVGF4cW5HM01XWVhIdTkvelp6OHd0VjZQMVNOb3pFN0ZOc1hrVGZ1dlBkCnIrL1ppY1N1aHYrYXR6ZmhXMzV5NTgwU3FDYnNlRGZhdmdoVGd1L0E3ZDlNS2pRM1loVnJiN2NIVHJRMwpZY1hTWVBOQ3FPdWlKbVhXbUoxcWxQNFZubDBjQ2drWTVoMkdUMTBjalh5Wlh1TmZRTjhYTk5oRzJqRDkKcDQyWXZtTXVReFJ2TUlIdTFDOXl0TFBwY0xlVm9iL2doV20wQVFvQjdaaExrYlhUVTFqOExBN2tGbm1FCjlJaG9BeVphM1R3K1JZbmpOUkJ2Q0RxVnllU28rTk1aWmROZ0ZWY1pKbkhXb05lcHBTRXhjamZ6N094Ugp5ekRzWDhLM0IyS0RPWHZJUTlWeVpQcG5kQWRaZi90RHRxdCtTckM4YW5mdnRhMkJlZkFaZFF4cEpnRjUKd0VCdVBVR20xeUd1N0ozWi9KR3UvdW5EdVBxa29mRFlFdXBxa1VxQXlDa3ZFQzNuUG4xazA3YmFHTFd2Cm5rSDRlYWcyWDY3blJuOEZKbER4b0E4bko1MEdmTDdlVFdTSG8zRzBpc0I2NjFicGxJRFkzNjd0Y0E5ZgpMcXczcnB5eFRjb2hXWUw3eG54VTYyWDA0VU1NS2FlVGowWm9melJ5Tzg0emxZOXNGbzBSOWNESUJFRVIKbVZ0MlZSckw1K29XaVExOTB2YXlWNTZGT2xEbE1hS2VCbWZpbHB0VW5aeU5iOXBEMFVaNWpwMUpobzhwCjdXL1J5S2xQNzhJelN1UGNEM0JOWlpydHlPdmIxMko3akpXSkJpM25TdGdjZ3pwSHBwS0JtNXV6SXo0TgpNVGNJOUJTWWsvOWtqZUc5YUJWZ1I5ejFidHlIVmhleTNFY3h3U0puMVVDSnJNMU42ZHhSMWs1K05WSSsKRHBnd1VmTDZwY2FseGdpdHJJZ1R1RGNiRkNaSFBKN0ZMZGt4eC9lMzh6bWdFUjJFM1BtSWZDREpheW55Cjg0a2tOUDRhdHpwdGZXaWZURzZRTzVFSFBIaTlnNHJwRVBHTXY4RFBncjFsOVhHSG4zdS9VY2xkSnVNKwpoT1Q4RFdaZWwxa0RBMVJrSWJRR1A5T0tVOCtpb2pUZzdNYVdINThPaG1kK0NVR2tPamg1ZzluZmMvUFIKclhGMjJyQmsK.f0fd867b6937164ceffece50e5f0ab8b\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p id=\"isPasted\">You are using Terraform to deploy a workspace in AWS and you get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Failed credential validation checks</span> error message.</p><pre>\u2502 Error: MALFORMED_REQUEST: Failed credential validation checks: please use a valid cross account IAM role with permissions setup correctly \r\n\u2502 \r\n\u2502 \u00a0 with databricks_mws_credentials.this,\r\n\u2502 \u00a0 on cross-account-role.tf line 29, in resource \"databricks_mws_credentials\" \"this\":\r\n\u2502 \u00a0 29: resource \"databricks_mws_credentials\" \"this\" {\r\n\u2502</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This issue can occur due to a race condition when the cross-account role configuration is applied by Terraform. If you re-run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">terraform apply</span> after getting the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Failed credential validation checks</span> error, the operation is successful and does not result in an error message.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p id=\"isPasted\">You should add an artificial delay as a dependency for the cross-account role configuration. This prevents the race condition from occurring when using Terraform.</p><ol>\n<li>In this example cross-account role configuration file, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">time_sleep.wait\u00a0</span>has been added as a dependency.<pre>// cross-account-role.tf\r\n\r\n// Properly configure the cross-account role for the creation of new workspaces within your AWS account.\r\n// See https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/mws_credentials\r\n\r\nresource \"databricks_mws_credentials\" \"this\" {\r\n\u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0 = databricks.mws\r\n\u00a0 account_id \u00a0 \u00a0 \u00a0 = var.databricks_account_id\r\n\u00a0 role_arn \u00a0 \u00a0 \u00a0 \u00a0 = aws_iam_role.cross_account_role.arn\r\n\u00a0 credentials_name = \"${local.prefix}-creds\"\r\n\u00a0 depends_on = [\r\n\u00a0 \u00a0 time_sleep.wait\r\n\u00a0 ]\r\n}\r\n\r\n</pre>\n</li>\n<li>The duration of the delay is set to 10 seconds. You can adjust the delay length as needed.<pre>resource \"time_sleep\" \"wait\" {\r\n\u00a0 depends_on = [\r\n\u00a0 \u00a0 aws_iam_role.cross_account_role\r\n\u00a0 ]\r\n\u00a0 create_duration = \"10s\"\r\n}</pre>\n</li>\n<li>Save the updated cross-account role configuration file.</li>\n<li>Run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">terraform init</span>.</li>\n<li>Run\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">terraform apply</span>.<br><br>\n</li>\n</ol><p>After the artificial delay has been added to the cross-account role configuration you can resume normal deployments with Terraform.</p><p>Review the <a href=\"https://registry.terraform.io/providers/hashicorp/time/latest/docs/resources/sleep\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Terraform time_sleep documentation</a> for more information.</p><p>You can also review the <a href=\"https://docs.databricks.com/dev-tools/terraform/index.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Databricks Terraform documentation</a>.</p><p><br></p>", "body_txt": "Problem You are using Terraform to deploy a workspace in AWS and you get a Failed credential validation checks error message. \u2502 Error: MALFORMED_REQUEST: Failed credential validation checks: please use a valid cross account IAM role with permissions setup correctly \u2502 \u2502 \u00a0 with databricks_mws_credentials.this, \u2502 \u00a0 on cross-account-role.tf line 29, in resource \"databricks_mws_credentials\" \"this\": \u2502 \u00a0 29: resource \"databricks_mws_credentials\" \"this\" { \u2502 Cause This issue can occur due to a race condition when the cross-account role configuration is applied by Terraform. If you re-run terraform apply after getting the Failed credential validation checks error, the operation is successful and does not result in an error message. Solution You should add an artificial delay as a dependency for the cross-account role configuration. This prevents the race condition from occurring when using Terraform. In this example cross-account role configuration file, time_sleep.wait\u00a0has been added as a dependency.// cross-account-role.tf // Properly configure the cross-account role for the creation of new workspaces within your AWS account. // See https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/mws_credentials resource \"databricks_mws_credentials\" \"this\" { \u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0 = databricks.mws \u00a0 account_id \u00a0 \u00a0 \u00a0 = var.databricks_account_id \u00a0 role_arn \u00a0 \u00a0 \u00a0 \u00a0 = aws_iam_role.cross_account_role.arn \u00a0 credentials_name = \"${local.prefix}-creds\" \u00a0 depends_on = [ \u00a0 \u00a0 time_sleep.wait \u00a0 ] } The duration of the delay is set to 10 seconds. You can adjust the delay length as needed.resource \"time_sleep\" \"wait\" { \u00a0 depends_on = [ \u00a0 \u00a0 aws_iam_role.cross_account_role \u00a0 ] \u00a0 create_duration = \"10s\" } Save the updated cross-account role configuration file.\nRun terraform init.\nRun\u00a0terraform apply. After the artificial delay has been added to the cross-account role configuration you can resume normal deployments with Terraform. Review the Terraform time_sleep documentation for more information. You can also review the Databricks Terraform documentation.", "format": "html", "updated_at": "2022-10-04T17:04:44.394Z"}, "author": {"id": 888181, "email": "cedric.law@databricks.com", "name": "Cedric Law", "first_name": "Cedric", "last_name": "Law", "role_id": "draft_writer", "created_at": "2022-06-22T10:28:09.373Z", "updated_at": "2023-04-28T11:11:08.692Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 313835, "name": "Terraform", "codename": "terraform", "accessibility": 1, "description": "These articles can help you with Terraform.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2766289, "name": "aws"}, {"id": 2766283, "name": "terraform"}], "url": "https://kb.databricks.com/terraform/failed-credential-validation-checks-error-with-terraform"}, {"id": 1418276, "name": "Cannot view table SerDe properties", "views": 5751, "accessibility": 1, "description": "SHOW CREATE TABLE only returns the Apache Spark DDL. It does not show the SerDe properties.", "codename": "cannot-view-table-serde-properties", "created_at": "2022-06-23T10:30:41.269Z", "updated_at": "2022-07-01T09:03:56.744Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.66d9a4d43140be8c6933a75247371b26\"></div><h1>Problem</h1><p>You are trying to view the SerDe properties on an Apache Hive table, but <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SHOW CREATE TABLE</span> just returns the Apache Spark DDL. It does not show the SerDe properties.</p><p>For example, given this sample code:</p><pre>%sql\r\n\r\nSHOW CREATE TABLE &lt;table-identifier&gt;</pre><p><br>You get a result that does not show the SerDe properties:<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656585368108-SerDe%2001.jpeg\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Result showing table without SerDe properties.\"></p><h1>Cause</h1><p>You are using Databricks Runtime 7.3 LTS or later, which uses Spark 3.0 and above.</p><p>The usage of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SHOW CREATE TABLE</span> changed with Spark 3.0.</p><h1>Solution</h1><p data-aura-rendered-by=\"471:763;a\">To view a table's SerDe properties in Spark 3.0 and above, you need to add the option <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">AS SERDE</span> at the end of the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SHOW CREATE TABLE</span> command.</p><p data-aura-rendered-by=\"471:763;a\">For example, given this sample code:</p><pre data-aura-rendered-by=\"471:763;a\">SHOW CREATE TABLE &lt;table-identifier&gt; AS SERDE</pre><p data-aura-rendered-by=\"471:763;a\">You get a result that shows the table's SerDe properties:<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656585404020-SerDe%2002.jpeg\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Result showing table's SerDe properties.\"></p><p data-aura-rendered-by=\"471:763;a\"><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"dcce2964340c5\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStlOVExTE5jd0hLR3F4aEN5ZFpPc1AyQVB1K1VCbkNhejNHTWpPT25lVlZMU2FRZHZqCjZNNHZBdVowYi9KL1BsWFNDbmhCODkxcDZoNUFTM2xNalF1VE1qOWErSmthOWZZdzBMN1Z4ZzVzZW9yTgpvZVpwR0doTWNHaDNsWXEwdExPeHhRL3MvKzlNeldkM2FlUmxUQjBZbUNUenlDMnY4UGtKcVpMUDJxdjMKTTBxbTVheFVtV1c0WTFRcTFzL2hrRUFmVzRWaXZCQ3NtVWYzRzh4SnZyQWk1VjNvd3VTbVpiR0RQY1dTCkhTS1hSWDFvUC8xMTZlNW1WVGdHSVI5bm5wZjRvRlZLTHo5eVlST2hlT3p4Lyt1TEk5QnRaay9oSkRJMQpFdjBDMDA2cmJRZkhSSHBYYWpQaFRxWGczT2JTTjdnUzFyUzYyWVN6RE1GMnZaRXhRUVdZYzNzcENjdE0KYmhGckFVY05nSjlwMkJYVldjZWpMUUtVSW9TWFJZWDFPZzJYTlZEMUFKcldqKzU2NDRtSGhVRXB3TkxxCk5Gcjd6a09nSEdScTBVbk5DRFJVR0wzZjJjRENTM2ViWDUydzN0YUhITTdaajBHNmFkZ1E1V2t3S202WQpCL1MzcFJGdEwxczg5OWUraHU4NHd5VzYxcWdxUnN0NmhXYS9QamlSRStBMUFUMTkrTTFLQ3NhaE1qcjQKT0NpZTNpSzBuRW1UOGM4cGRpYVEyLzdJYjM4aTk5aGZXeldpVVlDdDArTE9mZVJJeHpaaEwxcHFPaEhJCnYvWXRyNVNjRWluZWl2VUNBY1o4dy9TaUZpMnB3Z0IrRkNoaXUvVDZWWWhUS1pPRmowWm9lVnJjMk5aSwpXY1NUWnhWdkhEc1FJaE8zYmlzL1dHUkhuNmlPK3c4WDUrc2lxOW1rZHZDSGs3c2VyNWxFMnIwNXFhM0gKWStRRGxUcVYzQ0tWS3RMYWdmZSs4ZHY2NzB2S25RWks3bGlMV1JPOWRjQ0xwMFBENWhLcjZWMEFXbDcyCmFDcVFhWTFBM2FGZ2I2RjF6VHErN3VNcWFxYVZqM3JUNmdFaVdOZm9ZTGJ3OVA2N3ZpRU94WlNPa2xrNAp6QnJXNHpUTzhEMVNZd0x1TmtNd0p4MnNHa0o1cmRyMlZGR1Y4MllqRmliVnhlM3ZlK3NCWWVTcHkrdTQKS3BpeUJqU2dpbjg2T3Nzd3ljOW1oejdodTB5U1RuSEx4aCt1dGpvbTRhTFN4VFhwRXR5TkNFYVlWOXN5Ck8vY2VqYVF6ejY1Q3ZqTHVYbkM4ZlJvbEVUUGZwY0dsbytia1Nka0ZhV3pZdEpIdmpaZmxBK3NLYzBsRgpONG9Wb1ZoazFBZ0JTUWEvS0x0TWx1dUcvK2V0MEZTRGFDZWZZTE13cjZsa0xZVzlNT0RpZnRvRXFFQ2YKcHJwYjJoM1J1dTNPdmRCUURvZ01xUzRGbUUrT0hOWmdCZFpMSlExUHRjK2F4dS9jYWtkNkUvZVFlcjVIClY5aVo1R3RPeGduUWtIUnN5eXFGSHJWbkIwN1RzMENQNUlrSWc2RFNRdkcrUWtQSy9DM25aYkE9Cg==.615030235b93ea3e6c01cfc88a9134ba\"></div><p><br></p>", "body_txt": "Problem You are trying to view the SerDe properties on an Apache Hive table, but SHOW CREATE TABLE just returns the Apache Spark DDL. It does not show the SerDe properties. For example, given this sample code: %sql SHOW CREATE TABLE &lt;table-identifier&gt; You get a result that does not show the SerDe properties: Cause You are using Databricks Runtime 7.3 LTS or later, which uses Spark 3.0 and above. The usage of SHOW CREATE TABLE changed with Spark 3.0. Solution To view a table's SerDe properties in Spark 3.0 and above, you need to add the option AS SERDE at the end of the SHOW CREATE TABLE command. For example, given this sample code: SHOW CREATE TABLE &lt;table-identifier&gt; AS SERDE You get a result that shows the table's SerDe properties:", "format": "html", "updated_at": "2022-07-01T09:03:56.740Z"}, "author": {"id": 789805, "email": "saritha.shivakumar@databricks.com", "name": "saritha.shivakumar ", "first_name": "saritha.shivakumar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T14:42:15.447Z", "updated_at": "2023-04-17T04:09:51.795Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2766168, "name": "aws"}, {"id": 2766169, "name": "azure"}, {"id": 2766172, "name": "ddl"}, {"id": 2766170, "name": "gcp"}, {"id": 2766171, "name": "serde"}, {"id": 2766173, "name": "spark 3"}], "url": "https://kb.databricks.com/sql/cannot-view-table-serde-properties"}, {"id": 1417998, "name": "Get last modification time for all files in Auto Loader and batch jobs", "views": 4231, "accessibility": 1, "description": "Define a UDF to list all files in the path and return the last modification time for each one.", "codename": "get-last-modification-time-for-all-files-in-auto-loader-and-batch-jobs", "created_at": "2022-06-23T06:22:05.720Z", "updated_at": "2022-12-01T00:50:27.364Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStCclRCVjhhOS9tU0JGeWh1MSt6dWhxSDI5R2J4MTlvdXpsbHVqUlBLZ3R2dXVhNXNOCnRwNjNoR0FydFo2WlVpMW41YXFCYXpaejNCenJrVCtidm0vS1BRU0dia2JxVU9ROHpFV24zNHJVQldGVAp0NmdQdGRqZE9XNERFblVTSGJzVW5iWklVaU0yK0FPTzFvajZkZXZnc2lBM042UHQvY1V1c0FhT2tMSDEKOVlvdnNBQkJSdjZyQnJkN2diRVpKVXpWMjRNSlVLVkYwWjdEUjJoYmxaV0RvTjFGY1hzYzBkbHMvcGVoCi9CWVdLUnBnUFlnNkkyVm9lTTd4OFJFSmI5U2pDYkxrVjg5TEtGWXpuYXZuOElDTXZMVjlzS04wMXVTdApnZlhSVHZ1L2J3RVZOeUlhWktHN281NDJNbVZvVlhxeFo4SzJEa3l6OWZDd1dZbkxTdUNXRTVzYldpVGMKd2twSFh6YXlrQVpyRGJ3K2FNWE5ja0NaOEo0VGRqOTU2ZTV6NG43RWU3NWYzaWxYMFE0Vloxb2hCZFJhCm9peWduR3A2T3d1R0Jnckp4TFFQS3hvenV1NlQzVXlHMmFYMEZNMW5KeTdnWm9TVkpsVTA3M3UrOVhWTgpweDdBREZRSWxFanNJeGZCWGdMYXJZeEEyNjJlV0l6dWxLc0tyYWdkbWFScFo3bGt3a0N2bWtZc1Z5S1IKTURRa05EYXN6dWJUd1NwWkpySkFEMU5UbWFzNWhGQ1JEZkdVMzdJVWRlaFE2SW1HdjVocHNoaFp0VmJhCkJrSVJBOVZOYW95TFRqTGZzRG8wMU1MTDUrSnVXSmFjY2w4Q0ZVZmh3WVFGdGNJZDJvQ0pZZnJtZmViSAoxZXpzZEJmVFRwMUNaMENwYXJaNzkwbllXRW4xOUtKR2RIM1N2cUhHbnQyWUYzclhHT09qWkZJcmFDN1QKRmtNd05mRDNtOGdvamlFdEZzdG5iZWRZSU1UN3dKenliSnJabXRhVHF5L1kxcHM3ekl4a3JkMlhTYzJvCjN1TVlSbWVPaHRraXJWSk5aMkpaY1NoZ0djUGw4T0p3ME5aSmhHNWh1ZXI1TmNkMTk3VWxmNjlWTUtQeApWM2gxTkxSMUlBZG50R0JzZ2RlZVYwc0s2L1ZsSmdHbHRiNkhKeXFYS0hrbkgvS1FSTkZwdW9RQ0wzRGYKcmxlSnh6NzdxazdIVmZaQnhucDhmcmxnbGlKN0VlYkQ3K1l0d2poY09HUi9MNlZBc3MzZEUzWGRiTmwvCjVBWWYrNEtCaHI0SGhQdmFEQWJSc2dqb2FvQkdFRWI0R1lEaFRQTUY1dkR6bjlTRCs5VHAza0cxOUNsTQp5WVQwYWdjT3FHdHJzSGlIbnFlWG1Fb2xJS2R5cE5HbEhXc2NrNUFDcWZ4cENXcFA3VFdQaEFSTXZOT1EKVjJ4TkRobEhCYk9QRFV2MDBWY0JNN0haMCtLNENGRjZRL091YURaZkpTN0VsUnIwbUNmSnF1VFRMK2pUCkZHOEpnb08xN25GOG5MYXZ6a1VsZ2VYbWRtbmVuc3dUcXRyTmxpblpFaG9KYzJTQkVxSDRnRlRlenp6TQpxcjBkT2VpdEp6bU1tSW5zNnJxUEY5d2JZRUE3c2d1MlE1SG5VeWJmRS91NkNKa1ByRm9HdmdRVGp5TVAKZ25Ibi9uRWFxSjcreG83a0Uycnp2VjNNNTZrTXhzc0FlcGVsTU5VTmE3aFVvMWNDdmV5NDRDdncyNkhLCkVOMUlTVWxSdEJ2S0IxT2k3d3NMWkR1SVBNeHh3K2lEQ2pMQWZzQk4vNjlEWThDS3dlbnFrQ3EwcExBNgpOVnZlZGRwaWt3T3NhZkd4dVJZRENhcGhyQ09CVmtWR3c4azZmSWRKNXVrQmZ3d1MzZlAxVXhKODZEaTgKOVlXUjMwN0VxV24rS1BWeG55bm15WE82U3JFV3RPTzM0KzNoTWdTS3hUbmpMRlRsUWhOMWdBNnR0SDF6Cit4S2hOTXJuWGhLUzhXTXlEVG1pQzc3dUJ1OHl1dlNUMXZudmZTSXBEeVNvSlpJdUxEdDhHeUxvb3FBZwpXQ1lGMnRvRzJZcVBZVFNCT1IwVC84QzFBM1ZqTzE2TXphamNwQUlUMXJVdS9YOVE3Y3BaT3lwMzREMXMKSWZjakdXN1ByaDF1VFJWRVloTkREVjQ4c1M3czJkSCt4Ry9CVTh6NUdWK2kxT1hZU2dvMXU4QnVCVGRRCkp5dy8ySCt0Z2xoSEd3ME9UNmpOVFhleklnUnFDWTdsa3hMS0RHMVA2ZmJqc01TTzBzL2dvZC81OU1waAp6azA1dFh1aEZBL29aYVhEOERHSWdhcG8rZnlTSWlwL0VpNGozRXl6Mlhod2pYYTRRSWlXeGVXZ0xTR1MKU2hvUzZGT0w0SjNGNXVjRHhhWFFycjJ6Sng4dVJtdGpha2M9Cg==.a67574e494837fec523e38508fb568bf\"></div><p>You are running a streaming job with Auto Loader (<a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Auto Loader\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/spark/latest/structured-streaming/auto-loader\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Auto Loader\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/spark/latest/structured-streaming/auto-loader.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Auto Loader\">GCP</a>) and want to get the last modification time for each file from the storage account.</p><h1 data-toc=\"true\" id=\"instructions-0\">Instructions</h1><p>The <a href=\"/get-file-path-auto-loader.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Get the path of files consumed by Auto Loader</a> article describes how to get the filenames and paths for all files consumed by the Auto Loader. In this article, we build on that foundation and use sample code to show you how to apply a custom UDF and then extract the last modification time for the file.</p><ol data-aura-rendered-by=\"423:794;a\">\n<li>Start out by defining your imports and variables. You need to define the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;storage-base-path&gt;</span>, as well as the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;input-dir&gt;</span>, and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;output-dir&gt;\u00a0</span>you are using.<pre>import org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\r\nimport org.apache.spark.sql.functions.{input_file_name, col, udf, from_unixtime}\r\nimport org.apache.spark.sql.types._\r\nval basePath           = \"&lt;storage-base-path&gt;\"\r\nval inputLocation    = basePath + \"&lt;input-dir&gt;\"\r\nval outputLocation = basePath + \"&lt;output-dir&gt;\"</pre>\n</li>\n<li>For this example, we need to generate sample data and store it in a DataFrame. In a practical use case, you would be reading data from your storage bucket.<pre>import org.apache.spark.sql.types._\r\n\r\nval sampleData = Seq(\r\n  Row(1, \"James\",   10, \"M\", 1000),\r\n  Row(1, \"Michael\", 20, \"F\", 2000),\r\n  Row(2, \"Robert\",  30, \"M\", 3000),\r\n  Row(2, \"Maria\",   40, \"F\", 4000),\r\n  Row(3, \"Jen\",     50, \"M\", 5000)\r\n  )\r\n\r\nval sampleSchema = StructType(Array(\r\n  StructField(\"id\", IntegerType, true),\r\n  StructField(\"name\", StringType, true),\r\n  StructField(\"age\", IntegerType, true),\r\n  StructField(\"gender\", StringType, true),\r\n  StructField(\"salary\", IntegerType, true)\r\n  ))\r\n\r\nval df = spark.createDataFrame(sc.parallelize(sampleData), sampleSchema)\r\ndf.coalesce(1).write.format(\"parquet\").partitionBy(\"id\", \"age\").mode(\"append\").save(inputLocation);\r\nspark.read.format(\"parquet\").load(inputLocation).count();</pre>\n</li>\n<li>Create a custom UDF to list all files in the storage path and return the last modification time for each file.\u00a0<pre>val getModificationTimeUDF = udf((path: String) =&gt; {\r\n\u00a0 val finalPath = new Path(path)\r\n\u00a0 val fs = finalPath.getFileSystem(conf)\r\n\u00a0 if(fs.exists(finalPath)) {fs.listStatus(new Path(path)).head.getModificationTime}\r\n\u00a0 else {-1 // Or some other value based on business decision\r\n\u00a0 \u00a0 \u00a0 \u00a0}\r\n})</pre>\n</li>\n<li>Apply the UDF to the batch job. The UDF returns each file's last modification time in UNIX time format. To convert this into a human-readable format divide by 1000 and then cast it as the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">timestamp</span>.<pre>val df = spark.read.format(\"parquet\").load(inputLocation)\r\n.withColumn(\"filePath\", input_file_name())\r\n.withColumn(\"fileModificationTime\", getModificationTimeUDF(col(\"filePath\")))\r\n.withColumn(\"fileModificationTimestamp\", from_unixtime($\"fileModificationTime\" / 1000, \"yyyy-MM-dd HH:mm:ss\").cast(TimestampType).as(\"timestamp\")).drop(\"fileModificationTime\")\r\ndisplay(df)</pre>\n</li>\n<li>Apply the UDF to the Auto Loader streaming job.<pre>val sdf = spark.readStream.format(\"cloudFiles\")\r\n.schema(sampleSchema)\r\n.option(\"cloudFiles.format\", \"parquet\")\r\n.option(\"cloudFiles.includeExistingFiles\", \"true\")\r\n.option(\"cloudFiles.connectionString\", connectionString)\r\n.option(\"cloudFiles.resourceGroup\", resourceGroup)\r\n.option(\"cloudFiles.subscriptionId\", subscriptionId)\r\n.option(\"cloudFiles.tenantId\", tenantId)\r\n.option(\"cloudFiles.clientId\", clientId)\r\n.option(\"cloudFiles.clientSecret\", clientSecret)\r\n.option(\"cloudFiles.useNotifications\", \"true\")\r\n.load(inputLocation)\r\n.withColumn(\"filePath\", input_file_name())\r\n.withColumn(\"fileModificationTime\", getModificationTimeUDF(col(\"filePath\")))\r\n.withColumn(\"fileModificationTimestamp\", from_unixtime($\"fileModificationTime\" / 1000, \"yyyy-MM-dd HH:mm:ss\").cast(TimestampType).as(\"timestamp\"))\r\n.drop(\"fileModificationTime\")\r\n\r\ndisplay(sdf)</pre>\n</li>\n</ol><p><br></p><p>To recap, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">input_file_name()</span> is used to read an absolute file path, including the file name. We then created a custom UDF to list all files from the storage path. You can get the file's last modification time from each file but it is listed in UNIX time format. Convert the UNIX time format into a readable format by dividing UNIX time by 1000 and converting it to a timestamp.</p><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"62f51285f21a2\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStCL1V2M2IzSExVUTVSdjBmZnpIU0diT1hQeTg5QTFPNktndU1aS3FVTGZ4VlhkYWNLCjVscjdTZ2Z0aytzMlVGMlhsT3NpOUNhMjNpMURGOGlWSURuUUVDZ0VPRkVQd3hRdllvNU43SVNQL3Qwagp0anJWMXFyYUNIMStuakpOQUZOaUM0TnUzRk80V0xqSFRFTVRWSWFwcDZBa0tlSW92R2daM1IycnVWUkcKUmxWOS9zbTFpQnVLSjBpSEN2aTR4c0I1SXlrS213WkVjTEFwWVR3dDNqMjRQUHVFQ1A4K0VmOUxQcTQvCk5CREtqcFBoaVc2dkMzclZSMGhvYWxTdGR2UTVZUjltblJ1YkRKbUhDT0NhOUM2dEswOTZkcTVSQTlIUQowSCtrYWtsTVQ2SDNmYUkzdWlpblBodUxlOVJwVldVSmtLOHg2eEtFdFRtSlZPcHdJaFN1UEdoSHd4Q1IKUm9FMWdYMTRpb2h2dURwdTJKeVl2eElBZG1sb1N0dTBuMndnZCtZMDdTR3NBRFQ0YzJPOVlzVlRCNENpCi90Rnc2aGQwcWtCVkIzK0hUcSs4UC9STkdqVkREREIwMThFQ0pHVk1KVXEwQlpFNDFNS0hUclEyazdFSgpSTWRsNGJ6SXVGYTlNTDNIeGdPWGtNcG5hdHE3QmpSOGs3YWhmNUltMDhiKzVMKzFFdFdicStwcnd3TEwKb2Y0NTJIYzZ0SGZyV3I3WUNCcno1anJpOVkvOXhqTHhSeHhEblZseXhYd1dUMk12eStqQjJ0SlpRSE42ClZ0QlpMK2hwMlJBUWRaaDJMZmNxWm9oaVNTY1poaFVJK2JuUFFsU3BBQmN5SnlRMngwajhQZm83dTgxMwpKOWhqdE1lc0hJdnN2dHBvYUxnRWFCSnUrR1NwU3IvTW9aM3h4UXE2SzhmSnVUVFF1czBFOTJ6cGZsT24KZFlMd21KNVZGYUcrblR5SkJyUmlLRnNENjg1aklFYzFDQzlKNnV0aUc0eVgxTlRXVlAreGlCS0RkbTFtCmJnTmFyNUZ2YU1FbTZ4cWRMdng2SngwTVNvRWZTdVFqYTV6SXp4WVdRRTMyNTdmWXFvNHQrL1lhSTlrZgo5dWVGc2hiNzJpUjN5UERiZi9VMzlhSzRBRW9sRnVkcm9sVm5OSGQ5Y0g4d3ZCMEJWVkU4NjFSVDljamEKK3MzNUZKcFFUNjJUaXR3ZXcxVWFjMWhIaUFQckx2NXV4V0RiUmZlWTQ5aE9ZOGRuVGU3Y2tURTI3K0lQCnlKY2FWTi8vc0V5clYzWjRQaUZjZm40UFk2VkVmbE1SbnNkQzNWUjRXTzlSbkZENXcwcmltU09OQ0x4SAprNWRLQXdSOFN5RnJNT3lZUExmMkdPblZoeDNkazY1cVJYOVlwbWxmUWp0THQ5WT0K.05cfb1564ba8067be21c948c1f7d0def\"></div><p><br></p><p><br></p>", "body_txt": "You are running a streaming job with Auto Loader (AWS | Azure | GCP) and want to get the last modification time for each file from the storage account. Instructions The Get the path of files consumed by Auto Loader article describes how to get the filenames and paths for all files consumed by the Auto Loader. In this article, we build on that foundation and use sample code to show you how to apply a custom UDF and then extract the last modification time for the file. Start out by defining your imports and variables. You need to define the &lt;storage-base-path&gt;, as well as the &lt;input-dir&gt;, and &lt;output-dir&gt;\u00a0you are using.import org.apache.hadoop.conf.Configuration import org.apache.hadoop.fs.{FileStatus, FileSystem, Path} import org.apache.spark.sql.functions.{input_file_name, col, udf, from_unixtime} import org.apache.spark.sql.types._ val basePath = \"&lt;storage-base-path&gt;\" val inputLocation = basePath + \"&lt;input-dir&gt;\" val outputLocation = basePath + \"&lt;output-dir&gt;\" For this example, we need to generate sample data and store it in a DataFrame. In a practical use case, you would be reading data from your storage bucket.import org.apache.spark.sql.types._ val sampleData = Seq( Row(1, \"James\", 10, \"M\", 1000), Row(1, \"Michael\", 20, \"F\", 2000), Row(2, \"Robert\", 30, \"M\", 3000), Row(2, \"Maria\", 40, \"F\", 4000), Row(3, \"Jen\", 50, \"M\", 5000) ) val sampleSchema = StructType(Array( StructField(\"id\", IntegerType, true), StructField(\"name\", StringType, true), StructField(\"age\", IntegerType, true), StructField(\"gender\", StringType, true), StructField(\"salary\", IntegerType, true) )) val df = spark.createDataFrame(sc.parallelize(sampleData), sampleSchema) df.coalesce(1).write.format(\"parquet\").partitionBy(\"id\", \"age\").mode(\"append\").save(inputLocation); spark.read.format(\"parquet\").load(inputLocation).count(); Create a custom UDF to list all files in the storage path and return the last modification time for each file.\u00a0val getModificationTimeUDF = udf((path: String) =&gt; { \u00a0 val finalPath = new Path(path) \u00a0 val fs = finalPath.getFileSystem(conf) \u00a0 if(fs.exists(finalPath)) {fs.listStatus(new Path(path)).head.getModificationTime} \u00a0 else {-1 // Or some other value based on business decision \u00a0 \u00a0 \u00a0 \u00a0} }) Apply the UDF to the batch job. The UDF returns each file's last modification time in UNIX time format. To convert this into a human-readable format divide by 1000 and then cast it as the timestamp.val df = spark.read.format(\"parquet\").load(inputLocation) .withColumn(\"filePath\", input_file_name()) .withColumn(\"fileModificationTime\", getModificationTimeUDF(col(\"filePath\"))) .withColumn(\"fileModificationTimestamp\", from_unixtime($\"fileModificationTime\" / 1000, \"yyyy-MM-dd HH:mm:ss\").cast(TimestampType).as(\"timestamp\")).drop(\"fileModificationTime\") display(df) Apply the UDF to the Auto Loader streaming job.val sdf = spark.readStream.format(\"cloudFiles\") .schema(sampleSchema) .option(\"cloudFiles.format\", \"parquet\") .option(\"cloudFiles.includeExistingFiles\", \"true\") .option(\"cloudFiles.connectionString\", connectionString) .option(\"cloudFiles.resourceGroup\", resourceGroup) .option(\"cloudFiles.subscriptionId\", subscriptionId) .option(\"cloudFiles.tenantId\", tenantId) .option(\"cloudFiles.clientId\", clientId) .option(\"cloudFiles.clientSecret\", clientSecret) .option(\"cloudFiles.useNotifications\", \"true\") .load(inputLocation) .withColumn(\"filePath\", input_file_name()) .withColumn(\"fileModificationTime\", getModificationTimeUDF(col(\"filePath\"))) .withColumn(\"fileModificationTimestamp\", from_unixtime($\"fileModificationTime\" / 1000, \"yyyy-MM-dd HH:mm:ss\").cast(TimestampType).as(\"timestamp\")) .drop(\"fileModificationTime\") display(sdf) To recap, input_file_name() is used to read an absolute file path, including the file name. We then created a custom UDF to list all files from the storage path. You can get the file's last modification time from each file but it is listed in UNIX time format. Convert the UNIX time format into a readable format by dividing UNIX time by 1000 and converting it to a timestamp.", "format": "html", "updated_at": "2022-12-01T00:50:27.356Z"}, "author": {"id": 488152, "email": "dd.sharma@databricks.com", "name": "DD Sharma", "first_name": "DD", "last_name": "Sharma", "role_id": "admin", "created_at": "2021-10-07T02:59:41.776Z", "updated_at": "2023-02-16T02:48:14.950Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256866, "name": "Streaming", "codename": "streaming", "accessibility": 1, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2765353, "name": "autoloader"}, {"id": 2765350, "name": "aws"}, {"id": 2765351, "name": "azure"}, {"id": 2765352, "name": "gcp"}, {"id": 2907851, "name": "streaming"}], "url": "https://kb.databricks.com/streaming/get-last-modification-time-for-all-files-in-auto-loader-and-batch-jobs"}, {"id": 1415971, "name": "Understanding speculative execution", "views": 5427, "accessibility": 1, "description": "Learn how speculative execution works, how to identify it, and when you should use it.", "codename": "understanding-speculative-execution", "created_at": "2022-06-21T22:43:42.012Z", "updated_at": "2022-11-07T21:56:04.567Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"1dc36f69d06f5\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.c61c264fd99db804cba8b8b11b84fe98\"></div><h1 data-toc=\"true\" id=\"speculative-execution-0\">Speculative execution\u00a0</h1><p>Speculative execution can be used to automatically re-attempt a task that is not making progress compared to other tasks in the same stage.</p><p>This means if one or more tasks are running slower in a stage, they will be re-launched. The task that completes first is marked as successful. The other attempt gets killed.</p><h1 data-toc=\"true\" id=\"implementation-1\">Implementation\u00a0</h1><p data-toc=\"true\" id=\"when-a-job-hangs-intermittently-and-one-or-more-tasks-are-hanging-it-is-generally-a-good-idea-to-enable-speculative-execution-as-a-first-step-to-resolve-this-issue-as-a-result-of-speculative-execution-the-slow-hanging-task-that-is-not-progressing-can-be-re-attempted-in-another-node-to-help-solve-the-problem-2\">When a job hangs intermittently and one or more tasks are hanging, enabling speculative execution is often the first step to resolving the issue. As a result of speculative execution, the slow hanging task that is not progressing is re-attempted in another node.</p><p data-toc=\"true\" id=\"it-means-that-if-one-or-more-tasks-are-running-slower-in-a-stage-they-will-be-relaunched-upon-successful-completion-of-the-relaunched-task-it-will-mark-the-original-task-as-failed-if-the-original-task-attempt-gets-completed-first-then-it-will-make-the-original-task-attempt-as-successful-and-will-kill-the-duplicate-task-3\">It means that if one or more tasks are running slower in a stage, the tasks are relaunched. Upon successful completion of the relaunched task, the original task is marked as failed. If the original task completes before the relaunched task, the original task attempt is marked as successful and the relaunched task is killed.</p><p data-toc=\"true\" id=\"in-addition-to-the-speculative-configuration-there-are-a-few-additional-settings-that-can-be-tweaked-as-needed-speculative-execution-should-only-be-enabled-when-necessary-4\">In addition to the speculative execution, there are a few additional settings that can be tweaked as needed. Speculative execution should only be enabled when necessary.</p><p>Below are the major configuration options for speculative execution.</p><table><tbody>\n<tr>\n<td><div><strong>Configuration</strong></div></td>\n<td><div><strong>Description</strong></div></td>\n<td><div><strong>Databricks Default</strong></div></td>\n<td><div><strong>OSS Default</strong></div></td>\n</tr>\n<tr>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation</span></div></td>\n<td>If set to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span>, performs speculative execution of tasks. This means if one or more tasks are running slowly in a stage, they will be re-launched.</td>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span></div></td>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span></div></td>\n</tr>\n<tr>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation.interval</span></div></td>\n<td>How often Spark will check for tasks to speculate.</td>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">100ms</span></div></td>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">100ms</span></div></td>\n</tr>\n<tr>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation.multiplier</span></div></td>\n<td>How many times slower a task is than the median to be considered for speculation. \u00a0 \u00a0</td>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">3</span></div></td>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">1.5</span></div></td>\n</tr>\n<tr>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation.quantile</span></div></td>\n<td>Fraction of tasks which must be complete before speculation is enabled for a particular stage</td>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">0.9</span></div></td>\n<td><div><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">0.75</span></div></td>\n</tr>\n</tbody></table><h2 data-aura-rendered-by=\"423:770;a\" data-toc=\"true\" id=\"how-to-interpret-the-databricks-default-values-2\">How to interpret the Databricks default values</h2><p>If speculative execution is enabled (<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation</span>), then every 100 ms (<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation.interval</span>), Apache Spark checks for slow running tasks. A task is marked as a slow running task if it is running more than three times longer (<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation.multiplier</span>) the median execution time of completed tasks. Spark waits until 90% (<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation.quantile</span>) of the tasks have been completed before starting speculative execution.</p><h2 data-toc=\"true\" id=\"identifying-speculative-execution-in-action-3\">Identifying speculative execution in action</h2><ul>\n<li>Review the task attempts in the Spark UI. If speculative execution is running, you see one task with the <strong>Status</strong> as <strong>Success</strong> and the other task with a <strong>Status</strong> of <strong>TaskKilled</strong>.</li>\n<li>Speculative execution will not always start, even though there are slow tasks. This is because the criteria for speculative execution must be met before it starts running. This typically happens on stages with a small number of tasks, with only one or two tasks getting stuck. If the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation.quantile</span> is not met, speculative execution does not start.</li>\n</ul><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1667595232478-1667595232478.png\" class=\"fr-fic fr-dib\"></p><h2 data-aura-rendered-by=\"423:770;a\" data-toc=\"true\" id=\"when-to-enable-speculative-execution-4\">When to enable speculative execution</h2><ul data-aura-rendered-by=\"423:770;a\">\n<li>Speculative execution can be used to unblock a Spark application when a few tasks are running for longer than expected and the cause is undetermined. Once a root cause is determined, you should resolve the underlying issue and disable speculative execution.\u00a0</li>\n<li>Speculative execution ensures that the speculated tasks are not scheduled on the same executor as the original task. This means that issues caused by a bad VM instance are easily mitigated by enabling speculative execution.</li>\n</ul><h2 data-toc=\"true\" id=\"when-not-to-run-speculative-execution-5\">When not to run speculative execution</h2><ul data-aura-rendered-by=\"423:770;a\">\n<li>Speculative execution should not be used for a long time period on production jobs for a long time period. Extended use can result in failed tasks.</li>\n<li>If the operations performed in the task are not idempotent, speculative execution should not be enabled.\u00a0</li>\n<li>If you have data skew, the speculated task can take as long as the original task, leaving the original task to succeed and the speculated task to get killed. Speculative execution does not guaranteed the speculated task will finish first.</li>\n<li>Enabling speculative execution can impact performance so it should only be used for troubleshooting. If you require speculative execution to complete your workloads, open a <a href=\"https://help.databricks.com/s/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Databricks support request</a>. Databricks support can help determine the root cause of the task slowness.</li>\n</ul><h2 data-toc=\"true\" id=\"-6\"><br></h2>", "body_txt": "Speculative execution\u00a0 Speculative execution can be used to automatically re-attempt a task that is not making progress compared to other tasks in the same stage. This means if one or more tasks are running slower in a stage, they will be re-launched. The task that completes first is marked as successful. The other attempt gets killed. Implementation\u00a0 When a job hangs intermittently and one or more tasks are hanging, enabling speculative execution is often the first step to resolving the issue. As a result of speculative execution, the slow hanging task that is not progressing is re-attempted in another node. It means that if one or more tasks are running slower in a stage, the tasks are relaunched. Upon successful completion of the relaunched task, the original task is marked as failed. If the original task completes before the relaunched task, the original task attempt is marked as successful and the relaunched task is killed. In addition to the speculative execution, there are a few additional settings that can be tweaked as needed. Speculative execution should only be enabled when necessary. Below are the major configuration options for speculative execution. Configuration Description Databricks Default OSS Default spark.speculation If set to true, performs speculative execution of tasks. This means if one or more tasks are running slowly in a stage, they will be re-launched. false false spark.speculation.interval How often Spark will check for tasks to speculate. 100ms 100ms spark.speculation.multiplier How many times slower a task is than the median to be considered for speculation. \u00a0 \u00a0 3 1.5 spark.speculation.quantile Fraction of tasks which must be complete before speculation is enabled for a particular stage 0.9 0.75 How to interpret the Databricks default values If speculative execution is enabled (spark.speculation), then every 100 ms (spark.speculation.interval), Apache Spark checks for slow running tasks. A task is marked as a slow running task if it is running more than three times longer (spark.speculation.multiplier) the median execution time of completed tasks. Spark waits until 90% (spark.speculation.quantile) of the tasks have been completed before starting speculative execution. Identifying speculative execution in action Review the task attempts in the Spark UI. If speculative execution is running, you see one task with the Status as Success and the other task with a Status of TaskKilled.\nSpeculative execution will not always start, even though there are slow tasks. This is because the criteria for speculative execution must be met before it starts running. This typically happens on stages with a small number of tasks, with only one or two tasks getting stuck. If the spark.speculation.quantile is not met, speculative execution does not start. When to enable speculative execution Speculative execution can be used to unblock a Spark application when a few tasks are running for longer than expected and the cause is undetermined. Once a root cause is determined, you should resolve the underlying issue and disable speculative execution.\u00a0\nSpeculative execution ensures that the speculated tasks are not scheduled on the same executor as the original task. This means that issues caused by a bad VM instance are easily mitigated by enabling speculative execution. When not to run speculative execution Speculative execution should not be used for a long time period on production jobs for a long time period. Extended use can result in failed tasks.\nIf the operations performed in the task are not idempotent, speculative execution should not be enabled.\u00a0\nIf you have data skew, the speculated task can take as long as the original task, leaving the original task to succeed and the speculated task to get killed. Speculative execution does not guaranteed the speculated task will finish first.\nEnabling speculative execution can impact performance so it should only be used for troubleshooting. If you require speculative execution to complete your workloads, open a Databricks support request. Databricks support can help determine the root cause of the task slowness.", "format": "html", "updated_at": "2022-11-07T21:56:04.562Z"}, "author": {"id": 886320, "email": "mounika.tarigopula@databricks.com", "name": "mounika.tarigopula ", "first_name": "mounika.tarigopula", "last_name": "", "role_id": "draft_writer", "created_at": "2022-06-16T22:08:21.261Z", "updated_at": "2023-04-24T19:52:43.731Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2923229, "name": "aws"}, {"id": 2923230, "name": "azure"}, {"id": 2923233, "name": "best practices"}, {"id": 2923231, "name": "gcp"}, {"id": 2923234, "name": "how to"}, {"id": 2923232, "name": "speculate"}], "url": "https://kb.databricks.com/scala/understanding-speculative-execution"}, {"id": 1412564, "name": "Unable to infer schema for ORC error", "views": 4080, "accessibility": 1, "description": "Apache Spark returns an error for ORC files if no schema is defined when reading from an empty directory or a base path with multiple subfolders.", "codename": "unable-to-infer-schema-for-orc-error", "created_at": "2022-06-17T07:22:22.693Z", "updated_at": "2022-12-01T06:23:13.606Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.e9bf583894d3c792e973516d2987280e\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are trying to read ORC files from a directory when you get an error message:</p><pre data-aura-rendered-by=\"371:771;a\">org.apache.spark.sql.AnalysisException: Unable to infer schema for ORC. It must be specified manually.</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>An <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Unable to infer the schema for ORC</span> error occurs when the schema is not defined and Apache Spark cannot infer the schema due to:</p><ul>\n<li>An empty directory.</li>\n<li>Using the base path instead of the complete path to the files when there are multiple subfolders containing ORC files.</li>\n</ul><h2 data-toc=\"true\" id=\"empty-directory-example-2\">Empty directory example</h2><ol data-aura-rendered-by=\"421:771;a\">\n<li>Create an empty directory <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/tmp/testorc_empty</span>.<pre>%sh mkdir /dbfs/tmp/testorc_empty</pre>\n</li>\n<li>Attempt to read the directory.<pre>val df = spark.read.orc(\"dbfs:/tmp/testorc_empty\")</pre>\n</li>\n<li>The read fails with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Unable to infer the schema for ORC</span> error.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1663687375897-1663687375897.png\" class=\"fr-fic fr-dii\">\n</li>\n</ol><p><br></p><h2 data-toc=\"true\" id=\"base-path-example-3\">Base path example</h2><p>When only the base path is given (instead of the complete path) and there are multiple subfolders containing orc files, a read attempt returns the error: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Unable to infer the schema for ORC</span>.</p><ol>\n<li>Create multiple folders under <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/tmp/testorc</span>.<pre>import org.apache.hadoop.fs.Path\r\nval basePath = \"dbfs:/tmp/testorc\"\r\nspark.range(1).toDF(\"a\").write.orc(new Path(basePath, \"first\").toString)\r\nspark.range(1,2).toDF(\"a\").write.orc(new Path(basePath, \"second\").toString)\r\nspark.range(2,3).toDF(\"a\").write.orc(new Path(basePath, \"third\").toString)</pre>\n</li>\n<li>Attempt to read the directory <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/tmp/testorc</span>.<pre>val df = spark.read.orc(basePath)</pre>\n</li>\n<li>The read fails with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Unable to infer scheme for ORC</span> error.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1663687446202-1663687446202.png\" class=\"fr-fic fr-dib\">\n</li>\n</ol><h1 data-toc=\"true\" id=\"solution-4\">Solution</h1><h2 data-toc=\"true\" id=\"empty-directory-solution-5\">Empty directory solution</h2><ol>\n<li id=\"isPasted\">Create an empty directory <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/tmp/testorc_empty</span>.<pre>%sh mkdir /dbfs/tmp/testorc_empty</pre>\n</li>\n<li>Include the schema when you attempt to read the directory.<pre>val df_schema = spark.read.schema(\"a int\").orc(\"dbfs:/tmp/testorc_empty\") </pre>\n</li>\n<li>The read attempt does not return an error.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1663687482998-1663687482998.png\" class=\"fr-fic fr-dib\">\n</li>\n</ol><h2 data-toc=\"true\" id=\"base-path-solution-6\">Base path solution</h2><ol>\n<li>Create multiple folders under <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/tmp/testorc</span>.<pre>import org.apache.hadoop.fs.Path\r\nval basePath = \"dbfs:/tmp/testorc\"\r\nspark.range(1).toDF(\"a\").write.orc(new Path(basePath, \"first1\").toString)\r\nspark.range(1,2).toDF(\"a\").write.orc(new Path(basePath, \"second2\").toString)\r\nspark.range(2,3).toDF(\"a\").write.orc(new Path(basePath, \"third3\").toString)</pre>\n</li>\n<li>Include the schema and a full path to one of the subfolders when you attempt to read the directory. In this example, we are using the path to the folder <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/third3/</span>.<pre>val dfWithSchema = spark.read.schema(\"a long\").orc(basePath + \"/third3/\")</pre>\n</li>\n<li data-aura-rendered-by=\"471:771;a\">The read attempt does not return an error.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1663687532340-1663687532340.png\" class=\"fr-fic fr-dib\">\n</li>\n</ol><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1663687549779-1663687549779.png\" class=\"fr-fic fr-dib\"></p><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"3ef9a210e2c3b\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTkzcnQxQ3lIVEZleHZMNzZTSWFJK2dIVVUwL2FCaERGVW81ajM3eWRoeE5aNlVJTmw0CjRMbjVrb3dEWkxwT1FNSlNkWXJ3VDVsTThPQ25FdEdHZGFuMjErQk1XYlU0bTd2elEvZjlOeU5wVW5ZNQpZMVpWYm0zUmpvZ05zR1BLOFFySXZsSThPZWgvQ3BSZnU4ZzRHQTkzcVpQSFNxQjVaWHhYTVI0aUVSVkgKMFhqYkE1MnFUYWpZTERDamhKMGh4ZXJITkk1TC9PSlV6ZERjaW1iUmJwakNQa25Kbk9GL1F5UGxlbTRQCmxkWlZmMWk4akl4SG13Ym9FSHFpVmVWU3RHMm9vL2pFQ1RCalA0d1BMSXM2VFJMSTlhdExBNURQZ3IyVApXeG1FUFpyelgrT2dFZHpITDVZQkNnZHB0ZUVJVjVUWXFCenRBa0NsZURyazlGa0RqdCtHcU1sZzh4WjgKMkxZaWhuOU1NQUdZeDUxVGwyUnBzNWd0ckdQOWdwTTVicVIrdk4yZ0xsRGFnMXVQck5SZWJ4U2xqQTV2CmdiendnUzdPR29tUE4wdVZOMnRGL2pVM0Nrb3lTcnU3NUFkc1huczJoY3l3aEhrOC9hMExOYm5pdnJ5dgpLcHE2WUFtUFRMK2dBV2I5eVB1ajhRNWlXMTMyZ0h6M3RHQ1BtSEprMEg3eHgyKzZoSlJCQ0lyVnBCS1gKcldRQ0F3WTg1SlRxTW5idy8wUVR1UzVHMFpSWm54NVh0bTRtUUNvYWZVcG5ucXBnQWwzSXM4MlBqR3NFCmsyRzBFb1YydWx4aCs0SDJKZHVHajlNc2pvRSt3TjlrWTUwdEd6ZlVabGhGb2xNSTNkNGNIVUJydDQ2WQpLRzZ4Mk4vVG16enlRMHVFckpUaWEzazgxd0s5QkJaaCtaRTJ0MzZrNHI3L0t5YWtrYjZCV2VJRUN6cXUKcXhVQk5EZ2ovUGI1a3N0NFpnOVhCcGZ2NmhCWmk0cVpPTGpUUmFNa2FOemc5d01xT3poZS9PUjRmb0xCCkN3a2xqRHIwYXlla1NGKzZNSGRTRzNpU3BvNXNONzc4SlJwOHN0ZzBSMFhxVFZ5M2k0dVRUczNCdzZ5TQpDOERtdnIxWGxjK2dFdlNRVnNMTk5pcGFwZFdocnR1cllVYkFNRnRPa0QyUm05MGdKWUNBcEpERktCTjMKTlpXTis1VHRGd3F5cktCTDBlYVpCNHJXTTJENkU3cSt6TFhLeTBJPQo=.b78398bfb83265aa5647d9e9bd62442b\"></div><p><br></p><p><br></p>", "body_txt": "Problem You are trying to read ORC files from a directory when you get an error message: org.apache.spark.sql.AnalysisException: Unable to infer schema for ORC. It must be specified manually. Cause An Unable to infer the schema for ORC error occurs when the schema is not defined and Apache Spark cannot infer the schema due to: An empty directory.\nUsing the base path instead of the complete path to the files when there are multiple subfolders containing ORC files. Empty directory example Create an empty directory /tmp/testorc_empty.%sh mkdir /dbfs/tmp/testorc_empty Attempt to read the directory.val df = spark.read.orc(\"dbfs:/tmp/testorc_empty\") The read fails with an Unable to infer the schema for ORC error. Base path example When only the base path is given (instead of the complete path) and there are multiple subfolders containing orc files, a read attempt returns the error: Unable to infer the schema for ORC. Create multiple folders under /tmp/testorc.import org.apache.hadoop.fs.Path val basePath = \"dbfs:/tmp/testorc\" spark.range(1).toDF(\"a\").write.orc(new Path(basePath, \"first\").toString) spark.range(1,2).toDF(\"a\").write.orc(new Path(basePath, \"second\").toString) spark.range(2,3).toDF(\"a\").write.orc(new Path(basePath, \"third\").toString) Attempt to read the directory /tmp/testorc.val df = spark.read.orc(basePath) The read fails with an Unable to infer scheme for ORC error. Solution Empty directory solution Create an empty directory /tmp/testorc_empty.%sh mkdir /dbfs/tmp/testorc_empty Include the schema when you attempt to read the directory.val df_schema = spark.read.schema(\"a int\").orc(\"dbfs:/tmp/testorc_empty\") The read attempt does not return an error. Base path solution Create multiple folders under /tmp/testorc.import org.apache.hadoop.fs.Path val basePath = \"dbfs:/tmp/testorc\" spark.range(1).toDF(\"a\").write.orc(new Path(basePath, \"first1\").toString) spark.range(1,2).toDF(\"a\").write.orc(new Path(basePath, \"second2\").toString) spark.range(2,3).toDF(\"a\").write.orc(new Path(basePath, \"third3\").toString) Include the schema and a full path to one of the subfolders when you attempt to read the directory. In this example, we are using the path to the folder /third3/.val dfWithSchema = spark.read.schema(\"a long\").orc(basePath + \"/third3/\") The read attempt does not return an error.", "format": "html", "updated_at": "2022-12-01T06:23:13.575Z"}, "author": {"id": 838921, "email": "chandana.koppal@databricks.com", "name": "chandana.koppal ", "first_name": "chandana.koppal", "last_name": "", "role_id": "draft_writer", "created_at": "2022-04-04T10:55:10.687Z", "updated_at": "2023-03-20T04:23:14.357Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2955972, "name": "aws"}, {"id": 2955976, "name": "azure"}, {"id": 2955975, "name": "gcp"}], "url": "https://kb.databricks.com/data/unable-to-infer-schema-for-orc-error"}, {"id": 1412560, "name": "to_json() results in Cannot use null as map key error", "views": 5585, "accessibility": 1, "description": "You must filter or replace null values in your input data before using to_json().", "codename": "to_json-results-in-cannot-use-null-as-map-key-error", "created_at": "2022-06-17T06:53:15.316Z", "updated_at": "2022-07-22T11:09:55.007Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.9ebbf92b77999c8fab5887c920b71f67\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">to_json()</span> to convert data to JSON and you get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Cannot use null as map key</span> error:</p><pre>RuntimeException: Cannot use null as map key. </pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">to_json()</span> function does not support using null values as the input map keys.</p><p>This example code causes the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Cannot use null as map key</span> error when run, because of the null value used as a map key in the fourth line.</p><pre id=\"isPasted\">%sql\r\n\r\nselect\r\n\u00a0 to_json(\r\n\u00a0 \u00a0 map(\r\n\u00a0 \u00a0 \u00a0 1, 'Databricks',\r\n\u00a0 \u00a0 \u00a0 2, 'Map',\r\n\u00a0 \u00a0 \u00a0 3, 'Error',\r\n\u00a0 \u00a0 \u00a0 null, 'Data'\r\n\u00a0 \u00a0 )\r\n\u00a0 ) as json;</pre><h1 data-toc=\"true\" id=\"solution-2\">Solution\u00a0</h1><p>You should filter out any null values present in the input data before running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">to_json()</span>, or use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nvl()</span> to replace all of the null values with non-null values.</p><h2 data-toc=\"true\" id=\"filter-null-values-3\">Filter null values</h2><p>Consider this example DataFrame:</p><pre>+---+----------+-------+\r\n| Id| \u00a0 \u00a0 Value|address|\r\n+---+----------+-------+\r\n| \u00a01|Databricks| \u00a0 null|\r\n| \u00a02| \u00a0 \u00a0 \u00a0 Map| \u00a0 null|\r\n| \u00a03| \u00a0 \u00a0 Error| \u00a0 \u00a0xyz|\r\n+---+----------+-------+</pre><p>There are two null values in the example.</p><p>Attempting to use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">to_json()</span> on this DataFrame will return an error.</p><p>We can filter the null data by showing only the rows that have non-null values.</p><p>For example, filtering with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">df.filter(\"address is not null\").show()</span> returns:</p><pre id=\"isPasted\">+---+-----+-------+\r\n| Id|Value|address|\r\n+---+-----+-------+\r\n| \u00a03|Error| \u00a0 \u00a0xyz|\r\n+---+-----+-------+</pre><p>This filtered DataFrame does not contain any null values, so it can now be used as an input with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">to_json()</span>.</p><h2 data-toc=\"true\" id=\"replace-null-values-with-replacements-4\">Replace null values with replacements</h2><p>If you cannot filter out the null values, you can use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nvl()</span> to replace the null values with non-null values.\u00a0</p><p>The sample code originally had a null value as the map key for the fourth line. Since that results in an error, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nvl()</span> is used in this updated example to substitute 4 for the null value.</p><pre>%sql\r\n\r\nselect\r\n\u00a0 to_json(\r\n\u00a0 \u00a0 map(\r\n\u00a0 \u00a0 \u00a0 1, 'Databricks',\r\n\u00a0 \u00a0 \u00a0 2, 'Map',\r\n\u00a0 \u00a0 \u00a0 3, 'Error',\r\n nvl(null, 4), 'Data'\r\n\u00a0 \u00a0 )\r\n  ) as JSON;</pre><p><br></p><div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"a697675f46497\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlIeWhOSzlEWG5GVVMzeUd3b1AvcXgwNm9HaUdkdytUMmxhRGg4ZG5WVE53SWVnTXpuCkxnQWxoSzc2SUp4VTV5Si91YUY2NFN5OWtHYTdTR0pvQWpJV2o4cXFhenJ3ejc4ZUlhdkUyYWtDSzBkOAp2cXZVQWk5RzRYNnp1bnBpdWZxOXV5MGFNb3pEcTQwZ0xHajlUdm45YkdnMEZzcXBESWdydE1ZaUQ1QjYKUWI0NjdLUDFrR25HUEJURnhLVnlhNTNOYU01T0piZjV3RVZUL25TVk5YLzJNMGVuazYzdVFoSUUwTEJqCm5QZXJ5bmFESWM1dktsRVIxL3l1ZWJiY2pyRG40YWJrSVdQVUxZZ3VMS3QzZTc5N2crSmlmdDBkSXNPRgpNSFBhRURIVE1aN1JpaDdnblU4SklZS2U2WERoN1Bvdy9IdWZxRHJQRkdVNmtSazNjZXUyL21Md3p3d3MKVjFvRjQwYnBPRGRrUVBTTFNaenJoQ1QraHZYby83R3h4bVRlNlNDOXIwTHB6bXFKeU9BZ0laNWlnZGltCjBiUjZ3eFM4eGZQand3OVNRNFRPaks5dFpWUnVveFNuMEVLcy85Sk5aSXJqYmkwcTIySFpyUEFrd3pPMAp4Vy9WaGJ6ZURTZnR4eTlnMkx2NC93NkY0cjBaaWR4YWhDZU83OS9HZldlcVduVzg1cXVQeWdpeC9GZnEKUVVkSFowR2s0NFdJcGxpaEZlUGMwTlRwMmw4QWlDdFdBTWczdFRyUWVDaHhBQzdEMDBYbnhnZ3UrdC9NCkU4OGhGSmZ1RENwY1FpYmtzWTBpS2w4elV2dWVoMVVTRUxYaFE3b3lJT3NyTDkrOEpXbld1NzRDOHVhUgpiQW5lbWJWN01oZGpqMlByQzYrbC81T0ZRZDR3Zk1BZnU2UGtFdTlPQkFIWkFsNFRYNTAzUytDMHA2NysKZlc5TzBPb0Z2bng4dEpzVldDZXg2TmF0cjNIVmJaZGppM2k2cjIzKzR5RVZkcm1kdnJ4NDA2bDlNcVkyCkRVaDYra1YwM094NFJRa1FGcjRWQ0YzVm1aVjdOMXpnWUs4OC9aNGlrRmU4Ri9jQWVBS0tXakF2TXd5bQpDS0lZbVR5Rys0L1VzcjFMQ0tPOWtndTRGdGdqd1NHNzNncjg3QVVxbGg2cGV0dEtGYWkzV2tiMUJXQjIKWi9ndjdVdjVuTnZmcThFVXd1M2JiekRHZzE4UXlQMXFYbm9pVXhDazZ4blRGalBURDdaKzlINU9tbDVHCktoUG01bmxLSVF0N2gzaGtwV3RtU1E9PQo=.d13cee68bb443bf8fc1957d7b844e7e7\"></div><p><a href=\"https://cust-success.cloud.databricks.com/#notebook/2103904/command/2103909\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\"></a><a href=\"//s.cloud.databricks.com/#notebook/2103904/command/2103909\"></a></p><p><br></p>", "body_txt": "Problem You are using to_json() to convert data to JSON and you get a Cannot use null as map key error: RuntimeException: Cannot use null as map key. Cause to_json() function does not support using null values as the input map keys. This example code causes the Cannot use null as map key error when run, because of the null value used as a map key in the fourth line. %sql select \u00a0 to_json( \u00a0 \u00a0 map( \u00a0 \u00a0 \u00a0 1, 'Databricks', \u00a0 \u00a0 \u00a0 2, 'Map', \u00a0 \u00a0 \u00a0 3, 'Error', \u00a0 \u00a0 \u00a0 null, 'Data' \u00a0 \u00a0 ) \u00a0 ) as json; Solution\u00a0 You should filter out any null values present in the input data before running to_json(), or use nvl() to replace all of the null values with non-null values. Filter null values Consider this example DataFrame: +---+----------+-------+ | Id| \u00a0 \u00a0 Value|address| +---+----------+-------+ | \u00a01|Databricks| \u00a0 null| | \u00a02| \u00a0 \u00a0 \u00a0 Map| \u00a0 null| | \u00a03| \u00a0 \u00a0 Error| \u00a0 \u00a0xyz| +---+----------+-------+ There are two null values in the example. Attempting to use to_json() on this DataFrame will return an error. We can filter the null data by showing only the rows that have non-null values. For example, filtering with df.filter(\"address is not null\").show() returns: +---+-----+-------+ | Id|Value|address| +---+-----+-------+ | \u00a03|Error| \u00a0 \u00a0xyz| +---+-----+-------+ This filtered DataFrame does not contain any null values, so it can now be used as an input with to_json(). Replace null values with replacements If you cannot filter out the null values, you can use nvl() to replace the null values with non-null values.\u00a0 The sample code originally had a null value as the map key for the fourth line. Since that results in an error, nvl() is used in this updated example to substitute 4 for the null value. %sql select \u00a0 to_json( \u00a0 \u00a0 map( \u00a0 \u00a0 \u00a0 1, 'Databricks', \u00a0 \u00a0 \u00a0 2, 'Map', \u00a0 \u00a0 \u00a0 3, 'Error', nvl(null, 4), 'Data' \u00a0 \u00a0 ) ) as JSON;", "format": "html", "updated_at": "2022-07-22T11:09:55.002Z"}, "author": {"id": 801519, "email": "gopal.goel@databricks.com", "name": "gopal.goel ", "first_name": "gopal.goel", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-09T07:39:18.550Z", "updated_at": "2023-04-26T13:57:09.553Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2757713, "name": "aws"}, {"id": 2757715, "name": "azure"}, {"id": 2757714, "name": "gcp"}, {"id": 2779941, "name": "json"}, {"id": 2779943, "name": "map key"}, {"id": 2779942, "name": "null"}, {"id": 2781441, "name": "spark sql"}, {"id": 2781442, "name": "to_json"}], "url": "https://kb.databricks.com/sql/to_json-results-in-cannot-use-null-as-map-key-error"}, {"id": 1412158, "name": "Explicit path to data or a defined schema required for Auto loader", "views": 9476, "accessibility": 1, "description": "If you do not specify an explicit path to your data or define your data schema, you get an IllegalArgumentException error when you start an Auto loader job.", "codename": "explicit-path-to-data-or-a-defined-schema-required-for-auto-loader", "created_at": "2022-06-16T22:12:36.584Z", "updated_at": "2022-10-12T22:24:52.341Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.9b2f774fb6ec0ce3544283618f665479\"></div><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-0\">Info</h3>\n<p class=\"hj-alert-text\">This article applies to Databricks Runtime 9.1 LTS and above.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"problem-1\">Problem</h1><p>You are using Auto Loader to ingest data for your ELT pipeline when you get an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IllegalArgumentException: Please provide the source directory path with option `path`</span> error message.</p><p>You get this error when you start an Auto Loader job, if either the path to the data or the data schema is not defined.</p><pre>Error:\r\nIllegalArgumentException \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Traceback (most recent call last)\r\n&lt;command-1874749868040573&gt; in &lt;module&gt;\r\n\u00a0 \u00a0 \u00a01 df = (\r\n----&gt; 2 \u00a0 \u00a0spark\r\n\u00a0 \u00a0 \u00a03 \u00a0 \u00a0.readStream.format(\"cloudFiles\")\r\n\u00a0 \u00a0 \u00a04 \u00a0 \u00a0.options(**{\r\n\u00a0 \u00a0 \u00a05 \u00a0 \u00a0 \u00a0 \u00a0\"cloudFiles.format\": \"csv\",\r\n/databricks/spark/python/pyspark/sql/streaming.py in load(self, path, format, schema, **options)\r\n\u00a0 \u00a0480 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0return self._df(self._jreader.load(path))\r\n\u00a0 \u00a0481 \u00a0 \u00a0 \u00a0 \u00a0else:\r\n--&gt; 482 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0return self._df(self._jreader.load())\r\n\u00a0 \u00a0483\r\n\u00a0 \u00a0484 \u00a0 \u00a0def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\r\n/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)\r\n\u00a0 1302\r\n\u00a0 1303 \u00a0 \u00a0 \u00a0 \u00a0answer = self.gateway_client.send_command(command)\r\n-&gt; 1304 \u00a0 \u00a0 \u00a0 \u00a0return_value = get_return_value(\r\n\u00a0 1305 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0answer, self.gateway_client, self.target_id, self.name)\r\n\u00a0 1306\r\n/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\r\n\u00a0 \u00a0121 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Hide where the exception came from that shows a non-Pythonic\r\n\u00a0 \u00a0122 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# JVM exception message.\r\n--&gt; 123 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0raise converted from None\r\n\u00a0 \u00a0124 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0else:\r\n\u00a0 \u00a0125 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0raise\r\nIllegalArgumentException: Please provide the source directory path with option `path`</pre><h1 data-toc=\"true\" id=\"cause-2\">Cause</h1><p>Auto Loader requires you to provide the path to your data location, or for you to define the schema. If you provide a path to the data, Auto Loader attempts to infer the data schema. If you do not provide the path, Auto Loader cannot infer the schema and requires you to explicitly define the data schema.</p><p id=\"isPasted\">For example, if a value for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;input-path&gt;</span> is not included in this sample code, the error is generated when you start your Auto Loader job.</p><pre id=\"isPasted\">%python\r\n\r\ndf = spark.readStream.format(\"cloudFiles\") \\\r\n.option(&lt;cloudFiles-option&gt;, &lt;option-value&gt;) \\\r\n.load()</pre><p>If a value for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;input-path&gt;</span> is included in this sample code, the Auto Loader job can infer the schema when it starts and will not generate the error.</p><pre>%python\r\n\r\ndf = spark.readStream.format(\"cloudFiles\") \\\r\n.option(&lt;cloudFiles-option&gt;, &lt;option-value&gt;) \\\r\n.load(&lt;input-path&gt;)</pre><h1 data-toc=\"true\" id=\"solution-3\">Solution</h1><p id=\"isPasted\">You have to provide either the path to your data or the data schema when using Auto Loader.</p><p>If you do not specify the path, then the data schema MUST be defined.</p><p>For example, this sample code has the data schema defined, but no path specified. Because the data schema was defined, the path is optional. This does not generate an error when the Auto Loader job is started.</p><pre>%python\r\n\r\ndf = spark.readStream.format(\"cloudFiles\") \\\r\n.option(&lt;cloudFiles-option&gt;, &lt;option-value&gt;) \\\r\n.schema(&lt;schema&gt;) \\\r\n.load()</pre><p><br></p>", "body_txt": "Info\nThis article applies to Databricks Runtime 9.1 LTS and above. Problem You are using Auto Loader to ingest data for your ELT pipeline when you get an IllegalArgumentException: Please provide the source directory path with option `path` error message. You get this error when you start an Auto Loader job, if either the path to the data or the data schema is not defined. Error: IllegalArgumentException \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Traceback (most recent call last) &lt;command-1874749868040573&gt; in &lt;module&gt; \u00a0 \u00a0 \u00a01 df = ( ----&gt; 2 \u00a0 \u00a0spark \u00a0 \u00a0 \u00a03 \u00a0 \u00a0.readStream.format(\"cloudFiles\") \u00a0 \u00a0 \u00a04 \u00a0 \u00a0.options(**{ \u00a0 \u00a0 \u00a05 \u00a0 \u00a0 \u00a0 \u00a0\"cloudFiles.format\": \"csv\", /databricks/spark/python/pyspark/sql/streaming.py in load(self, path, format, schema, **options) \u00a0 \u00a0480 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0return self._df(self._jreader.load(path)) \u00a0 \u00a0481 \u00a0 \u00a0 \u00a0 \u00a0else: --&gt; 482 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0return self._df(self._jreader.load()) \u00a0 \u00a0483 \u00a0 \u00a0484 \u00a0 \u00a0def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None, /databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) \u00a0 1302 \u00a0 1303 \u00a0 \u00a0 \u00a0 \u00a0answer = self.gateway_client.send_command(command) -&gt; 1304 \u00a0 \u00a0 \u00a0 \u00a0return_value = get_return_value( \u00a0 1305 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0answer, self.gateway_client, self.target_id, self.name) \u00a0 1306 /databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw) \u00a0 \u00a0121 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Hide where the exception came from that shows a non-Pythonic \u00a0 \u00a0122 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# JVM exception message. --&gt; 123 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0raise converted from None \u00a0 \u00a0124 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0else: \u00a0 \u00a0125 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0raise IllegalArgumentException: Please provide the source directory path with option `path` Cause Auto Loader requires you to provide the path to your data location, or for you to define the schema. If you provide a path to the data, Auto Loader attempts to infer the data schema. If you do not provide the path, Auto Loader cannot infer the schema and requires you to explicitly define the data schema. For example, if a value for &lt;input-path&gt; is not included in this sample code, the error is generated when you start your Auto Loader job. %python df = spark.readStream.format(\"cloudFiles\") \\ .option(&lt;cloudFiles-option&gt;, &lt;option-value&gt;) \\ .load() If a value for &lt;input-path&gt; is included in this sample code, the Auto Loader job can infer the schema when it starts and will not generate the error. %python df = spark.readStream.format(\"cloudFiles\") \\ .option(&lt;cloudFiles-option&gt;, &lt;option-value&gt;) \\ .load(&lt;input-path&gt;) Solution You have to provide either the path to your data or the data schema when using Auto Loader. If you do not specify the path, then the data schema MUST be defined. For example, this sample code has the data schema defined, but no path specified. Because the data schema was defined, the path is optional. This does not generate an error when the Auto Loader job is started. %python df = spark.readStream.format(\"cloudFiles\") \\ .option(&lt;cloudFiles-option&gt;, &lt;option-value&gt;) \\ .schema(&lt;schema&gt;) \\ .load()", "format": "html", "updated_at": "2022-10-12T22:24:52.331Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256866, "name": "Streaming", "codename": "streaming", "accessibility": 1, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2898092, "name": "9.1"}, {"id": 2898096, "name": "autoloader"}, {"id": 2898093, "name": "aws"}, {"id": 2898094, "name": "azure"}, {"id": 2898091, "name": "dbr 9.1"}, {"id": 2898095, "name": "gcp"}, {"id": 2898097, "name": "pathless"}], "url": "https://kb.databricks.com/streaming/explicit-path-to-data-or-a-defined-schema-required-for-auto-loader"}, {"id": 1407867, "name": "Job fails with ExecutorLostFailure due to \u201cOut of memory\u201d error", "views": 6901, "accessibility": 1, "description": "Resolve executor failures where the root cause is due to the executor running out of memory..", "codename": "job-fails-with-executorlostfailure-due-to-out-of-memory-error", "created_at": "2022-06-15T16:43:18.684Z", "updated_at": "2022-11-07T21:56:23.711Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.5d5caec9b2cc3c9aac7e5b49a7754215\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Job fails with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ExecutorLostFailure</span> error message.</p><pre>ExecutorLostFailure (executor &lt;1&gt; exited caused by one of the running tasks) Reason: Executor heartbeat timed out after &lt;148564&gt; ms</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p id=\"isPasted\">The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ExecutorLostFailure</span> error message means one of the executors in the Apache Spark cluster has been lost. This is a generic error message which can have more than one root cause. In this article, we will look how to resolve issues when the root cause is due to the executor running out of memory\u00a0</p><p>Let's say your executor has too much data to process and the amount of memory available in the executor is not sufficient to process the amount of data, then this issue could occur. For e.g. if the executor in your cluster has 24GB capacity and if the cumulative amount of the data size corresponding to all the tasks that are getting executed on that executor is greater than 24GB, then this issue could occur</p><p data-toc=\"true\" id=\"how-do-you-find-if-the-oom-is-the-reason-for-the-executor-getting-lost-2\"><strong>How do you determine if OOM is the reason for the executor getting lost?\u00a0</strong></p><ol>\n<li>Open the Spark UI.</li>\n<li>Click <strong>Stages</strong>.</li>\n<li>Click <strong>Failed stages</strong>.</li>\n<li>Click the <strong>description</strong> that corresponds to the failed stage.</li>\n<li>Review the bottom of the <strong>stage details</strong> page.</li>\n<li>Sort the list of tasks on the <strong>error</strong> column.</li>\n</ol><p>The error messages describe why a specific task failed. If you see an error message that says <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">out of memory</span>, or a similar error like <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.lang.OutOfMemoryError\u00a0</span>it means the task failed because the executor ran out of memory.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>When an executor is failing due to running out of memory, you should review the following items.</p><h2 data-toc=\"true\" id=\"is-there-a-data-skew-3\">\n<strong>Is there a data skew?</strong>\u00a0</h2><p>Check whether the data is equally distributed across executors, or if there is any skew in the data.</p><p>You can find this by checking the <strong>stage summary</strong> table on the <strong>stage details</strong> page of the Spark UI.</p><p>If there is data skew and if this is the only executor that has more data in it, you need to resolve the skew to prevent the executor from running out of memory.\u00a0</p><p>In most cases Adaptive Query Execution (AQE) automatically detects data skew and resolves the issue. However, there are some edge cases where AQE may not detect data skew correctly. Please review Why didn\u2019t AQE detect my data skew? (<a href=\"https://docs.databricks.com/optimizations/aqe.html#why-didnt-aqe-detect-my-data-skew\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Why didn\u2019t AQE detect my data skew?\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/optimizations/aqe#why-didnt-aqe-detect-my-data-skew\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Why didn\u2019t AQE detect my data skew?\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/optimizations/aqe.html#why-didnt-aqe-detect-my-data-skew\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Why didn\u2019t AQE detect my data skew?\">GCP</a>) for more information.</p><p>If you are having trouble resolving data skew, you can try increasing the number of partitions or by explicitly mentioning the skew hints as explained in the <a href=\"https://kb.databricks.com/data/skew-hints-in-join.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">How to specify skew hints in dataset and DataFrame-based join commands</a> article.</p><p id=\"isPasted\">A partition is considered skewed when both <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">(partition size &gt; skewedPartitionFactor * median partition size)</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">(partition size &gt; skewedPartitionThresholdInBytes)</span> are true.</p><p>For example, given a median partition size of 200 MB, if any partition exceeds 1 GB (200 MB * 5 (five is the default <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">skewedPartitionFactor</span> value)), it is considered skewed. Under this example, if you have a partition size of 900 MB it wouldn't be considered as skewed with the default settings.</p><p>Now say your application code does a lot of transformations on the data (like <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">explode</span>, cartesian join, etc.). If you are performing a high number of transformations, you can overwhelm the executor, even if the partition isn't normally considered skewed.</p><p>Using our example defaults, you may find that a 900 MB partition is too much to successfully process. If that is the case, you should reduce the <span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard;'>skewedPartitionFactor</span> value. By reducing this value to 4, the system then considers any partition over 800 MB as skewed and automatically assigns the appropriate skew hints.</p><p>Please review the AQE documentation on dynamically handling skew join (<a href=\"https://docs.databricks.com/optimizations/aqe.html#dynamically-handle-skew-join\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"dynamically handling skew join\">AWS</a> | <a href=\"https://learn.microsoft.com/en-us/azure/databricks/optimizations/aqe#dynamically-handle-skew-join\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"dynamically handling skew join\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/optimizations/aqe.html#dynamically-handle-skew-join\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"dynamically handling skew join\">GCP</a>) for more information.</p><h2 data-toc=\"true\" id=\"is-the-executor-capable-enough-4\"><strong>Is the executor capable enough?</strong></h2><p>If data is equally distributed across all executors and you still see out of memory errors, the executor does not have enough resources to handle the load you are trying to run.</p><p>Increase horizontally by increasing the number of workers and/or increase vertically by selecting a <strong>Worker type</strong> with more memory when creating your clusters.</p><h2 data-toc=\"true\" id=\"is-it-a-properly-configured-streaming-job-5\"><strong>Is it a properly configured streaming job?</strong></h2><p>If there is no apparent data skew, but the executor is still getting too much data to process, you should use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">maxFilesPerTrigger</span> and/or the trigger frequency settings to reduce the amount of data that is processed at any one time.</p><p>Reducing the load on the executors also helps reduce the memory requirement, at the expense of slightly higher latency. In exchange for the increase in latency, the streaming job processed streaming events in a more controlled manner. A steady flow of events is reliably processed with every micro batch.</p><p>Please review the <a href=\"/_questions/1419188\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Optimize streaming transactions with .trigger\">Optimize streaming transactions with .trigger</a> article for more information. You should also review the Spark Structured Streaming Programming Guide documentation on <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"input sources\">input sources</a> and <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"triggers\">triggers</a>.\u00a0</p><p>If you want to increase the speed of the processing, you need to increase the number of executors in your cluster. You can also repartition the input streaming DataFrame, so the number of tasks is less than or equal to the number of cores in the cluster.</p><p><br></p>", "body_txt": "Problem Job fails with an ExecutorLostFailure error message. ExecutorLostFailure (executor &lt;1&gt; exited caused by one of the running tasks) Reason: Executor heartbeat timed out after &lt;148564&gt; ms Cause The ExecutorLostFailure error message means one of the executors in the Apache Spark cluster has been lost. This is a generic error message which can have more than one root cause. In this article, we will look how to resolve issues when the root cause is due to the executor running out of memory\u00a0 Let's say your executor has too much data to process and the amount of memory available in the executor is not sufficient to process the amount of data, then this issue could occur. For e.g. if the executor in your cluster has 24GB capacity and if the cumulative amount of the data size corresponding to all the tasks that are getting executed on that executor is greater than 24GB, then this issue could occur How do you determine if OOM is the reason for the executor getting lost?\u00a0 Open the Spark UI.\nClick Stages.\nClick Failed stages.\nClick the description that corresponds to the failed stage.\nReview the bottom of the stage details page.\nSort the list of tasks on the error column. The error messages describe why a specific task failed. If you see an error message that says out of memory, or a similar error like java.lang.OutOfMemoryError\u00a0it means the task failed because the executor ran out of memory. Solution When an executor is failing due to running out of memory, you should review the following items. Is there a data skew?\u00a0 Check whether the data is equally distributed across executors, or if there is any skew in the data. You can find this by checking the stage summary table on the stage details page of the Spark UI. If there is data skew and if this is the only executor that has more data in it, you need to resolve the skew to prevent the executor from running out of memory.\u00a0 In most cases Adaptive Query Execution (AQE) automatically detects data skew and resolves the issue. However, there are some edge cases where AQE may not detect data skew correctly. Please review Why didn\u2019t AQE detect my data skew? (AWS | Azure | GCP) for more information. If you are having trouble resolving data skew, you can try increasing the number of partitions or by explicitly mentioning the skew hints as explained in the How to specify skew hints in dataset and DataFrame-based join commands article. A partition is considered skewed when both (partition size &gt; skewedPartitionFactor * median partition size) and (partition size &gt; skewedPartitionThresholdInBytes) are true. For example, given a median partition size of 200 MB, if any partition exceeds 1 GB (200 MB * 5 (five is the default skewedPartitionFactor value)), it is considered skewed. Under this example, if you have a partition size of 900 MB it wouldn't be considered as skewed with the default settings. Now say your application code does a lot of transformations on the data (like explode, cartesian join, etc.). If you are performing a high number of transformations, you can overwhelm the executor, even if the partition isn't normally considered skewed. Using our example defaults, you may find that a 900 MB partition is too much to successfully process. If that is the case, you should reduce the skewedPartitionFactor value. By reducing this value to 4, the system then considers any partition over 800 MB as skewed and automatically assigns the appropriate skew hints. Please review the AQE documentation on dynamically handling skew join (AWS | Azure | GCP) for more information. Is the executor capable enough? If data is equally distributed across all executors and you still see out of memory errors, the executor does not have enough resources to handle the load you are trying to run. Increase horizontally by increasing the number of workers and/or increase vertically by selecting a Worker type with more memory when creating your clusters. Is it a properly configured streaming job? If there is no apparent data skew, but the executor is still getting too much data to process, you should use maxFilesPerTrigger and/or the trigger frequency settings to reduce the amount of data that is processed at any one time. Reducing the load on the executors also helps reduce the memory requirement, at the expense of slightly higher latency. In exchange for the increase in latency, the streaming job processed streaming events in a more controlled manner. A steady flow of events is reliably processed with every micro batch. Please review the Optimize streaming transactions with .trigger article for more information. You should also review the Spark Structured Streaming Programming Guide documentation on input sources and triggers.\u00a0 If you want to increase the speed of the processing, you need to increase the number of executors in your cluster. You can also repartition the input streaming DataFrame, so the number of tasks is less than or equal to the number of cores in the cluster.", "format": "html", "updated_at": "2022-11-07T21:56:23.707Z"}, "author": {"id": 790229, "email": "mathan.pillai@databricks.com", "name": "mathan.pillai ", "first_name": "mathan.pillai", "last_name": "", "role_id": "admin", "created_at": "2022-01-26T19:28:01.464Z", "updated_at": "2023-04-28T22:28:56.701Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2929158, "name": "aws"}, {"id": 2929159, "name": "azure"}, {"id": 2929161, "name": "best practices"}, {"id": 2929169, "name": "driver"}, {"id": 2929168, "name": "executor"}, {"id": 2929160, "name": "gcp"}, {"id": 2929162, "name": "guide"}, {"id": 2929163, "name": "overview"}, {"id": 2929167, "name": "partition"}, {"id": 2929171, "name": "streaming"}, {"id": 2929170, "name": "worker"}], "url": "https://kb.databricks.com/scala/job-fails-with-executorlostfailure-due-to-out-of-memory-error"}, {"id": 1407861, "name": "Job fails with ExecutorLostFailure because executor is busy", "views": 8625, "accessibility": 1, "description": "Resolve executor failures where the root cause is due to the executor being busy.", "codename": "job-fails-with-executorlostfailure-because-executor-is-busy", "created_at": "2022-06-15T16:34:58.544Z", "updated_at": "2022-11-07T21:56:16.540Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.0751e96229970659dc2332274e379d57\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>Job fails with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ExecutorLostFailure</span> error message.</p><pre>ExecutorLostFailure (executor &lt;1&gt; exited caused by one of the running tasks) Reason: Executor heartbeat timed out after &lt;148564&gt; ms</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ExecutorLostFailure</span> error message means one of the executors in the Apache Spark cluster has been lost. This is a generic error message which can have more than one root cause. In this article, we will look how to resolve issues when the root cause is due to the executor being busy.</p><p>This can happen if the load is too high and the executor is not able to send a heartbeat signal to the driver within a predefined threshold time. If this happens, the driver considers the executor lost.</p><p><strong>How do you determine if a high CPU load is the reason for the executor getting lost?\u00a0</strong></p><p>To confirm that the executors are busy, you should check the Ganglia metrics and review the CPU loads.</p><p>If the CPU loads are high:</p><ul>\n<li>you have too many small files\u00a0</li>\n<li>your job may be launching too many API calls/requests</li>\n<li>you don't have an optimum partition strategy\u00a0</li>\n<li>you don't have a compute intensive cluster</li>\n</ul><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Here is what you need to do based on different causes of failure.</p><h2 data-toc=\"true\" id=\"are-there-too-many-small-files-3\"><strong>Are there too many small files?</strong></h2><p>Compact small files to bigger files. Delta Lake supports <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> (<a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-optimize.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/spark/latest/spark-sql/language-manual/delta-optimize\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/spark/latest/spark-sql/language-manual/delta-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">GCP</a>) which is used to compact files. Auto optimize on Databricks (<a href=\"https://docs.databricks.com/optimizations/auto-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Auto optimize on Databricks\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/optimizations/auto-optimize\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Auto optimize on Azure Databricks\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/optimizations/auto-optimize.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Auto optimize on Databricks\">GCP</a>) automatically compacts small files during writes.</p><p>If you are not using Delta Lake, you should plan to create larger files before writing data to your tables. This can be achieved by applying <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">repartition()</span> before writing files to the table location.\u00a0</p><h2 data-toc=\"true\" id=\"are-there-too-many-api-requests-4\"><strong>Are there too many API requests?</strong></h2><p>Attempt to minimize the number of API calls made by your job. This applies to both external API services and any Databricks REST API calls. One way to do this is by repartitioning the source data so it contains a smaller number of partitions and then making the necessary API calls.</p><p>It is also good to determine why you have a high number of API requests. For example, if the root cause is too many small files, you should compact as many small files as possible into large files. A few large files is recommended over many small files.</p><h2 data-toc=\"true\" id=\"are-there-too-many-partitions-5\"><strong>Are there too many partitions?</strong></h2><p>One common mistake is over-partitioning of data sources. Instances with multiple thousands of partitions is not optimal. Ideally, you should have a small number of partitions that can be processed in parallel by the available cores your cluster.</p><p>Ensure that your partitions have as few levels as possible. For example, instead of using a partition structure like <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">yr=/month=/day=/</span>, you could reduce the date to a single level. For example, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date=yyyy-MM-dd</span>.</p><p>Avoid partitioning data based on a column that has high cardinality, like id columns. Instead pick a column that is commonly used in your queries, but has a lower cardinality.</p><h2 data-toc=\"true\" id=\"are-the-cpu-loads-in-ganglia-metrics-high-6\"><strong>Are the CPU loads in Ganglia metrics high?</strong></h2><ul>\n<li>If you have high CPU loads.</li>\n<li>If the cluster utilization is more than 80%.</li>\n<li>If you notice certain nodes in the Ganglia metrics are red.</li>\n</ul><p>It means you are not using the right type of cluster for the workload.</p><p>Make sure your selected cluster has enough CPU cores and available memory. You can also try using a <strong>Compute optimized</strong> worker type instead of the default <strong>Storage optimized</strong> worker type for your cluster.</p><p><br></p>", "body_txt": "Problem Job fails with an ExecutorLostFailure error message. ExecutorLostFailure (executor &lt;1&gt; exited caused by one of the running tasks) Reason: Executor heartbeat timed out after &lt;148564&gt; ms Cause The ExecutorLostFailure error message means one of the executors in the Apache Spark cluster has been lost. This is a generic error message which can have more than one root cause. In this article, we will look how to resolve issues when the root cause is due to the executor being busy. This can happen if the load is too high and the executor is not able to send a heartbeat signal to the driver within a predefined threshold time. If this happens, the driver considers the executor lost. How do you determine if a high CPU load is the reason for the executor getting lost?\u00a0 To confirm that the executors are busy, you should check the Ganglia metrics and review the CPU loads. If the CPU loads are high: you have too many small files\u00a0\nyour job may be launching too many API calls/requests\nyou don't have an optimum partition strategy\u00a0\nyou don't have a compute intensive cluster Solution Here is what you need to do based on different causes of failure. Are there too many small files? Compact small files to bigger files. Delta Lake supports OPTIMIZE (AWS | Azure | GCP) which is used to compact files. Auto optimize on Databricks (AWS | Azure | GCP) automatically compacts small files during writes. If you are not using Delta Lake, you should plan to create larger files before writing data to your tables. This can be achieved by applying repartition() before writing files to the table location.\u00a0 Are there too many API requests? Attempt to minimize the number of API calls made by your job. This applies to both external API services and any Databricks REST API calls. One way to do this is by repartitioning the source data so it contains a smaller number of partitions and then making the necessary API calls. It is also good to determine why you have a high number of API requests. For example, if the root cause is too many small files, you should compact as many small files as possible into large files. A few large files is recommended over many small files. Are there too many partitions? One common mistake is over-partitioning of data sources. Instances with multiple thousands of partitions is not optimal. Ideally, you should have a small number of partitions that can be processed in parallel by the available cores your cluster. Ensure that your partitions have as few levels as possible. For example, instead of using a partition structure like yr=/month=/day=/, you could reduce the date to a single level. For example, date=yyyy-MM-dd. Avoid partitioning data based on a column that has high cardinality, like id columns. Instead pick a column that is commonly used in your queries, but has a lower cardinality. Are the CPU loads in Ganglia metrics high? If you have high CPU loads.\nIf the cluster utilization is more than 80%.\nIf you notice certain nodes in the Ganglia metrics are red. It means you are not using the right type of cluster for the workload. Make sure your selected cluster has enough CPU cores and available memory. You can also try using a Compute optimized worker type instead of the default Storage optimized worker type for your cluster.", "format": "html", "updated_at": "2022-11-07T21:56:16.534Z"}, "author": {"id": 790229, "email": "mathan.pillai@databricks.com", "name": "mathan.pillai ", "first_name": "mathan.pillai", "last_name": "", "role_id": "admin", "created_at": "2022-01-26T19:28:01.464Z", "updated_at": "2023-04-28T22:28:56.701Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2929144, "name": "aws"}, {"id": 2929145, "name": "azure"}, {"id": 2929148, "name": "best practices"}, {"id": 2929156, "name": "driver"}, {"id": 2929155, "name": "executor"}, {"id": 2929146, "name": "gcp"}, {"id": 2929149, "name": "guide"}, {"id": 2929151, "name": "overview"}, {"id": 2929153, "name": "partition"}, {"id": 2929157, "name": "worker"}], "url": "https://kb.databricks.com/scala/job-fails-with-executorlostfailure-because-executor-is-busy"}, {"id": 1403220, "name": "KB Style Guide", "views": 74, "accessibility": 0, "description": "How to use the different formatting options within a KB article.", "codename": "kb-style-guide", "created_at": "2022-06-13T23:16:10.155Z", "updated_at": "2022-06-30T04:37:26.571Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStEcnpaa0RJcWJDa0x6NG1DMFZjMFhBQ2s1QS9pWlZqNzJoQm9ucjBPazFsVFhoRmZhCk9La2ZXeU1KTy9tdlNXYlhJWjhvL1ZmTDRCb1NTVnlDUFhySWw2TFJwNVJ6UUFLdkVFUmRneGM5VTYrQgpZbDF1Uk1ySCtoR3dsSmNOczVyQUl1cUNTU1NVNmUwem5yY1RjbTdrUUpiOGhNTDE5OHFJUDNCZXUrNGwKTHc3TW4yTHRJeFk2eEQwR2FqZlFMbWhwK3dEVXdqMU8xTFRWYlJjRHpOWW5TQkI4bmtzNkZ5Tm9CS0kzCkpoM0t5MmxJUmlKcTllb2RhTHU1WWUrdnNQSGZHOFpMNHZJalhiV21JdjdGZFBMSTRMeVhYZTZNWDhIWgo0VDhyZ2VLenU2R29qRTVob3FQK1VWaDg0N04zV3hKQUIzaVV6VGpXUUxFZmFNc0RBbmdYTEpocEowbFAKTE5aUjZOMGNxQ2xzUUx3QXdwaWVqeUd6WWZLZjhIK1NpTnZCZnRjQWtXRmpucTlMaWlySTR6ODVuUlNZCk9uSElXVFVkNGZoVlBOVTlVckNXbHdaYmI5VXp0VVNaanQ4SW52UmZNY2ZaYnVhTjM2WjZiMFVnT09rOQpGSzFtTGJVbklFcTlBM2NqUEFGamcrbXV1OVp1MHJQend4cXAwL1Fwb2puRlFQYjFxQkJucFcvMFFxbzQKc2RZRm1RUUxsZjZnU2hLc1k4ZWdic2tNOFNlTm14aFVJVlRuYUxlVVpTV05OSGZvQ09WUWlveVZ1SjlDCmNFNm1qdnFaWFNWU21DSGtSbmxsNTUvakd5R3BqZ0h2YVEvSEhuNGczbDBIZ2R4SzRnTUk1UVNPMngxKwpONkt1blMxV1ZuSm5RVTNPemwzU3k3VUpSRkVudGdxMWxCNU9CRDNnT05VUTh5bzBEOTZXVFRIRi9peUMKWWhLMW52WU44Qnk4OTZicDNCUTBCd05sWTNnczl3RUIvRnZxUmZVQ3BlOFRwZ3FhVGVaamltUmJxMkJICnlVNjVodnRja3BPZGh1cTQ0WHJtL1pFQkxWc0Q4SllSYW55b0RsV1lrZ2s5VkV6c0JsaklwOTZDTmFSUAp3N1ppcVhmMG03aEpnbEZhM21lV2RiaHQweTF1N3B3SjFLZUFyaHltZDc1UW8xQjVzNXd3TnVPdU9ubGoKdlMrWVZpOWVaV1B3cGlUdGhOQ1dnUEtMd1JXa1ZmL1BFUEh5U1MvL0NodzErQXNTaU15cEh2YjQzZHRWCnZvMEJVUTVQbXovOStQS0ZxZnlxV2Q3d1dBNVlVVFNXRm1XY1RmZWx6Zm9icjBxV0hHMVJyN2Q5eUg2SApDZHluZlZ5dmtVMlFhVGZ5dDBLN2JvNWFlMjMreC9VKzkrdnVGS00vZG1uSy9qTVRSNUVpZWtXMEp5ZGcKWXJLZXBYM3cvQnd1ZmNXMDFUMk03SWJvZDNxRGJ0UklqVTNCU1IzVWl0TU1BdkU4eklueE9EYjhHOGhFClZSTzVGMXhPUGRmYmJMWThvRmxHVWtTVlExMXVaR3c5dnRGSGNWRGIxdjFCQUVhOWJYbFd6dHI0Nlc5WApSbEhWbXA3cHludGd2TDk3dUd2ZDZTbUpMQ2pHa0JIVmVNNjgvYVRoK01BN3dhelg0ekhTUVZaZ1ZCOD0K.a4192495b517dcbcd415377e8940954f\"></div><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" id=\"info-0\">Info</h3>\n<p class=\"hj-alert-text\">This style guide provides an overview of the formatting options to use when authoring a KB article in Helpjuice. Bookmark this page, as it is subject to change.</p>\n</div>\n</div><h1>Article title</h1><p>Make sure that you enter an article title at the top of the article. DO NOT leave the default template title.</p><h1>Article subtitle</h1><p>This should be a one line sentence that summarizes your article.</p><h1 id=\"code-blocks-2\">Code blocks</h1><pre id=\"isPasted\">Pre-formatted code</pre><p>Pre-formatted code blocks use the <strong>Code</strong> text type. This should only be used for stand-alone blocks of code. It should NOT be used for in-line code.</p><p>Highlight the code block and then select <strong>Code</strong> from the text type menu to convert it from <strong>Normal Text</strong> to\u00a0<strong>Code</strong> format.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656563434114-Helpjuice%20code%20block%20formatting.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><p>Identify the type of code block, by placing the magic command (<a href=\"https://docs.databricks.com/notebooks/notebooks-use.html#mix-languages\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"magic command\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/notebooks/notebooks-use#mix-languages\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"magic command\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/notebooks/notebooks-use.html#mix-languages\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"magic command\">GCP</a>) for the language on the first line of the code block, followed by an empty line.</p><p>For example:</p><pre>%python\r\n\r\nThis is sample Python code.</pre><h1>Code inline</h1><p>Inline code should have the selected words using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Times New Roman</span> font. All other body text should be using the default font.<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656563545721-Helpjuice%20inline%20code%20example.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><h1>Images</h1><p>Images should be uploaded and inserted into the page with the\u00a0<strong>Insert Image</strong> button.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656563700215-Helpjuice%20Insert%20Image.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><ol>\n<li>Place the cursor where you want the image on the page.</li>\n<li>Click the <strong>Insert Image</strong> button.</li>\n<li>Drag and drop the image file onto the <strong>Drop image</strong> box.</li>\n</ol><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Warning</h3>\n<p class=\"hj-alert-text\">If you copy-and-paste an image from another tool it may look like it has uploaded to Helpjuice, but it is only copying a link to the image location. Other users will NOT be able to view the image. ALL images must be uploaded.</p>\n</div>\n</div><h1>Keywords</h1><p>Keywords are entered at the bottom of the article. Keywords should ALWAYS include the applicable cloud versions (ex AWS, Azure, and/or GCP). Keywords can also include any applicable items you think would be appropriate for SEO.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1655240731114-Helpjuice%20keywords.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><h1>Links</h1><p>Because articles can apply to multiple clouds, links to Docs must be supplied for each cloud. This is done in the format of &lt;link text&gt; (&lt;cloud&gt; | &lt;cloud&gt; | &lt;cloud&gt;). For example, if I wanted a link to the default Docs site for each cloud I might say \"review the Databricks documentation (<a href=\"https://docs.databricks.com/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks documentation\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks documentation\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks documentation\">GCP</a>) for more information.\" Off-site links that are not cloud specific, can be linked as normal. For example, \"You should review the Apache Spark documentation.\"</p><p>When adding a link, the following fields should be filled in:</p><ul>\n<li>\n<strong>URL:</strong> The target URL.</li>\n<li>\n<strong>Text:</strong> This is the text that has the clickable link applied.</li>\n<li>\n<strong>Title on Mouseover:</strong> If you are creating a multi-cloud link, this should be the link text. If you are linking to a single off-site location, this can be left blank.</li>\n<li>\n<strong>Open in new tab:</strong> This should be checked.</li>\n</ul><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1655241073072-Helpjuice%20link%20configuration.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></p><h1 id=\"metadata-3\">Metadata</h1><p>The beginning of each article has a restricted metadata section. This is used to keep track of internal information that tells us about the article and who is responsible for the content.</p><p><strong>DBR version:</strong> List the DBR versions (<a href=\"https://docs.databricks.com/release-notes/runtime/releases.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"DBR versions\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/release-notes/runtime/releases\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"DBR versions\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/release-notes/runtime/releases.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"DBR versions\">GCP</a>) that this KB applies to. Be specific. It is OK to list a range. Do NOT just list \"All\". DBR versions are constantly being released and going EoL. It is important for future updates to know what DBR versions this KB has been tested on and verified against. If you list it here, you are giving your word that the KB steps apply to the versions listed.</p><p><strong>Category:</strong> This is the primary category where the KB will be listed. Do NOT create a new category. Choose one of the existing categories from the KB.</p><p><strong>Secondary category:</strong> This is an optional secondary category to list the KB. Most articles will have this blank. If you fill this section out, do NOT create a new category. Choose one of the existing categories from the KB.</p><p><strong>Cloud version:</strong> List the Clouds that this KB applies to. Be specific. Do NOT just list \"All\".</p><p><strong>Author:</strong> This is the Databricks email of the author. Customers can not see this, but Databricks employees can. This is so that anyone who has questions knows who to follow up with when needed.</p><p><strong>Owning team:</strong> This is the Support team that is responsible for the KB content. If the author is no longer working in support, and there is a question or update required, the owning team is responsible.</p><p><strong>Ticket URL:</strong> This is a link to the original Salesforce support ticket or ES Jira ticket that is hte basis for the KB article.</p><p><strong>Last reviewed date:</strong> This lists the date and name of the TSE who provided technical review on the article.</p><p><strong>Review status:</strong> This field highlights the current state of the article in <strong>bold</strong> text.</p><h1 id=\"primary-section-header-4\">Primary section header</h1><p>The primary section headers should always be <strong>Heading 1</strong>. If you are using a template, the default section headers are already set to <strong>Heading 1</strong>.</p><h2 id=\"secondary-section-header-5\">Secondary section header</h2><p>Secondary section headers should be set to <strong>Heading 2</strong>. This is meant to be used when you want a distinct sub-section within a primary section. For example, if there are multiple solutions to a problem, you may have each solution separated out under secondary section headers within the <strong>Solution</strong> part of the article.</p><p><br></p><p>\u00a0</p>", "body_txt": "Info\nThis style guide provides an overview of the formatting options to use when authoring a KB article in Helpjuice. Bookmark this page, as it is subject to change. Article title Make sure that you enter an article title at the top of the article. DO NOT leave the default template title. Article subtitle This should be a one line sentence that summarizes your article. Code blocks Pre-formatted code Pre-formatted code blocks use the Code text type. This should only be used for stand-alone blocks of code. It should NOT be used for in-line code. Highlight the code block and then select Code from the text type menu to convert it from Normal Text to\u00a0Code format. Identify the type of code block, by placing the magic command (AWS | Azure | GCP) for the language on the first line of the code block, followed by an empty line. For example: %python This is sample Python code. Code inline Inline code should have the selected words using the Times New Roman font. All other body text should be using the default font. Images Images should be uploaded and inserted into the page with the\u00a0Insert Image button. Place the cursor where you want the image on the page.\nClick the Insert Image button.\nDrag and drop the image file onto the Drop image box. Warning\nIf you copy-and-paste an image from another tool it may look like it has uploaded to Helpjuice, but it is only copying a link to the image location. Other users will NOT be able to view the image. ALL images must be uploaded. Keywords Keywords are entered at the bottom of the article. Keywords should ALWAYS include the applicable cloud versions (ex AWS, Azure, and/or GCP). Keywords can also include any applicable items you think would be appropriate for SEO. Links Because articles can apply to multiple clouds, links to Docs must be supplied for each cloud. This is done in the format of &lt;link text&gt; (&lt;cloud&gt; | &lt;cloud&gt; | &lt;cloud&gt;). For example, if I wanted a link to the default Docs site for each cloud I might say \"review the Databricks documentation (AWS | Azure | GCP) for more information.\" Off-site links that are not cloud specific, can be linked as normal. For example, \"You should review the Apache Spark documentation.\" When adding a link, the following fields should be filled in: URL: The target URL. Text: This is the text that has the clickable link applied. Title on Mouseover: If you are creating a multi-cloud link, this should be the link text. If you are linking to a single off-site location, this can be left blank. Open in new tab: This should be checked. Metadata The beginning of each article has a restricted metadata section. This is used to keep track of internal information that tells us about the article and who is responsible for the content. DBR version: List the DBR versions (AWS | Azure | GCP) that this KB applies to. Be specific. It is OK to list a range. Do NOT just list \"All\". DBR versions are constantly being released and going EoL. It is important for future updates to know what DBR versions this KB has been tested on and verified against. If you list it here, you are giving your word that the KB steps apply to the versions listed. Category: This is the primary category where the KB will be listed. Do NOT create a new category. Choose one of the existing categories from the KB. Secondary category: This is an optional secondary category to list the KB. Most articles will have this blank. If you fill this section out, do NOT create a new category. Choose one of the existing categories from the KB. Cloud version: List the Clouds that this KB applies to. Be specific. Do NOT just list \"All\". Author: This is the Databricks email of the author. Customers can not see this, but Databricks employees can. This is so that anyone who has questions knows who to follow up with when needed. Owning team: This is the Support team that is responsible for the KB content. If the author is no longer working in support, and there is a question or update required, the owning team is responsible. Ticket URL: This is a link to the original Salesforce support ticket or ES Jira ticket that is hte basis for the KB article. Last reviewed date: This lists the date and name of the TSE who provided technical review on the article. Review status: This field highlights the current state of the article in bold text. Primary section header The primary section headers should always be Heading 1. If you are using a template, the default section headers are already set to Heading 1. Secondary section header Secondary section headers should be set to Heading 2. This is meant to be used when you want a distinct sub-section within a primary section. For example, if there are multiple solutions to a problem, you may have each solution separated out under secondary section headers within the Solution part of the article. \u00a0", "format": "html", "updated_at": "2022-06-30T04:37:26.565Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264386, "name": "Internal Articles", "codename": "internal-articles", "accessibility": 0, "description": "All articles in this section are only viewable by Databricks employees.", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [], "url": "https://kb.databricks.com/internal-articles/kb-style-guide"}, {"id": 1402932, "name": "Use Databricks Repos with Docker container services", "views": 4709, "accessibility": 1, "description": "Configure your cluster with a custom init script to use Databricks Repos with Docker container services.", "codename": "use-databricks-repos-with-docker-container-services", "created_at": "2022-06-13T16:50:14.904Z", "updated_at": "2022-09-28T06:49:07.854Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStueVdqTGc3akNERHJ5dC9aeWdXMk5oa1dPZXlGRU4xMVVBZEFQVURGcm5EYnpwL1FCCkM5NFFiYjFybG1yamo3K3FGV3ZaaVhwMWFaSGxUc1p2RGpHTkhlYkRGZEoxQjJPclRBRjlMZ2U5Wms5QQpXbU5BZkNBRGlET3hsbWpQd0pxQU4wYkQ4MnI2d0JHVFUyRWgxOWFrK0hjZjFUNlZQTmZLb2U0aGZhOGoKSGVzbk9RMWl2OVQ0aEVaeFlkNTdZa1lDWExPbGhmUit3dnRVOGhVWFhHQ0pLS1VLVHB5Zk03Rml2a1RVCnRUVGJZRVBSNy9adFQwNFNMYkZ1cnVIc3kydDdaOGs4Tng2Q0tPNlhodlRJbnRxT3ovbS9TaDQ2UTkwSAp2QlI0dEkrMlV6VDN6emp0S3VmeDFNd205d1QxYU5pM09oZ1NlVFI5amY2RXd2T0ppdHppN2hvd2s0RFkKaDhHd0FZakpSRlVtNHkzRHpXcm90eW5DMWNtN3pxaitNRDNZS3VDTzdFRFJITjVEbEVjajY1U1oxb1NhCjcveGJrejUrSDhlc0pWbllDdW9WNXBkSldGUnNSaXlpUHVPQldxMG9vM2IwWXUxWnVUTmpWS3dqSXVEVgoxYTRYYnJHNlp1aGFGd2pjK2E5eFpRUVFCR3ZTcm1YRzAvNG5CWVNQdi9GY2FiNXdiWVBxeEhSM2lQVzYKNGw0cmFxbVZ3cGM5TEFiRnVzR2hTeVEzV0FMb0UyZ3FlVXV5OEhDVDFYNWx1QVhGSE11S09IQUFuWVVXCkVidVlTL1M2ZTNVNmRhb2xHNGhhdnd1NkhKL1E5d1BOQUNsOVZVM1NpYURvRWtHMFplS1BKWjk1RXhlQQpjU0RxUUYyc2VZTXVTNDNiL1RQUjdZejlIblUzbUZTc29tNUR6b3lhV3Fmc2ZiNU5ZOWNTa2dUVWFjOTUKMUZTblFpeTZLU1hTeXpFNXZrRFNKTjRtY3Q1bWFZaHA4MTNuMHFyUGdhSTYzdVBtamY3QkQ4N2dyMC9qClJtMXlJT0VIT3hJUVJKbmk4VVM3Q21RN3pFK2srNTJ0MWhoa2ppR2VWY2pyaVVzaGQwU0hsL1pFYkZLVAp6VnR6ak1kTTZIL0lEanlXMENHZW5ORDB4S3FtVUM5cVdOakhmOUtIdkRHb0FaYkZnQ2hUTHA4cTE4ek4KSUZHZnF3aWIvaDBYT0N6bUtxeGgzdlNyOGRvaTNjdU1nNFJ0eWQ5Wmx3ZXVoOGsvNXV6dllDWlFJU0RSCmJXektUZGNZSldDUWU2MkJhSStyV1l2MkI4U0VJZWN0RzkrRXBJeTBaSHJYdGZheTBHUnExNUp3TjNYSwpBM3ZHSS9rMzlzTkJJbUhNZ2F0d1lxSzRxbnVzRVJKTzR0RUt6ZTRwdTIwTU8rcHFvVW1KWVRocWxkenMKdTJoNDRyUlZNdzUyTXFJb0RlM1pZUDN1czUxV3ozN0R4TDNpZ1lyT3Q0QnNLenZ2MzB5TEhiR2ZJQXB0CmlXYi93NGxrQStDN0pqN3FzTlVVQkhWeklVWHdOMkxzNmVkeGduV0p5Z1NSbXoyc3ZVQ083cUtoNmRPagpRWkhNQUtodW9WaUdkdU1uQ1ExQVFtK0MwZitEVDF0QWdsY05haHBROUtwbExGdlpVVjZlV0s3dGxEZUEKUiswckN4L2lHY0hrSWNJcXhiQ3E1dlVIazU0TThKdmdkMm9UQ3ZTV2RLa2hLNzlpS0ZybE5NZjJIM0lkClJEcVl4cUY5OW5yenlsZWV5bVMyYXdYVzRrdld4NGxHYkdEc0dmVml6NlFZbHpUckFrQXRuY3JycnFSTQpkT0lDNnljMmJXMFdWS1hkTHdzZTN0eG5kNzJCek1OQkF3NUpDUnN0b0VMcWtkTUVUR3h6Z3dGTm9MeDEKVUQ0cUdFTWpzNkk9Cg==.3a29c8c1b4003e417459e3a8246ad463\"></div><h1 data-toc=\"true\" id=\"introduction-0\">Introduction</h1><p>Depending on your use case, you may want to use both Docker Container Services (DCS) and Databricks Repos (<a href=\"https://docs.databricks.com/repos/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks Repos\">AWS</a> | <a href=\"https://learn.microsoft.com/azure/databricks/repos/\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks Repos\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/repos/index.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Databricks Repos\">GCP</a>) at the same time. DCS does not work with Databricks Repos by default, however you can use a custom init script to use both.</p><p>If you have not installed an init script to configure DCS with Databricks Repos you may see an error message when you try to start your cluster. This happens when the underlying filesystem becomes inaccessible.</p><p>You may see the below error while using repo without having the init script:</p><pre data-aura-rendered-by=\"4744:0\">py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\r\nFile \"/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 2442, in _call_proxy\r\nreturn_value = getattr(self.pool[obj_id], method)(*params)\r\nFile \"/databricks/python_shell/scripts/<a href=\"http://pythonshellimpl.py/\" rel=\"noopener\" target=\"_blank\">PythonShellImpl.py</a>\", line 935, in initStartingDirectory\r\nos.chdir(directory)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/Workspace/Repos/&lt;username&gt;/hello_world'</pre><h1 data-toc=\"true\" id=\"instructions-1\">Instructions</h1><p>You can use the example init script in this article to get DCS working with Databricks Repos.</p><p>This init script ensures the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">goofy-dbr</span> process is correctly running which ensures the filesystem remains accessible. The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">goofy-dbr</span> process is a Databricks internal fork of <a data-renderer-mark=\"true\" href=\"https://github.com/kahing/goofys\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"https://github.com/kahing/goofys\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">goofys</span></a>. Databricks' <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">goofy-dbr</span> adds support for Azure Data Lake Storage (ADLS) and Azure Blob Storage to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">goofys</span>, as well as ensuring that <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">goofys</span> can run on Databricks clusters.</p><h2 data-toc=\"true\" id=\"create-the-init-script-2\">Create the init script</h2><ol>\n<li>Ensure that you have a directory to store your init scripts. If you do not have one, create one.<pre>%scala\r\n\r\ndbutils.fs.mkdirs(\"dbfs:/databricks/&lt;init-script-folder&gt;/\")</pre>\n</li>\n<li>Create the init-script.</li>\n<li>Use this sample code to create an init script called <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">repo.sh</span> on your cluster. Replace <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;init-scripts&gt;</span>with the location of your init scripts.<pre>%scala\r\n\r\ndbutils.fs.put(\"dbfs:/databricks/&lt;init-script-folder&gt;/<a href=\"http://repo.sh/\" rel=\"noopener\" target=\"_blank\">repo.sh</a>\", \"\"\"\r\n#!/bin/bash\r\n\r\nset -o xtrace\r\n\r\nsource /databricks/spark/conf/<a href=\"http://spark-env.sh/\" rel=\"noopener\" target=\"_blank\">spark-env.sh</a>\r\n\r\nexport WSFS_ENABLE_DEBUG_LOG\r\n\r\nmkdir -p /Workspace\r\nmkdir -p /databricks/data/logs/\r\n\r\nnohup /databricks/spark/scripts/fuse/wsfs /Workspace &gt; /databricks/data/logs/wsfs.log 2&gt;&amp;1 &amp;\r\n\r\nWAIT_TIMEOUT=5\r\nCHECK_INTERVAL=0.1\r\nWAIT_UNTIL=$(($(date +%s) + $WAIT_TIMEOUT))\r\n\r\nuntil mountpoint -q /Workspace || [[ $(date +%s) -ge $WAIT_UNTIL ]]; do\r\n\u00a0 sleep $CHECK_INTERVAL\r\ndone\r\n\r\nmkdir -p /dbfs\r\nnohup /databricks/spark/scripts/fuse/goofys-dbr -f -o allow_other \\\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --file-mode=0777 --dir-mode=0777 -o bg --http-timeout 120s \\\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /: /dbfs &gt; /databricks/data/logs/dbfs_fuse_stderr 2&gt;&amp;1 &amp;\r\n\r\nWAIT_UNTIL=$(($(date +%s) + $WAIT_TIMEOUT))\r\n\r\nuntil mountpoint -q /dbfs || [[ $(date +%s) -ge $WAIT_UNTIL ]]; do\r\n\u00a0 sleep $CHECK_INTERVAL\r\ndone\r\n\r\n\"\"\",true)</pre>\n</li>\n<li>Verify that the init script was successfully created on your cluster.<pre>%scala\r\n\r\ndisplay(<a href=\"http://dbutils.fs.ls/\" rel=\"noopener\" target=\"_blank\">dbutils.fs.ls</a>(\"dbfs:/databricks/&lt;init-script-folder&gt;/<a href=\"http://repo.sh/\" rel=\"noopener\" target=\"_blank\">repo.sh</a>\"))</pre>\n</li>\n<li data-toc=\"true\" id=\"-3\">Make sure to record the full path to the init script. You will need it when you configure the init script.</li>\n</ol><h2 data-toc=\"true\" id=\"configure-the-init-script-3\">Configure the init script</h2><p>Follow the documentation to configure a cluster-scoped init script (<a href=\"https://docs.databricks.com/clusters/init-scripts.html#configure-a-cluster-scoped-init-script\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"configure a cluster-scoped init script\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/clusters/init-scripts#configure-a-cluster-scoped-init-script\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"configure a cluster-scoped init script\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/init-scripts.html#configure-a-cluster-scoped-init-script\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"configure a cluster-scoped init script\">GCP</a>).</p><p>Specify the path to the init script. Use the same path that you used in the sample script.</p><p>After configuring the init script, restart the cluster.</p>", "body_txt": "Introduction Depending on your use case, you may want to use both Docker Container Services (DCS) and Databricks Repos (AWS | Azure | GCP) at the same time. DCS does not work with Databricks Repos by default, however you can use a custom init script to use both. If you have not installed an init script to configure DCS with Databricks Repos you may see an error message when you try to start your cluster. This happens when the underlying filesystem becomes inaccessible. You may see the below error while using repo without having the init script: py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last): File \"/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 2442, in _call_proxy return_value = getattr(self.pool[obj_id], method)(*params) File \"/databricks/python_shell/scripts/PythonShellImpl.py\", line 935, in initStartingDirectory os.chdir(directory) FileNotFoundError: [Errno 2] No such file or directory: '/Workspace/Repos/&lt;username&gt;/hello_world' Instructions You can use the example init script in this article to get DCS working with Databricks Repos. This init script ensures the goofy-dbr process is correctly running which ensures the filesystem remains accessible. The goofy-dbr process is a Databricks internal fork of goofys . Databricks' goofy-dbr adds support for Azure Data Lake Storage (ADLS) and Azure Blob Storage to goofys, as well as ensuring that goofys can run on Databricks clusters. Create the init script Ensure that you have a directory to store your init scripts. If you do not have one, create one.%scala dbutils.fs.mkdirs(\"dbfs:/databricks/&lt;init-script-folder&gt;/\") Create the init-script.\nUse this sample code to create an init script called repo.sh on your cluster. Replace &lt;init-scripts&gt;with the location of your init scripts.%scala dbutils.fs.put(\"dbfs:/databricks/&lt;init-script-folder&gt;/repo.sh\", \"\"\" #!/bin/bash set -o xtrace source /databricks/spark/conf/spark-env.sh export WSFS_ENABLE_DEBUG_LOG mkdir -p /Workspace mkdir -p /databricks/data/logs/ nohup /databricks/spark/scripts/fuse/wsfs /Workspace &gt; /databricks/data/logs/wsfs.log 2&gt;&amp;1 &amp; WAIT_TIMEOUT=5 CHECK_INTERVAL=0.1 WAIT_UNTIL=$(($(date +%s) + $WAIT_TIMEOUT)) until mountpoint -q /Workspace || [[ $(date +%s) -ge $WAIT_UNTIL ]]; do \u00a0 sleep $CHECK_INTERVAL done mkdir -p /dbfs nohup /databricks/spark/scripts/fuse/goofys-dbr -f -o allow_other \\ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --file-mode=0777 --dir-mode=0777 -o bg --http-timeout 120s \\ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /: /dbfs &gt; /databricks/data/logs/dbfs_fuse_stderr 2&gt;&amp;1 &amp; WAIT_UNTIL=$(($(date +%s) + $WAIT_TIMEOUT)) until mountpoint -q /dbfs || [[ $(date +%s) -ge $WAIT_UNTIL ]]; do \u00a0 sleep $CHECK_INTERVAL done \"\"\",true) Verify that the init script was successfully created on your cluster.%scala display(dbutils.fs.ls(\"dbfs:/databricks/&lt;init-script-folder&gt;/repo.sh\")) Make sure to record the full path to the init script. You will need it when you configure the init script. Configure the init script Follow the documentation to configure a cluster-scoped init script (AWS | Azure | GCP). Specify the path to the init script. Use the same path that you used in the sample script. After configuring the init script, restart the cluster.", "format": "html", "updated_at": "2022-09-28T06:49:07.852Z"}, "author": {"id": 789480, "email": "darshan.bargal@databricks.com", "name": "darshan.bargal ", "first_name": "darshan.bargal", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T05:58:59.399Z", "updated_at": "2023-04-21T14:21:18.113Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 256852, "name": "Libraries", "codename": "libraries", "accessibility": 1, "description": "These articles can help you manage libraries in Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2899535, "name": "aws"}, {"id": 2899536, "name": "azure"}, {"id": 2899538, "name": "dcs"}, {"id": 2899539, "name": "docker"}, {"id": 2900249, "name": "filesystem"}, {"id": 2899537, "name": "gcp"}, {"id": 2899541, "name": "git"}, {"id": 2900248, "name": "goofys"}, {"id": 2899540, "name": "repos"}], "url": "https://kb.databricks.com/libraries/use-databricks-repos-with-docker-container-services"}, {"id": 1395664, "name": "Author a new KB article (Support)", "views": 98, "accessibility": 0, "description": "How to author a new KB article", "codename": "author-a-new-kb-article-support", "created_at": "2022-06-07T01:09:05.012Z", "updated_at": "2022-10-13T03:59:44.659Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.05394ff01f283bfb975338acaed14a23\"></div><p>This article explains how to author a new KB article in Helpjuice.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-0\">Info</h3>\n<p class=\"hj-alert-text\">If you have any questions during the process, reach out to\u00a0<a href=\"mailto:adam.pavlacka@databricks.com\">adam.pavlacka@databricks.com</a> via email or adam.pavlacka on Slack.<br><br>Adam is based in San Francisco and will respond as soon as possible.</p>\n</div>\n</div><h1 data-toc=\"true\" id=\"instructions-1\">Instructions</h1><ol>\n<li>Navigate to the <strong>Dashboard</strong> -&gt;\u00a0<strong>Support Article Drafts</strong> folder.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654573368290-Screen%20Shot%202022-06-06%20at%208.42.10%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>Select the correct folder for your team. For example, if you are on the US Spark team, select\u00a0<strong>spark-us-drafts</strong>.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654573466964-Screen%20Shot%202022-06-06%20at%208.44.17%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>Select the blue + icon on the left hand side.\u00a0<img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654573551206-Screen%20Shot%202022-06-06%20at%208.45.13%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>Click\u00a0<strong>New Article From Template</strong>.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654573596793-Screen%20Shot%202022-06-06%20at%208.44.58%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>Click the template you want to use. The most common KB article type is <strong>Problem Cause Solution</strong>.</li>\n<li>Enter a title for the article at the top of the page. The article URL automatically fills in after you enter an article title.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654573760109-Screen%20Shot%202022-06-06%20at%208.49.00%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>Enter an <strong>Article Subtitle</strong> in the line below the title. The subtitle should be a one sentence description of what the KB accomplishes. Think of it as a summary of the KB.</li>\n<li>Complete all of the metadata in the <strong>Restricted Content</strong> box. If something does not apply, enter N/A. Do NOT list \"All\" for <strong>DBR Version</strong> or\u00a0<strong>Cloud Version</strong>. Be specific.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654573918975-Screen%20Shot%202022-06-06%20at%208.51.49%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\">\n</li>\n<li>Complete the rest of the article content.</li>\n<li>Enter the applicable\u00a0<strong>Cloud Version</strong> types as keywords at the bottom of the article.<br><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654574024002-Screen%20Shot%202022-06-06%20at%208.53.34%20PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\">\n</li>\n<li>When the draft is finished, at-mention your lead and have them sign off on a technical review.</li>\n<li>When you are ready for a final review, click <strong>Submit for Review</strong>.</li>\n</ol>", "body_txt": "This article explains how to author a new KB article in Helpjuice. Info\nIf you have any questions during the process, reach out to\u00a0adam.pavlacka@databricks.com via email or adam.pavlacka on Slack. Adam is based in San Francisco and will respond as soon as possible. Instructions Navigate to the Dashboard -&gt;\u00a0Support Article Drafts folder. Select the correct folder for your team. For example, if you are on the US Spark team, select\u00a0spark-us-drafts. Select the blue + icon on the left hand side.\u00a0 Click\u00a0New Article From Template. Click the template you want to use. The most common KB article type is Problem Cause Solution.\nEnter a title for the article at the top of the page. The article URL automatically fills in after you enter an article title. Enter an Article Subtitle in the line below the title. The subtitle should be a one sentence description of what the KB accomplishes. Think of it as a summary of the KB.\nComplete all of the metadata in the Restricted Content box. If something does not apply, enter N/A. Do NOT list \"All\" for DBR Version or\u00a0Cloud Version. Be specific. Complete the rest of the article content.\nEnter the applicable\u00a0Cloud Version types as keywords at the bottom of the article. When the draft is finished, at-mention your lead and have them sign off on a technical review.\nWhen you are ready for a final review, click Submit for Review.", "format": "html", "updated_at": "2022-10-13T03:59:44.655Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264386, "name": "Internal Articles", "codename": "internal-articles", "accessibility": 0, "description": "All articles in this section are only viewable by Databricks employees.", "icon": "", "image_url": null}, "hierarchy": [], "keywords": [{"id": 3119546, "name": "create kb"}, {"id": 3119547, "name": "write kb"}], "url": "https://kb.databricks.com/internal-articles/author-a-new-kb-article-support"}, {"id": 1391287, "name": "ABFS client hangs if incorrect client ID or wrong path used", "views": 10355, "accessibility": 1, "description": "Trying to access an Azure Blob File System (ABFS) path results in a hung command when using Azure Data Lake Storage Gen2 (ADLS).", "codename": "abfs-client-hang", "created_at": "2022-06-01T20:54:34.335Z", "updated_at": "2022-06-01T20:58:17.124Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS80RjU5bGx0bmJkamcvTHJxbDBrWVl1NjlZTXpxejJSelNOUDhINWZKQ3Q1aFc1c1IvClhkSDJyK29RTDBqcXhwWFNiZ0s5NXVXUVpFUDhBV0hQTTlqOHpiS3FNTFM0b2xkNWxCbHo3bkFFZkY0Vwp1amYxdTVNQjN6NG5nM1M1dDJZeGNQbEp4aDg0aU52M0JoVHMycitTeFNyVzdIVTFGRUxpbG1EbGN6ZEQKMnl5N2VoU0l6cjRJdVVHcnc5QWFxclpLUkRvdXhmWFdXVlMvOVRuR05kSEs5NFNmT2VvYWVibkcrbTdSCmVKeHhxOHY1NjA5UFhVcGpGTU0yK2hsWDhXUEZ3RjJWYkorVDZqK09ZUmFPR3gxVVVXaUVCZ0syckkvcApGUitqRGNDQ2psRzNaM1doV05Wa0xZbGZkZSthTUVWblBHbGZXem44YWpGbkR5YlIzS1h1ejFObTFaMTEKNDFHOUw3Z3dGbVBrK0lOcUtya25pN1Zic2lEc2o3czRwV3djaGE2YzRaNnA3dzM1MDFORHJ4R0dyRmFYCmVnL2pDVGx0Z1hQQUxnVlZRaEY2K3N5TzYyUWFQSGl4NGRTVEJ1WEpKdm9xdkFrdVhsdEk4NklsNHo2Vwo3QUNLVVRzKzMrYkx6Vmx2KzI1bWlsWnpKT1REUDdkMFJUd0lLUzZ5L01PMTFJM3dQWnBMT2NQK0djaXcKNHYweURodzIxWWZDZk9TWG1oYm80RFp5cVU0SlpuTWxOeXY0YUo2ZWMzbGhySkI4UE9ZRWpzTzVaUFdBCk4xWUlaLzFFM3ZKbWFKV0ZkaG1KZFppMGVoRFR0SmNJN2Izb3dBaDNMUlhMek1LZUgzdHJHOUNXZk9BeApWZmVKQnJpQ3l6UkhwbHVOMktPZ3lrU21laGZnRkNmMXVWa2pWcVhYeE9YZjNRbktDeFdCc1NXQU5QUzQKYnl4b0pZUE4wOHgyNDRhZUhoV1RMSjN3a1BYeEErL0JWaEFobzNNMXRIMk81b0FkWHhDSndUUlRRQXFLCjRSSFlnSFpUd3hUek02TGRnR09jbzQvU2JPRUFYYWd5aWRPa3NWUERUQUFEWVpMRFNqK0lIRW1weUNmUwpHb1VWUkZ5VEZCK2dzL1V4dTNnOGlsRnV1RmNqUVlsL00zMUxoemNVa2JYMjl5NXA4bi9vdEU1Yy9tMmQKVVEyM0cyVW54K084aWNRWkJSVWc4dlhqeVBVR2V1L29XOVp3akJCSXdVSHVCTkhhZVRkWnJpN3ZvcWVtCg==.ad61a83e93308b9f0e89a04350d4c25b\"></div><h1 id=\"isPasted\">Problem</h1><p>You are using Azure Data Lake Storage (ADLS) Gen2. When you try to access an Azure Blob File System (ABFS) path from a Databricks cluster, the command hangs.</p><p>Enable the debug log and you can see the following stack trace in the driver logs:</p><pre>Caused by: java.io.IOException: Server returned HTTP response code: 400 for URL: https://login.microsoftonline.com/b9b831a9-6c10-40bf-86f3-489ed83c81e8/oauth2/token\r\n\u00a0 at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1894)\r\n\u00a0 at sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:91)\r\n\u00a0 at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1484)\r\n\u00a0 at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1482)\r\n\u00a0 at java.security.AccessController.doPrivileged(Native Method)\r\n\u00a0 at java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:782)\r\n\u00a0 at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1481)\r\n\u00a0 at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)\r\n\u00a0 at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347)\r\n\u00a0 at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:254)\r\n\u00a0 ... 31 more</pre><h1>Cause</h1><p>If ABFS is configured on a cluster with a wrong value for property <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">fs.azure.account.oauth2.client.id</span>, or if you try to access an explicit path of the form <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">abfss://myContainer@myStorageAccount.dfs.core.windows.net/...</span> where <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">myStorageAccount</span> does not exist, then the ABFS driver ends up in a retry loop and becomes unresponsive. The command will eventually fail, but because it retries so many times, it appears to be a hung command.</p><p>If you try to access an incorrect path with an existing storage account, you will see a 404 error message. The system does not hang in this case.</p><h1>Solution</h1><p>You must verify the accuracy of all credentials when accessing ABFS data. You must also verify the ABFS path you are trying to access exists. If either of these are incorrect, the problem occurs.</p>", "body_txt": "Problem You are using Azure Data Lake Storage (ADLS) Gen2. When you try to access an Azure Blob File System (ABFS) path from a Databricks cluster, the command hangs. Enable the debug log and you can see the following stack trace in the driver logs: Caused by: java.io.IOException: Server returned HTTP response code: 400 for URL: https://login.microsoftonline.com/b9b831a9-6c10-40bf-86f3-489ed83c81e8/oauth2/token \u00a0 at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1894) \u00a0 at sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:91) \u00a0 at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1484) \u00a0 at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1482) \u00a0 at java.security.AccessController.doPrivileged(Native Method) \u00a0 at java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:782) \u00a0 at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1481) \u00a0 at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) \u00a0 at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347) \u00a0 at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:254) \u00a0 ... 31 more Cause If ABFS is configured on a cluster with a wrong value for property fs.azure.account.oauth2.client.id, or if you try to access an explicit path of the form abfss://myContainer@myStorageAccount.dfs.core.windows.net/... where myStorageAccount does not exist, then the ABFS driver ends up in a retry loop and becomes unresponsive. The command will eventually fail, but because it retries so many times, it appears to be a hung command. If you try to access an incorrect path with an existing storage account, you will see a 404 error message. The system does not hang in this case. Solution You must verify the accuracy of all credentials when accessing ABFS data. You must also verify the ABFS path you are trying to access exists. If either of these are incorrect, the problem occurs.", "format": "html", "updated_at": "2022-06-01T20:58:17.120Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708974, "name": "aws"}, {"id": 2708975, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/abfs-client-hang"}, {"id": 1391283, "name": "Redshift JDBC driver conflict issue", "views": 6235, "accessibility": 1, "description": "Learn how to resolve a Redshift JDBC SQLDriverWrapper driver conflict.", "codename": "redshift-jdbc-driver-conflict", "created_at": "2022-06-01T20:49:20.436Z", "updated_at": "2022-06-01T20:54:20.911Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlNclFkdExqV2czTUpJWVlOd2pNckFHVFBCdSt6OHgrZ20rS24zSW5ROUVKbU5BN2FLCnJEVXIzbjhwczdFQUkxS0U3ZWtqTVhFRVl5WjRoZk1NZE5YZEpxdmhUZE56ZDluS3ZWY0FwWTZocHNFdQpqeGJmY2NCUldjWk5RemhrNzI5YU1ndEFXN2hHWFhJNHplaVJEREpzTGYxQXR1SkFGc041UU9MUTFCalcKcUN6OVJZMlpMZDVrUG9NakdjNlBWeHVnbDRuRGlCSVgwdThQazhUZlA2Rzk2S3k5ditnbENGVUl0MGtOCjlpTUsxeTg1N1pyWmtLOWlzL3FCcGVtK1hoNEJpbGZTU3ByQURpN1NHS3l6Q09BeHhnSnZHRVZ6OW8wMwpkMkQvZGljY0ZGZ1BZSzAwb0lNT21tVk5kVVFtS2V0bVJRVEpITm5kSk54L1BtTDRmMXV3UEpDUlFrV0sKalRlTXJDT3ZzNGM3cUk5VXZ0bTdzdlVkTU1XOHhmVlE5Y2ZjZGtkeVlvYmZ3ZGFPUGZNek9CaFZpYVJFCldDYkVCY2cwQ1JFelRxcVlmZ2RoTjNaQ0ovWk5FSzV4U01palVTNnVwaFl6a25FRDRMZ1dKVWVqeW9XTwp5bHlLSlpIUkZrT2tXRlFTZGlVZkd3Q3FlcXM4enRqK3Y5WnZ3OGFIZ0VPaVVoTWUxdkh2bEd3MjV3YXYKU1VGdWw5WVN2cTBWUWRvRUdaUHN4WlRkUjdueDFjejJCVnoveVFEb2pOam5IU2JBOVlhNzVsdnV2RkZQCjhOUnlVK1I5RmFuSHJYTjd2OTVmb3NiK21vZkhSWTM4MDlobDh5cHBMdDJTQ1hnMEp2NFhEWk5Ybys3SQpUbUpnUG5LcTZtODY5ZzgxbDBIUSt6SDNmbXFlVWxCNnE2SHora2xLTkYrUE1lTW02WnpyVk16YUE0TSsKcFhNaTRNUE9lbENHNzFrb0FwcnNxZHNuMFlnVEpkZVU4d3VoM1BqeXdCMm56ZzhBdnZxVkRhcm1nL0s1CjV4WTN1Uk1DU01PT1V6NUFQN1pvWUhTR1ZWTlNCNHpwNUgxTHhIWWlqeXAwVjFjZjY4QXFhMUxLMUIvMgpMOCt6OXFFdlViU3RIL2VPZm5ka3UwMzJtaGwvYjBnbTJkTWIxT1VGVWxvRER4aUpkRWk1VmIrRlF6MzEKZnk4TEF1cU05WEd1Z2JMWUpVdW1SWnQzRDlwMUU0OHZPd2VoTGM0PQo=.383c6ff53b5506d482b379955551287d\"></div><h1 id=\"isPasted\">Problem</h1><p>If you attach multiple Redshift JDBC drivers to a cluster, and use the Redshift connector, the notebook REPL might hang or crash with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SQLDriverWrapper</span> error message.</p><pre>19/11/14 01:01:44 ERROR SQLDriverWrapper: Fatal non-user error thrown in ReplId-9d455-9b970-b2042\r\njava.lang.NoSuchFieldError: PG_SUBPROTOCOL_NAMES\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.amazon.redshift.jdbc.Driver.getSubProtocols(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.amazon.redshift.jdbc.Driver.acceptsSubProtocol(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.amazon.jdbc.common.BaseConnectionFactory.acceptsURL(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.amazon.jdbc.common.AbstractDriver.connect(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper.connect(DriverWrapper.scala:45)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.databricks.spark.redshift.JDBCWrapper.getConnector(RedshiftJDBCWrapper.scala:355)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.databricks.spark.redshift.JDBCWrapper.getConnector(RedshiftJDBCWrapper.scala:376)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.databricks.spark.redshift.RedshiftRelation$$anonfun$schema$1.apply(RedshiftRelation.scala:75)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.databricks.spark.redshift.RedshiftRelation$$anonfun$schema$1.apply(RedshiftRelation.scala:72)</pre><h1>Cause</h1><p>Databricks Runtime does not include a Redshift JDBC driver. If you are using Redshift, you must attach the correct driver to your cluster. If you attach multiple Redshift JDBC drivers to a single cluster they may be incompatible, which results in a hang or a crash.</p><p>For example, the following Redshift JDBC jars are incompatible:</p><ul>\n<li>RedshiftJDBC41-1.1.7.1007.jar</li>\n<li>RedshiftJDBC42-no-awssdk-1.2.20.1043.jar</li>\n</ul><p>If you attach both of these to the same cluster, the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SQLDriverWrapper</span> error message will appear when you try to access Redshift.</p><h1>Solution</h1><p>You should only have one <a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html#download-jdbc-driver\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Redshift JDBC driver</a> attached to a cluster. Review the <a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Redshift JDBC Driver documentation</a> to choose the correct driver for your cluster.</p>", "body_txt": "Problem If you attach multiple Redshift JDBC drivers to a cluster, and use the Redshift connector, the notebook REPL might hang or crash with a SQLDriverWrapper error message. 19/11/14 01:01:44 ERROR SQLDriverWrapper: Fatal non-user error thrown in ReplId-9d455-9b970-b2042 java.lang.NoSuchFieldError: PG_SUBPROTOCOL_NAMES \u00a0 \u00a0 \u00a0 \u00a0 at com.amazon.redshift.jdbc.Driver.getSubProtocols(Unknown Source) \u00a0 \u00a0 \u00a0 \u00a0 at com.amazon.redshift.jdbc.Driver.acceptsSubProtocol(Unknown Source) \u00a0 \u00a0 \u00a0 \u00a0 at com.amazon.jdbc.common.BaseConnectionFactory.acceptsURL(Unknown Source) \u00a0 \u00a0 \u00a0 \u00a0 at com.amazon.jdbc.common.AbstractDriver.connect(Unknown Source) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper.connect(DriverWrapper.scala:45) \u00a0 \u00a0 \u00a0 \u00a0 at com.databricks.spark.redshift.JDBCWrapper.getConnector(RedshiftJDBCWrapper.scala:355) \u00a0 \u00a0 \u00a0 \u00a0 at com.databricks.spark.redshift.JDBCWrapper.getConnector(RedshiftJDBCWrapper.scala:376) \u00a0 \u00a0 \u00a0 \u00a0 at com.databricks.spark.redshift.RedshiftRelation$$anonfun$schema$1.apply(RedshiftRelation.scala:75) \u00a0 \u00a0 \u00a0 \u00a0 at com.databricks.spark.redshift.RedshiftRelation$$anonfun$schema$1.apply(RedshiftRelation.scala:72) Cause Databricks Runtime does not include a Redshift JDBC driver. If you are using Redshift, you must attach the correct driver to your cluster. If you attach multiple Redshift JDBC drivers to a single cluster they may be incompatible, which results in a hang or a crash. For example, the following Redshift JDBC jars are incompatible: RedshiftJDBC41-1.1.7.1007.jar\nRedshiftJDBC42-no-awssdk-1.2.20.1043.jar If you attach both of these to the same cluster, the SQLDriverWrapper error message will appear when you try to access Redshift. Solution You should only have one Redshift JDBC driver attached to a cluster. Review the Redshift JDBC Driver documentation to choose the correct driver for your cluster.", "format": "html", "updated_at": "2022-06-01T20:54:20.908Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708973, "name": "aws"}], "url": "https://kb.databricks.com/data-sources/redshift-jdbc-driver-conflict"}, {"id": 1391282, "name": "Accessing Redshift fails with NullPointerException", "views": 8261, "accessibility": 1, "description": "Learn how to resolve a `NullPointerException` error that occurs when you read a Redshift table.", "codename": "redshift-npe", "created_at": "2022-06-01T20:44:40.484Z", "updated_at": "2022-06-01T20:49:08.735Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlrclJsOXZ3UGFtZHY5dldUZ3lEcFZXRmxUTTBQVW1TRm1OR083TE1XRVRuWEZpdkhECnlpS0wxZ1RCdTJtZFE0M3NRU3h1L2FVWlg0eDJ1dTBTaFpON1dYT0pKSEdqRUNVamZkVFFHeHFsL2dEegppN0xIRHNiTDY0czQyQVY1OEJvR3pmdWJzVC9vdTU3RkJhVWZQeVhiVzFqdGZIbHRabEFJN09XODdBYnMKNVlBalB6azFzODlaQ0EvbTdrVk04MXZ6UW92QldPajdIYmtCdTM2MkdPcWdMU0wwLzZJbmVwWDVTZDR1CjAzZjNuWFBXVEd5cWU5bll1VjQ4ZnRsc2hvL3RFdGZ3cUJVbTNNcXRselFjd1hBQlk1WXh0THh2dkpLOAozYlZKckFMZjNEdy92NjVxa20vNXBWaEtnMFIvR2JLR2RDZGhiS01LQjZKalF5bG9WTFZ5WFJrU0FBVXgKWno3ekc3TlFiYjF1UnhFcElIbUI0andHT3hYU0NQTVU1ZFFnZ2p6ZlRJVnQ2RUE4VUowMWRmY3VPTmNZCmxZVVV5Z0RHWjZ3b1g2RFVSbWFYVGM4VkZwWHJZeW05RFJNZWRRK0ZYRm8rSGc0ajdLWWhBczc1L1R4SgpxOFBjV2poYnRKUnBNc002QlpTdjlNcHJLY0FuYW53M2pJUER3bUdwOUoxSThlN01FeFE3TXZFdEhQL0UKZm1uQ3JibmRaclR6ZFkvRXYwQkloRnlrUkVIaW5PdVBZVDNESzFRZHlIemNQbU1XQ0lXcWlIbkNtMjlUCnJ3eE5Pb0w3ajBiL0w4MDdFWDdFenN0L0pkaG5ONlBzVFR5V29uY2dnQk5xSVF3dlBwbXAvVGVRcG0waQpTeXE2THM2b2pPdG5jeWNZRGpZb1hsMHhtV1NiRk51ZVVNNnA1WlNRL0dHeVVWMUZmWkRpQWsxbm1XUkcKM2NRVEJTNGo2NHlzWG1hOWk0a0VxbmNoN1FrcGdSYnJpTDNaU0JUU2hWQ1ovUlBvRkxPaHlVRjFPQW1NCnhRK0lvNk05dS9Rb2NHUm5iR01Pa2h3UHMzWFVhZEZRZjZ0S3k0SUxYUXJRSVNHM2ZQa3BzVmtuWDZBTwpqUU5EU3lFWXcyYldSSzJ6TDhSbDJBcHMyVGV4S00vSnhYdVREK1kxN2dOYmwwV1Y2NFZzN2s5azJPTS8KWlJsZi93Mm9mUllqUUlnMWtyR1pZVi9OS0syUWtocDJ0dnBhbHp3PQo=.8277850785541462291f1f9abc864284\"></div><h1 id=\"isPasted\">Problem</h1><p>Sometimes when you read a Redshift table:</p><pre>%scala\r\n\r\nval original_df = spark.read.\r\n\u00a0 \u00a0 \u00a0 format(\"com.databricks.spark.redshift\").\r\n\u00a0 \u00a0 \u00a0 option(\"url\", url).\r\n\u00a0 \u00a0 \u00a0 option(\"user\", user).\r\n\u00a0 \u00a0 \u00a0 option(\"password\", password).\r\n\u00a0 \u00a0 \u00a0 option(\"query\", query).\r\n\u00a0 \u00a0 \u00a0 option(\"forward_spark_s3_credentials\", true).\r\n\u00a0 \u00a0 \u00a0 option(\"tempdir\", \"path\").\r\n\u00a0 \u00a0 \u00a0 load()</pre><p>The Spark job will throw a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">NullPointerException</span>:</p><pre>Caused by: java.lang.NullPointerException\r\n\u00a0 at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:194)</pre><h1>Cause</h1><p>The problem comes from the way Spark reads data from Redshift. The Amazon Redshift data source uses Redshift\u2019s unload format to read data from Redshift: Spark first issues an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">unload</span> command to Redshift to make it dump the contents of the table in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">unload format</span> to temporary files, and then Spark scans those temporary files. This text-based <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">unload format</span> does not differentiate between an empty string and a null string by default \u2013 both are encoded as an empty string in the resulting file. When spark-redshift reads the data in the unload format, there\u2019s not enough information for it to tell whether the input was an empty string or a null, and currently it simply deems it\u2019s a null.</p><h1>Solution</h1><p>In Scala, set the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nullable</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span> for all the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">String</span> columns:</p><pre>%scala\r\n\r\nimport org.apache.spark.sql.types.{StructField, StructType, StringType}\r\nimport org.apache.spark.sql.{DataFrame, SQLContext}\r\n\r\n\r\ndef setNullableStateForAllStringColumns(df: DataFrame, nullable: Boolean) = {\r\n\u00a0 StructType(df.schema.map {\r\n\u00a0 \u00a0 case StructField( c, StringType, _, m) =&gt; StructField( c, StringType, nullable = nullable, m)\r\n\u00a0 \u00a0 case StructField( c, t, n, m) =&gt; StructField( c, t, n, m)\r\n\u00a0 })\r\n}</pre><p>In Python:</p><pre>%python\r\n\r\ndef set_nullable_for_all_string_columns(df, nullable):\r\n\u00a0 \u00a0 from pyspark.sql.types import StructType, StructField, StringType\r\n\u00a0 \u00a0 new_schema = StructType([StructField(f.name, f.dataType, nullable, f.metadata)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0if (isinstance(f.dataType, StringType))\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0else StructField(f.name, f.dataType, f.nullable, f.metadata)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0for f in df.schema.fields])\r\n\u00a0 \u00a0 return new_schema</pre><p>To use this function, get the schema of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">original_df</span>, then modify the schema to make all <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">String</span> columns to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">nullable</span>, then re-read from Redshift:</p><pre>%scala\r\n\r\nval df = spark.read.schema(setNullableStateForAllStringColumns(original_df, true)).\r\n\u00a0 \u00a0 \u00a0 format(\"com.databricks.spark.redshift\").\r\n\u00a0 \u00a0 \u00a0 option(\"url\", url).\r\n\u00a0 \u00a0 \u00a0 option(\"user\", user).\r\n\u00a0 \u00a0 \u00a0 option(\"password\", password).\r\n\u00a0 \u00a0 \u00a0 option(\"query\", query).\r\n\u00a0 \u00a0 \u00a0 option(\"forward_spark_s3_credentials\", true).\r\n\u00a0 \u00a0 \u00a0 option(\"tempdir\", \"path\").\r\n\u00a0 \u00a0 \u00a0 load()</pre><br>", "body_txt": "Problem Sometimes when you read a Redshift table: %scala val original_df = spark.read. \u00a0 \u00a0 \u00a0 format(\"com.databricks.spark.redshift\"). \u00a0 \u00a0 \u00a0 option(\"url\", url). \u00a0 \u00a0 \u00a0 option(\"user\", user). \u00a0 \u00a0 \u00a0 option(\"password\", password). \u00a0 \u00a0 \u00a0 option(\"query\", query). \u00a0 \u00a0 \u00a0 option(\"forward_spark_s3_credentials\", true). \u00a0 \u00a0 \u00a0 option(\"tempdir\", \"path\"). \u00a0 \u00a0 \u00a0 load() The Spark job will throw a NullPointerException: Caused by: java.lang.NullPointerException \u00a0 at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:194) Cause The problem comes from the way Spark reads data from Redshift. The Amazon Redshift data source uses Redshift\u2019s unload format to read data from Redshift: Spark first issues an unload command to Redshift to make it dump the contents of the table in the unload format to temporary files, and then Spark scans those temporary files. This text-based unload format does not differentiate between an empty string and a null string by default \u2013 both are encoded as an empty string in the resulting file. When spark-redshift reads the data in the unload format, there\u2019s not enough information for it to tell whether the input was an empty string or a null, and currently it simply deems it\u2019s a null. Solution In Scala, set the nullable to true for all the String columns: %scala import org.apache.spark.sql.types.{StructField, StructType, StringType} import org.apache.spark.sql.{DataFrame, SQLContext} def setNullableStateForAllStringColumns(df: DataFrame, nullable: Boolean) = { \u00a0 StructType(df.schema.map { \u00a0 \u00a0 case StructField( c, StringType, _, m) =&gt; StructField( c, StringType, nullable = nullable, m) \u00a0 \u00a0 case StructField( c, t, n, m) =&gt; StructField( c, t, n, m) \u00a0 }) } In Python: %python def set_nullable_for_all_string_columns(df, nullable): \u00a0 \u00a0 from pyspark.sql.types import StructType, StructField, StringType \u00a0 \u00a0 new_schema = StructType([StructField(f.name, f.dataType, nullable, f.metadata) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0if (isinstance(f.dataType, StringType)) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0else StructField(f.name, f.dataType, f.nullable, f.metadata) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0for f in df.schema.fields]) \u00a0 \u00a0 return new_schema To use this function, get the schema of original_df, then modify the schema to make all String columns to nullable, then re-read from Redshift: %scala val df = spark.read.schema(setNullableStateForAllStringColumns(original_df, true)). \u00a0 \u00a0 \u00a0 format(\"com.databricks.spark.redshift\"). \u00a0 \u00a0 \u00a0 option(\"url\", url). \u00a0 \u00a0 \u00a0 option(\"user\", user). \u00a0 \u00a0 \u00a0 option(\"password\", password). \u00a0 \u00a0 \u00a0 option(\"query\", query). \u00a0 \u00a0 \u00a0 option(\"forward_spark_s3_credentials\", true). \u00a0 \u00a0 \u00a0 option(\"tempdir\", \"path\"). \u00a0 \u00a0 \u00a0 load()", "format": "html", "updated_at": "2022-06-01T20:49:08.712Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708961, "name": "aws"}, {"id": 3219415, "name": "redshift error"}], "url": "https://kb.databricks.com/data-sources/redshift-npe"}, {"id": 1391196, "name": "Apache Spark JDBC datasource query option doesn\u2019t work for Oracle database", "views": 8164, "accessibility": 1, "description": "Learn how to resolve an error that occurs when using the Apache Spark JDBC datasource to connect to Oracle Database from Databricks.", "codename": "query-option-not-work-oracle", "created_at": "2022-06-01T19:46:15.418Z", "updated_at": "2022-06-01T20:44:27.372Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStqaVgxVTdvRDdFN0ovNndaZmJjZG55ZHFhNWo0Z0Z6VG80SlN4eEszanNVVUwrZGc1CmEzY25lM0ptM3M4RlZXN2xROHZUQTBIYzdCMGlNR25iaWozWHIwT1BHZmVxb09NSXpETnNJanZTTWg5aQovcVhUNGpjanQ4NkNCenV1NC9tai9xd3AvZUFhMEhxNDZQQTZqN1VPVSt0dFhCcUVWdEJHTzl3aHU5T1gKcTRyMTdocUltN01ZaXgzOVNQK2VvNmFxNGdhOVhrQU1vYmlLdk5vZS9iUHJxQUFQdGwwZEhrQUNsQ3Z1CmdoVFBqc3hkNjdOOHVJRjZuZ3U4ZHNrTWM0M2NOMTRFZi9KR3BHWlZzTjJGcFRyOXdpRXpOUWlBSDc3dQo5Y1dUYWhxT2NIV1ZIb2EwU2l4cWJqL1hKMWx5ekF2U09oNEF2M011cTlvZXBYYlN5S2NFMkI5cjBNQnQKL3JwbmUyd0MrLzNOOUx3bFNtRkxRZ0JWRlNZY3pGeW9NYVdZaXBISStPUlFVN1gwekFBbzFEd3FPcG1lCnJhQS9xbk50OHp2VGl5dE9GK1ZDYmxmRXdBSXhlTDV4ZXp1ejExcTNzVWZrZ3dETEJsS3YydkM5N3daYgozeC9NWlgwV1VkNHRDakcyUzA0cmo0cjZucGEwVk9QL3lNU1BjcVYyWWpsSlJ4NDBoak56UEhFUTd0VkIKeURuODJwd1pueGVvTTJ5TUc2OHVSWXlISWpEc1hXZzVTd1FDNWp2aXZURHIwbjRxMlkxMlFTMW04cU5BClJYMFAvTzRnRytrL013TXIyU21GSVlaeHV2bFhvdHE5OWg4T0owTzhnaFVqWEs0ZDRaaDBCZ2U5SFF6dwpwZXJQYWdyNzNSQk5hRzM5OWRjLzdmMDRVa3hreEdNSXMwMUVSV2haOWhZY1JWTUEvd3U1MmVpTEtqUi8KSzE4UkJ1bU11a25UcVVpZmVYQkE5bnRmS1liTVA1Ulc0WkRKdVRrbmdDR21WeU1FemVLRm1pRS9qbjlQCjhZdE1YSUI4MkRVQUw5Qk5yRE5pNnAwOE9IQlV6TlBUblRuNlZCQzlqejlqUDRzZFdLMi9zNFRBd1hFNAoyMTFucCtCRWRNSVZ2NHI5VUM3N25OT3dCYytoYkwydTNSNEJJTWI0UEJOaVJzLzc0SUVMaUZRQ1ZXTlEKaWNUL2Uxb3NTcUxVU3hzZmVOOWk4TzdFTGlXRzBUVHRtV0RBc1NKV21xWWdTQ3ROWGR2TWFrNzU0ODI4Cjh6Q0RMMkRBTWdWRGtOSzZ6bTJVOUE9PQo=.8a29194f1da12a5de773b91d2bc67161\"></div><p><br></p><h1 id=\"isPasted\">Problem</h1><p>When you use the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">query</span> option with the Apache Spark JDBC datasource to connect to an Oracle Database, it fails with this error:</p><pre>java.sql.SQLSyntaxErrorException: ORA-00911: invalid character</pre><p>For example, if you run the following to make a JDBC connection:</p><pre>%scala\r\n\r\nval df = spark.read\r\n\u00a0 .format(\"jdbc\")\r\n\u00a0 .option(\"url\", \"&lt;url&gt;\")\r\n\u00a0 .option(\"query\", \"SELECT * FROM oracle_test_table)\")\r\n\u00a0 .option(\"user\", \"&lt;user&gt;\")\r\n\u00a0 .option(\"password\", \"&lt;password&gt;\")\r\n\u00a0 .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\r\n\u00a0 .load()\r\ndf.show()</pre><p>You will see this error Message:</p><pre>java.sql.SQLSyntaxErrorException: ORA-00911: invalid character\r\nat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447)\r\n\u00a0 \u00a0 \u00a0 at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)\r\n\u00a0 \u00a0 \u00a0 at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951)\r\n\u00a0 \u00a0 \u00a0 at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)</pre><h1>Cause</h1><p>The error is due to a Spark-generated subquery alias (generated with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">query</span> option) that does not conform to Oracle Database identifier naming conventions. This bug is tracked in Spark Jira ticket <a href=\"https://issues.apache.org/jira/browse/SPARK-27596\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">SPARK-27596</a>.</p><h1>Solution</h1><p>This issue is fixed in Apache Spark 2.4.4 and Databricks Runtime 5.4.</p><p>For clusters running on earlier versions of Spark or Databricks Runtime, use the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbtable</span> option instead of the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">query</span> option. The query must be enclosed in parentheses as a subquery.</p><pre>%scala\r\n\r\nval df = spark.read\r\n\u00a0 .format(\"jdbc\")\r\n\u00a0 .option(\"url\", \"&lt;url&gt;\")\r\n\u00a0 .option(\"dbtable\", \"(SELECT * FROM oracle_test_table)\")\r\n\u00a0 .option(\"user\", \"&lt;user&gt;\")\r\n\u00a0 .option(\"password\", \"&lt;password&gt;\")\r\n\u00a0 .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\r\n\u00a0 .load()\r\ndf.show()</pre><p>You can try the same workaround for other databases when the query option fails.</p>", "body_txt": "Problem When you use the query option with the Apache Spark JDBC datasource to connect to an Oracle Database, it fails with this error: java.sql.SQLSyntaxErrorException: ORA-00911: invalid character For example, if you run the following to make a JDBC connection: %scala val df = spark.read \u00a0 .format(\"jdbc\") \u00a0 .option(\"url\", \"&lt;url&gt;\") \u00a0 .option(\"query\", \"SELECT * FROM oracle_test_table)\") \u00a0 .option(\"user\", \"&lt;user&gt;\") \u00a0 .option(\"password\", \"&lt;password&gt;\") \u00a0 .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \u00a0 .load() df.show() You will see this error Message: java.sql.SQLSyntaxErrorException: ORA-00911: invalid character at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447) \u00a0 \u00a0 \u00a0 at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396) \u00a0 \u00a0 \u00a0 at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951) \u00a0 \u00a0 \u00a0 at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513) Cause The error is due to a Spark-generated subquery alias (generated with the query option) that does not conform to Oracle Database identifier naming conventions. This bug is tracked in Spark Jira ticket SPARK-27596. Solution This issue is fixed in Apache Spark 2.4.4 and Databricks Runtime 5.4. For clusters running on earlier versions of Spark or Databricks Runtime, use the dbtable option instead of the query option. The query must be enclosed in parentheses as a subquery. %scala val df = spark.read \u00a0 .format(\"jdbc\") \u00a0 .option(\"url\", \"&lt;url&gt;\") \u00a0 .option(\"dbtable\", \"(SELECT * FROM oracle_test_table)\") \u00a0 .option(\"user\", \"&lt;user&gt;\") \u00a0 .option(\"password\", \"&lt;password&gt;\") \u00a0 .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \u00a0 .load() df.show() You can try the same workaround for other databases when the query option fails.", "format": "html", "updated_at": "2022-06-01T20:44:27.368Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708931, "name": "aws"}], "url": "https://kb.databricks.com/data-sources/query-option-not-work-oracle"}, {"id": 1391174, "name": "Kafka client terminated with OffsetOutOfRangeException", "views": 7147, "accessibility": 1, "description": "Kafka client is terminated with `OffsetOutOfRangeException` when trying to fetch messages", "codename": "kafka-client-term-offsetoutofrange", "created_at": "2022-06-01T19:10:15.709Z", "updated_at": "2022-06-01T19:45:59.697Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlTM3Q2Qm4zcTZlUTNSek1mYmtBeXliaTMxVzRHK0hWZDJrVUhnV0Zhemd2ZXdKcTdhClpnTU9EUGVWUzRHQnlUTG4ySnhmTkR5RkNMK2ZsZXh2cFYxOCtOT2pmSnZ1LzJJMEx0ckF2SFVnSnAzdApBRjFIN1YyazliRzIwVWNwTzR3OWc3Z0QwMFpGM0IzaHE2bEpsWDFnODFFK09tbVgrQ3lFTzFtaXJscUoKeWh3NjR0WUxWV1BnUHJVNk8yZkFOUHNwV1VvT2lDclMzS0t4WndCSHFxWE5FN2kxbzNwRFdDaGhuZDNYCk5vVE43UUMvVC81dFJ2d3NIaS9LdHVBRXpGSVJibEsyZFBGbXJXajBrLy9HbTZDc1BSWk5BaStDc0hDTwpPRzY1b3B2ZDY2N3BYcCsvejFwMG0weHIzUDQycnN4V2hqVk1tb2tYM3czV1Q2Nkt5VDlzdWhXUmJRZFYKR1VQTFBpd1BTTGZJK3plRUFvbTJ2NTNzc0tYZ2ZCK2twSDllTHY1TkpybEJjcW9CdkhhYjJzV20rd242CnZBc3NFenVtMDMvcFNjZUNMRUtyODRJNlFMVkppRHQvNTRSbU14dzFGOHU3enFNLzFtRVdEQk81WTlsRQp0SEZnS0hWdlJmV055NmFPK3hzM2k2OXlwSGRBUnRKMEg3U2ZlaHA3RmowWTgwTnBwYTZFMEdNbExqalAKNW53Z3Rud2ZmRytZaTE0Tm9lS0t3Qm81OVUxM1FiVHcwbHFtMWdvbVVDTVBwd01tVU9wZWJUVERxdmJJCmhrcHZxVlZ2bUk1RzRQZ090R2VGZXV3ZjRuNGhSckxSaUErWEI3a25WakdLclRLVEJBK2hxemVaWlczbQpmVzVpNjgvcFUweVgyYlAvVHgyNzI5NUEyUkJORkcyTWpJMDFwYjlFbExZejVOU1dUNFBURnE3bWFxOU0Kam9ocithYk5lSlVaRGg5aWRHN3VwVTNQY1o3L1FLcGpjallBbC9RVE9nejhXbWEvSGZqazRTNVlXeG5PCi9ybzlaSTJJMStub0IxRytrNlBTR3Z4VXNqTU9CcVBlcXF0Z09GODdRenFpNzBKR2FaWmxBckgyVVZHdQp5TFk2T3V0bk4vWSswQVJQd0M0ejlxbi9odDlXbDVaWTVJZ3hpSE92Qkp3WU90c0VsN1haNGtSNXJWRVAKaVRLYVgxQjZtSVhVVkdmd3ZDY3hRbnhHV2ZJbGUzczdPaXhnaytWMzJxREVmdm1qR0NQS1F1YThVVFZ5Cg==.5ec3cb98200c8debe1c65d91b25eb09d\"></div><h1 id=\"isPasted\">Problem</h1><p>You have an Apache Spark application that is trying to fetch messages from an Apache Kafka source when it is terminated with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">kafkashaded.org.apache.kafka.clients.consumer.OffsetOutOfRangeException</span> error message.</p><h1>Cause</h1><p>Your Spark application is trying to fetch expired data offsets from Kafka.</p><p>We generally see this in these two scenarios:</p><h2>Scenario 1</h2><p>The Spark application is terminated while processing data. When the Spark application is restarted, it tries to fetch data based on previously calculated data offsets. If any of the data offsets have expired during the time the Spark application was terminated, this issue can occur.</p><h2>Scenario 2</h2><p>Your retention policy is set to a shorter time than the time require to process the batch. By the time the batch is done processing, some of the Kafka partition offsets have expired. The offsets are calculated for the next batch, and if there is a mismatch in the checkpoint metadata due to the expired offsets, this issue can occur.</p><h1>Solution</h1><h2>Scenario 1 - Option 1</h2><p>Delete the existing checkpoint before restarting the Spark application. A new checkpoint offset is created with the details of the newly fetched offset.</p><p>The downside to this approach is that some of the data may be missed, because the offsets have expired in Kafka.</p><h2>Scenario 1 - Option 2</h2><p>Increase the Kafka retention policy of the topic so that it is longer than the time the Spark application is offline.</p><p>No data is missed with this solution, because no offsets have expired before the Spark application is restarted.</p><p>There are two types of retention policies:</p><ul>\n<li>\n<strong>Time based retention</strong> - This type of policy defines the amount of time to keep a log segment before it is automatically deleted. The default time based data retention window for all topics is seven days. You can review the Kafka documentation for <a href=\"https://kafka.apache.org/documentation/#brokerconfigs_log.retention.hours\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">log.retention.hours</span></a>, <a href=\"https://kafka.apache.org/documentation/#brokerconfigs_log.retention.minutes\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">log.retention.minutes</span></a>, and <a href=\"https://kafka.apache.org/documentation/#brokerconfigs_log.retention.ms\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">log.retention.ms</span></a> for more information.</li>\n<li>\n<strong>Size based retention</strong> - This type of policy defines the amount of data to retain in the log for each topic-partition. This limit is per-partition. This value is unlimited by default. You can review the Kafka documentation for <a href=\"https://kafka.apache.org/documentation/#brokerconfigs_log.retention.bytes\" id=\"\" title=\"\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">log.retention.bytes</span></a> for more information.</li>\n</ul><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">If multiple retention policies are set, the more restrictive one controls. This can be overridden on a per topic basis.</p>\n</div>\n</div><p>Review Kafka\u2019s <a href=\"http://kafka.apache.org/081/documentation.html#topic-config\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Topic-level configuration</a> for more information on how to set a per topic override.</p><h2>Scenario 2 - Option 1</h2><p>Increase the retention policy of the partition. This is accomplished in the same way as the solution for <strong>Scenario 1 - Option 2</strong>.</p><h2>Scenario 2 - Option 2</h2><p>Increase the number of parallel workers by configuring <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.option(\"minPartitions\",&lt;X&gt;)</span> for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">readStream</span>.</p><p>The option <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">minPartitions</span> defines the minimum number of partitions to read from Kafka. By default, Spark uses a one-to-one mapping of Kafka topic partitions to Spark partitions when consuming data from Kafka. If you set the option <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">minPartitions</span> to a value greater than the number of your Kafka topic partitions, Spark separates the Kafka topic partitions into smaller pieces.</p><p>This option is recommended at times of data skew, peak loads, and if your stream is falling behind. Setting this value greater than the default results in the initialization of Kafka consumers at each trigger. This can impact performance if you use SSL when connecting to Kafka.</p>", "body_txt": "Problem You have an Apache Spark application that is trying to fetch messages from an Apache Kafka source when it is terminated with a kafkashaded.org.apache.kafka.clients.consumer.OffsetOutOfRangeException error message. Cause Your Spark application is trying to fetch expired data offsets from Kafka. We generally see this in these two scenarios: Scenario 1 The Spark application is terminated while processing data. When the Spark application is restarted, it tries to fetch data based on previously calculated data offsets. If any of the data offsets have expired during the time the Spark application was terminated, this issue can occur. Scenario 2 Your retention policy is set to a shorter time than the time require to process the batch. By the time the batch is done processing, some of the Kafka partition offsets have expired. The offsets are calculated for the next batch, and if there is a mismatch in the checkpoint metadata due to the expired offsets, this issue can occur. Solution Scenario 1 - Option 1 Delete the existing checkpoint before restarting the Spark application. A new checkpoint offset is created with the details of the newly fetched offset. The downside to this approach is that some of the data may be missed, because the offsets have expired in Kafka. Scenario 1 - Option 2 Increase the Kafka retention policy of the topic so that it is longer than the time the Spark application is offline. No data is missed with this solution, because no offsets have expired before the Spark application is restarted. There are two types of retention policies: Time based retention - This type of policy defines the amount of time to keep a log segment before it is automatically deleted. The default time based data retention window for all topics is seven days. You can review the Kafka documentation for log.retention.hours , log.retention.minutes , and log.retention.ms for more information. Size based retention - This type of policy defines the amount of data to retain in the log for each topic-partition. This limit is per-partition. This value is unlimited by default. You can review the Kafka documentation for log.retention.bytes for more information. Info\nIf multiple retention policies are set, the more restrictive one controls. This can be overridden on a per topic basis. Review Kafka\u2019s Topic-level configuration for more information on how to set a per topic override. Scenario 2 - Option 1 Increase the retention policy of the partition. This is accomplished in the same way as the solution for Scenario 1 - Option 2. Scenario 2 - Option 2 Increase the number of parallel workers by configuring .option(\"minPartitions\",&lt;X&gt;) for readStream. The option minPartitions defines the minimum number of partitions to read from Kafka. By default, Spark uses a one-to-one mapping of Kafka topic partitions to Spark partitions when consuming data from Kafka. If you set the option minPartitions to a value greater than the number of your Kafka topic partitions, Spark separates the Kafka topic partitions into smaller pieces. This option is recommended at times of data skew, peak loads, and if your stream is falling behind. Setting this value greater than the default results in the initialization of Kafka consumers at each trigger. This can impact performance if you use SSL when connecting to Kafka.", "format": "html", "updated_at": "2022-06-01T19:45:59.694Z"}, "author": {"id": 867531, "email": "vikas.yadav@databricks.com", "name": "vikas.yadav ", "first_name": "vikas.yadav", "last_name": "", "role_id": "draft_writer", "created_at": "2022-05-10T10:04:06.319Z", "updated_at": "2022-05-11T20:22:24.392Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708928, "name": "aws"}, {"id": 2708929, "name": "azure"}, {"id": 2708930, "name": "gcp"}], "url": "https://kb.databricks.com/data-sources/kafka-client-term-offsetoutofrange"}, {"id": 1391109, "name": "Inconsistent timestamp results with JDBC applications", "views": 6565, "accessibility": 1, "description": "Timestamp records are inconsistent with JDBC applications when daylight saving time adjustments are made.", "codename": "inconsistent-timestamp-results-jdbc", "created_at": "2022-06-01T18:36:56.867Z", "updated_at": "2022-06-01T19:09:59.717Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlkT01aQlRvYU1mQXpadG9LU3IvemVSVUhyT3YyL3ZISnV1eWE3Z3k4a3hMVGtHS2E1Cmo1SXhlbkMxQ2FReHp1RGhUTXpBNVBWY0RSTjNNai9lYUZGanY0bE03c0cxRS9XV1dQajFxd2E1M2J1LwpFN2xUWWk5TTNhQ1prc1J0SVQxU0p4aEsxL1kxemdsSjVTdDJ3RUdtS1pybW9WYVNvemE4R2RMWXU0cEUKT28xWDB0MW5Jc3NBczhBalpzbzc0ZDA0VVN3bEljVFBOMkpGbXIzdDdtay9Wc0dlU29WVHlkbFZJQVpzCmdLTkRiT2hwUHREUjVjU0J1eUJhcDc2eDBHelRYZzRkelQ4ditWVWlzU0U1a3FER1A1OGpmVjRmQXlHVAoyRC8wMTJaWW80Rm1Fbk1yalNuM0VxVEt3WlJQdEJSVEpSMmNnc05YWGxGTEQ5aFJIM1RwTmsvM2RsMncKMTNhOHE3ZUdEeUdUK1d2UjZTN1BUbnphV0IvQ3hIWCtHRmVnR3lyOG9ibEJkNDA3RHRiRFhlMzkxTW5SCm5DZjhiK1JQTnVnU0Q1R01hZU1jRHJEc2RhM0VuYzBCMDdPUVJmT01ROXNSV1JkZTBZSHpESit6c3hyNApNM3dhbnpRS2ZjZy9jZ1hhcWxDS05SVkl3aFoxZFNQdFNvRnl1L3FRT1lDeG9zK1h6NmFhb1VDQUFyVkEKM1hxNFRndUFJMEF4bzdpWkdvTDVvV0RvMkJ1R3J6dldtdll4QldNTG05Qk84RXdhbGdBRTNvYzcxSUJqCmJQOVIxd0lPUmp6NUFqYXNwRXVIbVFGd1lDajZ5dkRnYTZMNzZlOHMvL0FpVU96cy8vUWdZWlhvR0Rjcgp1aDFCbEh1bEs0VUJiSzd6Zmc0MmtjanVtSSt2cWo4aTRISlJDbmhOYkwvenhic1VVSFlyWDdyMUoxYysKNWVlc1BLTW9SeXV0bjFJR0RpSWVmSjA5bHJUL01NRVhySmF0Zk1VQ2tuYm43VWRoVUhCRmxERzRLcHgvCmFuay9DWEt2ak4vYm9Jdm5Lc1c3eS93ekNrYUJHTmJjcW9ock9aSU45dEFuRUZxZDk2RVpIZ2psWmtRQgpYMlNhelhxMlRQNWJ0V25Ja2xGTDdac0pkRHZXL0J5Q3R3d05YNkFocUlQWjRuTTNDdnpqbnJOUSs0MTUKSUJhcTV1RVpkRHpIcDBMQ1YyTGRiaWVYazJLV3lwQ1ZGSmtwUTZNU1c4QytOQzgvSGNVWlJiaWI3aHBGCmx0U0JNWWVMSmlYSlVCUVNxdWpyS0g3Z2xvaWRZTmMzd3Zic0JicG42aTlhVkE4bXBCdTlMOWsvenlEVQpQZGJ6NUlvM2tJa0wwdjBxdjJkVGh0b2hreERCWUNCZkpGdzF0a1hUQi9PK09YZkYybk4ycDNGVTIzbEEKN3NzenZ4S0ppc3ZGeHpZQWtDWDQxcHR0SU9sSk5RPT0K.2e2b40d3ba36ef02e683d70d6c2814f2\"></div><h1 id=\"isPasted\">Problem</h1><p>When using JDBC applications with Databricks clusters you see inconsistent <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.sql.Timestamp</span> results when switching between standard time and daylight saving time.</p><h1>Cause</h1><p>Databricks clusters use UTC by default.</p><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.sql.Timestamp</span> uses the JVM\u2019s local time zone.</p><p>If a Databricks cluster returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2021-07-12 21:43:08</span> as a string, the JVM parses it as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2021-07-12 21:43:08</span> and assumes the time zone is local.</p><p>This works normally for most of the year, but when the local time zone has a DST adjustment, it causes an issue as UTC does not change.</p><p>For example, on March 14, 2021, the US switched from standard time to daylight saving time. This means that local time went from 1:59 am to 3:00 am.</p><p>If a Databricks cluster returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2021-03-14 02:10:55</span>, the JVM automatically converts it to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2021-03-14 03:10:55</span> because <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">02:10:55</span> does not exist in local time on that date.</p><h1>Solution</h1><p><strong>Option 1</strong>: Configure the JVM time zone to UTC.</p><p>Set the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">user.timezone</span> property to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">GMT</span>.</p><p>Review the <a href=\"https://docs.oracle.com/javase/9/troubleshoot/time-zone-settings-jre.htm#JSTGD377\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Java time zone settings</a> documentation for more information.</p><p><strong>Option 2</strong>: Use ODBC instead of JDBC. ODBC interprets timestamps as UTC.</p><ul>\n<li>Install the <a href=\"https://databricks.com/spark/odbc-drivers-download\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Databricks ODBC Driver</a>.</li>\n<li>Connect pyodbc (<a href=\"https://docs.databricks.com/dev-tools/pyodbc.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pyodbc\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/dev-tools/pyodbc\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pyodbc\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/dev-tools/pyodbc.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"pyodbc\">GCP</a>) to Databricks.</li>\n</ul><p>You can also use <a href=\"https://turbodbc.readthedocs.io/en/latest/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">turbodbc</a>.</p><p><strong>Option 3</strong>: Set the local time zone to UTC in your JDBC application.</p><p>Review the documentation for your JDBC application to learn how to configure the local time zone settings.</p>", "body_txt": "Problem When using JDBC applications with Databricks clusters you see inconsistent java.sql.Timestamp results when switching between standard time and daylight saving time. Cause Databricks clusters use UTC by default. java.sql.Timestamp uses the JVM\u2019s local time zone. If a Databricks cluster returns 2021-07-12 21:43:08 as a string, the JVM parses it as 2021-07-12 21:43:08 and assumes the time zone is local. This works normally for most of the year, but when the local time zone has a DST adjustment, it causes an issue as UTC does not change. For example, on March 14, 2021, the US switched from standard time to daylight saving time. This means that local time went from 1:59 am to 3:00 am. If a Databricks cluster returns 2021-03-14 02:10:55, the JVM automatically converts it to 2021-03-14 03:10:55 because 02:10:55 does not exist in local time on that date. Solution Option 1: Configure the JVM time zone to UTC. Set the user.timezone property to GMT. Review the Java time zone settings documentation for more information. Option 2: Use ODBC instead of JDBC. ODBC interprets timestamps as UTC. Install the Databricks ODBC Driver.\nConnect pyodbc (AWS | Azure | GCP) to Databricks. You can also use turbodbc. Option 3: Set the local time zone to UTC in your JDBC application. Review the documentation for your JDBC application to learn how to configure the local time zone settings.", "format": "html", "updated_at": "2022-06-01T19:09:59.714Z"}, "author": {"id": 831506, "email": "manjunath.swamy@databricks.com", "name": "manjunath.swamy ", "first_name": "manjunath.swamy", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T05:55:43.385Z", "updated_at": "2023-03-24T10:04:17.174Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708923, "name": "aws"}, {"id": 2708924, "name": "azure"}, {"id": 2708925, "name": "gcp"}], "url": "https://kb.databricks.com/data-sources/inconsistent-timestamp-results-jdbc"}, {"id": 1391082, "name": "Failure to detect encoding in JSON", "views": 8156, "accessibility": 1, "description": "Learn how to resolve a failure to detect encoding of input JSON files when using BOM with Databricks.", "codename": "json-unicode", "created_at": "2022-06-01T18:09:05.060Z", "updated_at": "2022-06-01T18:36:24.492Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStLbEhyQlNuMklyOXFhZWNTMm5OUnoyVFl0UDQyR1pyUEw0ZEJ6WXBBNGgrRU1BVktmCnM2NVl1a04xMzZzNnlYWXFFUVdvZ0ZZcFJxblcrMjJwSzRmNHQwVG5rcWtSL2Y5cGx0QnRRR2k1MEJ1TwpIQ3VlOU1oOU5MUEZPamI5VVpxLzdlNFhyNEdPYnlWbW03VFc0QXNtc2xkQTV0YStjUFIyK2dxWWI1TnAKSGszWUpiYkljVFQ0YU5Iblh0WVFEU3RBMlkwZVBVZzJ1dkVRemhYQU9nZFZRRVU1d2Y3MXBINW5xcGFuClFaUnFGNG9Ecm5kVHVnN0pLc2RnSlFCdS9kd2QrNDBySkZqTG1XejFsOFBreGJKMzlDTHJ1ajgxYXdYVwplUWwzUGdGZUhERHRxVkpySU1XeFpreEJrMUZHN1hWcGtrUG5HQTNkNE0wcHFTS0paS2lQRjFFN01SaEwKdzJEajY5UjVwMkNRZU51eldVYUo2VXRML0d1ZVFTbElHRUdpZnZjam05TnBWKzgraEVNZ25ZeUI3WXltCkM2QXgzZE1oeGc5bDZHanBhTzZoNEdIV0dvbGpCSllmdnlyc3lHVkpQRldjNGs1c0srOG43MXpTVUdjVgozN2Y3TXpTcFNjQ2NjcHhyM0gwbWVSTkdPczd0VElhZk9zWXJ4eStoc0tQa3FQWVBnamk3NDduaWQrd2cKZlBXZ0ZWcG1EdTZPNUE4YTZwbkZ0MHdmTXF6Q1FUd0tEWllVK01CRHVObFM3STZ1M2ZmNUcrSERlM0k3CmQ4NkluTUdzTnFOMFdGWkJKTjV0alZwYUJMRmtlSUJ4U2NKYWtVQVo1MXFKbGFDZ1pDeExRTWNucVRvbQpmZWk0VTNtWlZLTkVyWW1CdzBYdnRsNnRzTmxKUytMeklWcVFnWFlSd25Oc2svb3ZCQnZQSzZGb2hoTmcKWHpLNnNMakpxaHVBdFN4OVVKUXNBQ2xLRmV4WHUrMzRGRENZYkFmZGtEdjBEWXlLeUlUaUIxQ3BEUkZ4CnU1WlpYZ2tPUU5iaFR5d3p1ZWxqU0g2aGtTNGtlVWpLQ2o1TVE4dXZsa3M0QlFOQ2NvV1FUeCtwQnRsbAorZ3F5YkZBSWcxRUtkVmZ1MjMvU3BSOGcyMm1vVWh5ZXBDTVRrNkN5MUJoMTFhZkIvQ2pXRElKQ292d2cKNEk2NXR6RUo1UzJWd0c0b3pIQTlLR0o5VHV6MXNSKys4clJzeHIvVmNIdW9ldFE5ODJpUVRySVl4SU81Cg==.a45f0bec0fecb8d7579e0cf5faa5423a\"></div><h1 id=\"isPasted\">Problem</h1><p>Spark job fails with an exception containing the message:</p><pre>Invalid UTF-32 character 0x1414141(above 10ffff) \u00a0at char #1, byte #7)\r\nAt org.apache.spark.sql.catalyst.json.JacksonParser.parse</pre><h1>Cause</h1><p>The JSON data source reader is able to automatically detect encoding of input JSON files using <a href=\"https://en.wikipedia.org/wiki/Byte_order_mark\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">BOM</a> at the beginning of the files.</p><p>However, BOM is not mandatory by Unicode standard and prohibited by <a href=\"https://tools.ietf.org/html/rfc7159\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">RFC 7159</a>.</p><p>For example, section 8.1 says, <em>\"</em>Implementations MUST NOT add a byte order mark to the beginning of a JSON text.\"</p><p>As a consequence, Spark is not always able to detect the charset correctly and read the JSON file.</p><h1>Solution</h1><p>To solve the issue, disable the charset auto-detection mechanism and explicitly set the charset using the encoding option:</p><pre>%scala\r\n\r\n.option(\"encoding\", \"UTF-16LE\")</pre><p><br></p>", "body_txt": "Problem Spark job fails with an exception containing the message: Invalid UTF-32 character 0x1414141(above 10ffff) \u00a0at char #1, byte #7) At org.apache.spark.sql.catalyst.json.JacksonParser.parse Cause The JSON data source reader is able to automatically detect encoding of input JSON files using BOM at the beginning of the files. However, BOM is not mandatory by Unicode standard and prohibited by RFC 7159. For example, section 8.1 says, \"Implementations MUST NOT add a byte order mark to the beginning of a JSON text.\" As a consequence, Spark is not always able to detect the charset correctly and read the JSON file. Solution To solve the issue, disable the charset auto-detection mechanism and explicitly set the charset using the encoding option: %scala .option(\"encoding\", \"UTF-16LE\")", "format": "html", "updated_at": "2022-06-01T18:36:24.486Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708884, "name": "aws"}, {"id": 2708885, "name": "azure"}, {"id": 2708886, "name": "gcp"}], "url": "https://kb.databricks.com/data-sources/json-unicode"}, {"id": 1390611, "name": "CosmosDB-Spark connector library conflict", "views": 7711, "accessibility": 1, "description": "Learn how to resolve conflicts that arise when using the CosmosDB-Spark connector library with Databricks.", "codename": "cosmosdb-connector-lib-conf", "created_at": "2022-06-01T01:25:04.514Z", "updated_at": "2022-06-01T18:08:37.862Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStmajdnZXVzNnpDSVJOSDBIRWpGYnp2anNNRkltd0cwckpIOEhQTS9yeW01Sm5TVGo2CkpNaS9SZWd0NkZMTDQrWFhTYnJoU2JMSURXWlk2QVlmUTBVQ291VksyNnhaM0dSY1ltWVQ2TisyZVhSOApxRUExS1pIV1hCWFkvNVRVVGFMS29FSVdEWXpISkxCZWxBTjl1VGR1RmJmZXBYT1hTcU9JZ0xMamdISUoKWDE1SytTbUZxS1BuZkpEN24rc0ltajh1RnVUZEdiSFJGb0hrMldKenZVNGx1c2NmZEcyVjI4RzVBOUxRCnBoMXJUbGNjNW5hcnFYSlB0NU5aY3o1ZTYyV2pEN1BTdHRVcjJnajgwZlFzdXFzL09rWDVJdFh1STRXTApZa3h2Vk9sL05aclE1cTF3cnd4V1JXWE9zQ3R3RmFqcHdzeUlmWFhGUGU4TzBvZzdjTnVUcVZFdVlKeVQKYy85elVqSS9BR3JuYlV3QW0rREVXYzRPSWtZUlR2N2k2UktqTjE4TFM3Si91R1ZMQlJMaENDV1pRbW1vCjRhaUgvRW1Pb1QrUkRIem1LSHk0Q1FsRjNhT2FnOFA0U1pndnRFMlFhdmtaR3ZRbUdIZ1M5Vk5DclJJRQpjYTV1dVdRTS8xemt1eWo4dXk4MjdtekoraTcrWWhTQ3lReUV2WGhvZ3JFZm82UmJxbXl0MFpBWm5Ma0sKZ3JoMTQxTzMzdTloTHAvRnpqbldMVEhwdlowRGVvZ3B4T1VQMXI4MUpEMDZBbUMrZ1dRa3FuNnJBb25ECjM3Wi9xckJUN1lyT2NyYW1FaHZFS2ZYZmZDd0N4a3RyeW5ZQ05RS0tjSGl4cWJMOXIydm8xOHFSQ3Z1LwpFQ2NMT3dGeUVucEZUUWRDVVNrT2xnYU5kNlFZYUUvdGNYVDdDQks5T1NEQytKMmdWbmhWbTE4MWxiMm4KTnZGWnByTUwyK0dJZ2taVzFkNlZzSEQ0bGNCY295Y1lHanJVaW9jYkxVMGMxVUpFNGh6QU5jN0kvTnpRCmZFZkpxSHU4bXU5ODF5eGhWVUZsR2dPOXMrdGtJa0lYeGp6ZTZ0dXdXeVlZMjgvQnk2R1NOaDR6cUtoZApmSm9rSEcwY040cUZrdkc3dlFNRStTb3BzN0k0WkQ3WXFNcEx4OFRqM1RBV2dEM09XV3BwTWlUQTAzMlUKWVdxM0ZoT1hML1A1RG03eVFlSDd4Q1JrNk1pRTZaN0g5emFWd3lnQ2tocHUyZkVDSjZOaW9zemVuYmNuCkpldWJNamRib0JMQkRpQjZja3N1L2c9PQo=.d777a8dafb03d41007e9d2928cd67714\"></div><p id=\"isPasted\">This article explains how to resolve an issue running applications that use the CosmosDB-Spark connector in the Databricks environment.</p><h1>Problem</h1><p>Normally if you add a Maven dependency to your Spark cluster, your app should be able to use the required connector libraries. But currently, if you simply specify the CosmosDB-Spark connector\u2019s Maven co-ordinates as a dependency for the cluster, you will get the following exception:</p><pre>java.lang.NoClassDefFoundError: Could not initialize class com.microsoft.azure.cosmosdb.Document</pre><h1>Cause</h1><p>This occurs because Spark 2.3 uses <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">jackson-databind-2.6.7.1</span>, whereas the CosmosDB-Spark connector uses <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">jackson-databind-2.9.5</span>. This creates a library conflict, and at the executor level you observe the following exception:</p><pre>java.lang.NoSuchFieldError: ALLOW_TRAILING_COMMA\r\nat com.microsoft.azure.cosmosdb.internal.Utils.&lt;clinit&gt;(Utils.java:69)</pre><h1>Solution</h1><p>To avoid this problem:</p><ol>\n<li>Directly download the CosmosDB-Spark connector Uber JAR: <a href=\"https://repo1.maven.org/maven2/com/microsoft/azure/azure-cosmosdb-spark_2.3.0_2.11/1.2.2/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar</a>.</li>\n<li>Upload the downloaded JAR to Databricks following the instructions in Upload a Jar, Python egg, or Python wheel (<a href=\"https://docs.databricks.com/libraries/workspace-libraries.html#uploading-libraries\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Upload a Jar, Python egg, or Python wheel\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/libraries/workspace-libraries#uploading-libraries\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Upload a Jar, Python egg, or Python wheel\">Azure</a>).</li>\n<li>Install the uploaded library as a Cluster-installed library (<a href=\"https://docs.databricks.com/libraries/cluster-libraries.html#cluster-installed-library\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Cluster-installed library\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/libraries/cluster-libraries#cluster-installed-library\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Cluster-installed library\">Azure</a>)</li>\n</ol><p>For more information, see Azure Cosmos DB (<a href=\"https://docs.databricks.com/data/data-sources/azure/cosmosdb-connector.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Azure Cosmos DB\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/data/data-sources/azure/cosmosdb-connector\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Azure Cosmos DB\">Azure</a>).</p>", "body_txt": "This article explains how to resolve an issue running applications that use the CosmosDB-Spark connector in the Databricks environment. Problem Normally if you add a Maven dependency to your Spark cluster, your app should be able to use the required connector libraries. But currently, if you simply specify the CosmosDB-Spark connector\u2019s Maven co-ordinates as a dependency for the cluster, you will get the following exception: java.lang.NoClassDefFoundError: Could not initialize class com.microsoft.azure.cosmosdb.Document Cause This occurs because Spark 2.3 uses jackson-databind-2.6.7.1, whereas the CosmosDB-Spark connector uses jackson-databind-2.9.5. This creates a library conflict, and at the executor level you observe the following exception: java.lang.NoSuchFieldError: ALLOW_TRAILING_COMMA at com.microsoft.azure.cosmosdb.internal.Utils.&lt;clinit&gt;(Utils.java:69) Solution To avoid this problem: Directly download the CosmosDB-Spark connector Uber JAR: azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar.\nUpload the downloaded JAR to Databricks following the instructions in Upload a Jar, Python egg, or Python wheel (AWS | Azure).\nInstall the uploaded library as a Cluster-installed library (AWS | Azure) For more information, see Azure Cosmos DB (AWS | Azure).", "format": "html", "updated_at": "2022-06-01T18:08:37.858Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708606, "name": "aws"}, {"id": 2708607, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/cosmosdb-connector-lib-conf"}, {"id": 1390610, "name": "Troubleshooting JDBC/ODBC access to Azure Data Lake Storage Gen2", "views": 7762, "accessibility": 1, "description": "Learn how to troubleshoot JDBC and ODBC access to Azure Data Lake Storage Gen2 from Databricks.", "codename": "access-blobstore-odbc", "created_at": "2022-06-01T01:21:09.365Z", "updated_at": "2022-06-01T01:24:52.560Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlYNmFzVVNlaktZeTBXL1FsSUsyUUJtNGN1NU1NZGo1bmNuK2I3ZjY2YXlnUVJqLzdNCkl3Z29RVHlEbVJUK3UxSTVaWXBtU2VORW5YUEJZMEFmdDRwTHorZGZQdzErNVB5TkNyNG9HZEJlQW1lbgp6THd4am5oVG5LR2pKRXJVclFqbkxWQW96ZWYybHZxemw2MHdMaW51MnJXbUxWNUhSYjIxYkxQOTd5amMKT0kwT0ZIenJhWlVRSVdBYnNaOVBucGlPd2w3S256QnhPNm5ERlIvbnU1R2lUNFgzM3pYUWRCY2tXUlptCkFBb3Y4Q2dUTXJLdThJUU9lTlVoTHRjODgxcWVLc2wwSmFzMjBiK1o5YmxDeWw0R3FWQlI3TThZNTJNNAp5NzZyZS9vQUJaRFByajlBL2Y3ZEt2Y3d5cVU4QlZrLzltUW9heUJ4dWdNZFVDQllQMHRLQkNEcEFtc0gKT3d4eEtqcENtSlVwdC9kcE5vMjRRdEdlQ01paHhURkdXeFlBTTZ3MS8vY0lDYlVyNTRZZUkxb3RBVU5nCndLb0N6THJSTGJzWU5kd3dqZytQQWhXOEhsVFNTeUNrY0l0d1VnWUV2YlV2M2c2WmU0Z2RVcjhIUU5kbQpQRzM1QzRBZVNKc0xnbyswQmVrRVpDZDlFL3FGOVF6dFJtd3N6NjVZTmV1REpSaE9UbzRtTkl6TUVGMmIKVmVjd1ZUdDl6dEtnaUErR2M3b3M2YnBrSTNDd3Uyb2dYckpPUFFJcXFSRzN6Q0sraHJGK0QxSFVhVStyCm9mSWMxNTFLa3hTYThNNnBvc2pTSnp0QVRDQjJkK1RlOC9OeEdrOW9KNFhydytTWFI2eFloZERSQ29hVwp2Ui9XcUFObHNYYkx5TG5laHptRG83UjJ3UEpydkI3VWdiV043aDhVVFhoUDEvaTJ3ckM5ZWNjRzJNd1MKa2dTZWN4R0kyL3dPc1JzdjFDcjdJNXdtY3M0QUs3RVBUS2ZoTDBEbmVuTUVoa1JtUDFqbDdJZDlrdWRXCkVkZkdOdUdGQlVyTVA1UGIxallsN3JibjNRcE8yS0NldjhKNHZDb1F1RElLTzVGek1UUi9uNDNGQU44Kwp0RDlIbWpSS3RzWGxJUTNBVTRVU2QrRUN5OXBzMC9vUjZTdndYWGRtdHBIWFBKdjJhYmpHbThBdWdycWMKNllGNW9PdHBpRkRrOXE4SzNsTm0vOElPemtLaGFtK0pRRHdaUEg3aGlKbm14NnI0enF1RzVCeDNuR3dTCg==.e6c671c2c83e870585972afc65e7daf5\"></div><h1 id=\"isPasted\">Problem</h1><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">In general, you should use Databricks Runtime 5.2 and above, which include a built-in Azure Blob File System (ABFS) driver, when you want to access Azure Data Lake Storage Gen2 (ADLS Gen2). This article applies to users who are accessing ADLS Gen2 storage using JDBC/ODBC instead.</p>\n</div>\n</div><p>When you run a SQL query from a JDBC or ODBC client to access ADLS Gen2, the following error occurs:</p><pre>com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalArgumentException: No value for dfs.adls.oauth2.access.token.provider found in conf file.\r\n\r\n18/10/23 21:03:28 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING,\r\njava.util.concurrent.ExecutionException: java.io.IOException: There is no primary group for UGI (Basic token)chris.stevens+dbadmin (auth:SIMPLE)\r\n\u00a0 at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\r\n\u00a0 at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\r\n\u00a0 at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\r\n\u00a0 at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\r\n\u00a0 at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344)\r\n\u00a0 at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316)\r\n\u00a0 at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\r\n\u00a0 at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\r\n\u00a0 at com.google.common.cache.LocalCache.get(LocalCache.java:3932)\r\n\u00a0 at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\r\n\u00a0 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:158)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:257)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:313)\r\n\u00a0 at\r\n\u00a0 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\r\n\u00a0 at scala.collection.immutable.List.foldLeft(List.scala:84)\r\n\u00a0 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:87)\r\n\u00a0 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:79)</pre><p>When you run the query from the SQL client, you get the following error:</p><pre>An error occurred when executing the SQL command:\r\nselect * from test_databricks limit 50\r\n\r\n[Simba][SparkJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: com.google.common.util.concurrent.UncheckedExecutionException: com.databricks.backend.daemon.data.common.InvalidMountException: Error while using path /mnt/crm_gen2/phonecalls for resolving path '/phonecalls' within mount at '/mnt/crm_gen2'., Query: SELECT * FROM `default`.`test_databricks` `default_test_databricks` LIMIT 50. [SQL State=HY000, DB Errorcode=500051]\r\n\r\nWarnings:\r\n[Simba][SparkJDBCDriver](500100) Error getting table information from database.</pre><h1>Cause</h1><p>The root cause is incorrect configuration settings to create a JDBC or ODBC connection to ABFS via ADLS Gen2, which cause queries to fail.</p><h1>Solution</h1><p>Set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.hadoop.hive.server2.enable.doAs</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span> in the cluster configuration settings.</p>", "body_txt": "Problem Info\nIn general, you should use Databricks Runtime 5.2 and above, which include a built-in Azure Blob File System (ABFS) driver, when you want to access Azure Data Lake Storage Gen2 (ADLS Gen2). This article applies to users who are accessing ADLS Gen2 storage using JDBC/ODBC instead. When you run a SQL query from a JDBC or ODBC client to access ADLS Gen2, the following error occurs: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalArgumentException: No value for dfs.adls.oauth2.access.token.provider found in conf file. 18/10/23 21:03:28 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING, java.util.concurrent.ExecutionException: java.io.IOException: There is no primary group for UGI (Basic token)chris.stevens+dbadmin (auth:SIMPLE) \u00a0 at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299) \u00a0 at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286) \u00a0 at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) \u00a0 at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135) \u00a0 at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2344) \u00a0 at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2316) \u00a0 at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) \u00a0 at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) \u00a0 at com.google.common.cache.LocalCache.get(LocalCache.java:3932) \u00a0 at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) \u00a0 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:158) \u00a0 at org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:257) \u00a0 at org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:313) \u00a0 at \u00a0 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) \u00a0 at scala.collection.immutable.List.foldLeft(List.scala:84) \u00a0 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:87) \u00a0 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:79) When you run the query from the SQL client, you get the following error: An error occurred when executing the SQL command: select * from test_databricks limit 50 [Simba][SparkJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: com.google.common.util.concurrent.UncheckedExecutionException: com.databricks.backend.daemon.data.common.InvalidMountException: Error while using path /mnt/crm_gen2/phonecalls for resolving path '/phonecalls' within mount at '/mnt/crm_gen2'., Query: SELECT * FROM `default`.`test_databricks` `default_test_databricks` LIMIT 50. [SQL State=HY000, DB Errorcode=500051] Warnings: [Simba][SparkJDBCDriver](500100) Error getting table information from database. Cause The root cause is incorrect configuration settings to create a JDBC or ODBC connection to ABFS via ADLS Gen2, which cause queries to fail. Solution Set spark.hadoop.hive.server2.enable.doAs to false in the cluster configuration settings.", "format": "html", "updated_at": "2022-06-01T01:24:52.555Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.197Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708604, "name": "aws"}, {"id": 2708605, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/access-blobstore-odbc"}, {"id": 1390609, "name": "Optimize read performance from JDBC data sources", "views": 8644, "accessibility": 1, "description": "Learn how to optimize performance when reading from JDBC data sources in Databricks.", "codename": "jdbc-optimize-read", "created_at": "2022-06-01T01:15:44.726Z", "updated_at": "2022-06-01T01:20:59.838Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStTMkltNjRncXJVZmlpUzlSRjBsZnZzdHNKb0czaG5sb25ZOEdUcXBjbDNMQ2h1UGQzCnNzNnhLaU16cHI2aVJhc3BJUU5MVFRFVlBVTEdLY1NPNE9ma2p2dEpuRGRHd2phUHBla2lOVGJsdjgzcwpZSGRqSGhFRWlGVENGSFJXQjZiS29kNnJUMm0vVHZ5bUpjSUhVakNLVmFCUDRPRUZFM29GNlVJVGpLR0YKYmdjTlNRazVSOWQzSTZab2IvRXNIK01Vc09DZTA5ay9pdWl4T1YyMTdrbjBoT0k4MEp2UUpSUEdrYzZlCnkyaUNqNi9vRmZaZTZCZ0RJcDgzRi9BaE1LbjNWZ2h6aVRNNERKdjlLejdPVm16aFpYaEF2OENaS1MrOApROXZrLzlLUTFtR1BxVktLOTlJU0oxYkFjZE1YdDhvRm1jcW5CU3YvSEFwU2JvY3pIaXFVb0g5emp0MjAKMEQ4aTR3ZnVvd0p3WVZ2Q3psUW9aaC9VNnhlSkQzMExzTS94MUlXS1JUNEMwUmpVSnpSYzd3YXU3bFl1CnBGRUc3ZGxLc0RaVk81SExYR3NtdWFJYjZ0QTdKVDB0N2lHbGtYWnptQmlNaFlZQlFhTElSYktaSitVcAp0TGduS0pqSENSVmc5RnFoNXFEcytGME0vUUtHdjdtdTJ5SnNraWxqSnBjeDZOSDBlOG9CaHNQUThPNXYKdGZ1OVk5Y1hkVTc2cU5ERjdJQTdVdnl5Q0FzZVZ0WlovM0dhYUtabm80WmZ6Y1Z0MksxRTNtZ1lVOXdYCkNrQnlQSFhOSGxZZ0dPM2JGUDZ2RXlRcG8vYmV1bXNNUFdkQlNTTzdVQ2dCL0J4QU1UejQwOUZ2aE9tbQoxbnRLVHQ4TkhJampBbmJZUktIanpsZjM0QVhXWGY0YWZHM2NFcTFUaEIyOHVMdHVnWG5PQ0FJZnR4aXAKWUdndk1SeGtRNzU4QlpmamIvRDRwNWk5aGN2ZURmcGcyelorM0lGcm13UTRYR0dhdzFLZnpad2FxVHFICkp0bWFLNUJQcGZYeUtSNlN1ZWhFdG00WjFZRXNUUUN1MmF4cjNMakZtVHZYZjcxWjRlMDFMQW12OTJwTApReTVhc1VTREdZVG1DWFhJcmJ6UGc2b3N6c0dUQno0UlFUdGt3YkZLekI4TUY4VEtnRmpTczBYemhweVkKR1EremR4NXZDMWNHdWhtcjIrVndsRmZaVHdwei9mZGpUeit4RXdTbVo2cjV3NjVDT0paaUNpNUpjVnpKCg==.9391b673e2f2568566af5fe44760df7f\"></div><h1 id=\"isPasted\">Problem</h1><p>Reading data from an external JDBC database is slow. How can I improve read performance?</p><h1>Solution</h1><p>See the detailed discussion in the Databricks documentation on how to optimize performance when reading data (<a href=\"https://docs.databricks.com/data/data-sources/sql-databases.html#optimize-performance-when-reading-data\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"optimize performance when reading data\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/data/data-sources/sql-databases#optimize-performance-when-reading-data\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"optimize performance when reading data\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/data/data-sources/sql-databases.html#optimize-performance-when-reading-data\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"optimize performance when reading data\">GCP</a>) from an external JDBC database.</p>", "body_txt": "Problem Reading data from an external JDBC database is slow. How can I improve read performance? Solution See the detailed discussion in the Databricks documentation on how to optimize performance when reading data (AWS | Azure | GCP) from an external JDBC database.", "format": "html", "updated_at": "2022-06-01T01:20:59.837Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708601, "name": "aws"}, {"id": 2708602, "name": "azure"}, {"id": 2708603, "name": "gcp"}], "url": "https://kb.databricks.com/data-sources/jdbc-optimize-read"}, {"id": 1390593, "name": "Unable to read files and list directories in a WASB filesystem", "views": 8271, "accessibility": 1, "description": "Learn how to interpret errors that occur when accessing WASB append blob types in Databricks.", "codename": "wasb-check-blob-types", "created_at": "2022-05-31T23:13:25.654Z", "updated_at": "2022-06-01T01:14:56.744Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlicFRrNEVxSjBWMkIwVXdDbDJuT1Q2Y05iR282S3p3ZzRVSkRLdmxBK1pweEZLTTVKCjR5ZTl2U212VWk5ZTVtYlJVSFhlTXhtMHpnVy95UjZZUlNaQ0JNUHh2Z1BTTlIzcFFGZjhFazQwUkRCQgo1eFZJU2tLRVZiVjBYcE1UTm1YRG5kaldGM3J5MTY5ZytXdkdCMXhUY0RpVUx3akUwZm1qWENUY0F3R0sKdmh5V0RObDJZdU5VZ01VM1ByWjBteDd5REVMVE9ROXlVQlRBM2xqMFRrSzBheEp1MURyeEwwb3BuU1FaCm9YekVvUDZPcGQ0TnMxYmEwcDY1N3A0UExMNlBOVFhnOS9iRS9CY042eFJVOFNCZlgxc3lVM3I3MG1JYwpucUZTMGdHTWxCSTJXRjlSc3d0d3lWSm5VMUlNcVZnbEZNdERaZzlxbXlNb2RFYUdwdWpEbHZmYWxVcmEKL1Bhc2svdStUUmZIVWtRQU81Y0VJVDl4N1NhZ2ZLWHlqT2Rha2N3WkRyZHIvT3NHTUJCKzBjVFNpMldpCnJnSGJpVzkxSFBmU3FuTkFvcFNBcTZyOFh4L3R6dWlHMkgwck9ESHdXNFVKcWo2OWhZMG1vczV6M0lyQgpFTWtUNDl0S1RES2dQVXNWQmt2RVFLeUNSVmp3Wm1FRWNkWXNYK3YvNmNTOW16N3g3R2NVZ0dzc2xjN3MKZDY0UjVNeVFWViszMFJjT1V5aUp5MEFrVGY4M2hkTVp4TW9SVzlIK0c0M3V0dktxMStBY2htTk1VRWtVCnU3SXBNWmV1SjFqa2V2M0djL2NQbjNUeVhqejNVcktVY284QzJhc0ErZWJqT0pOOXcvOE9NN1JWZ0E2RwpodFkzNlJ2VU1ZdzJ0eFFScVBFblRzQ0tNTGdkUlBxdC9hcDF2NWdGc2FMSk55VmV6bkVXbVBPU1dDbFUKR0dPdFphZ1B5TGF6YUE5R1ZvWmI5dnh1TVlUeDlYWUFjMnZGVFpoT1IwVDNhR1NyN3lSKzVjVC96amQ1CnVlSEErK2pLekFGWGlZU3NXUThnYTZhZEpMVHZPdmpONjJEbGpiMldyRnhreVFZRnhWc3Q3YVRPaFBaSQowZW5vSU5VZVBSaWtQZkpxL0tqSkFycWNSbkthOW5WS0JjNUdiY1lpRDN2OXpoc1pSNUlpRUMyV1VsSzYKMkRYNGdib2JoNjBNVHBYcURqWHNKVHBYQW1nOTJUdVFDM3ZobWxPM1RHQ2FWMFlTWWdwWktSSHlrS0J5Cg==.71730dbf565e023eb25832e1895d7f67\"></div><h1 id=\"isPasted\">Problem</h1><p>When you try reading a file on WASB with Spark, you get the following exception:</p><pre>org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 19, 10.139.64.5, executor 0): shaded.databricks.org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: Incorrect Blob type, please use the correct Blob type to access a blob on the server. Expected BLOCK_BLOB, actual APPEND_BLOB.</pre><p>When you try listing files in WASB using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.fs.ls</span> or the Hadoop API, you get the following exception:</p><pre>java.io.FileNotFoundException: File/&lt;some-directory&gt; does not exist.</pre><h1>Cause</h1><p>The WASB filesystem supports three types of blobs: block, page, and append.</p><ul>\n<li>Block blobs are optimized for upload of large blocks of data (the default in Hadoop).</li>\n<li>Page blobs are optimized for random read and write operations.</li>\n<li>Append blobs are optimized for append operations.</li>\n</ul><p>See <a href=\"https://docs.microsoft.com/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Understanding block blobs, append blobs, and page blobs</a> for details.</p><p>The errors described above occur if you try to read an append blob or list a directory that contains only append blobs. The Databricks and <a href=\"https://hadoop.apache.org/docs/current/hadoop-azure/index.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Hadoop Azure</a> WASB implementations do not support reading append blobs. Similarly when listing a directory, append blobs are ignored.</p><p>There is no workaround to enable reading append blobs or listing a directory that contains only append blobs. However, you can use either Azure CLI or Azure Storage SDK for Python to identify if a directory contains append blobs or a file is an append blob.</p><p>You can verify whether a directory contains append blobs by running the following Azure CLI command:</p><pre>az storage blob list \\\r\n\u00a0 --auth-mode key \\\r\n\u00a0 --account-name &lt;account-name&gt; \\\r\n\u00a0 --container-name &lt;container-name&gt; \\\r\n\u00a0 --prefix &lt;path&gt;</pre><p>The result is returned as a JSON document, in which you can easily find the blob type for each file.</p><p>If directory is large, you can limit number of results with the flag <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">--num-results &lt;num&gt;</span>.</p><p>You can also use Azure Storage SDK for Python to list and explore files in a WASB filesystem:</p><pre>%python\r\n\r\n\r\niter = service.list_blobs(\"container\")\r\nfor blob in iter:\r\n\u00a0 if blob.properties.blob_type == \"AppendBlob\":\r\n\u00a0 \u00a0 print(\"\\t Blob name: %s, %s\" % (blob.name, blob.properties.blob_type))</pre><p>Databricks does support accessing append blobs using the Hadoop API, but only when appending to a file.</p><h1>Solution</h1><p>There is no workaround for this issue.</p><p>Use Azure CLI or Azure Storage SDK for Python to identify if the directory contains append blobs or the object is an append blob.</p><p>You can implement either a Spark SQL UDF or custom function using RDD API to load, read, or convert blobs using Azure Storage SDK for Python.</p>", "body_txt": "Problem When you try reading a file on WASB with Spark, you get the following exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 19, 10.139.64.5, executor 0): shaded.databricks.org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: Incorrect Blob type, please use the correct Blob type to access a blob on the server. Expected BLOCK_BLOB, actual APPEND_BLOB. When you try listing files in WASB using dbutils.fs.ls or the Hadoop API, you get the following exception: java.io.FileNotFoundException: File/&lt;some-directory&gt; does not exist. Cause The WASB filesystem supports three types of blobs: block, page, and append. Block blobs are optimized for upload of large blocks of data (the default in Hadoop).\nPage blobs are optimized for random read and write operations.\nAppend blobs are optimized for append operations. See Understanding block blobs, append blobs, and page blobs for details. The errors described above occur if you try to read an append blob or list a directory that contains only append blobs. The Databricks and Hadoop Azure WASB implementations do not support reading append blobs. Similarly when listing a directory, append blobs are ignored. There is no workaround to enable reading append blobs or listing a directory that contains only append blobs. However, you can use either Azure CLI or Azure Storage SDK for Python to identify if a directory contains append blobs or a file is an append blob. You can verify whether a directory contains append blobs by running the following Azure CLI command: az storage blob list \\ \u00a0 --auth-mode key \\ \u00a0 --account-name &lt;account-name&gt; \\ \u00a0 --container-name &lt;container-name&gt; \\ \u00a0 --prefix &lt;path&gt; The result is returned as a JSON document, in which you can easily find the blob type for each file. If directory is large, you can limit number of results with the flag --num-results &lt;num&gt;. You can also use Azure Storage SDK for Python to list and explore files in a WASB filesystem: %python iter = service.list_blobs(\"container\") for blob in iter: \u00a0 if blob.properties.blob_type == \"AppendBlob\": \u00a0 \u00a0 print(\"\\t Blob name: %s, %s\" % (blob.name, blob.properties.blob_type)) Databricks does support accessing append blobs using the Hadoop API, but only when appending to a file. Solution There is no workaround for this issue. Use Azure CLI or Azure Storage SDK for Python to identify if the directory contains append blobs or the object is an append blob. You can implement either a Spark SQL UDF or custom function using RDD API to load, read, or convert blobs using Azure Storage SDK for Python.", "format": "html", "updated_at": "2022-06-01T01:14:56.702Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708596, "name": "aws"}, {"id": 2708597, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/wasb-check-blob-types"}, {"id": 1390566, "name": "Failure when mounting or accessing Azure Blob storage", "views": 9291, "accessibility": 1, "description": "Learn how to resolve a failure when mounting or accessing Azure Blob storage from Databricks.", "codename": "access-blob-fails-wasb", "created_at": "2022-05-31T22:07:00.730Z", "updated_at": "2022-05-31T23:12:53.094Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThsckdsamxodnVFaHhVZHRWMXVLOGhNZStHaWF1c0tzd0lqYU5xaTZQK1ZvTEZ5MnVsCkIxaCtwSCt4UkdvY2dXU3RwbHpEbTdrSFNFeTJNWkhIa0NYODEyby9Ta1ZqZTJ0NXpyK3ZoajRRekordAp0SDE2cUdBcjJQeVhmaDFFKzltNUJ4UG5oMHBQSVhuMHJOejI0YlRkWnBDdkFMRjNHR3Y4QzA4SStYTloKNXV1UmRBNlpFTThOTno4MUQ3MC8raVEwa1lCRVArREVQZ2pBRDV1U29TaWh0RGhCUGVibHRpUmFwbkFqCnBrVWFSZUlwSFM1djhHWGZHSmp6cG1OUUVxR3RNdmcrZnMra0tkVWplQysxUGIyUE1RWmI5d25sNjREcQpKYlZUb2FDVWRpRlh1VmlTSnh3UmRKSGNLSVY4YzN5aDJva2dsdEs3dXB1aWJUcmJlSjNYSEt0bVRqTW8KaENQTkZoZEpMbFArVGh5SW9UQkNUSTF2Ukd3NzJJVURjYkFSdzRSOUxZY1l5YzJHOU9nS1FYMVIxbkhKCnh5UmxVRXBnZmxPdXdML1M2NFBmVjdSTmJUR0orQzZUVGtybWlMZ3ZCdkJEamYvWThCTVdvVmhtVHZCSwpuZ0ZBV3hibEJ4Uk4reEJhT0tmUlByenhLWXo4dnFidTdRT2lRc21heXRvODFCdzgzVzNjVjNCblozWFEKRDkxYnYxcU83MUJ5d1lMdWN2N04yMktYb0FFZy9GS3RlNE9iUjFsVFdtUENpM1dDdkV2UVRoSmdNd2l4CkZPZ2VHYTNHMEhGV2h4WjhVUkdVN2tRNWthNGpyVEtxRWt4M2ZEczJ0Zk5lQWc0RkE1M3NBVzVqMUFUbwozZDN0OStZdkw4K3pHdnpZcEZtS1pneDFrN0RwcDd0UmR6RCtCZFJUTFVLSC9XN3FNNEVsMERFQW0xRkoKRGl3b25zdHFXZnBFQnhINW9NbXcyTlloUnRRNi8yd091YUhZejBWK28waUdOeHVvdVV5Ny9LSlNlRTJkCjhSbGppUDNxQWdoYWRlWVVaa1dFWFpiVlE5d1owYnc1U2RVd0lZcGNnRkh3aVZ3VkZhNi95WGtqL01haQpxRUd6RU5hcE1QanlKM3Ezc2k2SEcrS1BRS05IdU9rOWNYMmY4SXczKys1M0o5eElUR3BBNUNYc0ZpWjUKUTF3Rm5HNkhBRGFqMFFnVUJhb243TGx1aUh1UkZXaElPTm1PQjhHRnk5NUlOMWNMdm9wR3pmN1RPVVVRCg==.6cf85eb3073f24b5c0bfb3ad43819137\"></div><h1 id=\"isPasted\">Problem</h1><p>When you try to access an already created mount point or create a new mount point, it fails with the error:</p><pre>WASB: Fails with java.lang.NullPointerException</pre><h1>Cause</h1><p>This error can occur when the root mount path (such as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/mnt/</span>) is also mounted to blob storage. Run the following command to check if the root path is also mounted:</p><pre>%python\r\n\r\ndbutils.fs.mounts()</pre><p>Check if <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/mnt</span> appears in the list.</p><h1>Solution</h1><p>Unmount the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">/mnt/</span> mount point using the command:</p><pre>%python\r\n\r\ndbutils.fs.unmount(\"/mnt\")</pre><p>Now you should be able to access your existing mount points and create new ones.</p>", "body_txt": "Problem When you try to access an already created mount point or create a new mount point, it fails with the error: WASB: Fails with java.lang.NullPointerException Cause This error can occur when the root mount path (such as /mnt/) is also mounted to blob storage. Run the following command to check if the root path is also mounted: %python dbutils.fs.mounts() Check if /mnt appears in the list. Solution Unmount the /mnt/ mount point using the command: %python dbutils.fs.unmount(\"/mnt\") Now you should be able to access your existing mount points and create new ones.", "format": "html", "updated_at": "2022-05-31T23:12:53.091Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708558, "name": "aws"}, {"id": 2708559, "name": "azure"}], "url": "https://kb.databricks.com/data-sources/access-blob-fails-wasb"}, {"id": 1390547, "name": "Delete table when underlying S3 bucket is deleted", "views": 7651, "accessibility": 1, "description": "Do not delete the contents of a S3 bucket before dropping a table that stores data in the bucket.", "codename": "delete-table-if-s3-bucket-deleted", "created_at": "2022-05-31T21:32:16.940Z", "updated_at": "2022-05-31T22:06:51.642Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThhZlpseHdGV2FpWkdyZlNiSXp1TVJXeTUxbTF6a1JrYldtNFIyMnR0eCtKMVBEQk1SCkNBV2RiMlFHdzNadXpQcmptTFkzZGgxNksvcnZGeHpmVHlOOW1mdUI4Q0tzWUFYWVJEb0Z2ek5QeEFOUgpuQUpjTHBmSnh3T28wMW4vNWJwNkhaSTNlZXpKeitkekhBbnI0alp2Q2V4VDJVdjNmMmRnZUNOSmFxcG0KMDVjUjUvYmE3QmJuN29aSTQzeHhmZDNvd1B2eU1Ic2VaRWVlQWNtWEZ5d0JSc0dhaUc1SG9CVzh4WDN4Cnk3ZVRhd3ZicGlLVXlhbjVML0ZYQUNsSlJEMm96V2RYR0lFdUo4U2NzT1gwcXBvSG9XUUxFb2QvS0ZwaAo0alBBdGJzSC9nTlc4SUV1MHZYa0RuaktkRm5RbzF2M3AveTd6LzBlZ0xMZHhQMFB2TW94TUk2RVZsNWwKZll3OVVBK1E5OXo0R1FpY2d0eTkyYmxySUJ0VG1VUSs4b1h4c0tlenVLQmdlNk9lNHAwdXFLZUpVN3AzCjZxcTVWL2JkQjNKYWpLVUd4OUQ3TStySG55VW5lMVpuMWpuTm1iZ2MyQXhZaDlqSUV1VStialpZVmxZVwp0R3U1dEhIRklJZ2FiK1NPRlhXM0RaRWQzOGZIb0FTT1k3UlA5bC9JNXlPWGFhLytuWHdMc3RRZnhYS1YKU3g1bVZoNW9zRUc4czc3ekdIN1RoNlFpUjZLZEhROVNNUTE3QVZuWVJVZ1hlaWJZOTBRbG9keFFVOHZyCkRla0prU0hnYzZKV3RLRTU0LzlUWXFaSEI4Zk5ZUEdFVGJMYSs4STBOYlEzTjRsVHhkT3pDRmhpM2pRSAorRXE3OUdwMkFQbDRvZzdWaW9UR1ZRc3dwOG80ZmdtVjVtUlVwL0tDVXIxWmYrRkxrbzZwNlZ3Nk9SVFgKZWJhZjRiZHEwQ1NnbmtIK0IyOEoxWWVXK1hkVEdNYzJOR1VGYVpsLys5Z2tkdDVhdmNnT3F1elg5dFJwClMrZTZaVzk2ZVczZVlVY2NYbU9XVFhnQk1wRGVHOWtIVjFBK0RPSnMxaFMwSWEyQjhHczdRU1RJK3RBaQpyeGNkSTJMS0tIU3FkMy9GYjhpRXpOeUlQayt0QW1JUS9PL3hLN29HVkdqZmhtb3I3cjYvRDNTcXdSSjkKdnBjWkRiaU5NQmROSVVVNkxJbTRaaUJHcndIWVIzei9kckJONzRTU1FFUXVyaTM5UldFc05YYm92ek9mCkppaWdWald4L1ozdlVrbmozNU1nSzVjSEh0Y2RjcHRFRzRRQW85cnZHYTNnS0dPamFsTjIwQk9zNmRiUwpBYzFQb0NpRGE1YUhRcGYwVEJURWt5OG1FUzlQQmM0Vi9sQUNQalNPSkZFMG5sNnZpMllSdlZDMEN3Si8Kb3NiU2pPMU0K.8cbcc895d0506e9b796a80826cd71d9a\"></div><h1 id=\"isPasted\">Problem</h1><p>You are trying to drop or alter a table when you get an error.</p><pre>Error in SQL statement: IOException: Bucket_name \u2026 does not exist</pre><p>You can reproduce the error with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DROP TABLE</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ALTER TABLE</span> command.</p><pre>%sql\r\n\r\nDROP TABLE &lt;database-name.table-name&gt;;</pre><pre>%sql\r\n\r\nALTER TABLE &lt;database-name.table-name&gt; SET LOCATION \"&lt;file-system-location&gt;\";</pre><h1>Cause</h1><p>You deleted the contents of the underlying S3 bucket before dropping the tables.</p><p>Because the data no longer exists, you get an error when trying to drop the table.</p><h1>Solution</h1><p>You can use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sessionState.catalog.externalCatalog.dropTable</span> to delete the table.</p><pre>%scala\r\nimport org.apache.spark.sql.hive.HiveUtils\r\nspark.sessionState.catalog.externalCatalog.dropTable(\"&lt;database-name&gt;\", \"&lt;table-name&gt;\", ignoreIfNotExists = false, purge = false)</pre><p><br></p>", "body_txt": "Problem You are trying to drop or alter a table when you get an error. Error in SQL statement: IOException: Bucket_name \u2026 does not exist You can reproduce the error with a DROP TABLE or ALTER TABLE command. %sql DROP TABLE &lt;database-name.table-name&gt;; %sql ALTER TABLE &lt;database-name.table-name&gt; SET LOCATION \"&lt;file-system-location&gt;\"; Cause You deleted the contents of the underlying S3 bucket before dropping the tables. Because the data no longer exists, you get an error when trying to drop the table. Solution You can use spark.sessionState.catalog.externalCatalog.dropTable to delete the table. %scala import org.apache.spark.sql.hive.HiveUtils spark.sessionState.catalog.externalCatalog.dropTable(\"&lt;database-name&gt;\", \"&lt;table-name&gt;\", ignoreIfNotExists = false, purge = false)", "format": "html", "updated_at": "2022-05-31T22:06:51.628Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708527, "name": "aws"}], "url": "https://kb.databricks.com/data-sources/delete-table-if-s3-bucket-deleted"}, {"id": 1390524, "name": "Create tables on JSON datasets", "views": 12067, "accessibility": 1, "description": "Create tables on JSON datasets; requires SerDe JAR.", "codename": "create-table-json-serde", "created_at": "2022-05-31T21:11:07.880Z", "updated_at": "2022-05-31T21:31:41.372Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS93dmdab3JtTVJRbHRhcHJPM0xNenpjVUd1bW54K0J0NytZTlBTWkdVdUpKWnZkWTBTCm9acmRmdUNNWURpNDdhbjc3QmJiZG80SFBZeUFmazh3YkVTbUdFM1B5S2hySTBacmhjbGlKeHhSdnB5cwoyQU1sUGUvNmdyV2VGWmFybytFeVZEbEdsTVRjaVdLSk9SZk8xczNpWW5IMElUbW15KzFvYUhMQnArSmYKcWxxOVBaOTFqaEJQUCtVaTFpYXdyWUQ5NGFncnhRYitWc28wamZVQzBqS2d4TndJSU0rNk9pS2x5MzI0CnQ2YmVWT2xySTlyQWRTNzZnVm9kZ1JJS212Z1BWVHJEVU00MUd1OWZuOTBpbkwxbmhILzdvZFlMSS93SQpxV2QyeE9YdTl5UEdYbitlNlZXUGRTQWNuVnB6SmxLM0JtRVhKeURVY2FTWGk5T2Z6R2ZwdDdBdklFOGUKR1pwcUtFN2w3UTNWNUtWTndvengxcTMvUzB6M09aOGJvUHdzRTM0Q0xIdm0ydXQrR1BsUkNsRTZTYmpPCkE1K2FkMldscDJPRk9rYWxhWkFKNTk5dmU3T0w5ZkMwVWtwZ3QyK1d2ZVUxUDZXRWRVMzRPdXliK1ZaMgorYW5PekhLRFNpMlNWTERaSkRVT2lSNjB4U3NjcmFTbjBYSDNSb1VSRVJNOGJiK3dFQmtJSVRtdi91a0gKQXJuekFTRmpnNGluWXgxK2hHZEd1amV3ZmZPa1F3R1BNT0VNNVB1SHdvTW56NHJsTTdUdVVFRmI4VEQ5Cm4zNWxIaVRGL05mSmZhemFpRWh5bVFWd0MrMmNPNCtiQVFxRUYzRHFwV1Y0S3FlNEE3WGw2dGtBUWgvcgpLWWxuZGtKaHNhbjlWRHhqM1ZPSkw3MGwvWTJESjhpa294UThxNCtJMUdERGlSaGVHWVZwVHV3bkpjREYKQjBsMTR3SnBEMlhlNGZ4YUFueHpyMHZjdy9EdzJuUDV1aGl5ZHorUmp4aEVkRFRpbVh6U3lNUDhTeWM2ClFvZmsxWmR6TDFTV1d4d0I2dHdPNmg0cVd1VmpOYXpoZzBUOEZnRE9NRi84NFEveUNuS3JHRUQ0S2QyZgo3clVKOVhYbnlsK3lUQng4RFlwMC9ZN3Q2cmc1Q3h1M3RLMjhjaFpmZStRMEZ3OC83T1BZalk4eGRZZXQKRGpGQ2xNbHR2T21mSUp5MWRyOCszUnQyKzJnMGRxYWoyMXRqbDVGd0tQVWIzak9oNFBCN2t2QVA5UHgxCg==.e10e1a91bef7420e6e2ed173bef15607\"></div><p id=\"isPasted\">In this article we cover how to create a table on JSON datasets using SerDe.</p><h1>Download the JSON SerDe JAR</h1><ol>\n<li>Open the <a href=\"http://www.congiu.net/hive-json-serde/1.3.8/hdp23/\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">hive-json-serde 1.3.8</a> download page.</li>\n<li>Click on <strong>json-serde-1.3.8-jar-with-dependencies.jar</strong> to download the file <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">json-serde-1.3.8-jar-with-dependencies.jar</span>.</li>\n</ol><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">You can review the <a href=\"https://github.com/rcongiu/Hive-JSON-Serde\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Hive-JSON-Serde</a> GitHub repo for more information on the JAR, including source code.</p>\n</div>\n</div><h1>Install the JSON SerDe JAR on your cluster</h1><ol>\n<li>Select your cluster in the workspace.</li>\n<li>Click the <strong>Libraries</strong> tab.</li>\n<li>Click <strong>Install new</strong>.</li>\n<li>In the Library Source button list, select <strong>Upload</strong>.</li>\n<li>In the Library Type button list, select <strong>JAR</strong>.</li>\n<li>Click <strong>Drop JAR here</strong>.</li>\n<li>Select the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">json-serde-1.3.8-jar-with-dependencies.jar</span> file.</li>\n<li>Click <strong>Install</strong>.</li>\n</ol><h1>Configure SerDe properties in the create table statement</h1><pre>%sql\r\n\r\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\r\nSTORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\r\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\r\nLOCATION '&lt;path-to-json-files&gt;'</pre><p>For example:</p><pre>%sql\r\n\r\ncreate table &lt;name-of-table&gt; (timestamp_unix string, comments string, start_date string, end_date string)\r\npartitioned by (yyyy string, mm string, dd string)\r\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\r\nSTORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\r\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\r\nLOCATION '&lt;path-to-json-files&gt;'\r\nThis example creates a table that is partitioned by the columns yyyy, mm, and dd.</pre><h1>Run a repair table statement after the table is created</h1><p>For example:</p><pre>%sql\r\n\r\nmsck repair table &lt;name-of-table&gt;</pre><p><br></p>", "body_txt": "In this article we cover how to create a table on JSON datasets using SerDe. Download the JSON SerDe JAR Open the hive-json-serde 1.3.8 download page.\nClick on json-serde-1.3.8-jar-with-dependencies.jar to download the file json-serde-1.3.8-jar-with-dependencies.jar. Info\nYou can review the Hive-JSON-Serde GitHub repo for more information on the JAR, including source code. Install the JSON SerDe JAR on your cluster Select your cluster in the workspace.\nClick the Libraries tab.\nClick Install new.\nIn the Library Source button list, select Upload.\nIn the Library Type button list, select JAR.\nClick Drop JAR here.\nSelect the json-serde-1.3.8-jar-with-dependencies.jar file.\nClick Install. Configure SerDe properties in the create table statement %sql ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION '&lt;path-to-json-files&gt;' For example: %sql create table &lt;name-of-table&gt; (timestamp_unix string, comments string, start_date string, end_date string) partitioned by (yyyy string, mm string, dd string) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION '&lt;path-to-json-files&gt;' This example creates a table that is partitioned by the columns yyyy, mm, and dd. Run a repair table statement after the table is created For example: %sql msck repair table &lt;name-of-table&gt;", "format": "html", "updated_at": "2022-05-31T21:31:41.366Z"}, "author": {"id": 791511, "email": "ram.sankarasubramanian@databricks.com", "name": "ram.sankarasubramanian ", "first_name": "ram.sankarasubramanian", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T01:51:47.575Z", "updated_at": "2023-04-10T05:38:44.201Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256845, "name": "Data sources", "codename": "data-sources", "accessibility": 1, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708522, "name": "aws"}, {"id": 2708523, "name": "azure"}, {"id": 2708524, "name": "gcp"}], "url": "https://kb.databricks.com/data-sources/create-table-json-serde"}, {"id": 1390521, "name": "Invalid timestamp when loading data into Amazon Redshift", "views": 7786, "accessibility": 1, "description": "Learn how to resolve an invalid timestamp error when loading data into AWS Redshift.", "codename": "redshift-loading-timestamp-problem", "created_at": "2022-05-31T21:06:32.605Z", "updated_at": "2022-05-31T21:10:23.415Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlsT2FiQTh0dkpoMzczd2o4eEFmdkhSUktjOGZrNitOZkZFTnY2OHFqNDdIcmYzRENlCkFXSzZMUlkwL3prWlA1eFdhRlgzM1JsWGJ4RlFHQS9Tc3FBUzR6NEQyS1VCK1ZpczdZUDdoSE1MUE91eQo0UTlMQlJUK3VQQ3pOeVVmVEZQeXZmL0czN0ZjOWdyb1pzL2RDS29XR1lBb3NMVUorbWlBcG4rMHIrMFYKbzVqbDlpWlI2dVNTN3h4L1NqMEpscGRlL1lwTUhOdWxPbXZ1REQvQitXYmhmQllLLzU2U25DNkhPVml5ClZac1BRREt6UGJ1Syt0bTRERVBHZ1FwUzZMM1lSUDJteURvTFpnMmwxaXRkMC9zNkdDME9LT2xmd0lqdApSaytjRmgzcUFJWVpnS3JSMkhvVGErMjl4M3R4Q0xhNlovL3BNNkxjNkV5V3FIQllDNXF0ejJuRGZ3eDcKRzRNZGhvNWxKWGNnYmJxNVByUTZvOSs5OWZlWG5HOEt2MXRnNldRTzd5bzRTajhSWERoMm5pWjlyY0NrCkcvcm5sVUpKREVBZUh1andrNmVQZU1Pb1JrV29rNGo1Q29qdUtzTUYxeXhGaThLak05N1RYcm5UayttZQo3LzEveVVHbDhFSXlPZjV5dW83S2pnZERNQUdodTQxcU0vUkZLcGtucFVFSlJ6RlU0NklESWFFb1VDTFQKK3pVMXQ0U2NLN0V1ZnFBekYvOVR4VlBWTFdKQWRrQ2cyNXBMbnhPeFJ2cW9jNnY2dlVvYWdPNzJ1MFhxCkpWbGtLd3NpaVlqZmIzM2hEZ2lzY3lmMjhBZUh5WGpMay9iQncxZXBDM2F5a0dKelRrU043NmM5bFNlcApvcDFvbGJOUGVRdjMrc1JWWUNVL2dCL3MvbWxOMkN4UklpSlI2NUpqNFJzMlMwYWhnZlN1M0pyU2RGc3IKZzdsWlNUL1FKMlF5bG1INlhUQWwwdU9UK29zWTkzRUx6N1ExUU5iTFZja2RORjAxbXJjT2FQUFJMaktZCjhhbzZIeHFmNDhLdExwc3pYRzFKNmpkd2E3WGs5VnNKbHRTN2hSaCs0dGR2SEh6SENPSXp4dG1HTi9FRgpUM3oxYkZJN2IxMjF5NGxnU09oWFZIbys1ZXV6aldkeGNRVFhlK2Y5YjlpcHkySkFDbGRxSkYzaXhRaDAKVnhXaHZjZWk4dDhBSFowcDBER3lQaDZLQ3ZQbmJNVy9VTm1SWEJRPQo=.31742352ef24ba6498b0e71cd0fd8139\"></div><h1 id=\"isPasted\">Problem</h1><p>When you use a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark-redshift</span> write operation to save timestamp data to Amazon Redshift, the following error can occur if that timestamp data includes timezone information.</p><pre>Error (code 1206) while loading data into Redshift: \"Invalid timestamp format or value [YYYY-MM-DD HH24:MI:SSOF]\"</pre><h1>Cause</h1><p>The Redshift table is using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Timestamp</span> data type that doesn\u2019t store timezone information.</p><h1>Solution</h1><p>Include the option <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.options(\"extracopyoptions\", \"TIMEFORMAT 'auto'\")</span> as shown in the following Scala code:</p><pre>%scala\r\n\r\ndf.write\r\n.format(\"com.databricks.spark.redshift\")\r\n.options(...)\r\n.options(\"extracopyoptions\", \"TIMEFORMAT 'auto'\")\r\n.mode(\"append\")\r\n.save()</pre><p>If you specify <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">auto</span> as the argument for the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DATEFORMAT</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TIMEFORMAT</span> parameter, Amazon Redshift automatically recognizes and converts the date format or time format in your source data.</p>", "body_txt": "Problem When you use a spark-redshift write operation to save timestamp data to Amazon Redshift, the following error can occur if that timestamp data includes timezone information. Error (code 1206) while loading data into Redshift: \"Invalid timestamp format or value [YYYY-MM-DD HH24:MI:SSOF]\" Cause The Redshift table is using the Timestamp data type that doesn\u2019t store timezone information. Solution Include the option .options(\"extracopyoptions\", \"TIMEFORMAT 'auto'\") as shown in the following Scala code: %scala df.write .format(\"com.databricks.spark.redshift\") .options(...) .options(\"extracopyoptions\", \"TIMEFORMAT 'auto'\") .mode(\"append\") .save() If you specify auto as the argument for the DATEFORMAT or TIMEFORMAT parameter, Amazon Redshift automatically recognizes and converts the date format or time format in your source data.", "format": "html", "updated_at": "2022-05-31T21:10:23.413Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708471, "name": "aws"}], "url": "https://kb.databricks.com/data/redshift-loading-timestamp-problem"}, {"id": 1390450, "name": "Access denied when writing to an S3 bucket using RDD", "views": 8082, "accessibility": 1, "description": "Learn how to resolve an access denied error when writing to an S3 bucket using RDD.", "codename": "writing-with-dataframe-api-aws", "created_at": "2022-05-31T19:58:07.309Z", "updated_at": "2022-05-31T21:06:03.297Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlQYk1lOHRCM2krOGxyVGovVXczcGVMWHlQTUhKbkk0Q0ZqMFJmMzdzTFNlbzBiSExrCk9acXpMdDVrRnp1OUFhWEtMb1VZQ0pRL21DWEQ3R1lqZ1FuTW1FMTNEM3IwajRJNThVVUtEVktFSW40agpZZGVKZVNUQ1drRWc3MDZZUGdGbXlRdFVmV2VkSGN4NUVWWmtGWkkyYXM5Q2o3VVQ3OUxlTHhUZjczclgKZkNqOFNrVmpOQ0tHWU1ObTdnSzJlcXU5WHQrbG1kYWZVKytGSFByWkkvVHJ6OCtObTRQL29mZUhLbndhClFQMC9UN0tlOXBwU1F5Nkx3LzR4UWpNTlY2MS93b0EvbkdnbXpURCsvQ281ekdkSGFFY0xXejZJWGIrUgpKa0JFb25ieXNUVVNmTzU1Sm4wUzJkcUtDUDR3bGQydHB1VURoYVUrTlFMOEtpNDhnOFFnbHU4NzBPZy8KZkZWN25PVGVsRHFvVmpMck5XNW55N2d5ZGx6dUhqSG1XUlJmMUV4RTR0emlMa01qZExDR1MvMXlxY0tSCkRzdXlNR1lTNWpweDFIaFNqaXZJaUtCWlh0S1hGNlJwS3FSZk0zMzBkMk8rSVVjRmljT1lkTnFkcEJKdgp2ZU5zbWJQRkh0OEJrK0ZkR0d2NUlwcytleUk4enJ4eUtvTzJJUzc0czIySk9DNGwyeEE0RjJoaHlhNWcKNEN0bVVndU94RkRaUUtIZWgrY05XdmZTWGt3NUZHUzNGUTA3VzZJUVZOR2l6YXVCUzg0bFNsVTJjbjFuCmdkZnpHampmNzIxZ3ZoRmJQYjVESEtkYkhKWnEzWFdNTDFud0VNN1paOTc4c3o4R3pvcG5qK29WeEwwcApHdzNrS28vM1AzU3F4cDcwbjhxMS82Z0JybHA1N1ZQZExrSzNzb2x0ckRTMDk0UHlnakVDSGxjTVo2Z08KUmdLYkFnYlNhTG1ibkRzMm1WbEU4MmRtbjFlVUNmVkVSci9HTlprM3BHMTZnY0ZuTWlJa3BHSHRBQzRECkhreCs5YnNSMmRHSEJVc3pxNlRNVWNRTTFuQVBZUmU2VWgrRzlhNFhPd1BTSVk1NHNqWEkvb3BCM2dMQwpwbm85bkNGSHQ1VkpUM1lJNmQrNTNxdTArZDNJQUlFQnlLTmt6Mzc2RUlTcHhLWkttR09LbE4rMFNsQnQKRjRXYXlZbXZBUkpQVytCVnlEZ0RMSldQRWtncWxadmEwZGdMZXNNPQo=.cc4496108ab3db370540079dc17c25b4\"></div><h1 id=\"isPasted\">Problem</h1><p>Writing to an S3 bucket using RDDs fails. The driver node can write, but the worker (executor) node returns an access denied error. Writing with the DataFrame API, however works fine.</p><p>For example, let\u2019s say you run the following code:</p><pre>%scala\r\n\r\nimport java.io.File\r\nimport java.io.Serializable\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\nimport org.apache.hadoop.fs.{FileSystem, Path}\r\nimport org.apache.hadoop.conf.Configuration\r\nimport java.net.URI\r\nimport scala.collection.mutable\r\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\r\nimport org.apache.spark.streaming.dstream.InputDStream\r\n\r\nval ssc = new StreamingContext(sc, Seconds(10))\r\nval rdd1 = sc.parallelize(Seq(1,2))\r\nval rdd2 = sc.parallelize(Seq(3,4))\r\nval inputStream = ssc.queueStream[Int](mutable.Queue(rdd1,rdd2))\r\nval result = inputStream.map(x =&gt; x*x)\r\nval count = result.foreachRDD { rdd =&gt;\r\nval config = new Configuration(sc.hadoopConfiguration) with Serializable\r\n\u00a0rdd.mapPartitions {\r\n\u00a0 \u00a0_.map { entry =&gt;\r\n\u00a0 \u00a0 \u00a0 \u00a0val fs = FileSystem.get(URI.create(\"s3://dx.lz.company.fldr.dev/part_0000000-123\"), config)\r\n\u00a0 \u00a0 \u00a0 \u00a0val path = new Path(\"s3://dx.lz.company.fldr.dev/part_0000000-123\")\r\n\u00a0 \u00a0 \u00a0 \u00a0val file = fs.create(path)\r\n\u00a0 \u00a0 \u00a0 \u00a0file.write(\"foobar\".getBytes)\r\n\u00a0 \u00a0 \u00a0 \u00a0file.close()\r\n\u00a0 \u00a0}\r\n\u00a0}.count()\r\n}\r\n\r\nprintln(s\"Count is $count\")\r\n\r\nssc.start()</pre><p>The following error is returned:</p><pre>org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 4 times, most recent failure:\r\nLost task 3.3 in stage 0.0 (TID 7, 10.205.244.228, executor 0): java.rmi.RemoteException: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied; Request ID: F81ADFACBCDFE626,\r\nExtended Request ID: 1DNcBUHsmUFFI9a1lz0yGt4dnRjdY5V3C+J/DiEeg8Z4tMOLphZwW2U+sdxmr8fluQZ1R/3BCep,</pre><h1>Cause</h1><p>When you write to the worker node using RDD, the IAM policy denies access if you use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Serializable</span>, as in <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">val config = new Configuration(sc.hadoopConfiguration) with Serializable</span>.</p><h1>Solution</h1><p>There are two ways to solve this problem:</p><h2>Option 1: Use DataFrames</h2><pre>%scala\r\n\r\ndbutils.fs.put(\"s3a://dx.lz.company.fldr.dev/test-gopi/test0.txt\", \"foobar\")\r\nval df = spark.read.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/test0.txt\")\r\ndf.write.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/text1.txt\")\r\nval df1 = spark.read.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/text1.txt\")</pre><h2>Option 2: Use SerializableConfiguration</h2><p>If you want to use RDDs, use:</p><pre>%scala\r\n\r\nval config = sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration))</pre><p>For example:</p><pre>%scala\r\n\r\nimport java.io.File\r\nimport java.io.Serializable\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\nimport org.apache.hadoop.fs.{FileSystem, Path}\r\nimport org.apache.hadoop.conf.Configuration\r\nimport java.net.URI\r\nimport scala.collection.mutable\r\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\r\nimport org.apache.spark.streaming.dstream.InputDStream\r\nimport org.apache.spark.util.SerializableConfiguration\r\n\r\nval ssc = new StreamingContext(sc, Seconds(10))\r\nval rdd1 = sc.parallelize(Seq(1,2))\r\nval rdd2 = sc.parallelize(Seq(3,4))\r\nval inputStream = ssc.queueStream[Int](mutable.Queue(rdd1,rdd2))\r\nval result = inputStream.map(x =&gt; x*x)\r\nval count = result.foreachRDD { rdd =&gt;\r\n//val config = new Configuration(sc.hadoopConfiguration) with Serializable\r\nval config = sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration))\r\nrdd.mapPartitions {\r\n\u00a0 \u00a0_.map { entry =&gt;\r\n\u00a0 \u00a0 \u00a0 \u00a0val fs = FileSystem.get(URI.create(\"s3://pathpart_0000000-123\"), config.value.value)\r\n\u00a0 \u00a0 \u00a0 \u00a0val path = new Path(\"s3:///path/part_0000000-123\")\r\n\u00a0 \u00a0 \u00a0 \u00a0val file = fs.create(path)\r\n\u00a0 \u00a0 \u00a0 \u00a0file.write(\"foobar\".getBytes)\r\n\u00a0 \u00a0 \u00a0 \u00a0file.close()\r\n\u00a0 \u00a0}\r\n\u00a0}.count()\r\n}\r\n\r\nprintln(s\"Count is $count\")\r\n\r\nssc.start()</pre><p><br></p>", "body_txt": "Problem Writing to an S3 bucket using RDDs fails. The driver node can write, but the worker (executor) node returns an access denied error. Writing with the DataFrame API, however works fine. For example, let\u2019s say you run the following code: %scala import java.io.File import java.io.Serializable import org.apache.spark.{SparkConf, SparkContext} import org.apache.hadoop.fs.{FileSystem, Path} import org.apache.hadoop.conf.Configuration import java.net.URI import scala.collection.mutable import org.apache.spark.streaming.{Seconds, StreamingContext} import org.apache.spark.streaming.dstream.InputDStream val ssc = new StreamingContext(sc, Seconds(10)) val rdd1 = sc.parallelize(Seq(1,2)) val rdd2 = sc.parallelize(Seq(3,4)) val inputStream = ssc.queueStream[Int](mutable.Queue(rdd1,rdd2)) val result = inputStream.map(x =&gt; x*x) val count = result.foreachRDD { rdd =&gt; val config = new Configuration(sc.hadoopConfiguration) with Serializable \u00a0rdd.mapPartitions { \u00a0 \u00a0_.map { entry =&gt; \u00a0 \u00a0 \u00a0 \u00a0val fs = FileSystem.get(URI.create(\"s3://dx.lz.company.fldr.dev/part_0000000-123\"), config) \u00a0 \u00a0 \u00a0 \u00a0val path = new Path(\"s3://dx.lz.company.fldr.dev/part_0000000-123\") \u00a0 \u00a0 \u00a0 \u00a0val file = fs.create(path) \u00a0 \u00a0 \u00a0 \u00a0file.write(\"foobar\".getBytes) \u00a0 \u00a0 \u00a0 \u00a0file.close() \u00a0 \u00a0} \u00a0}.count() } println(s\"Count is $count\") ssc.start() The following error is returned: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 4 times, most recent failure: Lost task 3.3 in stage 0.0 (TID 7, 10.205.244.228, executor 0): java.rmi.RemoteException: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied; Request ID: F81ADFACBCDFE626, Extended Request ID: 1DNcBUHsmUFFI9a1lz0yGt4dnRjdY5V3C+J/DiEeg8Z4tMOLphZwW2U+sdxmr8fluQZ1R/3BCep, Cause When you write to the worker node using RDD, the IAM policy denies access if you use Serializable, as in val config = new Configuration(sc.hadoopConfiguration) with Serializable. Solution There are two ways to solve this problem: Option 1: Use DataFrames %scala dbutils.fs.put(\"s3a://dx.lz.company.fldr.dev/test-gopi/test0.txt\", \"foobar\") val df = spark.read.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/test0.txt\") df.write.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/text1.txt\") val df1 = spark.read.text(\"s3a://dx.lz.company.fldr.dev/test-gopi/text1.txt\") Option 2: Use SerializableConfiguration If you want to use RDDs, use: %scala val config = sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration)) For example: %scala import java.io.File import java.io.Serializable import org.apache.spark.{SparkConf, SparkContext} import org.apache.hadoop.fs.{FileSystem, Path} import org.apache.hadoop.conf.Configuration import java.net.URI import scala.collection.mutable import org.apache.spark.streaming.{Seconds, StreamingContext} import org.apache.spark.streaming.dstream.InputDStream import org.apache.spark.util.SerializableConfiguration val ssc = new StreamingContext(sc, Seconds(10)) val rdd1 = sc.parallelize(Seq(1,2)) val rdd2 = sc.parallelize(Seq(3,4)) val inputStream = ssc.queueStream[Int](mutable.Queue(rdd1,rdd2)) val result = inputStream.map(x =&gt; x*x) val count = result.foreachRDD { rdd =&gt; //val config = new Configuration(sc.hadoopConfiguration) with Serializable val config = sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration)) rdd.mapPartitions { \u00a0 \u00a0_.map { entry =&gt; \u00a0 \u00a0 \u00a0 \u00a0val fs = FileSystem.get(URI.create(\"s3://pathpart_0000000-123\"), config.value.value) \u00a0 \u00a0 \u00a0 \u00a0val path = new Path(\"s3:///path/part_0000000-123\") \u00a0 \u00a0 \u00a0 \u00a0val file = fs.create(path) \u00a0 \u00a0 \u00a0 \u00a0file.write(\"foobar\".getBytes) \u00a0 \u00a0 \u00a0 \u00a0file.close() \u00a0 \u00a0} \u00a0}.count() } println(s\"Count is $count\") ssc.start()", "format": "html", "updated_at": "2022-05-31T21:06:03.294Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708468, "name": "aws"}], "url": "https://kb.databricks.com/data/writing-with-dataframe-api-aws"}, {"id": 1390440, "name": "Incompatible schema in some files", "views": 15724, "accessibility": 1, "description": "Learn how to resolve incompatible schema in Parquet files with Databricks.", "codename": "wrong-schema-in-files", "created_at": "2022-05-31T19:50:17.235Z", "updated_at": "2022-05-31T19:57:52.325Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTk4ay9Rd2U5T1RQVWxBckpYU0ZWSEh0WThtcFdCZlZZQ0xzZW5CbHRZVGtjYmw2TTdVCjBrVnhOMDlheGNHUktDWVFGWW5oL2xtRjMxMEVHeENXcmxYanRqNHJKM0dBeHJsTHlaMHNSYXkyK0R1NApQTWw0b3M3NkFwenQ5TmF1Q3JURmdwUHpidE56S0VGd0xQYXN2Vmp5SzJnbzM3N2M1MHZLY250L0p5K2IKWUVqczM1RU1NVEtkMzQ1eHJEWUExS0g3S0N4akYrTEl4SDB3NWJYanQzeDFpY210S3pVUWI2blQ0ZDZKCkRRZ2dXcC9PdkU0Yml1RjhTYm5sUEEydnhRdmdsM01hMGpKR2tTYWY1RFdycDI3N01LbzhkR2pnckFjWQpyL2pqb3d2dUw3ZS9XMGFsdnFsbUFXREU4d3ZrV0lNSllNVVE4TCswOTZNd1puZE5qdU83djlhYzJQNHEKNGsyS1BGZXV2VWZXRjBtMWJ6NDZoMXdISDhPd05iMVl3SUJ2enByNUVXU0ZqOHpoT1k1L2ppcUwvYVVtCnBoMnJrYUFCUStGRWZTTThGZzZMeUlCZFVRSnJoWmpuOEUzampxM2xOTzQ1QkU4RnVQWVBobU1WanY5Ugpsa2tFMmVXMXJ2cUN0REw4RHNxY2pIVk1zZVMzZlREV04xWU51d3lkbEpWR0w4cW9SdnBDajhEYjArNHAKNWhRNTg5UmFGZFRZTll2T2VVbWVTOW9UeS9wRUN6ejFlUGF2d1NuTXRZUmorMUJvNithckdOK3JsZDNUCkxyY0tLb2ZsQVlmdDFkVUFVd1puSGwybUlGbm5HKzFXQzU5T1ZvODY1UTQyRWRaYU9YZEVMaTBCR0dBRApzZFAzUnk4MFNIV29DckdQM1BMOFBCclg3NnJrZ2xXTkpBUFg4NGFsVkFxeU4vQzRUSjUrKzY1aDRiWmIKenFsUFN4eWtWQk1yTnFwMW1UbDNsdXZiS01TVnJ5M2xOSkRGOUgzVHZ3YmcvNjBhQjBqTE9ud1hNZ0drCjg3WW1URTZVeGRKamtsUlJiODhReWdGUWgwUTlvcERaMzY1aElnejMyM0JGT0x1WXhiK1dNcEFZL1MrSwpvWmZ4RWg5eXpCVnRicTB5Y2xzeWxacld6QnQyd0p0RXdhandVS0MvMFhtMUhYaDRxVUt2Z0xsN3EyTUcKSFJKSDJSWnE4MGZlUkVPM3pZSFBaUURRdDRSb0VoQVB0ckhLdXRoT0xMZjJYek1UQkJqQlF3MmVJQ05TCg==.450b7b200b5f761f935b72f2e98e9971\"></div><h1 id=\"isPasted\">Problem</h1><p>The Spark job fails with an exception like the following while reading Parquet files:</p><pre>Error in SQL statement: SparkException: Job aborted due to stage failure:\r\nTask 20 in stage 11227.0 failed 4 times, most recent failure: Lost task 20.3 in stage 11227.0\r\n(TID 868031, 10.111.245.219, executor 31):\r\njava.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\r\n\u00a0 \u00a0 at org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:52)</pre><h1>Cause</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.lang.UnsupportedOperationException</span> in this instance is caused by one or more Parquet files written to a Parquet folder with an incompatible schema.</p><h1>Solution</h1><p>Find the Parquet files and rewrite them with the correct schema. Try to read the Parquet dataset with schema merging enabled:</p><pre>%scala\r\n\r\nspark.read.option(\"mergeSchema\", \"true\").parquet(path)</pre><p>or</p><pre>%scala\r\n\r\nspark.conf.set(\"spark.sql.parquet.mergeSchema\", \"true\")\r\nspark.read.parquet(path)</pre><p>If you do have Parquet files with incompatible schemas, the snippets above will output an error with the name of the file that has the wrong schema.</p><p>You can also check if two schemas are compatible by using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">merge</span> method. For example, let\u2019s say you have these two schemas:</p><pre>%scala\r\n\r\nimport org.apache.spark.sql.types._\r\n\r\nval struct1 = (new StructType)\r\n\u00a0 .add(\"a\", \"int\", true)\r\n\u00a0 .add(\"b\", \"long\", false)\r\n\r\nval struct2 = (new StructType)\r\n\u00a0 .add(\"a\", \"int\", true)\r\n\u00a0 .add(\"b\", \"long\", false)\r\n\u00a0 .add(\"c\", \"timestamp\", true)</pre><p>Then you can test if they are compatible:</p><pre>%scala\r\n\r\nstruct1.merge(struct2).treeString</pre><p>This will give you:</p><pre>%scala\r\n\r\nres0: String =\r\n\"root\r\n|-- a: integer (nullable = true)\r\n|-- b: long (nullable = false)\r\n|-- c: timestamp (nullable = true)\r\n\"</pre><p>However, if <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">struct2</span> has the following incompatible schema:</p><pre>%scala\r\n\r\nval struct2 = (new StructType)\r\n\u00a0 .add(\"a\", \"int\", true)\r\n\u00a0 .add(\"b\", \"string\", false)</pre><p>Then the test will give you the following <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SparkException</span>:</p><pre>org.apache.spark.SparkException: Failed to merge fields 'b' and 'b'. Failed to merge incompatible data types LongType and StringType</pre><p><br></p>", "body_txt": "Problem The Spark job fails with an exception like the following while reading Parquet files: Error in SQL statement: SparkException: Job aborted due to stage failure: Task 20 in stage 11227.0 failed 4 times, most recent failure: Lost task 20.3 in stage 11227.0 (TID 868031, 10.111.245.219, executor 31): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary \u00a0 \u00a0 at org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:52) Cause The java.lang.UnsupportedOperationException in this instance is caused by one or more Parquet files written to a Parquet folder with an incompatible schema. Solution Find the Parquet files and rewrite them with the correct schema. Try to read the Parquet dataset with schema merging enabled: %scala spark.read.option(\"mergeSchema\", \"true\").parquet(path) or %scala spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"true\") spark.read.parquet(path) If you do have Parquet files with incompatible schemas, the snippets above will output an error with the name of the file that has the wrong schema. You can also check if two schemas are compatible by using the merge method. For example, let\u2019s say you have these two schemas: %scala import org.apache.spark.sql.types._ val struct1 = (new StructType) \u00a0 .add(\"a\", \"int\", true) \u00a0 .add(\"b\", \"long\", false) val struct2 = (new StructType) \u00a0 .add(\"a\", \"int\", true) \u00a0 .add(\"b\", \"long\", false) \u00a0 .add(\"c\", \"timestamp\", true) Then you can test if they are compatible: %scala struct1.merge(struct2).treeString This will give you: %scala res0: String = \"root |-- a: integer (nullable = true) |-- b: long (nullable = false) |-- c: timestamp (nullable = true) \" However, if struct2 has the following incompatible schema: %scala val struct2 = (new StructType) \u00a0 .add(\"a\", \"int\", true) \u00a0 .add(\"b\", \"string\", false) Then the test will give you the following SparkException: org.apache.spark.SparkException: Failed to merge fields 'b' and 'b'. Failed to merge incompatible data types LongType and StringType", "format": "html", "updated_at": "2022-05-31T19:57:52.322Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708427, "name": "aws"}, {"id": 2708428, "name": "azure"}, {"id": 2708429, "name": "gcp"}], "url": "https://kb.databricks.com/data/wrong-schema-in-files"}, {"id": 1390436, "name": "How to update nested columns", "views": 9838, "accessibility": 1, "description": "Learn how to update nested columns in Databricks.", "codename": "update-nested-column", "created_at": "2022-05-31T19:44:45.877Z", "updated_at": "2022-05-31T19:50:01.000Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStkREFNaHdiQVJha0c1dW5iekRCUUVNZWQ1RVNub0Q4VXJCRWEyL0p0Qk10RjZ6d0NZCmNJUVVzM0NCNy9XUTA0NEVuaHNOVVNoamIvRW9hUXM2QjZSak9Fem5vYTlVSlJlZFNlZmxEdk5LcVpUcApzaGNENlRjcDQ4MEFtM1FwYUNaMWFVcWJyd0xud3EwNWxhVklFc2k3SUUrREFPeENraHFaZXRNWkVuTEgKU1ArTDdGZk92N1g1RHhySk83L21vOTB1Tk1PakRHRk82a0taNWV2ZW1EMXV5b2g3cUhqSnNPWDkva3BsCndwWWtqMUZ4cC94a1I3MXFnb1RhdmYyRmtCSmVzUVpNWmpNYUZtdUdyTXNvbEFkR0xqZWNNS29zUG91Wgo5OUpPMHNkNm1jLzhsUHNCenE3QWVQcFIrK2ZWbVhHU2RCTGtRZytlTi9zWFl4VDJ4UU5aWlE0SThIZmMKRWlpKy9IUTVEbDlCSlVQWXcwV1Fxb1BtOVgydm1IUVdEekRjMkplblpuZ3J6QzR5MFllMU5qZkgweEdSClZwY2UvNzRhUjYzZUVESGxFTHd5S1lyMmlMSVc4ZXRvN0tzZTc2b0VNTXI4KzBmSmZTUFoyeGFHKzhTTwpLUzBWYVJiZUcvRzByYTJ0OGhTRE9qK0FZalVscUVwVzA4R1oya3RWZ0ZHRW0vOHd6NUJUcEpwODNnSUwKNEZaY3o0ZXRIWGlnNGZIWmZJK2FBSlZHL1ZlK1FPSFoxNlRNNzk4Z2xvWWJEcmtMd1Z4YUhhN2UzcldGCk4ydXRUNzcwZmc4d3hZeUxIM2JUUEpkQm54UFo1NlhlL2x1eStDbmVrV0YyTFVwcVZydzFFaDhqbGpCcwoxOVhyZlBlU2VQZ2JCdWorV1B1cUxCYzdJRmxmZy92K3FGUFdsK05odUpOZW5LTmpkTzNFd0ZMWlJTcmIKQlNrNXgveFlXK0lXQzNNUjdIWmxudld1S0RNOHovbXh3bUtidXF5bWNucUpLV1cvT0QwazZMV1U4VDhXCi9rN0MwOGZtaEpSRlcwMTZWaEtTeEdka3k1cFFhZnFLMkJVWUdZSFJ0VDR4Vk1tcEtSZ0d4Kzg5VjE2OQpMemhzekU0MWRuejU2UjVaSkRFUGdqM2N3TVVsWmVGSng4Q2puTWxjbS9MOExFdTVhZk0wQ3RyQ2MxalYKZEIrSU5pNzNLVS9EOG5XRjZtaG9CQVVYMWloTmNGUFhmbnJkTUZKMXkvTzdud1MveFJwQWptRFRFUTVuCg==.a3f21ed971c32a1d193fba416c942188\"></div><p id=\"isPasted\">Spark doesn\u2019t support adding new columns or dropping existing columns in nested structures. In particular, the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">withColumn</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">drop</span> methods of the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Dataset</span> class don\u2019t allow you to specify a column name different from any top level columns. For example, suppose you have a dataset with the following schema:</p><pre>%scala\r\n\r\nval schema = (new StructType)\r\n\u00a0 \u00a0 \u00a0 .add(\"metadata\",(new StructType)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"eventid\", \"string\", true)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"hostname\", \"string\", true)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"timestamp\", \"string\", true)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0, true)\r\n\u00a0 \u00a0 \u00a0 .add(\"items\", (new StructType)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"books\", (new StructType).add(\"fees\", \"double\", true), true)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"paper\", (new StructType).add(\"pages\", \"int\", true), true)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0,true)\r\nschema.treeString</pre><p>The schema looks like:</p><pre>root\r\n\u00a0|-- metadata: struct (nullable = true)\r\n\u00a0| \u00a0 \u00a0|-- eventid: string (nullable = true)\r\n\u00a0| \u00a0 \u00a0|-- hostname: string (nullable = true)\r\n\u00a0| \u00a0 \u00a0|-- timestamp: string (nullable = true)\r\n\u00a0|-- items: struct (nullable = true)\r\n\u00a0| \u00a0 \u00a0|-- books: struct (nullable = true)\r\n\u00a0| \u00a0 \u00a0| \u00a0 \u00a0|-- fees: double (nullable = true)\r\n\u00a0| \u00a0 \u00a0|-- paper: struct (nullable = true)\r\n\u00a0| \u00a0 \u00a0| \u00a0 \u00a0|-- pages: integer (nullable = true)</pre><p>Suppose you have the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span>:</p><pre>%scala\r\n\r\nval rdd: RDD[Row] = sc.parallelize(Seq(Row(\r\n\u00a0 Row(\"eventid1\", \"hostname1\", \"timestamp1\"),\r\n\u00a0 Row(Row(100.0), Row(10)))))\r\nval df = spark.createDataFrame(rdd, schema)\r\ndisplay(df)</pre><p>You want to increase the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">fees</span> column, which is nested under <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">books</span>, by 1%. To update the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">fees</span> column, you can reconstruct the dataset from existing columns and the updated column as follows:</p><pre>%scala\r\n\r\nval updated = df.selectExpr(\"\"\"\r\n\u00a0 \u00a0 named_struct(\r\n\u00a0 \u00a0 \u00a0 \u00a0 'metadata', metadata,\r\n\u00a0 \u00a0 \u00a0 \u00a0 'items', named_struct(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'books', named_struct('fees', items.books.fees * 1.01),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'paper', items.paper\r\n\u00a0 \u00a0 \u00a0 \u00a0 )\r\n\u00a0 \u00a0 ) as named_struct\r\n\"\"\").select($\"named_struct.metadata\", $\"named_struct.items\")\r\nupdated.show(false)</pre><p>Then you will get the result:</p><pre>+-----------------------------------+-----------------+\r\n| metadata \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| items \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |\r\n+===================================+=================+\r\n| [eventid1, hostname1, timestamp1] | [[101.0], [10]] |\r\n+-----------------------------------+-----------------+</pre><p><br></p>", "body_txt": "Spark doesn\u2019t support adding new columns or dropping existing columns in nested structures. In particular, the withColumn and drop methods of the Dataset class don\u2019t allow you to specify a column name different from any top level columns. For example, suppose you have a dataset with the following schema: %scala val schema = (new StructType) \u00a0 \u00a0 \u00a0 .add(\"metadata\",(new StructType) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"eventid\", \"string\", true) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"hostname\", \"string\", true) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"timestamp\", \"string\", true) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0, true) \u00a0 \u00a0 \u00a0 .add(\"items\", (new StructType) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"books\", (new StructType).add(\"fees\", \"double\", true), true) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.add(\"paper\", (new StructType).add(\"pages\", \"int\", true), true) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0,true) schema.treeString The schema looks like: root \u00a0|-- metadata: struct (nullable = true) \u00a0| \u00a0 \u00a0|-- eventid: string (nullable = true) \u00a0| \u00a0 \u00a0|-- hostname: string (nullable = true) \u00a0| \u00a0 \u00a0|-- timestamp: string (nullable = true) \u00a0|-- items: struct (nullable = true) \u00a0| \u00a0 \u00a0|-- books: struct (nullable = true) \u00a0| \u00a0 \u00a0| \u00a0 \u00a0|-- fees: double (nullable = true) \u00a0| \u00a0 \u00a0|-- paper: struct (nullable = true) \u00a0| \u00a0 \u00a0| \u00a0 \u00a0|-- pages: integer (nullable = true) Suppose you have the DataFrame: %scala val rdd: RDD[Row] = sc.parallelize(Seq(Row( \u00a0 Row(\"eventid1\", \"hostname1\", \"timestamp1\"), \u00a0 Row(Row(100.0), Row(10))))) val df = spark.createDataFrame(rdd, schema) display(df) You want to increase the fees column, which is nested under books, by 1%. To update the fees column, you can reconstruct the dataset from existing columns and the updated column as follows: %scala val updated = df.selectExpr(\"\"\" \u00a0 \u00a0 named_struct( \u00a0 \u00a0 \u00a0 \u00a0 'metadata', metadata, \u00a0 \u00a0 \u00a0 \u00a0 'items', named_struct( \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'books', named_struct('fees', items.books.fees * 1.01), \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'paper', items.paper \u00a0 \u00a0 \u00a0 \u00a0 ) \u00a0 \u00a0 ) as named_struct \"\"\").select($\"named_struct.metadata\", $\"named_struct.items\") updated.show(false) Then you will get the result: +-----------------------------------+-----------------+ | metadata \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| items \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | +===================================+=================+ | [eventid1, hostname1, timestamp1] | [[101.0], [10]] | +-----------------------------------+-----------------+", "format": "html", "updated_at": "2022-05-31T19:50:00.996Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708416, "name": "aws"}, {"id": 2708417, "name": "azure"}, {"id": 2708418, "name": "gcp"}], "url": "https://kb.databricks.com/data/update-nested-column"}, {"id": 1390430, "name": "How to specify skew hints in dataset and DataFrame-based join commands", "views": 13116, "accessibility": 1, "description": "Learn how to specify skew hints in Dataset and DataFrame-based join commands in Databricks.", "codename": "skew-hints-in-join", "created_at": "2022-05-31T19:37:03.897Z", "updated_at": "2022-05-31T19:44:29.221Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTl3VW4wSDI4TTU4Q1FGMjd4SXgzNS9HU2NGMlVYUWVGNUNkVnNUY25yWGIzbHoxaFhlClV2NlZIVnY5STBQV2dlUUFCa3FyREtLY2hGZi9CY1hYclFWTklqcmVrVTBFeWhXUWVnVEdHVEZxdk1mQgpDVUk5eFh2cnROVFJpTHNSMDJYeTczVHI3N1h3bERNT3hrNG0yc1E0MzJhMFMza3NIUFZTM1JuOExSN3AKWUI1c1FvaDVYb1hmcS9SNUhaaHNWVXAydk1FcEdLRGV5dm9QN3Zqa2J3MHgyYm0yL3pyRVY0SmJZUG1QCkdyemNSMUFJaUVrOUp2d2JQVnV2TzdaaVpqL0lHRVBjSHRjcjA3QUJKQXcwZFo3d3VkOGl6N3NPLzlKTQpRWkUxY3NBU2NhR0xBR0FPLzI1YXBYdlNzcDVHRDlmVHQ0OXh0M3Y4dUExWUt4Mkt6Vm82OHh3bXFyK0wKSWZ3NHdKRXVES0dIUjR0bitISzRYc0E0UnJuazRmRUQwYWw4M1I4eFVKRDAyOHN0cWF5Ym5nQW5mNzcvCmZ3ZTdqQTFGTlZQUFUxL2Vqb3F2dCtJQWs2M2pGNFM2UXlhaXVKdnk0T1kyTzJjTDd3TTR6d2tCVXlDZAp0N0FGZHpGWmZCUkc2ZFVNK2MrakYwZzBUUWc2MkV3WEpFSnpOUlA0TlVPMysyc244aE1BQjkyTWE2NlQKajVtWUpXb29MbEtVd0hxSzVsN1R5T3IreklWWG1XcUhoK0xMVFVidTFjSUFnb0RGVmw0dVFvTWo0SWdPCjcvY21vTXQ5Y0Y5MVB4L21nd1Fwc2FSMGN4M0dITFdyOXZDU2pERnN5ZWVGdFlaWnRWaGtic2g5M3QraAp3RzVMQm1pa3QwQndmWDN3QU1haUFGTmVCQ0pEUDVob3UrNTVxQWc4ZTNZaGIxM1E1N0JoOHVYajYrKzgKTjhHQ3FSVFc3Z3I2SjVHUXpNZWhJMkgwc2VYOXlOWi9uUnFSMzA1MnJYTDJmaVJNaldVZEt6d1ZwMlpsCnNWZXRJU3lEZExJY1JwaGtyRnkxWkc1RWlSc2hOR214UnFqcUlxOHJFWEJpNUdIVjJkTlhVQUtUSkdjRgpYcmRvYmRpbk1URVZmSW9Bd3R6UWhqMEQxcElvTmhOQlNSZ2RydmdWa2dxQXhSUnEzQWk5aFZkQit0blcKaXNMYWdwQTFOTEV4WTNzVnpiVWZ2L1dCbHA1N2EvTWloNUl2ZGNsakZucm5hZWpRTzNRTlNsVjNaNFBECg==.5397186e37a20acbf9735ce353e58d42\"></div><p id=\"isPasted\">When you perform a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">join</span> command with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Dataset</span> objects, if you find that the query is stuck on finishing a small number of tasks due to <a href=\"https://en.wikipedia.org/wiki/Skewness\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">data skew,</a> you can specify the skew hint with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">hint(\"skew\")</span> method: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">df.hint(\"skew\")</span>. The skew join optimization (<a href=\"https://docs.databricks.com/delta/join-performance/skew-join.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"skew join optimization\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/delta/join-performance/skew-join\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"skew join optimization\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/delta/join-performance/skew-join.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"skew join optimization\">GCP</a>) is performed on the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span> for which you specify the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">skew</span> hint.</p><p>In addition to the basic hint, you can specify the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">hint</span> method with the following combinations of parameters: column name, list of column names, and column name and skew value.</p><ul>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span> and column name. The skew join optimization is performed on the specified column of the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span>.<pre>%python\r\n\r\ndf.hint(\"skew\", \"col1\")</pre>\n</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span> and multiple columns. The skew join optimization is performed for multiple columns in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span>.<pre>%python\r\n\r\ndf.hint(\"skew\", [\"col1\",\"col2\"])</pre>\n</li>\n<li>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span>, column name, and skew value. The skew join optimization is performed on the data in the column with the skew value.<pre>%python\r\n\r\ndf.hint(\"skew\", \"col1\", \"value\")</pre>\n</li>\n</ul><h1>Example</h1><p>This example shows how to specify the skew hint for multiple <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span> objects involved in a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">join</span> operation:</p><pre>%scala\r\n\r\nval joinResults = ds1.hint(\"skew\").as(\"L\").join(ds2.hint(\"skew\").as(\"R\"), $\"L.col1\" === $\"R.col1\")</pre><p><br></p>", "body_txt": "When you perform a join command with DataFrame or Dataset objects, if you find that the query is stuck on finishing a small number of tasks due to data skew, you can specify the skew hint with the hint(\"skew\") method: df.hint(\"skew\"). The skew join optimization (AWS | Azure | GCP) is performed on the DataFrame for which you specify the skew hint. In addition to the basic hint, you can specify the hint method with the following combinations of parameters: column name, list of column names, and column name and skew value. DataFrame and column name. The skew join optimization is performed on the specified column of the DataFrame.%python df.hint(\"skew\", \"col1\") DataFrame and multiple columns. The skew join optimization is performed for multiple columns in the DataFrame.%python df.hint(\"skew\", [\"col1\",\"col2\"]) DataFrame, column name, and skew value. The skew join optimization is performed on the data in the column with the skew value.%python df.hint(\"skew\", \"col1\", \"value\") Example This example shows how to specify the skew hint for multiple DataFrame objects involved in a join operation: %scala val joinResults = ds1.hint(\"skew\").as(\"L\").join(ds2.hint(\"skew\").as(\"R\"), $\"L.col1\" === $\"R.col1\")", "format": "html", "updated_at": "2022-05-31T19:44:29.218Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708388, "name": "aws"}, {"id": 2708389, "name": "azure"}, {"id": 2708390, "name": "gcp"}], "url": "https://kb.databricks.com/data/skew-hints-in-join"}, {"id": 1390428, "name": "Generate schema from case class", "views": 8633, "accessibility": 1, "description": "Learn how to generate a schema from a Scala case class.", "codename": "schema-from-case-class", "created_at": "2022-05-31T19:34:32.690Z", "updated_at": "2022-05-31T19:36:47.196Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9NTVZGczJ1RVEwK0x4Tmt5b2tUVzI0SXMvampTajFnWTdmQkdpTmViVExVc1pPTTQrCnRaWlJVYXJEWUpBOVl1SThWMnAyNElUSVlhOGNFTTZjZzBud0lBR3c2MUorYWd6MFhzbFBpVkU1eStvTgp1bHkrNGhWcjF5M0RmbkVsL2lYYURabFNKaEozRWJEb1F3L1pJM1k1bGlVMmlpYmtmQ2RGL1NuemlxaHEKbm4xaXBYMStUc01xeG15bEx0MHFoUDQyOE1pNS9IMm1lTWlMSHFIR2RUN0I3N0VOT3RhNXdyS21RWFZXClRMUnB1N1UzaFB3dU14V05FbWZZbTdCUTU4ZVFCK0xGTCs0TDBUV0kvV1NyZkN2d0JUakRwM0JYdlVnZQpDY29naEZIR1FucWVqNGlnbE5YaTVrRnRyRlNJaTdUYTZ6R0tRNnVGOXhCSXd5OWZMYVNRS25xVDdBTlgKa1Z4NXlISjgxMUZJNGQrMGdDeXFGczhFRHVtRzdRYmd0bk0rUHptdTJMM1dxRWROZmVMN2NRWjBtcW0wCnFERFp3aHZJQ3VuYlJJMGpHOWFCRlRHS05nS0dyTkNNZnphbTBPeWp1RGN2VXFTUzF1b0RHbDFYSXAwdgpJNXdLUG9FNTVKTTNhQ0Z3L29rcDhSemRSSldjR0JRVlFHNXVTMFJ0WUNuc3lNQ1czZWpWWXBmRElkVlkKV2V1eXZ2NmkwQjR0ZXVxWldvS1VBQ2Z6a25yMmE0VDR3eDF4cCtMVS85WTZLVjJZdCtMVURyNE9GNHQzCmoxRkVqRGE3VmZ3a1dYaEpsUXpFUU1TV0UzY0lUdk0yRVVuQkFGYTJPYTlPa0FIV29HYm8yQi8xS1hDVApFbjVFUFJiU1pHMWdLbnFpSUxKclR6VmFDNzRmQS9tUnlJSzlMWlZnNlV5RnBWRzJOVzhtd2hzckdVSGsKcGJZLzlPZmFnZWdrampKcFgvLzBQZ0FyY3lEUy9sNlpOM2VLVEUzeTlXaTdZR0lOeXBKeXdNeFNZcGxMCk9NU1ZoZ2NEbGhFWWpYUHRxQVM2SzU1cFFIWEQzT2oyOU05TjliYTZDUkJpYjRYeUpPTVlacXpWS1VMKwo0OTB5NXdpV0p5Z1RpZU1naFA3Um1DY29VRklvOHZJV3ZWR0Z2KzltTDlSdytiUEpKVnJ2OTY5cFhsc3IKRWFId05Kc2pTV3orUkU1eW0ySW9naW1HT1J0QTNVOU9wc01MS0RrMmZaUVlpL2FFY2lzYWxja3ZVd2JpCg==.8220ccd0636ddac271e39ffc06244449\"></div><p id=\"isPasted\">Spark provides an easy way to generate a schema from a Scala case class. For case class <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">A</span>, use the method <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]</span>.</p><p>For example:</p><pre>%scala\r\n\r\nimport org.apache.spark.sql.types.StructType\r\nimport org.apache.spark.sql.catalyst.ScalaReflection\r\n\r\ncase class A(key: String, time: java.sql.Timestamp, date: java.sql.Date, decimal: java.math.BigDecimal, map: Map[String, Int], nested: Seq[Map[String, Seq[Int]]])\r\nval schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]\r\nschema.printTreeString</pre><p><br></p>", "body_txt": "Spark provides an easy way to generate a schema from a Scala case class. For case class A, use the method ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]. For example: %scala import org.apache.spark.sql.types.StructType import org.apache.spark.sql.catalyst.ScalaReflection case class A(key: String, time: java.sql.Timestamp, date: java.sql.Date, decimal: java.math.BigDecimal, map: Map[String, Int], nested: Seq[Map[String, Seq[Int]]]) val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] schema.printTreeString", "format": "html", "updated_at": "2022-05-31T19:36:47.192Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708373, "name": "aws"}, {"id": 2708374, "name": "azure"}, {"id": 2708375, "name": "gcp"}], "url": "https://kb.databricks.com/data/schema-from-case-class"}, {"id": 1390425, "name": "Job fails when using Spark-Avro to write decimal values to AWS Redshift", "views": 6693, "accessibility": 1, "description": "Learn how to resolve job failures when writing decimal values to AWS Redshift with Spark-Avro.", "codename": "redshift-fails-decimal-write", "created_at": "2022-05-31T19:31:30.962Z", "updated_at": "2022-05-31T19:34:19.496Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSt2VUxPc3JHcHhzNnU1cnIxNVltREdyUCt1ZFM4Ukg4RmpFaGFBYUNJcXBNWC85VnBiCjV1bDdHQm1QY056eStQUEpPZ0xSSDdlVm9ITnVGcy9nQlpkTDEwd0FPNVRESmJKaTY3R053ZXU4VTQ3NAplL3pBM2JIUW9DVWovL3BqNUwyREdmTENvQUh6ZEYwYTNjRzRaR202RTBvenYrSXBqRmNYb2s0TTNXcDcKSHBvWFE2REZPVzlsRDc2UVF3UVlnOFVGSElrajZPcTBKT25WaUtIVCtyKzF5Zkt6U0diWXgyeUJKREUwCmFqbDl6TVJINHhweGlIc1RWWG1oSkZYRndGblZ0ZmVyTWZkYVRhNEcwRWpMektHR05rTDNGbXZXTTRLUApGL21lQnFDcXFTcFFtL2QxWnRzWC9CeEpMV09UdmM3TzNOeHFmSlhIZjRmemRYQ29SS3UxWFdUTmoyc1IKQzJwa3JSZytadEF3TzVHd1dBQ0FONFFrZm8zTzFtU21jYXhVTGZ0VXhPdGM0bGJaTjdiU0h6RmNERWEvCmdlZWlNZ2RRZDBzWXhuVHMwOUZiWlkyNmV0bE1zUmRIeitWUXM5bkhrRUQvVTFRd3A5QlJxdHFqeEI0RgptbHdiM1ZWRkNxemFuL1gzOWZVdXQ1c25FUGtaeTl6TWZQdWVhU0M0MXpGMGlmYmo3aXFCMkRVYXVldGsKUm5aRCtCUSt4OE9JWG93RW9GNTVDTWdaOVN2aTRVRWRod3NPSG0remxqRzZoM3QwY0c5VGJSOTJJOUVBCklKT2VOaGNiSkZvZms4UWRNWitaMDhUL2FyUlIyYXpaYTBNN1lsWmFxeFN6S3IxYVVHOUc0b1pwa25nSApXZW1zdW5CYUhoUGpTVFFKdDgzM2lKVU56SjhIVFFCVEF3VndVazJQSDRla0QxNmlUYXhndEdVVlFSMXcKTlBkWlE0aXl3aFJ2UTFYL2dFTnVXVEdESXkxOUIreGJuMEZQdE9uTjZhZDZBZzVPZU5yZmQwdExRMUpGCks3N2NXcUNhSXpnckRScEZNSU9jN2ZqTjFHc2t0aFZqT0pmUDgyc3lGVUlURVZpaW9zeGlHSWo5b2ZMbAo3Zy84cnpKazZ5cmh4TEFZZEZ1UThualNvelNDTnFYMUppWEFRV2x0WndzL0I5cVZaVG1PbzBNNDhCOHUKRW9wUUc2dldnUmVXdEE0YzRHZnBWZDAzUzRSN0dnbUZLemsyWXBjPQo=.492d09735a260fb2f302ab00ffa48d0b\"></div><h1 id=\"isPasted\">Problem</h1><p>In Databricks Runtime versions 5.x and above, when writing decimals to <a href=\"https://docs.databricks.com/data/data-sources/aws/amazon-redshift.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Amazon Redshift</a> using Spark-Avro as the default temp file format, either the write operation fails with the exception:</p><pre>Error (code 1207) while loading data into Redshift: \"Invalid digit, Value '\"', Pos 0, Type: Decimal\"</pre><p>or the write operation writes nulls in place of the decimal values.</p><h1>Cause</h1><p>When writing to Redshift, data is first stored in a temp folder in S3 before being loaded into Redshift. The default format used for storing temp data between Apache Spark and Redshift is Spark-Avro. However, Spark-Avro stores a decimal as a binary, which is interpreted by Redshift as empty strings or nulls.</p><h1>Solution</h1><p>Change the temp file format to CSV using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">tempformat</span> option. You can use this sample Scala code:</p><pre>%scala\r\n\r\n//Create sample data\r\ncase class createDec(value: BigDecimal)\r\nval df = Seq(createDec(45.24)).toDS\r\n//Write to Redshift\r\n(df.write\r\n\u00a0 .format(\"com.databricks.spark.redshift\")\r\n\u00a0 .option(\"url\", jdbcUrl)\r\n\u00a0 .option(\"tempdir\", tempDir)\r\n\u00a0 .option(\"dbtable\", \"testtable\")\r\n\u00a0 .option(\"aws_iam_role\", \"your_aws_iam_role\")\r\n\u00a0 .option(\"tempformat\", \"CSV\")\r\n\u00a0 .mode(\"append\")\r\n\u00a0 .save())</pre><p><br></p>", "body_txt": "Problem In Databricks Runtime versions 5.x and above, when writing decimals to Amazon Redshift using Spark-Avro as the default temp file format, either the write operation fails with the exception: Error (code 1207) while loading data into Redshift: \"Invalid digit, Value '\"', Pos 0, Type: Decimal\" or the write operation writes nulls in place of the decimal values. Cause When writing to Redshift, data is first stored in a temp folder in S3 before being loaded into Redshift. The default format used for storing temp data between Apache Spark and Redshift is Spark-Avro. However, Spark-Avro stores a decimal as a binary, which is interpreted by Redshift as empty strings or nulls. Solution Change the temp file format to CSV using the tempformat option. You can use this sample Scala code: %scala //Create sample data case class createDec(value: BigDecimal) val df = Seq(createDec(45.24)).toDS //Write to Redshift (df.write \u00a0 .format(\"com.databricks.spark.redshift\") \u00a0 .option(\"url\", jdbcUrl) \u00a0 .option(\"tempdir\", tempDir) \u00a0 .option(\"dbtable\", \"testtable\") \u00a0 .option(\"aws_iam_role\", \"your_aws_iam_role\") \u00a0 .option(\"tempformat\", \"CSV\") \u00a0 .mode(\"append\") \u00a0 .save())", "format": "html", "updated_at": "2022-05-31T19:34:19.490Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708372, "name": "aws"}], "url": "https://kb.databricks.com/data/redshift-fails-decimal-write"}, {"id": 1390419, "name": "Behavior of the randomSplit method", "views": 9674, "accessibility": 1, "description": "Learn about inconsistent behaviors when using the randomSplit method in Databricks.", "codename": "random-split-behavior", "created_at": "2022-05-31T19:25:26.695Z", "updated_at": "2022-05-31T19:31:15.969Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlWYno1bS9SWXliRCtubU53SUEwNU9kUkZGS1dkUkxzcnZ5cjJvUmUzT1k0OHVCMnd2ClRSMHFQTnhjVlV0VkVqOG5icHdpeTlEUVZIdGpyZkZQdkttNnN4clRTYjZlTUlMdWx3VlVNRWkzRzlmNwpRd3VDWEZlVEhTNGpGODJLeVlkK1RMSU5FWmRnMFZ6cVcrQ3ZMRlZia0lsQjRaU0ZqWngwN2pRc1hlbWsKQWlQNkYwUlRraDZHM3krM2x5dEoyMjVZYkU0V3lSKzNqWWVhUlh2cmFQVy91UHgxOGdPZ1FRb0xJZjNSCk8zWDQxOUJJNEd0T2UzRlVXSDhPc056TEgrYk0rc21hU1ZoM1ZBd1BIWnpXTS9uVW5oWWQ3VVdzZHJiMApJTkMzQkZSUS9mYTlEWVJScmZmNXFRczM5eUgzVVovMVdFb01OMCsvbzJRbWhGa2dheHBkQjB6bzltemkKaXU2cndmYzZWWkt1RDBOQ1FpcjU4cUlNQjhXK2hROEpKZGJUMUhJdUhvS2VwdU1jZkllREdBdFNxSWt4ClVkR0FoTk5xd3R0K05FbXU0ZXRiZCtZUjROUTVwRkdxZlhsT1RwZFlLcWpQb3o2amY4T2pvbVJSZ2lmZgpSdldVNm5nYmN3bjNGMitNaS9TT1ZENC9jSTh2SlMrQjh2TTBJRXg4WmpxZ0RyUExWWXVtQ1A2SkJYeFgKUEtIbnY2aEowOUlkZVhsOTZIazBwR0lVNmNyMHpDallnTm1aUHdzTFlsR0ZhVWVLS0x4OFkvT1RqUW5lCndxc1d3Ymk3MFlQUEFKREdLcFk0RVF3dmNPZVJTOExqSHVrTEZzNDdHeXl4U1M2cGJSWjJ1QnRsZ2NGRQo3NndVTWxCY2JSY0cxK2N3SllQZVdXWFhKdmk4SXp6cnM5VU54QzVjMkxyWDA1b3JlZHNWWjNsT0hCcXgKOXY5WXhTcHRZK1NsQlNnbmVadktXc3NtcHcrT1BYUXJsUWZHWmhFLzVxdzkwYVY2cWxHSlRTeGY2dFZaClBlbzJoK0I1REcvL2FJZDVEWVg4ZXpNWTV6NWxJQWIvTTRkaHlRZVhWcnlneWxlYnZ0NllBaHVDVzBxTwpvMTg1M3VXL0FlZjQrWXV3TWo1RHFNUW5HU0dIYk1OeHZ0SHNoOUFYTHZUSkRLRUYxL2JLYzRzMkFrVVUKSlFNVUFaRHBVeDE2M2wwVjlUc2Z0aTkrRFZ0dnlkeTVycW82YVFHd2k4Qi95UU5uSGJvUEcvckNlbEozCg==.35c464ad525735c78e3c24080111141e\"></div><p id=\"isPasted\">When using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">randomSplit</span> on a DataFrame, you could potentially observe inconsistent behavior. Here is an example:</p><pre>%python\r\n\r\ndf = spark.read.format('inconsistent_data_source').load()\r\na,b = df.randomSplit([0.5, 0.5])\r\na.join(broadcast(b), on='id', how='inner').count()</pre><p>Typically this query returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">0</span>. However, depending on the underlying data source or input DataFrame, in some cases the query could result in more than 0 records.</p><p>This unexpected behavior is explained by the fact that data distribution across RDD partitions is not idempotent, and could be rearranged or updated during the query execution, thus affecting the output of the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">randomSplit</span> method.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p id=\"isPasted\">Spark DataFrames and RDDs preserve partitioning order; this problem only exists when query output depends on the actual data distribution across partitions, for example, <em>values from files 1, 2 and 3 always appear in partition 1</em>.</p>\n<p>The issue could also be observed when using Delta cache (<a href=\"https://docs.databricks.com/delta/optimizations/delta-cache.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Delta cache\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/delta/optimizations/delta-cache\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Delta cache\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/delta/optimizations/delta-cache.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Delta cache\">GCP</a>). All solutions listed below are still applicable in this case.</p>\n</div>\n</div><h1>Solution</h1><p>Do one of the following:</p><ul>\n<li>Use explicit Apache Spark RDD caching<pre>%python\r\n\r\ndf = inputDF.cache()\r\na,b = df.randomSplit([0.5, 0.5])</pre>\n</li>\n<li>Repartition by a column or a set of columns<pre>%python\r\n\r\ndf = inputDF.repartition(100, 'col1')\r\na,b = df.randomSplit([0.5, 0.5])</pre>\n</li>\n<li>Apply an aggregate function<pre>%python\r\n\r\ndf = inputDF.groupBy('col1').count()\r\na,b = df.randomSplit([0.5, 0.5])</pre>\n</li>\n</ul><p>These operations persist or shuffle data resulting in the consistent data distribution across partitions in Spark jobs.</p>", "body_txt": "When using randomSplit on a DataFrame, you could potentially observe inconsistent behavior. Here is an example: %python df = spark.read.format('inconsistent_data_source').load() a,b = df.randomSplit([0.5, 0.5]) a.join(broadcast(b), on='id', how='inner').count() Typically this query returns 0. However, depending on the underlying data source or input DataFrame, in some cases the query could result in more than 0 records. This unexpected behavior is explained by the fact that data distribution across RDD partitions is not idempotent, and could be rearranged or updated during the query execution, thus affecting the output of the randomSplit method. Info\nSpark DataFrames and RDDs preserve partitioning order; this problem only exists when query output depends on the actual data distribution across partitions, for example, values from files 1, 2 and 3 always appear in partition 1.\nThe issue could also be observed when using Delta cache (AWS | Azure | GCP). All solutions listed below are still applicable in this case. Solution Do one of the following: Use explicit Apache Spark RDD caching%python df = inputDF.cache() a,b = df.randomSplit([0.5, 0.5]) Repartition by a column or a set of columns%python df = inputDF.repartition(100, 'col1') a,b = df.randomSplit([0.5, 0.5]) Apply an aggregate function%python df = inputDF.groupBy('col1').count() a,b = df.randomSplit([0.5, 0.5]) These operations persist or shuffle data resulting in the consistent data distribution across partitions in Spark jobs.", "format": "html", "updated_at": "2022-05-31T19:31:15.967Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708369, "name": "aws"}, {"id": 2708370, "name": "azure"}, {"id": 2708371, "name": "gcp"}], "url": "https://kb.databricks.com/data/random-split-behavior"}, {"id": 1390412, "name": "Nulls and empty strings in a partitioned column save as nulls", "views": 12812, "accessibility": 1, "description": "Learn why nulls and empty strings in a partitioned column save as nulls in Databricks.", "codename": "null-empty-strings", "created_at": "2022-05-31T19:16:37.707Z", "updated_at": "2022-05-31T19:25:09.201Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStUT01MVFZWSHdrYm5HNHNUbnk5akFROFhvSTVFbmF2MEZuVjFoa2FmWnB0dHAwdUNyCndLd05KTUYzekdZVGxpZkNHalJvL3pGMURCa1VBYmVPVlZoZEtUcURhWGgyWm9wR2J6WitLbXJuVGZzTQpRcEFERGdCWHhsZFdaQzFDQ3Z6NlVjdXJSYkJJay9tMXNBWnBaUG91SHdEdXZiMXBBOUlvaHdlZEcrc0UKRXNsSzZTOXJjMllwNGc1NTNqYW9CYWVpQ05hYlJ2aCt3MHJQc21keERzV0l4S1NIVzlZRVBaOHpESUhSCnJUZE5NeFBzdiswdEFKSHQyUW45UGR5NG5sUG5tMVVUVURYTDVnczNCZjJUZ1FHeUREZzdKVDZlSkFISApIdVgzM3NFeUc0amxicVVuZmwyMEVQaGxWYTZMTTNsM2ttZXQ4OHlXbVpxWFd1VDdFR254OE9MQ1V3NjgKTnFqMzhrQ3JpQi9wOHJ1MStXUkhXYmdjRElCVEZhcDJ3YmxMeDRDeFJSdmpuUkZUaDNDQXdzRjR0ZzMrClB5c3Q5cS9BTVlkYVo1NlgxaGlDTitESDlSMzkvVWx5Q25EV0JUczNSR1JWZWlFRU5pQVBZTUVMOE5xWgpxVWNxMlptZ3dvUEY2ODROUzdwOVcwZkFqaEpyMkNmTkZ0TzNMTlBFZ3hiQno3RG5BWmkzMzVSZFFmbHMKZGxVS2l4c09KSjg0TEx3QzV3SXlwQUtDTHR5VGFyc3FFS2x5ZWZiVmhkVGtzSmhqYklONGRuS0pqSnFUCjRFbmdTMUl2S1RxT3FEaHNSMjBTN1lMeVk2WWtUSFVsYlFjWnBSVHQvYWtHc2YxR3FUcnZ6M0Z1enNuLwpmUHNTMFNpRmF0S1MxQmhtTFliYnlTc1YzdzRDQ3ZEd0dveHBCMUNuM1RWVlNJTWZzSDFaSVpGVFBXb0sKNlA5OGRpQUUwbjd3TDRoTmE0OXR3TFo4QUFYNEpzK2dpbzNkTjVUVzZEcVB0Sk5UNWQxaUVGUVdqVnlrCk9oemxYNURNMVk3ak1TU080TmE4VlRzeEc2NmdZN04wdHZiMzdkQkNRNy80eUNXTlpRTzNBUUs3eTJZVgp1Z2Q5TzhxZ2ozUWE3TjE4QWg3eUZzVjZUZiswbHFUSTZSZldXckg0WmhIZFBScDlhSHFPcXlQdXdhVDkKcktTSm5GbHkzV1lmZmVlNnRhRWhabDRUcDNnUWk3MHJmRFcwUTBxS2pPUmpjbGRWS0tXekhzeXU5QWliCg==.3e3a0965ef72ff37d2232daf08dbe022\"></div><h1 id=\"isPasted\">Problem</h1><p>If you save data containing both empty strings and null values in a column on which the table is partitioned, both values become null after writing and reading the table.</p><p>To illustrate this, create a simple <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span>:</p><pre>%scala\r\n\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\r\nval data = Seq(Row(1, \"\"), Row(2, \"\"), Row(3, \"\"), Row(4, \"hello\"), Row(5, null))\r\nval schema = new StructType().add(\"a\", IntegerType).add(\"b\", StringType)\r\nval df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)</pre><p>At this point, if you display the contents of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">df</span>, it appears unchanged:</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654024983040-null-empty-strings-1.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Null values and empty strings displayed.\"></p><p>Write <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">df</span>, read it again, and display it. The empty strings are replaced by null values:</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1654024994976-null-empty-strings-2.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Null values replace all empty strings.\"></p><h1>Cause</h1><p>This is the expected behavior. It is inherited from Apache Hive.</p><h1>Solution</h1><p>In general, you shouldn\u2019t use both null and empty strings as values in a partitioned column.</p>", "body_txt": "Problem If you save data containing both empty strings and null values in a column on which the table is partitioned, both values become null after writing and reading the table. To illustrate this, create a simple DataFrame: %scala import org.apache.spark.sql.types._ import org.apache.spark.sql.catalyst.encoders.RowEncoder val data = Seq(Row(1, \"\"), Row(2, \"\"), Row(3, \"\"), Row(4, \"hello\"), Row(5, null)) val schema = new StructType().add(\"a\", IntegerType).add(\"b\", StringType) val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema) At this point, if you display the contents of df, it appears unchanged: Write df, read it again, and display it. The empty strings are replaced by null values: Cause This is the expected behavior. It is inherited from Apache Hive. Solution In general, you shouldn\u2019t use both null and empty strings as values in a partitioned column.", "format": "html", "updated_at": "2022-05-31T19:25:09.197Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708364, "name": "aws"}, {"id": 2708365, "name": "azure"}, {"id": 2708366, "name": "gcp"}], "url": "https://kb.databricks.com/data/null-empty-strings"}, {"id": 1390405, "name": "No USAGE permission on database", "views": 7523, "accessibility": 1, "description": "User does not have USAGE permission on the database.", "codename": "no-use-perms-db", "created_at": "2022-05-31T19:10:14.464Z", "updated_at": "2022-05-31T19:16:23.813Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStVQXVSUFJZL2ticm9KZHlXWTV0K3hreWE0TlMrK3krRkRqYVQzZlg2bk5wNWFvNVFMCkJnZ2RQdkJwQ0Y4YWVzRjRuVHZQaGhlTFhzOVBPYTIybStPd1RFUkdRdktpN1BUeis3Q0piN0tlMWZragpnZk50OURkL3lST0VWdzc2ODlSbjNiM3AzYlNQNlVUQ0FVLzNZMlVuNjNaMUwwOXdmZlZDZkhXd3hJU2wKb1dMNGpTRUl6dWp4bmpBbHY2c3o2VWsyWlpGcUlYSkRWaVhGQzkyYjBkMkFCM2dQWkM5akV5eTZnQVFMCjdaRWczS20rSXNqSFJ1eS9kLzNwNkszV3VpRWloZ29HVWlrbE8xQlJ5OStwSXJyZmtFY3AxVmwyR0VVdQpzNnY3WWJtV1VQQW1NOHN3Y2Rib1R2djdKQys2dUVJOHEwQ3ZvWkFXOTdTcyt1SDI0RnB3QTgrU3J1UjkKU3RIYitSbllhUnFsQ1VvN214U1dHcmc0dEYvNmhnL29yZ1h6c1hmM1hhSkpBQytxS0VXSFhaRkUwVmN4CkttalFWQTg1V29MSEVzVitnNnVmS2lqUkJ5U1ZWbjIxdUVkL2hBelllN1htdHpFaUc4dnNXSjZrWjVESAp1YUx4ODVRMzJZYjBORzIzRXltSzlYb1pab3JNeFdzR3RJYnFTSW5jVEhFU1M1bGwrT3FZMHBoZ2gxdHQKVWtvd2tSdnRJRGdiamJxa3NmR0VybHNja0s0SVJ3UVlpRFhMNGFRWXF4L0swY1BwTjRZUkhhQzlGa0FSCkN2ZE4wcXZSUUZqOXJyam5LQjVhZkdBdHh3c2JPQ3pYdHRjL2x5RVBlazFWMkgraExGYXZ5aHVVTVUzNQpoUkNsYSsxOXU1U0kwYllMNGFqZE1iZnJFblduSnN0TDNOditobXNrYVBlaGh5UTAzK01ONkJTcTMzalgKVWZubmw1MC80VnRLR1dWWDVDSGtjc3hFVk45eUhjanNucGRla3Y3SnhUN1VTQndJbVJZRVgyTkZMME13Cnp6ZU5jNzN6SHN2Ny93SW55VG5QRHBsWmp4cFlWcUoreisrdTdHVWxOcU5VNStxbVVvYWpBc3NZT3VrUwpSQjJMOTIyWjFiRU1TVFNzS1gzQmlkbWtGMWtCR1ExRU5IZU9SbGtYSmx3NGF6WEFYaDEvMW1wdXZSQUwKSEd0SmhBZFZ4TDR2S2xISGN5UTYzcU9MakxXSTBnRVpXT01KdnZuT1M2VWpyY1gwR2JnV0FTMjQ5SDhzCg==.df3bd4295cbcee8dd092d2e7f33c4e23\"></div><h1 id=\"isPasted\">Problem</h1><p>You are using a cluster running Databricks Runtime 7.3 LTS and above.</p><p>You have enabled table access control for your workspace (<a href=\"https://docs.databricks.com/administration-guide/access-control/table-acl.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"enabled table access control for your workspace\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/administration-guide/access-control/table-acl\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"enabled table access control for your workspace\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/administration-guide/access-control/table-acl.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"enabled table access control for your workspace\">GCP</a>) as the admin user, and granted the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SELECT</span> privilege to a standard user-group that needs to access the tables.</p><p>A user tries to access an object in the database and gets a SecurityException error message.</p><pre>Error in SQL statement: SecurityException: User does not have permission USAGE on database &lt;databasename&gt;</pre><h1>Cause</h1><p>A new <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">USAGE</span> privilege was added to the available data access privileges. This privilege is enforced on clusters running Databricks Runtime 7.3 LTS and above.</p><h1>Solution</h1><p>Grant the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">USAGE</span> privilege to the user-group.</p><ol>\n<li>Login to the workspace as an admin user.</li>\n<li>Open a notebook.</li>\n<li>Run the following command:<pre>%sql\r\n\r\nGRANT USAGE ON DATABASE &lt;databasename&gt; TO &lt;user-group&gt;;</pre>\n</li>\n</ol><p>Review the USAGE privilege (<a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#usage-privilege\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"USAGE privilege\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/administration-guide/access-control/table-acl\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"USAGE privilege\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/administration-guide/access-control/table-acl.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"USAGE privilege\">GCP</a>) documentation for more information.</p>", "body_txt": "Problem You are using a cluster running Databricks Runtime 7.3 LTS and above. You have enabled table access control for your workspace (AWS | Azure | GCP) as the admin user, and granted the SELECT privilege to a standard user-group that needs to access the tables. A user tries to access an object in the database and gets a SecurityException error message. Error in SQL statement: SecurityException: User does not have permission USAGE on database &lt;databasename&gt; Cause A new USAGE privilege was added to the available data access privileges. This privilege is enforced on clusters running Databricks Runtime 7.3 LTS and above. Solution Grant the USAGE privilege to the user-group. Login to the workspace as an admin user.\nOpen a notebook.\nRun the following command:%sql GRANT USAGE ON DATABASE &lt;databasename&gt; TO &lt;user-group&gt;; Review the USAGE privilege (AWS | Azure | GCP) documentation for more information.", "format": "html", "updated_at": "2022-05-31T19:16:23.810Z"}, "author": {"id": 488150, "email": "rakesh.parija@databricks.com", "name": "rakesh.parija ", "first_name": "rakesh.parija", "last_name": "", "role_id": "admin", "created_at": "2021-10-07T02:59:41.577Z", "updated_at": "2023-04-21T14:21:18.111Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708355, "name": "aws"}, {"id": 2708356, "name": "azure"}, {"id": 2708357, "name": "gcp"}], "url": "https://kb.databricks.com/data/no-use-perms-db"}, {"id": 1390399, "name": "How to handle corrupted Parquet files with different schema", "views": 9067, "accessibility": 1, "description": "Learn how to read Parquet files with a specific schema using Databricks.", "codename": "match-parquet-schema", "created_at": "2022-05-31T19:05:27.503Z", "updated_at": "2022-05-31T19:10:02.055Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS93b3AxQVNQY3dmNGlUVWkvOHF1NjhPOGc1NmVRWWg5d05pbTV6ekdjMDR3RXBSRFNoCi9JK1d6TFZwWGliZUVpdXNjNFExU3VFWnBBWUFnR1lNWENtSnN1Y2Q4WUR3WjYxYmYwRXE3S3U5cjZDNwpudGwzOFZ6TnRjMTB2YUlRVEZsdytoVDBDSmVWYldsZ1k2SnZjY2o3R1pYYVFpOFFGY0d2YXA4OTBjcHgKMlZZc3VENFJFdjVjLzNteUk2WG9yL2VQaG9DZnBEK2V6UXJhRk8yOEplbWJmcnlkdWwvVzRvS25mVFMxCitZL0YvMFV5d0NPMkNGRDIzbnA4eUR2N2tubUxJeW1UazdDek44SXlUTE1GSkoyRGI5K3VxNWxsZ3A3ZApqbld1YitwaWZRUEthcEhHenRlTVF1dFc1OWk4bjh1Ry84ekZCbkRKRWpsL0hPc0c1QXZYUGwwUFhPV3AKVEVrQURMUEI5ZVprWDZuNmhNQSs1TXpQYzhjNUV1UmFaZFpFNDV0WkZuWTJoV3lIdWtGR3hIbUNQTzFlCkFqYUwwSzFaNGJIVllvcVRqVmYwT2R4bEdvaVEzNldZZkRsL210Vk02bmc4VDZhZTExeEN0YlpRNHdCWQorZlFHVmhuSVkzZW1RdDU0b2ZlTTBMSjdnOU1uT21kMEsxZmZnZkp1ekFNc1VDaDU2ZUVBeVZnZGRVcmcKTVpRS2VkZ0djZjhhT0pHaS9aLzJ4cUZnbnpJUG03TnBoK25pNUNwL1RFa1pZNlFmQXg1OTRvVHJxS1BXCmVNc2V5QTFJajZlblJBdzBINzVsM2Z0RUdybDgxN2xkRll3eS83ek5hd0IrM2NNR1RxNW1yL3RBWVFkOApYRzIwaUVabTlLOXhEakdRVUsxSFM4S1JEWmFWOWc4Y2tNbEFUOEc4RHBDL0hJdE16aS9yT2pvaXdZMmIKVnNSRDByOXdYcjh2TURYQmp1bzM1NkFibEVuYkw5WWlOdVZPUVZ2UE1IeFZOWEhEclloSWl5MlVDS1IrCndRejBMbVZDNDRKV1RmMVhTd0FVREFuSldMQi9jVjdDZlMwSEhwVUhLZm5jbmc5dEIyeGFmTlVYMXFGQgp5NGtodGNZN05FV095R1FYWUpQbE5Ed3NuWXA4QVJIdERKZVVKR3JRTWtyaDB3aG1BSVp0aTQvVU1RcmcKc3RqVlVrUFBETW44NWZ1SFAydWprWGRoaGM4NmVySU1MeHQ5MlNlcC9Ya3RUVUNCcjZsOG91N29nQ3IwCg==.1ef60ea73e5ee84b00b49467c03ba486\"></div><h1 id=\"isPasted\">Problem</h1><p>Let\u2019s say you have a large list of essentially independent Parquet files, with a variety of different schemas. You want to read only those files that match a specific schema and skip the files that don\u2019t match.</p><p>One solution could be to read the files in sequence, identify the schema, and union the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrames</span> together. However, this approach is impractical when there are hundreds of thousands of files.</p><h1>Solution</h1><p>Set the Apache Spark property <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.files.ignoreCorruptFiles</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span> and then read the files with the desired schema. Files that don\u2019t match the specified schema are ignored. The resultant dataset contains only data from those files that match the specified schema.</p><p>Set the Spark property using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.conf.set</span>:</p><pre>spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")</pre><p>Alternatively, you can set this property in your <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>).</p>", "body_txt": "Problem Let\u2019s say you have a large list of essentially independent Parquet files, with a variety of different schemas. You want to read only those files that match a specific schema and skip the files that don\u2019t match. One solution could be to read the files in sequence, identify the schema, and union the DataFrames together. However, this approach is impractical when there are hundreds of thousands of files. Solution Set the Apache Spark property spark.sql.files.ignoreCorruptFiles to true and then read the files with the desired schema. Files that don\u2019t match the specified schema are ignored. The resultant dataset contains only data from those files that match the specified schema. Set the Spark property using spark.conf.set: spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\") Alternatively, you can set this property in your Spark config (AWS | Azure | GCP).", "format": "html", "updated_at": "2022-05-31T19:10:02.050Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708352, "name": "aws"}, {"id": 2708353, "name": "azure"}, {"id": 2708354, "name": "gcp"}], "url": "https://kb.databricks.com/data/match-parquet-schema"}, {"id": 1388990, "name": "How to list and delete files faster in Databricks", "views": 32638, "accessibility": 1, "description": "Learn how to list and delete files faster in Databricks.", "codename": "list-delete-files-faster", "created_at": "2022-05-31T06:34:10.496Z", "updated_at": "2022-05-31T18:49:23.912Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThqTWZHbUxnUHJQb2pGT3gwUyt4a1RDNHhLcERkcUw5OVNrMHpYU2RGdVdKTmwwMFE1CitGWlhMdnU3ZFFxMUJ1c0wxOFh5dFMxcG1CYmdrVGxLNDFXemF1VDcwMlo4UmtYRC9aSDZIT3pQZndrWApOSGNBeFlBa3RuNlk2VFRNWFRkUjFqUkk4WkVTcERoTldDZ1ZvbG1SYWVZRjBNTzhtemRaZ3dQWFpVSmYKMVlFc1FIaWM4TkZIWmlIekloMS9sMTVnUnRMSTVKcVpYZlgvb0hobkFZMEVKRC9OK2VMa2s5Tyt3VUlzCnR5THg1cENvd0tnczZHanpGbUQ3MDlxSkhSdWhMU21ISE1YTU16MnVRaUhCZlRpOWRSdFNIOFg0cUdkZApscEVwTTdBZ3dGMkxQdm4zQlMxQVorZFowQjZKdjBBa0ZsdmUxMUtXaVFHNkxML0NNTWNuUmZDdVQvK2EKWHNzYWRENVpER0V0ZDlBYnJTZHpwQW5qcjh6NXNodkIza0YyanVNN3BmUGUwL2d5Tmg1a0ZaZ3BzQXJQCncvbDdHK0ltTkc5dE1UMkdLZDZvYXY4WWhzeFJFVENXY0VBbzRIMFoyZjdXMmEzaUE4ejg0RXYzK0FRcApMQ0hRNlhmOXNBZVpwMS96MWRCdEdRcXVrcWkwYlVjMmRnRjhQdHY3QVhHeDgvQzc4OVlsWUJ6d3RhMVEKeEtuYU0zU1Erdk9Tejl1ZHZqQ1FGQVp4MllZL3pSS0tRbDRrMnZrbXRaT29aeUp3ZXpoNC9JMEF6dXFqCm9mM0FlWmw5eWtVRFNKQ1dQb2RTZCs4UEk1UzVWQnAzUkprV3kvL1RzMy9NMmk0TWkvOUYvVTd4RWo2SQpBdzVTV2s3eUc5Y2R0UGVWbEdVN2VGTE51OU9oMG5SOFNIcWUzRmpvZWFDRG5MYnlVTkZldzVjc3h6R0MKeHBmeVJydEJNV1NYNkxSR2tLc0p1VVVaS1pQT2tkYWY0ajRJaDlQYWcwM1ZEZU5xMm12aG9pd1VtVDNMCnFmNkQrcDNoM3FHTzZnMmpLUmhjOFFuaGxabTd6QTl4Yk10ck5acXFRdUJNOGpmaXZPWk1oTUVDc1RGSgpSNFpOVXBRdnY5ditjY3laRXZyMjZMbUJxa3RiNXV4bXVxMzM4Q3gvTG9odW1NaGFVK0RseVE3cFY5VkoKSGJteHIvVWtvRk1EbFdNTjN1YjJ2ZUlwRWdYYWtEY2tqVmIrbEpFNitJejVFSmRPOUxKaGVZSUg4ajBCCg==.4e14c67ac1c1418695766a18eaa1ace5\"></div><h1 id=\"isPasted\">Scenario</h1><p>Suppose you need to delete a table that is partitioned by <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">year</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">month</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">region</span>, and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">service</span>. However, the table is huge, and there will be around 1000 <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">part</span> files per partition. You can list all the files in each partition and then delete them using an Apache Spark job.</p><p>For example, suppose you have a table that is partitioned by a, b, and c:</p><pre>%scala\r\n\r\nSeq((1,2,3,4,5),\r\n\u00a0 (2,3,4,5,6),\r\n\u00a0 (3,4,5,6,7),\r\n\u00a0 (4,5,6,7,8))\r\n\u00a0 .toDF(\"a\", \"b\", \"c\", \"d\", \"e\")\r\n\u00a0 .write.mode(\"overwrite\")\r\n\u00a0 .partitionBy(\"a\", \"b\", \"c\")\r\n\u00a0 .parquet(\"/mnt/path/table\")</pre><h1>List files</h1><p>You can list all the part files using this function:</p><pre>%scala\r\n\r\nimport org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs.{Path, FileSystem}\r\nimport org.apache.spark.deploy.SparkHadoopUtil\r\nimport org.apache.spark.sql.execution.datasources.InMemoryFileIndex\r\nimport java.net.URI\r\n\r\ndef listFiles(basep: String, globp: String): Seq[String] = {\r\n\u00a0 val conf = new Configuration(sc.hadoopConfiguration)\r\n\u00a0 val fs = FileSystem.get(new URI(basep), conf)\r\n\r\n\u00a0 def validated(path: String): Path = {\r\n\u00a0 \u00a0 if(path startsWith \"/\") new Path(path)\r\n\u00a0 \u00a0 else new Path(\"/\" + path)\r\n\u00a0 }\r\n\r\n\u00a0 val fileCatalog = InMemoryFileIndex.bulkListLeafFiles(\r\n\u00a0 \u00a0 paths = SparkHadoopUtil.get.globPath(fs, Path.mergePaths(validated(basep), validated(globp))),\r\n\u00a0 \u00a0 hadoopConf = conf,\r\n\u00a0 \u00a0 filter = null,\r\n\u00a0 \u00a0 sparkSession = spark, areRootPaths=true)\r\n\r\n\u00a0// If you are using Databricks Runtime 6.x and below,\r\n\u00a0// remove &lt;areRootPaths=true&gt; from the bulkListLeafFiles function parameter.\r\n\r\n\u00a0 fileCatalog.flatMap(_._2.map(_.path))\r\n}\r\n\r\nval root = \"/mnt/path/table\"\r\nval globp = \"[^_]*\" // glob pattern, e.g. \"service=webapp/date=2019-03-31/*log4j*\"\r\n\r\nval files = listFiles(root, globp)\r\nfiles.toDF(\"path\").show()</pre><pre>+------------------------------------------------------------------------------------------------------------------------------+\r\n|path \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|\r\n+------------------------------------------------------------------------------------------------------------------------------+\r\n|dbfs:/mnt/path/table/a=1/b=2/c=3/part-00000-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-5.c000.snappy.parquet|\r\n|dbfs:/mnt/path/table/a=2/b=3/c=4/part-00001-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-6.c000.snappy.parquet|\r\n|dbfs:/mnt/path/table/a=3/b=4/c=5/part-00002-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-7.c000.snappy.parquet|\r\n|dbfs:/mnt/path/table/a=4/b=5/c=6/part-00003-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-8.c000.snappy.parquet|\r\n+------------------------------------------------------------------------------------------------------------------------------+</pre><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">listFiles</span> function takes a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">base</span> path and a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">glob</span> path as arguments, scans the files and matches with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">glob</span> pattern, and then returns all the leaf files that were matched as a sequence of strings.</p><p>The function also uses the utility function <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">globPath</span> from the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SparkHadoopUtil</span> package. This function lists all the paths in a directory with the specified prefix, and does not further list leaf children (files). The list of paths is passed into <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">InMemoryFileIndex.bulkListLeafFiles</span> method, which is a Spark internal API for distributed file listing.</p><p>Neither of these listing utility functions work well alone. By combining them you can get a list of top-level directories that you want to list using globPath function, which will run on the driver, and you can distribute the listing for all child leaves of the top-level directories into Spark workers using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">bulkListLeafFiles</span>.</p><p>The speed-up can be around 20-50x faster according to Amdahl\u2019s law. The reason is that, you can easily control the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">glob</span> path according to the real file physical layout and control the parallelism through <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.sources.parallelPartitionDiscovery.parallelism</span> for <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">InMemoryFileIndex</span>.</p><h1>Delete files</h1><p>When you delete files or partitions from an unmanaged table, you can use the Databricks utility function <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.fs.rm</span>. This function leverages the native cloud storage file system API, which is optimized for all file operations. However, you can\u2019t delete a gigantic table directly using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.fs.rm(\"path/to/the/table\")</span>.</p><p>You can list files efficiently using the script above. For smaller tables, the collected paths of the files to delete fit into the driver memory, so you can use a Spark job to distribute the file deletion task.</p><p>For gigantic tables, even for a single top-level partition, the string representations of the file paths cannot fit into the driver memory. The easiest way to solve this problem is to collect the paths of the inner partitions recursively, list the paths, and delete them in parallel.</p><pre>%scala\r\n\r\nimport scala.util.{Try, Success, Failure}\r\n\r\ndef delete(p: String): Unit = {\r\n\u00a0 dbutils.fs.ls(p).map(_.path).toDF.foreach { file =&gt;\r\n\u00a0 \u00a0 dbutils.fs.rm(file(0).toString, true)\r\n\u00a0 \u00a0 println(s\"deleted file: $file\")\r\n\u00a0 }\r\n}\r\n\r\nfinal def walkDelete(root: String)(level: Int): Unit = {\r\n\u00a0 dbutils.fs.ls(root).map(_.path).foreach { p =&gt;\r\n\u00a0 \u00a0 println(s\"Deleting: $p, on level: ${level}\")\r\n\u00a0 \u00a0 val deleting = Try {\r\n\u00a0 \u00a0 \u00a0 if(level == 0) delete(p)\r\n\u00a0 \u00a0 \u00a0 else if(p endsWith \"/\") walkDelete(p)(level-1)\r\n\u00a0 \u00a0 \u00a0 //\r\n\u00a0 \u00a0 \u00a0 // Set only n levels of recursion, so it won't be a problem\r\n\u00a0 \u00a0 \u00a0 //\r\n\u00a0 \u00a0 \u00a0 else delete(p)\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 deleting match {\r\n\u00a0 \u00a0 \u00a0 case Success(v) =&gt; {\r\n\u00a0 \u00a0 \u00a0 \u00a0 println(s\"Successfully deleted $p\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 dbutils.fs.rm(p, true)\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 case Failure(e) =&gt; println(e.getMessage)\r\n\u00a0 \u00a0 }\r\n\u00a0 }\r\n}</pre><p>The code deletes inner partitions while ensuring that the partition that is being deleted is small enough. It does this by searching through the partitions recursively by each level, and only starts deleting when it hits the level you set. For instance, if you want to start with deleting the top-level partitions, use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">walkDelete(root)(0)</span>. Spark will delete all the files under <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbfs:/mnt/path/table/a=1/</span>, then delete <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.../a=2/</span>, following the pattern until it is exhausted.</p><p>The Spark job distributes the deletion task using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">delete</span> function shown above, listing the files with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.fs.ls</span> with the assumption that the number of child partitions at this level is small. You can also be more efficient by replacing the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">dbutils.fs.ls</span> function with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">listFiles</span> function shown above, with only slight modification.</p><h1>Summary</h1><p>These two approaches highlight methods for listing and deleting gigantic tables. They use some Spark utility functions and functions specific to the Databricks environment. Even if you cannot use them directly, you can create your own utility functions to solve the problem in an analogous way.</p>", "body_txt": "Scenario Suppose you need to delete a table that is partitioned by year, month, date, region, and service. However, the table is huge, and there will be around 1000 part files per partition. You can list all the files in each partition and then delete them using an Apache Spark job. For example, suppose you have a table that is partitioned by a, b, and c: %scala Seq((1,2,3,4,5), \u00a0 (2,3,4,5,6), \u00a0 (3,4,5,6,7), \u00a0 (4,5,6,7,8)) \u00a0 .toDF(\"a\", \"b\", \"c\", \"d\", \"e\") \u00a0 .write.mode(\"overwrite\") \u00a0 .partitionBy(\"a\", \"b\", \"c\") \u00a0 .parquet(\"/mnt/path/table\") List files You can list all the part files using this function: %scala import org.apache.hadoop.conf.Configuration import org.apache.hadoop.fs.{Path, FileSystem} import org.apache.spark.deploy.SparkHadoopUtil import org.apache.spark.sql.execution.datasources.InMemoryFileIndex import java.net.URI def listFiles(basep: String, globp: String): Seq[String] = { \u00a0 val conf = new Configuration(sc.hadoopConfiguration) \u00a0 val fs = FileSystem.get(new URI(basep), conf) \u00a0 def validated(path: String): Path = { \u00a0 \u00a0 if(path startsWith \"/\") new Path(path) \u00a0 \u00a0 else new Path(\"/\" + path) \u00a0 } \u00a0 val fileCatalog = InMemoryFileIndex.bulkListLeafFiles( \u00a0 \u00a0 paths = SparkHadoopUtil.get.globPath(fs, Path.mergePaths(validated(basep), validated(globp))), \u00a0 \u00a0 hadoopConf = conf, \u00a0 \u00a0 filter = null, \u00a0 \u00a0 sparkSession = spark, areRootPaths=true) \u00a0// If you are using Databricks Runtime 6.x and below, \u00a0// remove &lt;areRootPaths=true&gt; from the bulkListLeafFiles function parameter. \u00a0 fileCatalog.flatMap(_._2.map(_.path)) } val root = \"/mnt/path/table\" val globp = \"[^_]*\" // glob pattern, e.g. \"service=webapp/date=2019-03-31/*log4j*\" val files = listFiles(root, globp) files.toDF(\"path\").show() +------------------------------------------------------------------------------------------------------------------------------+ |path \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| +------------------------------------------------------------------------------------------------------------------------------+ |dbfs:/mnt/path/table/a=1/b=2/c=3/part-00000-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-5.c000.snappy.parquet| |dbfs:/mnt/path/table/a=2/b=3/c=4/part-00001-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-6.c000.snappy.parquet| |dbfs:/mnt/path/table/a=3/b=4/c=5/part-00002-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-7.c000.snappy.parquet| |dbfs:/mnt/path/table/a=4/b=5/c=6/part-00003-tid-5046671251912249212-afa32967-b7db-444e-b895-d12d68c05500-8.c000.snappy.parquet| +------------------------------------------------------------------------------------------------------------------------------+ The listFiles function takes a base path and a glob path as arguments, scans the files and matches with the glob pattern, and then returns all the leaf files that were matched as a sequence of strings. The function also uses the utility function globPath from the SparkHadoopUtil package. This function lists all the paths in a directory with the specified prefix, and does not further list leaf children (files). The list of paths is passed into InMemoryFileIndex.bulkListLeafFiles method, which is a Spark internal API for distributed file listing. Neither of these listing utility functions work well alone. By combining them you can get a list of top-level directories that you want to list using globPath function, which will run on the driver, and you can distribute the listing for all child leaves of the top-level directories into Spark workers using bulkListLeafFiles. The speed-up can be around 20-50x faster according to Amdahl\u2019s law. The reason is that, you can easily control the glob path according to the real file physical layout and control the parallelism through spark.sql.sources.parallelPartitionDiscovery.parallelism for InMemoryFileIndex. Delete files When you delete files or partitions from an unmanaged table, you can use the Databricks utility function dbutils.fs.rm. This function leverages the native cloud storage file system API, which is optimized for all file operations. However, you can\u2019t delete a gigantic table directly using dbutils.fs.rm(\"path/to/the/table\"). You can list files efficiently using the script above. For smaller tables, the collected paths of the files to delete fit into the driver memory, so you can use a Spark job to distribute the file deletion task. For gigantic tables, even for a single top-level partition, the string representations of the file paths cannot fit into the driver memory. The easiest way to solve this problem is to collect the paths of the inner partitions recursively, list the paths, and delete them in parallel. %scala import scala.util.{Try, Success, Failure} def delete(p: String): Unit = { \u00a0 dbutils.fs.ls(p).map(_.path).toDF.foreach { file =&gt; \u00a0 \u00a0 dbutils.fs.rm(file(0).toString, true) \u00a0 \u00a0 println(s\"deleted file: $file\") \u00a0 } } final def walkDelete(root: String)(level: Int): Unit = { \u00a0 dbutils.fs.ls(root).map(_.path).foreach { p =&gt; \u00a0 \u00a0 println(s\"Deleting: $p, on level: ${level}\") \u00a0 \u00a0 val deleting = Try { \u00a0 \u00a0 \u00a0 if(level == 0) delete(p) \u00a0 \u00a0 \u00a0 else if(p endsWith \"/\") walkDelete(p)(level-1) \u00a0 \u00a0 \u00a0 // \u00a0 \u00a0 \u00a0 // Set only n levels of recursion, so it won't be a problem \u00a0 \u00a0 \u00a0 // \u00a0 \u00a0 \u00a0 else delete(p) \u00a0 \u00a0 } \u00a0 \u00a0 deleting match { \u00a0 \u00a0 \u00a0 case Success(v) =&gt; { \u00a0 \u00a0 \u00a0 \u00a0 println(s\"Successfully deleted $p\") \u00a0 \u00a0 \u00a0 \u00a0 dbutils.fs.rm(p, true) \u00a0 \u00a0 \u00a0 } \u00a0 \u00a0 \u00a0 case Failure(e) =&gt; println(e.getMessage) \u00a0 \u00a0 } \u00a0 } } The code deletes inner partitions while ensuring that the partition that is being deleted is small enough. It does this by searching through the partitions recursively by each level, and only starts deleting when it hits the level you set. For instance, if you want to start with deleting the top-level partitions, use walkDelete(root)(0). Spark will delete all the files under dbfs:/mnt/path/table/a=1/, then delete .../a=2/, following the pattern until it is exhausted. The Spark job distributes the deletion task using the delete function shown above, listing the files with dbutils.fs.ls with the assumption that the number of child partitions at this level is small. You can also be more efficient by replacing the dbutils.fs.ls function with the listFiles function shown above, with only slight modification. Summary These two approaches highlight methods for listing and deleting gigantic tables. They use some Spark utility functions and functions specific to the Databricks environment. Even if you cannot use them directly, you can create your own utility functions to solve the problem in an analogous way.", "format": "html", "updated_at": "2022-05-31T18:49:23.909Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2708318, "name": "aws"}, {"id": 2708320, "name": "azure"}, {"id": 2708321, "name": "gcp"}], "url": "https://kb.databricks.com/data/list-delete-files-faster"}, {"id": 1388987, "name": "Revoke all user privileges", "views": 9234, "accessibility": 1, "description": "Use a regex and a series of for loops to revoke all privileges for a single user.", "codename": "revoke-all-user-privileges", "created_at": "2022-05-31T06:20:15.787Z", "updated_at": "2022-05-31T06:33:57.632Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStHT0I4REROUHN0RFg5WDVWRVBnVm1zY1IyQ0NpN1hvdWZyTitZN1dXelFqSWZPalhFCkQ3SlNWdTIvcXVXanNyY05mejBBMGZiVFJreW1TVUs0d2VydDNUWTQzNnBta2crc2lPOTBwMi9Bb0dURwpxcW9rV2pueGl5M1FOdTZwYkRBc3hrRjl3Q3poV04wNjVPTnMzYlJFeFAzNmtORTFSNk9PZ1VkUVlMbUIKMVV2R0Jqa2toQzF3RytMM21IVDJGeHBVOGlmdG5iS2VNaSt5NEZucEtIbXpvdU1kRGxTRkxxNW4rL2VyCjMvKzUxdHpNaXJOWkdqbEZKVGI2MUZTQTZsbUN5NDRRcUVKWlNHakpxTjBKSnJkdTJZdjVTNDNweHpNegpQcUhpOUxhTHZLNFErVzJXQkpBQlNWcFQwbEYxaFFRdnZKZWpNQlczRUw4WFJBaithT3hINmFmTnFrZHkKLzhaOHVXdkN0RUlIN0NJczFXd0FlNHRPR2hCc05sOWdIbUVjck1VMEllTWxEelJ3ci94YmgrM212YThrCkdGZzJHRkMvT1lTVnFadHcrUzNYYTBIZ0lJdm4yOWhrV3ZHdElLcXJKQ3VJWU8yM3BLcXJVa0wvdndDZwp3a0JQTlJNVC9MWTMxQWJwWUViZ3FPb3BFVDBwaGFXVit4KzAxS2lGdVlOM3U5YUZieFdINjUrdXpQVWcKOXlzRUxkQXBMNSswZTlabEEwMi9ZTEhSaHRBLzQ0Nlk5dmZ6UDQweWJpcU0wSlNTeG9HTFdFUVZ1WkZuCnNuQkQvVjBKV1ZuN3ZOMU1qUzhRV3hscitsTkphaUFFWGM2eDRIMjBJQTJSSEdQVGtUWmlrSEZ2aW9XMApSK1Q5V0wyMCtFc1R3eVdOYnUxTFhyV1VhT0lBY05aUzlkaDNuMmZ3RlBpOWcrN3VJL0RsK1R6bW5DU2MKNlh2Y2hSN00vR2RVZWxqMW8rWHV1SjVmUDJDcTVkQkkxVnhYTjJkV29CUVk0am1PS0E5VlpHZUhKUGZTClZQZWk5S2ZIOWs2NjFrZlBsT1l5amF2NUM3clJFUHNxNExQemFHZWJmZWY0ZndQTFRMaStGZy9IOE5tcQpXRVVsYUVQNzk3Q3pyYTZZRWR4azFPSnV4S0piWEpuNmd3RjMvZ1daVGdGL2tScU52MFR6d3dUNQo=.19b62b777a451fc651d3129dfccac99b\"></div><p id=\"isPasted\">When user permissions are explicitly granted for individual tables and views, the selected user can access those tables and views even if they don\u2019t have permission to access the underlying database.</p><p>If you want to revoke a user\u2019s access, you can do so with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">REVOKE</span> command. However, the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">REVOKE</span> command is explicit, and is strictly scoped to the object specified in the command.</p><p>For example:</p><pre>%sql\r\n\r\nREVOKE ALL PRIVILEGES ON DATABASE &lt;database-name&gt; FROM `&lt;user&gt;@&lt;domain-name&gt;`\r\nREVOKE SELECT ON &lt;table-name&gt; FROM `&lt;user&gt;@&lt;domain-name&gt;`</pre><p>If you want to revoke all privileges for a single user you can do it with a series of multiple commands, or you can use a regular expression and a series of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">for</span> loops to automate the process.</p><h1>Example code</h1><p>This example code matches the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;search-string&gt;</span> pattern to the database name and the table name and then revokes the user\u2019s privileges. The search is recursive.</p><pre>%python\r\n\r\nfrom re import search\r\ndatabaseQuery = sqlContext.sql(\"show databases\")\r\ndatabaseList = databaseQuery.collect()\r\n# This loop revokes at the database level.\r\nfor db in databaseList:\r\n\u00a0 listTables = sqlContext.sql(\"show tables from \"+db['databaseName'])\r\n\u00a0 tableRows = listTables.collect()\r\n\u00a0 if search(&lt;search-string&gt;, db['databaseName']):\r\n\u00a0 \u00a0 revokeDatabase=sqlContext.sql(\"REVOKE ALL PRIVILAGES ON DATABASE \"+db['databaseName']+\" to `&lt;username&gt;`\")\r\n\u00a0 \u00a0 display(revokeDatabase)\r\n\u00a0 \u00a0 print(\"Ran the REVOKE query on \"+db['databaseName']+\" for &lt;username&gt;\")\r\n\u00a0 # This loop revokes at the table level.\r\n\u00a0 for table in tableRows:\r\n\u00a0 \u00a0 if search(&lt;search-string&gt;,table['tableName']):\r\n\u00a0 \u00a0 \u00a0 revokeCommand=sqlContext.sql(\"REVOKE SELECT ON \"+table['database']+\".\"+table['tableName']+\" FROM `&lt;username&gt;`\")\r\n\u00a0 \u00a0 \u00a0 display(revokeCommand)\r\n\u00a0 \u00a0 \u00a0 print(\"Revoked the SELECT permissions on \"+table['database']+\".\"+table['tableName']+\" for &lt;username&gt;\")</pre><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">These commands only work if you have enabled table access control for the cluster (<a href=\"https://docs.databricks.com/security/access-control/table-acls/table-acl.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"enabled table access control for the cluster\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/security/access-control/table-acls/table-acl\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"enabled table access control for the cluster\">Azure</a> |\u00a0<a href=\"https://docs.gcp.databricks.com/security/access-control/table-acls/table-acl.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"enabled table access control for the cluster\">GCP</a>).</p>\n</div>\n</div><p><br></p>", "body_txt": "When user permissions are explicitly granted for individual tables and views, the selected user can access those tables and views even if they don\u2019t have permission to access the underlying database. If you want to revoke a user\u2019s access, you can do so with the REVOKE command. However, the REVOKE command is explicit, and is strictly scoped to the object specified in the command. For example: %sql REVOKE ALL PRIVILEGES ON DATABASE &lt;database-name&gt; FROM `&lt;user&gt;@&lt;domain-name&gt;` REVOKE SELECT ON &lt;table-name&gt; FROM `&lt;user&gt;@&lt;domain-name&gt;` If you want to revoke all privileges for a single user you can do it with a series of multiple commands, or you can use a regular expression and a series of for loops to automate the process. Example code This example code matches the &lt;search-string&gt; pattern to the database name and the table name and then revokes the user\u2019s privileges. The search is recursive. %python from re import search databaseQuery = sqlContext.sql(\"show databases\") databaseList = databaseQuery.collect() # This loop revokes at the database level. for db in databaseList: \u00a0 listTables = sqlContext.sql(\"show tables from \"+db['databaseName']) \u00a0 tableRows = listTables.collect() \u00a0 if search(&lt;search-string&gt;, db['databaseName']): \u00a0 \u00a0 revokeDatabase=sqlContext.sql(\"REVOKE ALL PRIVILAGES ON DATABASE \"+db['databaseName']+\" to `&lt;username&gt;`\") \u00a0 \u00a0 display(revokeDatabase) \u00a0 \u00a0 print(\"Ran the REVOKE query on \"+db['databaseName']+\" for &lt;username&gt;\") \u00a0 # This loop revokes at the table level. \u00a0 for table in tableRows: \u00a0 \u00a0 if search(&lt;search-string&gt;,table['tableName']): \u00a0 \u00a0 \u00a0 revokeCommand=sqlContext.sql(\"REVOKE SELECT ON \"+table['database']+\".\"+table['tableName']+\" FROM `&lt;username&gt;`\") \u00a0 \u00a0 \u00a0 display(revokeCommand) \u00a0 \u00a0 \u00a0 print(\"Revoked the SELECT permissions on \"+table['database']+\".\"+table['tableName']+\" for &lt;username&gt;\") Info\nThese commands only work if you have enabled table access control for the cluster (AWS | Azure |\u00a0GCP).", "format": "html", "updated_at": "2022-05-31T06:33:57.629Z"}, "author": {"id": 789491, "email": "pavan.kumarchalamcharla@databricks.com", "name": "pavan.kumarchalamcharla ", "first_name": "pavan.kumarchalamcharla", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:11:25.774Z", "updated_at": "2023-03-28T06:33:31.123Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2692327, "name": "aws"}, {"id": 2692328, "name": "azure"}, {"id": 2692329, "name": "gcp"}], "url": "https://kb.databricks.com/data/revoke-all-user-privileges"}, {"id": 1388980, "name": "Prevent duplicated columns when joining two DataFrames", "views": 41625, "accessibility": 1, "description": "Learn how to prevent duplicated columns when joining two DataFrames in Databricks.", "codename": "join-two-dataframes-duplicated-columns", "created_at": "2022-05-31T06:09:36.415Z", "updated_at": "2022-10-13T05:24:02.629Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTg0M0kvWmUzb2xuZllxbFJnRER5YWNtMGF6VWdLTUNZcldvekp0TDlJbFloYUhHa3F0CnVOY0YyajZDcXc4ME5NRHl6OXhXbTRORFI1OUxpK2RacjhIanNsamkvYzVWVzU3SHlEWityTUNUTFhDRApZTnQyWXJ5VzQwUkRlZnlXSWxvbEs1UGxGei9QV2c3M25uQkpIakVGdzBBL0dCNHhHKzFaWWhldHdhVFIKRkFUbGRPRUZGY29BM3ZZSUU5S0JHQ0JRamUvTE1ncFZnNk5PK3ZpcFVmQ1dWalI0Y2hmMDBXMnBQZ0UwCkRyM0wvS29jYmZqYzI2K3pIM09zTkpDSG9lOW02bFhsR3hTRlRQek42L3htYmdxZWxLZk1zMk5EeDZ3YQo4aG1HNFhDYjlHM002a0NLNVZPd0U3L0w5MWFxVUFsZkhtdVBwVWF2azlLL2pYSFlmbmFFRk9paTF2aVUKVVE4V0pFYXVSS3VobTJsbTAyTkFvdExnVlpaUFZlZ2U4cHFYMkFudTFKbXdrMHlpblFSRkJZUXRSUEc4CmppR3YrbjRBakZ6OEZIN25QeXVYUE1UUytramVWVGVHTmk3ODQzRHpDYkM3akVwRzB5MTBZdHZ4T2hKdQphZit5cE1XVCtuOVJ3ck1OanF2bGVDcDMxWHYwNEVNeWo5SHovV0VVa0E5ZUxzYVZRY0MvajNOc3RZQnYKTTk3Qi9xMnlrSXU3aWRHdkZkNXBLSFVzdFRKcnZJV1p6YThmeXNJais0NzgvQm5QdVYvVVJ3MXZ4MHlUCnNmZFNxdzZWckZra0VQRDhkdWhGaVo1LzhTWFZ1cXNNZ09WVzdQNG44S1J5bmhadnMyRGJDclREazZhTQpBay9sWmIzK09ldzNzeGtiK2JGNkYzZHdxdU9RTkFkRm9GWlJUeHNOWGpicUgrN09qM3UxQUFPK1VobnEKbzVrMXExTzVEVGtwR3hSYm1IeExIdFZtRStRbEo5RHQ3eDhSb0t2VUh3b0k1c2lhOXJQU2owY253a1hkCmVZM2YrVFJYL3FPNHJzWnNVVEhKUjRhR2EvMWJLS1NXbkVGZ2hqaDRjc0JiTnYrUWFuN24zTVc5Z0dLYwprdVlYN0RqMnYrS3pSMXhJdVh5MDlqOUxYRSswZlRGVDdtU3JDOS8zTE5LTnZ6ZGUyUDF1UnZPcVVYUHMKN1hNVjRtajZETTVuek8rbEoydFdXM3hnUitOb2VzdFZoT2FodE5OQ1V6UVEvelZBOXBzUGlwOVBxY0RHCg==.c05e0705768091ab68c6bf92834d41c3\"></div><p id=\"isPasted\">If you perform a join in Spark and don\u2019t specify your join correctly you\u2019ll end up with duplicate column names. This makes it harder to select those columns. This article and notebook demonstrate how to perform a join so that you don\u2019t have duplicated columns.</p><h1>Join on columns</h1><p>If you join on columns, you get duplicated columns.</p><p><strong>Scala</strong></p><pre>%scala\r\n\r\nval llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10))\r\nval left = llist.toDF(\"name\",\"date\",\"duration\")\r\nval right = Seq((\"alice\", 100),(\"bob\", 23)).toDF(\"name\",\"upload\")\r\n\r\nval df = left.join(right, left.col(\"name\") === right.col(\"name\"))</pre><p><strong>Python</strong></p><pre>%python\r\n\r\nllist = [('bob', '2015-01-13', 4), ('alice', '2015-04-23',10)]\r\nleft = spark.createDataFrame(llist, ['name','date','duration'])\r\nright = spark.createDataFrame([('alice', 100),('bob', 23)],['name','upload'])\r\n\r\ndf = left.join(right, left.name == right.name)</pre><h1>Solution</h1><p>Specify the join column as an array type or string.</p><h2>Scala</h2><pre>%scala\r\n\r\nval df = left.join(right, Seq(\"name\"))</pre><pre>%scala\r\n\r\nval df = left.join(right, \"name\")</pre><h2>Python</h2><pre>%python\r\n\r\ndf = left.join(right, [\"name\"])</pre><pre>%python\r\n\r\ndf = left.join(right, \"name\")</pre><h2>R</h2><p>First register the DataFrames as tables.</p><pre>%python\r\n\r\nleft.createOrReplaceTempView(\"left_test_table\")\r\nright.createOrReplaceTempView(\"right_test_table\")</pre><pre>%r\r\n\r\nlibrary(SparkR)\r\nsparkR.session()\r\nleft &lt;- sql(\"SELECT * FROM left_test_table\")\r\nright &lt;- sql(\"SELECT * FROM right_test_table\")</pre><p>The above code results in duplicate columns. The following code does not.</p><pre>%r\r\n\r\nhead(drop(join(left, right, left$name == right$name), left$name))</pre><h1>Join DataFrames with duplicated columns notebook</h1><p>Review the <a href=\"https://docs.databricks.com/_static/notebooks/kb/data/join-two-dataframes-duplicated-column-notebook.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Join DataFrames with duplicated columns example notebook</a>.</p>", "body_txt": "If you perform a join in Spark and don\u2019t specify your join correctly you\u2019ll end up with duplicate column names. This makes it harder to select those columns. This article and notebook demonstrate how to perform a join so that you don\u2019t have duplicated columns. Join on columns If you join on columns, you get duplicated columns. Scala %scala val llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10)) val left = llist.toDF(\"name\",\"date\",\"duration\") val right = Seq((\"alice\", 100),(\"bob\", 23)).toDF(\"name\",\"upload\") val df = left.join(right, left.col(\"name\") === right.col(\"name\")) Python %python llist = [('bob', '2015-01-13', 4), ('alice', '2015-04-23',10)] left = spark.createDataFrame(llist, ['name','date','duration']) right = spark.createDataFrame([('alice', 100),('bob', 23)],['name','upload']) df = left.join(right, left.name == right.name) Solution Specify the join column as an array type or string. Scala %scala val df = left.join(right, Seq(\"name\")) %scala val df = left.join(right, \"name\") Python %python df = left.join(right, [\"name\"]) %python df = left.join(right, \"name\") R First register the DataFrames as tables. %python left.createOrReplaceTempView(\"left_test_table\") right.createOrReplaceTempView(\"right_test_table\") %r library(SparkR) sparkR.session() left &lt;- sql(\"SELECT * FROM left_test_table\") right &lt;- sql(\"SELECT * FROM right_test_table\") The above code results in duplicate columns. The following code does not. %r head(drop(join(left, right, left$name == right$name), left$name)) Join DataFrames with duplicated columns notebook Review the Join DataFrames with duplicated columns example notebook.", "format": "html", "updated_at": "2022-10-13T05:24:02.625Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2692324, "name": "aws"}, {"id": 2692325, "name": "azure"}, {"id": 2692326, "name": "gcp"}], "url": "https://kb.databricks.com/data/join-two-dataframes-duplicated-columns"}, {"id": 1388968, "name": "Hive UDFs", "views": 9005, "accessibility": 1, "description": "Learn how to create and use a Hive UDF for Databricks.", "codename": "hive-udf", "created_at": "2022-05-31T05:33:45.192Z", "updated_at": "2022-05-31T06:09:23.779Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9vYWlRcGNGUThIYjhRK09MaW11blVBanMrVVZXcldWR1lYZ21NamptZUVXVURRdnFoCko0Y3RaQktOQS93OEpQQnhjRnFXTjJhamtFZ2p6KzRYOWxzYjhLMjExWFZXa2FUVXlVQ2F4bStYdUZOagpEMkg5eVc4QlhiWXQ3N21YS0x2dkRxajIwV3VXZFBQeS9sWVozVmNoYlBTTW9vb0ZPVi9SUXZSYnhFMGQKSXh3TGxaV3hnQUFFNmZnZ2FlRCtjTStpYTNwcHNYRTJ6WThXc1JPYmN3Mis0VkpBdVJQTFhsTHAyTnZCCmZmRWtqSzdNeGE2bzZkU29CY2JVNnlxQmxzRGc0L1pTSkp1TkN5emhBQXN5UDkxWm56M1V6ODI0MlF2awplMW5RcnBDQU8veXA0dkV4NFNLVkN3TVFyS1BPZjVjTG5EQmd1bjZXS0NmMFhEOGlDNHloMEFXMlRkMmYKczcvcmxQWTZ4d213WC9hZTJITWIwOFE3OUp4QUgweU1VR2dBZ2hCOFh4ZjBOam83L3VkYzI3RFVqRUdUCmtHN0FERnhmVnR0eGp1MmJJSXB5VkJHczV1bThMeHZlZlhVM1E5cWtGVWY4d05xN1hQOHB2SjA4Nm9zcQpDTXk1LzI0TXhXTm4rbHd4VE5FQTgvV1RaeUhva2txZ0szY1dMb2k3dURBY1g1aVg0MUZVWVZERFJzNUcKN2dmUlVVQ0tEUFNpZVlONmh4RThGRHY4Y3h5ZFA5VG9rdGlKaFMwV1BXaWxsU0JlL05Jekd6V1c4aTJICm11ckFCaVFoUXAweW9ha1ZKYWxUZmc0R1Q0YWR6R0gvYjJjbzA0UE9UbVB2UDlCbnpZT1ZmMEkrNW84TQpmZ1dqWnA5NHMxT2VMTUU2eG5OYzZxVE5rWE5BcVpZRTRqNnZjcHRzVzE3cHhZR1ZVMTNHcElYcU9SYkkKOVUvUGRuZW9FQlhwQkNIUkFIOTBBOUJaNUNpWWkzUU9qckp1OTVyNXZkQzdiOHovbUtyZTdDTStOY2pkCi84MHdTbURZL3JhWlpZaXFraW1ITm1VeE5xVFgxb3BBSzc3NXkrVHQvdEcxQ2YrRDRFR0hBK0tUTUtPOQpXamhNMkluNHpQc3RoQjlYeG5mTnZYZ2NQV0ttREdsN1ZGU0YwZFp6VXM5WUNQaXh1aDJDNm5rSERoRzcKZTl3VFNxRm81NUNoRkYyblRBZ2ZSeWU3bndrSEM2MkJHM3cvUVVPME9NWVJPQTc3RUMyVUkvUG8rTUtoCg==.5da76b48f93e178e7056762b68e1573b\"></div><p id=\"isPasted\">This article shows how to create a Hive UDF, register it in Spark, and use it in a Spark SQL query.</p><p>Here is a Hive UDF that takes a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">long</span> as an argument and returns its hexadecimal representation.</p><pre>%scala\r\n\r\nimport org.apache.hadoop.hive.ql.exec.UDF\r\nimport org.apache.hadoop.io.LongWritable\r\n\r\n// This UDF takes a long integer and converts it to a hexadecimal string.\r\n\r\nclass ToHex extends UDF {\r\n\u00a0 def evaluate(n: LongWritable): String = {\r\n\u00a0 \u00a0 Option(n)\r\n\u00a0 \u00a0 .map { num =&gt;\r\n\u00a0 \u00a0 \u00a0 \u00a0 // Use Scala string interpolation. It's the easiest way, and it's\r\n\u00a0 \u00a0 \u00a0 \u00a0 // type-safe, unlike String.format().\r\n\u00a0 \u00a0 \u00a0 \u00a0 f\"0x${num.get}%x\"\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 .getOrElse(\"\")\r\n\u00a0 }\r\n}</pre><p>Register the function:</p><pre>%scala\r\n\r\nspark.sql(\"CREATE TEMPORARY FUNCTION to_hex AS 'com.ardentex.spark.hiveudf.ToHex'\")</pre><p>Use your function as any other registered function:</p><pre>%scala\r\n\r\nspark.sql(\"SELECT first_name, to_hex(code) as hex_code FROM people\")</pre><p>You can find more examples and compilable code at the <a href=\"https://github.com/bmc/spark-hive-udf\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Sample Hive UDF project</a>.</p>", "body_txt": "This article shows how to create a Hive UDF, register it in Spark, and use it in a Spark SQL query. Here is a Hive UDF that takes a long as an argument and returns its hexadecimal representation. %scala import org.apache.hadoop.hive.ql.exec.UDF import org.apache.hadoop.io.LongWritable // This UDF takes a long integer and converts it to a hexadecimal string. class ToHex extends UDF { \u00a0 def evaluate(n: LongWritable): String = { \u00a0 \u00a0 Option(n) \u00a0 \u00a0 .map { num =&gt; \u00a0 \u00a0 \u00a0 \u00a0 // Use Scala string interpolation. It's the easiest way, and it's \u00a0 \u00a0 \u00a0 \u00a0 // type-safe, unlike String.format(). \u00a0 \u00a0 \u00a0 \u00a0 f\"0x${num.get}%x\" \u00a0 \u00a0 } \u00a0 \u00a0 .getOrElse(\"\") \u00a0 } } Register the function: %scala spark.sql(\"CREATE TEMPORARY FUNCTION to_hex AS 'com.ardentex.spark.hiveudf.ToHex'\") Use your function as any other registered function: %scala spark.sql(\"SELECT first_name, to_hex(code) as hex_code FROM people\") You can find more examples and compilable code at the Sample Hive UDF project.", "format": "html", "updated_at": "2022-05-31T06:09:23.777Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2692317, "name": "aws"}, {"id": 2692318, "name": "azure"}, {"id": 2692319, "name": "gcp"}], "url": "https://kb.databricks.com/data/hive-udf"}, {"id": 1385765, "name": "Get and set Apache Spark configuration properties in a notebook", "views": 38182, "accessibility": 1, "description": "", "codename": "get-and-set-spark-config", "created_at": "2022-05-25T22:25:59.350Z", "updated_at": "2022-05-26T00:41:08.494Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStNK09YbFE2NEd3MWNtOWJob3I5Vm5tV0d0TFlzdUtaUko0UkVxK2ZZd05pWTFlVEkyCmQyZmNTZk96NnpYTWFoQUJjYkx3a1VRZnJCNGhodjZkVUM4KzFJd05WSkhGd29iTVNxYWtIQitEZE1CVApUWXdoTVZJZmZuZmc0NGprM0E2VVJkRmJBeEZDUk1hWTVzLzduNW0wNk1mdHpuWWdhS3h1RVZrQzlFRVoKVlV1ZVcrWjhJd3QxbHRNWXZOY2RDTmdPYWMrOUxxZHZhOXlTQi8yZlFHU1A0VkY4UW9zQUpMUXltZDJqCmtQejRObGU3TEhlanBmbDFaQWMyY3QyNEhOeFVsOVVwNjlsdFR0YjJ4OUZHTGlncGpRRWZtayttYmg0Two5L3pZTUw4d041OVkvclR2dmozMWFBWG9tRFBpS0FVL0JaWlNSNTFmSUltYSthcmxsU1AxZy9XRy9KbkUKa3prN1FINHoyUU1ZODIwZUpxYnBjVU90RlM2QUJVcHBxWkpPSDZMUjVrWXNrNndzMzFKaUU5RW9oaTVKClp6YnNwMmNTanhFRFVkckhyR05PN1g0TG5ZMnBTdGlGdmFpZUw1Tm8rK0lpc2RtVk5IYkw5b05jTVZDaApjcWZoRUg5Q3c5OG9jczdLWDNadXpOWUlHSVBnam1Ub0p3QkFCOWQzSFhDT2NkVHRLQzUxVmNLeVM1VE8KTzlnU1FaZzhNYm5ObDBoQXk5dEI0L3Z0eW5NbUx0T25qUTdZS3ZpNW9WZVFzN0x3WWtacllZY2d5ZXZNClI2b2VKQlY2YjIwMjF1OWEwa1RtTkN0aUQzVWovZjVzMXA0ck5lZHRxUWhtZFk5bGZyYk9XcXlwaVNKdAo4Ty9GRFBxL1NjUGRqT0JPS2IwZ3p4TFlIdjM4RGNLUWw2QkNyNFNZdDhHT09Nd2puY1BRc2hHYmMxdUsKcldaUFlVOWErdzRkck5WUnBkSEE0NjErbmhUc0liMDdtdllCVFdHNlNVMHVDeWJ6QkRyczVESjdrelZuCk44Qi9veXgyaXJlQlNpYlh3Z0FDU3BrVzVNeW11S0VQczVsN1ZrR0JEQjVRYmE5a2ZBeW42STlHNHlIRQpjY3FJZFBlWFlJU1RjTkJDVklzNzdmWE8xK3ROMVp6SGR6WkJlaVFoZklJZlpkWmQzZmh4VVZ2Sgo=.d29ea149911a1156962e6914e40452ba\"></div><p id=\"isPasted\">In most cases, you set the <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a><a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\"></a>) at the cluster level. However, there may be instances when you need to check (or set) the values of specific Spark configuration properties in a notebook.</p><p>This article shows you how to display the current value of a Spark configuration property in a notebook. It also shows you how to set a new value for a Spark configuration property in a notebook.</p><h1>Get Spark configuration properties</h1><p>To get the current value of a <strong>Spark config</strong> property, evaluate the property without including a value.</p><h2>Python</h2><pre>%python\r\n\r\nspark.conf.get(\"spark.&lt;name-of-property&gt;\")</pre><h2>R</h2><pre>%r\r\n\r\nlibrary(SparkR)\r\nsparkR.conf(\"spark.&lt;name-of-property&gt;\")</pre><h2>Scala</h2><pre>%scala\r\n\r\nspark.conf.get(\"spark.&lt;name-of-property&gt;\")</pre><h2>SQL</h2><pre>%sql\r\n\r\nGET spark.&lt;name-of-property&gt;;</pre><h1>Set Spark configuration properties</h1><p>To set the value of a Spark configuration property, evaluate the property and assign a value.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">You can only set Spark configuration properties that start with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql</span> prefix.</p>\n</div>\n</div><h2>Python</h2><pre>%python\r\n\r\nspark.conf.set(\"spark.sql.&lt;name-of-property&gt;\", &lt;value&gt;)</pre><h2>R</h2><pre>%r\r\n\r\nlibrary(SparkR)\r\nsparkR.session()\r\nsparkR.session(sparkConfig = list(spark.sql.&lt;name-of-property&gt; = \"&lt;value&gt;\"))</pre><h2>Scala</h2><pre>%scala\r\n\r\nspark.conf.set(\"spark.sql.&lt;name-of-property&gt;\", &lt;value&gt;)</pre><h2>SQL</h2><pre>%sql\r\n\r\nSET spark.sql.&lt;name-of-property&gt; = &lt;value&gt;;</pre><h1>Examples</h1><p>Get the current value of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.rpc.message.maxSize</span>.</p><pre>%sql\r\n\r\nSET spark.rpc.message.maxSize;</pre><p>Set the value of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.autoBroadcastJoinThreshold</span> to -1.</p><pre>%python\r\n\r\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)</pre><p><br></p>", "body_txt": "In most cases, you set the Spark config (AWS | Azure ) at the cluster level. However, there may be instances when you need to check (or set) the values of specific Spark configuration properties in a notebook. This article shows you how to display the current value of a Spark configuration property in a notebook. It also shows you how to set a new value for a Spark configuration property in a notebook. Get Spark configuration properties To get the current value of a Spark config property, evaluate the property without including a value. Python %python spark.conf.get(\"spark.&lt;name-of-property&gt;\") R %r library(SparkR) sparkR.conf(\"spark.&lt;name-of-property&gt;\") Scala %scala spark.conf.get(\"spark.&lt;name-of-property&gt;\") SQL %sql GET spark.&lt;name-of-property&gt;; Set Spark configuration properties To set the value of a Spark configuration property, evaluate the property and assign a value. Info\nYou can only set Spark configuration properties that start with the spark.sql prefix. Python %python spark.conf.set(\"spark.sql.&lt;name-of-property&gt;\", &lt;value&gt;) R %r library(SparkR) sparkR.session() sparkR.session(sparkConfig = list(spark.sql.&lt;name-of-property&gt; = \"&lt;value&gt;\")) Scala %scala spark.conf.set(\"spark.sql.&lt;name-of-property&gt;\", &lt;value&gt;) SQL %sql SET spark.sql.&lt;name-of-property&gt; = &lt;value&gt;; Examples Get the current value of spark.rpc.message.maxSize. %sql SET spark.rpc.message.maxSize; Set the value of spark.sql.autoBroadcastJoinThreshold to -1. %python spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)", "format": "html", "updated_at": "2022-05-26T00:41:08.489Z"}, "author": {"id": 790229, "email": "mathan.pillai@databricks.com", "name": "mathan.pillai ", "first_name": "mathan.pillai", "last_name": "", "role_id": "admin", "created_at": "2022-01-26T19:28:01.464Z", "updated_at": "2023-04-28T22:28:56.701Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2679662, "name": "aws"}, {"id": 2679663, "name": "azure"}], "url": "https://kb.databricks.com/data/get-and-set-spark-config"}, {"id": 1385757, "name": "How to dump tables in CSV, JSON, XML, text, or HTML format", "views": 10226, "accessibility": 1, "description": "Learn how to output tables from Databricks in CSV, JSON, XML, text, or HTML format.", "codename": "dump-table", "created_at": "2022-05-25T22:05:34.452Z", "updated_at": "2022-05-25T22:25:21.369Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStUY1dOTHlTcEY5Mm1RbFliSzhJK0lmZEh2UGZSaXorbUZHdXhRNDBFRGVXTStZVmZRCnlDMUN2RURMdjlnR3dYR3hra0J5dTBwcUplejhSbXppOVZmU3BxUHdkd3dJOTFrbnR1Qzd5VkpHRGR6agpVZGx5bk52Yy9jaGtjUFlKYmlac0NmN3JreEdDQUxvMXZmeWpWdXJRUDFpOTFzUVVJYlR1QldxaytGTU4KSmlaWE1VKzY1NnBGTkdVNXdsZ09SWTRvSnptTzNNVGE1SmZ6c0ZlRTU5aE1qMy9BVnQ3MHVZbmtjVVRlCjBjeTVyQ0hac0Q1Y1Y0K1BlRDFUd0xadzFBQ1c5Yk4wZnpMWlR2SU9KNWdKc0RYZk5iS2pSYWhUMEQ4QwpvQmI1MDV4cTl0aDBhaFMwZ0VoQzArSlduQkZWTlZZVkNSdHVEQzYrTjF2ZkJWYmR6YWMwd0wvcUVuV3kKdTRKSVczdWpZdjFBaTVDcWNkeTNiZUwvYVVyM3YyaitTYTArK0ovQnd3MTlsR0srdXZBcElpTVdWNk1YCk82MlhLM2NLRzYzTjl6SXdtdVhXR3RObytqbkxySGZXSXZvNVphQjdiUCsyYVFvSUlBV2FFQzl5ZFY3OAptL001QUVQc0tJZW9ybXN2MDBTbjkyRW1JaXhqQmtSMUhLT3JKUWxQMURwZnZqd25YTTY3aXZ4ejJoOWIKQTZvVk12d2ZabXFRNEtXT3NXZkNBUE42cGd1bTloZmpJdXlWTmIxMmZ5UkNYKzhMTk40UHpIMDlBWXVEClQwNEIyWkI0RThxdVdXZXloRUdPMFdjSm5ITWlCSk8wbmhFT3VSY0QwOUdQSVBCUSsyVVo3V3ZMSmd4OApIYzhQaWJZTStaNktWMXVMK2NSMEhtRFdwTVpJTVpwcjd5M0R4QVR4L3pqQnVQNXZTUjNvYzI5Q1BMalAKSkNJRnpUcnBYZU5vbnJzK0lMM1BIblV5VzlJOXJkMkRWa254MkVPUlhTTmhqdXJoWkFpREpYNWdlRW5KCnJQbXhRdnV4bjhUTWg2R1hFeUF3WmtvNWo4a2M5SldZdG1UaFlteHNKOUViekpsRUs5YmIxTnpPMlQvSApQek5wbm9zMnBuOTc1YldRVURlRy9zTVZPOW11NUFVZVFHRnZ5cmdjWTU2dVkwbEtSOGwvMkdvdTc1blYKdTlabEQ5UVpzOUdxVXVjbm45SlgxVDI5b1hscTkwU2ZGeitFOFRTa2xkbkthcGJVbUlQUi80SFZWWXRKClNRZGs2amlkSVlST2FxUWRhanEvR1dSc0tpOWZzNVlrSXArL0JuK2lXRWs9Cg==.e39426f94fdffbe80b0f8db05776c139\"></div><p id=\"isPasted\">You want to send results of your computations in Databricks outside Databricks. You can use BI tools to connect to your cluster via JDBC and export results from the BI tools, or save your tables in DBFS or blob storage and copy the data via REST API.</p><p>This article introduces JSpark, a simple console tool for executing SQL queries using JDBC on Spark clusters to dump remote tables to local disk in CSV, JSON, XML, Text, and HTML format.\u00a0</p><p>For example:</p><pre>%sh\r\n\r\njava -Dconfig.file=mycluster.conf -jar jspark.jar -q \"select id, type, priority, status from tickets limit 5\"</pre><p>returns:</p><pre>+----+--------+--------+------+\r\n| \u00a0id|type \u00a0 \u00a0|priority|status|\r\n+----+--------+--------+------+\r\n|9120|problem |urgent \u00a0|closed|\r\n|9121|question|normal \u00a0|hold \u00a0|\r\n|9122|incident|normal \u00a0|closed|\r\n|9123|question|normal \u00a0|open \u00a0|\r\n|9124|incident|normal \u00a0|solved|\r\n+----+--------+--------+------+</pre><p>Instructions for use, example usage, source code, and a link to the assembled JAR is available at the <a href=\"https://github.com/MaxGekk/jspark\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">JSpark GitHub repo</a>.</p><p>You can specify the parameters of JDBC connection using arguments or using a config file, for example: <a href=\"https://github.com/MaxGekk/jspark/blob/master/src/main/resources/mycluster.conf\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">mycluster.conf</a>.</p><p>To check or troubleshoot JDBC connections, download the fat JAR <a href=\"https://bintray.com/maxgekk/generic/download_file?file_path=jspark.jar\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">jspark.jar</a> and launch it as a regular JAR. It includes hive-jdbc 1.2.1 and all required dependencies.</p>", "body_txt": "You want to send results of your computations in Databricks outside Databricks. You can use BI tools to connect to your cluster via JDBC and export results from the BI tools, or save your tables in DBFS or blob storage and copy the data via REST API. This article introduces JSpark, a simple console tool for executing SQL queries using JDBC on Spark clusters to dump remote tables to local disk in CSV, JSON, XML, Text, and HTML format.\u00a0 For example: %sh java -Dconfig.file=mycluster.conf -jar jspark.jar -q \"select id, type, priority, status from tickets limit 5\" returns: +----+--------+--------+------+ | \u00a0id|type \u00a0 \u00a0|priority|status| +----+--------+--------+------+ |9120|problem |urgent \u00a0|closed| |9121|question|normal \u00a0|hold \u00a0| |9122|incident|normal \u00a0|closed| |9123|question|normal \u00a0|open \u00a0| |9124|incident|normal \u00a0|solved| +----+--------+--------+------+ Instructions for use, example usage, source code, and a link to the assembled JAR is available at the JSpark GitHub repo. You can specify the parameters of JDBC connection using arguments or using a config file, for example: mycluster.conf. To check or troubleshoot JDBC connections, download the fat JAR jspark.jar and launch it as a regular JAR. It includes hive-jdbc 1.2.1 and all required dependencies.", "format": "html", "updated_at": "2022-05-25T22:25:21.367Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2679660, "name": "aws"}, {"id": 2679661, "name": "azure"}], "url": "https://kb.databricks.com/data/dump-table"}, {"id": 1385754, "name": "Simplify chained transformations", "views": 10634, "accessibility": 1, "description": "Learn how to simplify chained transformations on your DataFrame in Databricks.", "codename": "chained-transformations", "created_at": "2022-05-25T21:46:27.853Z", "updated_at": "2022-05-25T22:05:15.576Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStNRnVLNFM3bzVYZkd5UW1MekJFclpqbHRmOHhveTdHNkNVa1o5TVdWQUQ0THlJNlQzCkhzQjNITHdDU00yS25oK1hsT2hkWDJ6c054MHMvTDliSU5POS82Z3dHOGp4SUJ0anY3SFRoT0JjVWc0RQpVZ2toZjB3Qm8wbjRMNXdtdmZWSVNUWDA3UnB5Mk9Va3c5dG5CbVRleTZRVkNRcXhLcjhmWWE1WERJRi8KNmJqZHo3bnp0R0hyNVM2S3BGMm8ySnFmYVNDUXBlaGNQS1VSZlZ5TUl6OGRmeTNQRmFaTTFRSytHcHczCklMS3dzaGZJb2ZJQ0F1N3VjdFc0SVdXK3NqNXJ2QmIxZWI0OVlYbXU4UGkrcFFhcVBmanIvcWNqTWJRKwprUkZQN2xJaEliZGliTTlHdDMzNmJ0ejJ6UWZFNHArQUZmYUtwQkVEK0lleUZjRGlrNEI2VHJza3B4YzUKdCtSSlVVK3lNcXJxbzFraGhJVmFqV2JmeDI4VWs2b3FzcENHRTZLR0JIZEEwaTZEQ04xQm9LMW5tV0RMCll3ZXg4NFZXbmxudU1vZFBzdTZua1RJeHkxTWpGRXRZY0VwN1FKeUhlRWNjZmRZYkM4MWsvdHkyM3IrKwp1N09JeVF5akF6TUEycnk5OFhXY0lsdnpEWlQ0M21SVWtVQTBHbXoxbktkd25GQkhWQ2dLQUx6WDlPOXYKT2tiR1hGbWlCSC9NRmt2aytzeGF3R0dMQlg2dTRIVWptVWk1N3E0Qy9iNDNiYmV3R3ZvOVZRdi9jR013CnpjMXEzRGpocmQ5Wmxtb0MrVm1ESzQvTVh3aWtKU0FWQ2thK3VNcHU0NnFvT2QrcEdJV1RPU1lIMWxjZgpHVTBRWjF1aFhzZmMxYTlPL3F5R1BGWmhPZ01IQzBEaU1uMExqdHVMemU3ZnoxRStRUkZya2srbDdNR2UKTFBSTWEwanhVemZma25nOEdxMDNsNGhEc2hDZDhaQ3NjY3RNRy9KVE5aVUNpK1BZdHh5clczSVNXWlJKCm1lb1dTd0doY29vZUYzQ25VTEJzY3BZUUVZNzhRTjFQeGlIVE4zbmhleVl2OUVDVjhseTA5VWtrLys2dQp1YUQzekNlVG5URnUwNXNSNWw4ZmI3SmVjdzNsL3ljUUZEdzhMMXhMUmN6MmxxdVVMcE8wNzJjL3BMY2MKOFh2RndwNHowSjMxOXpsVUhsbXpzbUYyQmswd3pjMjhIVTlSZ2NWampqVExWeUswck9EN2N4S3VVeUV2Cg==.63c5369b05fd0863da319f1a579c243b\"></div><p id=\"isPasted\">Sometimes you may need to perform multiple transformations on your DataFrame:</p><pre>%scala\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.DataFrame\r\n\r\nval testDf = (1 to 10).toDF(\"col\")\r\n\r\ndef func0(x: Int =&gt; Int, y: Int)(in: DataFrame): DataFrame = {\r\n\u00a0 in.filter('col &gt; x(y))\r\n}\r\ndef func1(x: Int)(in: DataFrame): DataFrame = {\r\n\u00a0 in.selectExpr(\"col\", s\"col + $x as col1\")\r\n}\r\ndef func2(add: Int)(in: DataFrame): DataFrame = {\r\n\u00a0 in.withColumn(\"col2\", expr(s\"col1 + $add\"))\r\n}</pre><p>When you apply these transformations, you may end up with spaghetti code like this:</p><pre>%scala\r\n\r\ndef inc(i: Int) = i + 1\r\n\r\nval tmp0 = func0(inc, 3)(testDf)\r\nval tmp1 = func1(1)(tmp0)\r\nval tmp2 = func2(2)(tmp1)\r\nval res = tmp2.withColumn(\"col3\", expr(\"col2 + 3\"))</pre><p>This article describes several methods to simplify chained transformations.</p><h1 id=\"dataframe-transform-api-0\">DataFrame <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">transform</span> API</h1><p>To benefit from the functional programming style in Spark, you can leverage the DataFrame <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">transform</span> API, for example:</p><pre>%scala\r\n\r\nval res = testDf.transform(func0(inc, 4))\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .transform(func1(1))\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .transform(func2(2))\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"col3\", expr(\"col2 + 3\"))</pre><h1 id=\"functionchain-api-1\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Function.chain</span> API</h1><p>To go even further, you can leverage the Scala Function library, to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">chain</span> the transformations, for example:</p><pre>%scala\r\n\r\nval chained = Function.chain(List(func0(inc, 4)(_), func1(1)(_), func2(2)(_)))\r\nval res = testDf.transform(chained)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"col3\", expr(\"col2 + 3\"))</pre><h1 id=\"implicit-class-2\">\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">implicit</span> class</h1><p>Another alternative is to define a Scala <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">implicit</span> class, which allows you to eliminate the DataFrame <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">transform</span> API:</p><pre>%scala\r\n\r\nimplicit class MyTransforms(df: DataFrame) {\r\n\u00a0 \u00a0 def func0(x: Int =&gt; Int, y: Int): DataFrame = {\r\n\u00a0 \u00a0 \u00a0 \u00a0 df.filter('col &gt; x(y))\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 def func1(x: Int): DataFrame = {\r\n\u00a0 \u00a0 \u00a0 \u00a0 df.selectExpr(\"col\", s\"col + $x as col1\")\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 def func2(add: Int): DataFrame = {\r\n\u00a0 \u00a0 \u00a0 \u00a0 df.withColumn(\"col2\", expr(s\"col1 + $add\"))\r\n\u00a0 \u00a0 }\r\n}</pre><p>Then you can call the functions directly:</p><pre>%scala\r\n\r\nval res = testDf.func0(inc, 1)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .func1(2)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .func2(3)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"col3\", expr(\"col2 + 3\"))</pre><p><br></p>", "body_txt": "Sometimes you may need to perform multiple transformations on your DataFrame: %scala import org.apache.spark.sql.functions._ import org.apache.spark.sql.DataFrame val testDf = (1 to 10).toDF(\"col\") def func0(x: Int =&gt; Int, y: Int)(in: DataFrame): DataFrame = { \u00a0 in.filter('col &gt; x(y)) } def func1(x: Int)(in: DataFrame): DataFrame = { \u00a0 in.selectExpr(\"col\", s\"col + $x as col1\") } def func2(add: Int)(in: DataFrame): DataFrame = { \u00a0 in.withColumn(\"col2\", expr(s\"col1 + $add\")) } When you apply these transformations, you may end up with spaghetti code like this: %scala def inc(i: Int) = i + 1 val tmp0 = func0(inc, 3)(testDf) val tmp1 = func1(1)(tmp0) val tmp2 = func2(2)(tmp1) val res = tmp2.withColumn(\"col3\", expr(\"col2 + 3\")) This article describes several methods to simplify chained transformations. DataFrame transform API To benefit from the functional programming style in Spark, you can leverage the DataFrame transform API, for example: %scala val res = testDf.transform(func0(inc, 4)) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .transform(func1(1)) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .transform(func2(2)) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"col3\", expr(\"col2 + 3\")) Function.chain API To go even further, you can leverage the Scala Function library, to chain the transformations, for example: %scala val chained = Function.chain(List(func0(inc, 4)(_), func1(1)(_), func2(2)(_))) val res = testDf.transform(chained) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"col3\", expr(\"col2 + 3\")) implicit class Another alternative is to define a Scala implicit class, which allows you to eliminate the DataFrame transform API: %scala implicit class MyTransforms(df: DataFrame) { \u00a0 \u00a0 def func0(x: Int =&gt; Int, y: Int): DataFrame = { \u00a0 \u00a0 \u00a0 \u00a0 df.filter('col &gt; x(y)) \u00a0 \u00a0 } \u00a0 \u00a0 def func1(x: Int): DataFrame = { \u00a0 \u00a0 \u00a0 \u00a0 df.selectExpr(\"col\", s\"col + $x as col1\") \u00a0 \u00a0 } \u00a0 \u00a0 def func2(add: Int): DataFrame = { \u00a0 \u00a0 \u00a0 \u00a0 df.withColumn(\"col2\", expr(s\"col1 + $add\")) \u00a0 \u00a0 } } Then you can call the functions directly: %scala val res = testDf.func0(inc, 1) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .func1(2) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .func2(3) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"col3\", expr(\"col2 + 3\"))", "format": "html", "updated_at": "2022-05-25T22:05:15.563Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:29.827Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256844, "name": "Data management", "codename": "data", "accessibility": 1, "description": "These articles can help you with Datasets, DataFrames, and other ways to structure data using Apache Spark and Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2679645, "name": "aws"}, {"id": 2679646, "name": "azure"}, {"id": 2679647, "name": "gcp"}], "url": "https://kb.databricks.com/data/chained-transformations"}, {"id": 1383931, "name": "SHOW DATABASES command returns unexpected column name", "views": 5674, "accessibility": 1, "description": "Running the `SHOW DATABASES` command returns an unexpected column name.", "codename": "show-databases-unexpected-name", "created_at": "2022-05-24T00:51:00.176Z", "updated_at": "2022-05-24T00:56:32.565Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9UNm9nYjNab2t0MWlVZDUzbzBEcnRBTHhocDg1ak9kVVhWOEltK0E4V0VVbVI1dFZPClFuRjJwZUxHL3JkVnBEYU1KenQyZGtTZU5kVTd4eDVmbTF0WUpGUzhWa2k2cysrOUhiYTV2eGhmUy9vKwoxaDRDQmdUc3hHMVNDWVVIcWF6SldvdzY3K01yUUthMmNQMnZrUlVabGxqNm92d3pIUjFKY0o3VkNwVUEKYklZTVNPUUdicldQTnJ6TlpxcUNKNlZIK0xOZVpIYVg0bDJEVm8xWlNyNk8zNE9hcGl0VGJlVjZTcEowCithUVAvbEs4ZTMxSld0ZmRwUG1ZczVaQ3lHbFZaSWJ3VThwT2l0bXpQR2ZBbENjYWJCTHR5OGxvcjRPZQpKTWltc3hidXhCVnZqcVR1V2xpdmRiRkd2SFAxbW92SW5XYUlNSTkydEJNVHN5eTB5M29uK2w3ZFBnTFcKUWhpWEZhL2ZwQklHdW04UVVTQklZWDJHMk9nNDFYVDNRakNyV3g0SElvSVRRUXNXRzJpczM1d2F5TzV2CjlJS1RkVWd0emliRWd3eWMxRVVlYnBsMng1T1NGUTNOR2xLVmZVYmJGTWZNeDBFQTVkNGhJT1dQU1NaZAovY08wSmlvaXFQWEM1YmY1S0dLdUZZWGZvdWpwYlExTWVDOTdxTGJqMnlPVmVkNUtXUlpTVFdNQ0pOemkKdVp1TEpWbU1NQTdwa1Bqa2lwUXNvNVc4OG91RkZJUmNYTnZsRVRmSFJKM2ZvZm5HVDBsTHhuY3gwVXBrCjV3eDRMK2FNRWc5NlhaREYvMCs3aENRVUd2NUxwc05KL3EyU1ptU0twc0V0RjNSVGVCbWs1WExObGlveQpqaXdiVU5Ga2twcEVoNkJkbFNLak42bDRndWFrUWh6QTYzNlRiNCs0UzdnL2NuVEI4K0NOZDlEZlhHU3kKdnFtYVR4UHZXNWl4UjkyWk1UYmhpVFY3M3RrbklTR3YrUjlnQmo2blFUb1Bib0VWOFZQRjJya3JnMXdjCndxaXp4REpGUjFBTW5vejVmQyswMXNnWENZYkZKcVh0MjVCaEFJL1Bsd1N3Q2UxY0F1aWFFQUlveUFMaAphb2JEL2VsUTUvY0hBMCtpMGYvcU53Yk45eVdmck9BQ1htUElXV0QrZ3hqTGM3UXZqWGJENkV2QkFQSGwKQy81MjlPSGM0NHVyLytNM3FycHdDbDNJMi82bWxISDh0ajhKSWJGdDF6eUJsRXRNbUhZUm52VFR5aWxkCmJPZ0dHVmRLWS9FeXIzdWRiWDE2am56d0RCMzN0RllqMmsyWTJpREJFUi82aHUxN2NNbnlXQjE4UVZ4QwpLNFIzdzNRQWIvanl2MWw3Vml0MkZIYVQvV3g5ak9wQ084SEEwZlZjenowdTIyZXMwS3BsWVd0NnJWMjAKQnA0QjZ1OTVQS1dzdm1ENy9RajJnNDR0c2U3YXVnPT0K.7ba09eff09afdd2f5d868b75c7ade0d5\"></div><h1 id=\"isPasted\">Problem</h1><p>You are using the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SHOW DATABASES</span> command and it returns an unexpected column name.</p><h1>Cause</h1><p>The column name returned by the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SHOW DATABASES</span> command changed in Databricks Runtime 7.0.</p><ul>\n<li>Databricks Runtime 6.4 Extended Support and below: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SHOW DATABASES</span> returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">namespace</span> as the column name.</li>\n<li>Databricks Runtime 7.0 and above: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SHOW DATABASES</span> returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">databaseName</span> as the column name.</li>\n</ul><h1>Solution</h1><p>You can enable legacy column naming by setting the property <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.legacy.keepCommandOutputSchema</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span> in the cluster\u2019s <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">GCP</a>).</p>", "body_txt": "Problem You are using the SHOW DATABASES command and it returns an unexpected column name. Cause The column name returned by the SHOW DATABASES command changed in Databricks Runtime 7.0. Databricks Runtime 6.4 Extended Support and below: SHOW DATABASES returns namespace as the column name.\nDatabricks Runtime 7.0 and above: SHOW DATABASES returns databaseName as the column name. Solution You can enable legacy column naming by setting the property spark.sql.legacy.keepCommandOutputSchema to false in the cluster\u2019s Spark config (AWS | Azure | GCP).", "format": "html", "updated_at": "2022-05-24T00:56:32.561Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678358, "name": "aws"}, {"id": 2678359, "name": "azure"}, {"id": 2678360, "name": "gcp"}], "url": "https://kb.databricks.com/sql/show-databases-unexpected-name"}, {"id": 1383930, "name": "Query does not skip header row on external table", "views": 9336, "accessibility": 1, "description": "External Hive tables do not skip the header row when queried from Spark SQL.", "codename": "query-not-skip-header-ext-table", "created_at": "2022-05-24T00:42:44.175Z", "updated_at": "2022-05-24T00:50:31.359Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThkZ0VVWkJrcE9NY2x4cG1BbzJMWUNNUGRZVzZwdCtTV2dMWmU0dWFMVkk2aTRlOTNRCmVDZ0NqVmRPNkxZTGZxbHZYZ29jemlyOVJqZzVUZzNqZDJLZHBqanhEU001cmxrdTUxQzRwUy80aVZDLwpvUlZqeWdCcGxNSFFIRTM1a1J4aklQZlNadFZtY2R0dENiMmw0MHB4V2JQeW5Td2lWYlFJcHJEWkZadDYKRDZ5dEtjWlZtQU1yeUk2TVNxblg4K2ZCenhHc3k0QkZXREdyWTVIQTZEUFVoMDh4T1NUWUt1YnpmUnpaCjUvNVhaQWxrc3hRUVZ2WGtMeWdIL1dZa09IT0ovMWprQ2tISVllTWNVWUlSN1QwOGVoYnpPM2hKZnFVUApzMW1NMGl6V215cDFSdDBJbm1CUm9nZFluMUZBRVFvWjN1bis3b1E0M3hiZzhnWVZpSTJLYnQvMDZOcFMKNzRkdG84dkdkMVM5cld2ekx1L0NFcTl6N3kvSGo3NEpkN21haU9VSW5NeGRpSHF0QUxmS1RPWERsQndlCkt5YjBOemV4TTdwaTE5TnYvWHhub0hPQzgzc0t0b3FXVnRXNm96OVZiRXFFMkp4TE5oSWt1UUpCaTFJTwpKVXZ4L1ZDMlRORmd6cE9BNkErQWxxanAyYUZaK3hubU13QnpOQk5ZR2NYcDF3NExlaG9IMFVGSEdoT1IKcVdCR0RkbEpRR2M0TFMrN2JpVGh4U0wwRUZBTzViaHlNQjVidUhzTjBHUDJOM0xQeWY5TnJPcCtyZlN2ClovQ1JaZ2ozODIwVUFXTDA0YjkreHVWS0lJbk8rUmIyMXR3SmlwWENLT0g3aGV1NE1XNEEyaUFHY1hvZApvcHFUQ2NycUROTGszTm5hTkd4NURwQVNqdWVaVDBicVhvQ1ZOYVFXSkg1NkZpNC94RUw3bmZUUkVLSmkKV2xHT2dNU2l2TTdMeUFvRjdsZlk5NVk3L2k1M0trbHQwL0tLRENLbm1uMkpFYWI3czA2QUxTVEVMVnNRClI3Y3hqZGx3c3EzUGg1QTVqRy9ndi9YN3FlNVVGZUgrajNpMy9CR00wL2RrdUxQbEFxNFhQcSttcTd3VgoyR1RzRGQ1emxkQUNnRmQxYk9IMmFqSFlORXVnN1RIUlA4OGRuMXJ1TFZpbTdOWnp3MUlQeC9WMXJSNlYKR2JzdDBKMm1sRXNjQmVwRVYweDZoV0VnanlmMENvc0ZIOXlzcjN0T0hkZXNjMklHZmdjYllQaFpuOFRjCmp0a1BmQm1ZeHkyNFo3cUlaOVFMaHNMQW1UcU1LZVVzb3Q0M2FyeHArTlJEZmZuTnBtUHkzbkZkbkpaaApHdmM0QXh6T1JNenR4YS9qQmpNcVVaSlJLQUhzVldqeHM4d2MvV0s2RTVFWXYzVXJHdzFEODVwTGpNazUKQUtCU2RKbUhGU2VZT0tVdDRneEx4b242aE5WZUdBPT0K.d673e79436cef84f3115c6cfac9e5406\"></div><h1 id=\"isPasted\">Problem</h1><p>You are attempting to query an external Hive table, but it keeps failing to skip the header row, even though <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">TBLPROPERTIES ('skip.header.line.count'='1')</span> is set in the HiveContext.</p><p>You can reproduce the issue by creating a table with this sample code.</p><pre>%sql\r\n\r\nCREATE EXTERNAL TABLE school_test_score (\r\n\u00a0 `school` varchar(254),\r\n\u00a0 `student_id` varchar(254),\r\n\u00a0 `gender` varchar(254),\r\n\u00a0 `pretest` varchar(254),\r\n\u00a0 `posttest` varchar(254))\r\nROW FORMAT DELIMITED\r\n\u00a0 FIELDS TERMINATED BY ','\r\n\u00a0 LINES TERMINATED BY '\\n'\r\nSTORED AS INPUTFORMAT\r\n\u00a0 'org.apache.hadoop.mapred.TextInputFormat'\r\nOUTPUTFORMAT\r\n\u00a0 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\r\nLOCATION\r\n\u00a0 'dbfs:/FileStore/table_header/'\r\nTBLPROPERTIES (\r\n\u00a0 \u00a0'skip.header.line.count'='1'\r\n)</pre><p>If you try to select the first five rows from the table, the first row is the header row.</p><pre>%sql\r\n\r\nSELECT * FROM school_test_score LIMIT 5</pre><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1653353204966-header-row-not-skipped.jpg\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Table output with header row as first data row.\"></p><h1>Cause</h1><p>If you query directly from Hive, the header row is correctly skipped. Apache Spark does not recognize the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">skip.header.line.count</span> property in HiveContext, so it does not skip the header row.</p><p>Spark is behaving as designed.</p><h1>Solution</h1><p>You need to use Spark options to create the table with a header option.</p><pre>%sql\r\n\r\nCREATE TABLE student_test_score (school String, student_id String, gender String, pretest String, posttest String) USING CSV\r\nOPTIONS (path \"dbfs:/FileStore/table_header/\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 delimiter \",\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 header \"true\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 ;</pre><p>Select the first five rows from the table and the header row is not included.</p><pre>%sql\r\n\r\nSELECT * FROM school_test_score LIMIT 5</pre><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1653353241436-header-row-skipped.jpg\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Table output with header row skipped.\"></p><p><br></p>", "body_txt": "Problem You are attempting to query an external Hive table, but it keeps failing to skip the header row, even though TBLPROPERTIES ('skip.header.line.count'='1') is set in the HiveContext. You can reproduce the issue by creating a table with this sample code. %sql CREATE EXTERNAL TABLE school_test_score ( \u00a0 `school` varchar(254), \u00a0 `student_id` varchar(254), \u00a0 `gender` varchar(254), \u00a0 `pretest` varchar(254), \u00a0 `posttest` varchar(254)) ROW FORMAT DELIMITED \u00a0 FIELDS TERMINATED BY ',' \u00a0 LINES TERMINATED BY '\\n' STORED AS INPUTFORMAT \u00a0 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT \u00a0 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION \u00a0 'dbfs:/FileStore/table_header/' TBLPROPERTIES ( \u00a0 \u00a0'skip.header.line.count'='1' ) If you try to select the first five rows from the table, the first row is the header row. %sql SELECT * FROM school_test_score LIMIT 5 Cause If you query directly from Hive, the header row is correctly skipped. Apache Spark does not recognize the skip.header.line.count property in HiveContext, so it does not skip the header row. Spark is behaving as designed. Solution You need to use Spark options to create the table with a header option. %sql CREATE TABLE student_test_score (school String, student_id String, gender String, pretest String, posttest String) USING CSV OPTIONS (path \"dbfs:/FileStore/table_header/\", \u00a0 \u00a0 \u00a0 \u00a0 delimiter \",\", \u00a0 \u00a0 \u00a0 \u00a0 header \"true\") \u00a0 \u00a0 \u00a0 \u00a0 ; Select the first five rows from the table and the header row is not included. %sql SELECT * FROM school_test_score LIMIT 5", "format": "html", "updated_at": "2022-05-24T00:50:31.356Z"}, "author": {"id": 790787, "email": "manisha.jena@databricks.com", "name": "manisha.jena ", "first_name": "manisha.jena", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T13:04:07.172Z", "updated_at": "2022-11-21T15:31:32.115Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678355, "name": "aws"}, {"id": 2678356, "name": "azure"}, {"id": 2678357, "name": "gcp"}], "url": "https://kb.databricks.com/sql/query-not-skip-header-ext-table"}, {"id": 1383929, "name": "JDBC write fails with a PrimaryKeyViolation error", "views": 6522, "accessibility": 1, "description": "JDBC write to a SQL database fails with a `PrimaryKeyViolation` error or results in duplicate data", "codename": "jdbc-write-fails-primarykeyviolation", "created_at": "2022-05-24T00:29:18.350Z", "updated_at": "2022-05-24T00:42:28.454Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThnVlhFN2NBUEFGMGwyaFUwRzJUbmR5THgzTGxuVjFSaXV6RHdvUWo0K2k4K1dCWUpCCnM2eGJxNHZPN2NBREVuc29EbkxMaHBxbVB6ZTR5ejFvMWo3b0VyM2RsblJzZDJiVzJmb2o4dS90cUZTVgp2M05NLzVtajRmL2p3SGlTL0UyanpXQ0VYek1WUUdQd3ZGYmExRHlNV0Q5TS9CYVIrZUVZZk9IMk9uMDAKUFkxT1RKMTYxRUU5cUZ1YllYSWhMZjc2NEFQalNhYlhJUkFqMWtMNmdQS3R0Zmw3dnpvajh2OUhqMy82CkNDUHpyR1NlTTV3dXFOQXc2am5HZnJZUm9VWmo4N2JEcEhaMS8rTEdQNmJJZXF2WlpxZFBwNlIrUitQQwp6ZzRYSDdtbnYyTXZKUGZYMzBnbmswVVBxUE55MVZJUUpSYnYvU2NZVEJmNmtGS1FrSWRENWlNTUU4TDMKQ0RCSG9UbnlWNU1XcTRIUUZCMXozZGF4ZGM4VHN1NExOWmZWWWlKSS8yL1A3SHRvMU51clduZUVqZFhsCjJwRmhNdzN2ZDVQTk5WeVhjMk5aVjFKVVpVVlZuT0hJaHQvTVFabVBlUTlDRGEvQTUwcisvWUtOeUMzUApWYXJwRWNDKzB4Y2N4S001ZGxVUERoVi9tVHdOOUw1NWVxMnBOZ2FQeEtFM2FwUTZ3dWIrdU4yN0hxYUwKVHVudjI5a1VHV2FpQzVLeW5TT3QyK2dPQjhxSzJCOElzSzBIeHZIKzBONjFLRWtzY2V2ZzJDVVgwVFF3CnpUT2hwQi9oajl5eS9UdkMxeG1zSVpWTHFWUlhiQ3IvR2twQkVWb2t3VnIyNW9OQmxkcVNsREZIMWozZwpWenRsRjNURDJST0Y2Tmkzd1NoYWdmZUM1OFdMdk45QkV2YmhGOFVSd085WVZXWWZBWW8zQmpJYnRyaW8KRnEyWm1kcG43dnVtSkhTYVlIcmwzMGtsS3p4UUNzUDZSaG9BRFYxZDNIMEl1UDcwOFFtRlBVZXlQbFYvClBNeGFsWlFqZ1I5aWRveklwMlVjQXR1ODNTTnc2cTZkV2doUVpEOWFQWG5ESEhOdVd6NU1GMDhsdmxGMQpRMmdaMUsvNGM2ZzdBaW8wdzRqMjFHU1N1YStST3prOGpCZkFjaThsSjVncGZYUTdUaFdjb1RhYzd4S0cKMmEveExnRjEzMC9GRTQvU2VZU1ZwQTl6dCtFeWV6T2hnYzhobEw4RWMvZEdwL1RuTjN5dzBQdERNb1lnCnpEOVl6eStzRHJEc3M2NDJMenpOcmc9PQo=.0900ab33c154f779e005825520b3d7b5\"></div><h1 id=\"isPasted\">Problem</h1><p>You are using JDBC to write to a SQL table that has primary key constraints, and the job fails with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PrimaryKeyViolation</span> error.</p><p>Alternatively, you are using JDBC to write to a SQL table that does not have primary key constraints, and you see duplicate entries in recently written tables.</p><h1>Cause</h1><p>When Apache Spark performs a JDBC write, one partition of the DataFrame is written to a SQL table. This is generally done as a single JDBC transaction, in order to avoid repeatedly inserting data. However, if the transaction fails after the commit occurs, but before the final stage completes, it is possible for duplicate data to be copied into the SQL table.</p><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">PrimaryKeyViolation</span> error occurs when a write operation is attempting to insert a duplicate entry for the primary key.</p><h1>Solution</h1><p>You should use a temporary table to buffer the write, and ensure there is no duplicate data.</p><ol>\n<li>Verify that speculative execution is disabled in your Spark configuration: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.speculation false</span>. This is disabled by default.</li>\n<li>Create a temporary table on your SQL database.</li>\n<li>Modify your Spark code to write to the temporary table.</li>\n<li>After the Spark writes have completed, check the temporary table to ensure there is no duplicate data.</li>\n<li>Merge the temporary table with the target table on your SQL database.</li>\n<li>Delete the temporary table.</li>\n</ol><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Info</h3>\n<p class=\"hj-alert-text\">This workaround should only be used if you encounter the listed data duplication issue, as there is a small performance penalty when compared to Spark jobs that write directly to the target table.</p>\n</div>\n</div><p><br></p>", "body_txt": "Problem You are using JDBC to write to a SQL table that has primary key constraints, and the job fails with a PrimaryKeyViolation error. Alternatively, you are using JDBC to write to a SQL table that does not have primary key constraints, and you see duplicate entries in recently written tables. Cause When Apache Spark performs a JDBC write, one partition of the DataFrame is written to a SQL table. This is generally done as a single JDBC transaction, in order to avoid repeatedly inserting data. However, if the transaction fails after the commit occurs, but before the final stage completes, it is possible for duplicate data to be copied into the SQL table. The PrimaryKeyViolation error occurs when a write operation is attempting to insert a duplicate entry for the primary key. Solution You should use a temporary table to buffer the write, and ensure there is no duplicate data. Verify that speculative execution is disabled in your Spark configuration: spark.speculation false. This is disabled by default.\nCreate a temporary table on your SQL database.\nModify your Spark code to write to the temporary table.\nAfter the Spark writes have completed, check the temporary table to ensure there is no duplicate data.\nMerge the temporary table with the target table on your SQL database.\nDelete the temporary table. Info\nThis workaround should only be used if you encounter the listed data duplication issue, as there is a small performance penalty when compared to Spark jobs that write directly to the target table.", "format": "html", "updated_at": "2022-05-24T00:42:28.451Z"}, "author": {"id": 789479, "email": "harikrishnan.kunhumveettil@databricks.com", "name": "harikrishnan.kunhumveettil ", "first_name": "harikrishnan.kunhumveettil", "last_name": "", "role_id": "collaborator", "created_at": "2022-01-26T05:57:22.814Z", "updated_at": "2023-04-07T07:50:41.846Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678352, "name": "aws"}, {"id": 2678353, "name": "azure"}, {"id": 2678354, "name": "gcp"}], "url": "https://kb.databricks.com/sql/jdbc-write-fails-primarykeyviolation"}, {"id": 1383925, "name": "Data is incorrect when read from Snowflake", "views": 7541, "accessibility": 1, "description": "Data read from Snowflake is incorrect when time zone value is not set correctly.", "codename": "wrong-data-read-from-snowflake", "created_at": "2022-05-24T00:20:53.720Z", "updated_at": "2022-05-24T00:29:05.409Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTkwaUNsTTVKaVBodFdmRzY2QTFxL3lZekk1d2p5VEJveW1qWnZGVzJiNjFxK25LeUw5CkpHbFREcmVCWHVtRmJ6RjZBdGtZYXdQU1prbWEzUFQzbzZLbDVINE1xaUtjYWJaV2R0OVhPc1pkdnd3Swo3TVNpbUt1N1pRU2VmdkxJOWtVbW1ndGlrWkpPU1UwNnpCTEh1enBCNTAxK3FpYmRwM0FvVzYxWHZPRFIKL0NBdjE4R0VwUFhEZVR1V1FFQWtsUWg4bU1paW9rVnhMZkRsY1QzR0dJdFJqUEFQNEc4QUp3d0JkdkNUCnlLVUJYNXRjUVJaOG5sMGpyenhkdkpwYWJNOUxGOXJML1FnUlNOQWQwanRoakVldTd0aUNxa1Q3TEhZWgp3cXdyRzVFRnRYTk9TS292KzVVa093TW1wWldsUUJIbWhnQ3ZvSzVYekttMlZJWTh3ZlJOVHFUdzhpbEQKL3kxNXVXOE5oT3dmZ2R4RWRMckxuZHdGM0dzYUZ2NWVMSW9qb3lCbWg1U1lqSDJXUlc4Uk1qNzdnbm9NCmFhU1lUd1lEQXNKMTNicU1KS1FhSVVldHZrN1ZvSlQ0dzYvZ1VRYm15V0sxdmNMNFNMUFI1VHFWYmNjagpoZWp3R1hoUXNNdEJDc2l6ZzZ0VDM4Q1draVc4UXV2L0RHdzVXUmJYamllNnRaU013dE5zeU1QWE9VVVkKUE9nT3FnWXUzNW5XT0U2c0g2THU1UUdRVzRUT3dPR2tTbVlLRk8ySVZUcmtESVluMmxoQVNyL2VZT1A1CmhlRkFhRUlINlpHNVZiOWNFWDJ4eVpQcWpmMzNFc0VQRm1XZmlTL3FxODJYV2w4bmhjQ2FGMFFsK3pkQwpsTzlMSlprMEJIMVRMeEpmbEZ5RGdRTkplZ3dmQWEydjU1cTkxQjUwYXZhMDZNakVpVVpzTVZ4UjFMWncKNUJJWTI2eGpIMDA5bVh4VzNaeW1RckM4WFFDZ21OS1JoQkgvaks1aS8ybU9iL0FmTjhtRGlTVm40V01ICnBQOEY0UzBZOWtybnZ6bHFxS25lUzdOb3lrS0NIdVJGTEpoczFwc2IxbFZOK2NLL2pmSkRvNGQ4UHJhVQp1TVBuT3FaamdiTTZhb0pXR2tvdkFpMWZWZ0JhY085UjF6ZXFDS29EV1BVMG01Qm5CTGkzMVpYQ3lpa2YKUUdzU2xlcWMreE5JeENCOWUyOForUXk4UWlzd1BvZXBGRktWS2owPQo=.e4ef19ea40f98ec731a164be519b58ec\"></div><h1 id=\"isPasted\">Problem</h1><p>You have a job that is using Apache Spark to read from a Snowflake table, but the time data that appears in the Dataframe is incorrect.</p><p>If you run the same query directly on Snowflake, the correct time data is returned.</p><h1>Cause</h1><p>The time zone value was not correctly set. A mismatch between the time zone value of the Databricks cluster and Snowflake can result in incorrect time values, as explained in Snowflake\u2019s <a href=\"https://docs.snowflake.com/en/user-guide/spark-connector-use.html#working-with-timestamps-and-time-zones\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">working with timestamps and time zones</a> documentation.</p><h1>Solution</h1><p>Set the time zone in Databricks and do not explicitly set a time zone in Snowflake.</p><h2>Option 1: Set the time zone for SQL statements in Databricks</h2><ol>\n<li>Open the Databricks workspace.</li>\n<li>Select <strong>Clusters</strong>.</li>\n<li>Select the cluster you want to modify.</li>\n<li>Select <strong>Edit</strong>.</li>\n<li>Select <strong>Advanced Options</strong>.</li>\n<li>Enter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.session.timeZone &lt;timezone&gt;</span> in the <strong>Spark config</strong> field.</li>\n<li>Select <strong>Confirm</strong>.</li>\n</ol><h2>Option 2: Set the time zone for all nodes with an init script</h2><ol>\n<li>Create the init script with the following command:<pre>%python\r\n\r\ndbutils.fs.put(\"/databricks/scripts/set_timezone.sh\",\"\"\"\r\n#!/bin/bash\r\ntimedatectl set-timezone America/Los_Angeles\r\n\"\"\", True)</pre>\n</li>\n<li>Verify the full path of the init script.<pre>%python\r\n\r\n%fs ls /databricks/scripts/set_timezone.sh</pre>\n</li>\n<li>Open the Databricks workspace.</li>\n<li>Select <strong>Clusters</strong>.</li>\n<li>Select the cluster you want to modify.</li>\n<li>Select <strong>Edit</strong>.</li>\n<li>Select <strong>Advanced Options</strong>.</li>\n<li>Select <strong>Init Scripts</strong>.</li>\n<li>Enter the <strong>Init Script Path</strong>.</li>\n<li>Select <strong>Add</strong>.</li>\n<li>Select <strong>Confirm</strong>.</li>\n</ol>", "body_txt": "Problem You have a job that is using Apache Spark to read from a Snowflake table, but the time data that appears in the Dataframe is incorrect. If you run the same query directly on Snowflake, the correct time data is returned. Cause The time zone value was not correctly set. A mismatch between the time zone value of the Databricks cluster and Snowflake can result in incorrect time values, as explained in Snowflake\u2019s working with timestamps and time zones documentation. Solution Set the time zone in Databricks and do not explicitly set a time zone in Snowflake. Option 1: Set the time zone for SQL statements in Databricks Open the Databricks workspace.\nSelect Clusters.\nSelect the cluster you want to modify.\nSelect Edit.\nSelect Advanced Options.\nEnter spark.sql.session.timeZone &lt;timezone&gt; in the Spark config field.\nSelect Confirm. Option 2: Set the time zone for all nodes with an init script Create the init script with the following command:%python dbutils.fs.put(\"/databricks/scripts/set_timezone.sh\",\"\"\" #!/bin/bash timedatectl set-timezone America/Los_Angeles \"\"\", True) Verify the full path of the init script.%python %fs ls /databricks/scripts/set_timezone.sh Open the Databricks workspace.\nSelect Clusters.\nSelect the cluster you want to modify.\nSelect Edit.\nSelect Advanced Options.\nSelect Init Scripts.\nEnter the Init Script Path.\nSelect Add.\nSelect Confirm.", "format": "html", "updated_at": "2022-05-24T00:29:05.388Z"}, "author": {"id": 488152, "email": "dd.sharma@databricks.com", "name": "DD Sharma", "first_name": "DD", "last_name": "Sharma", "role_id": "admin", "created_at": "2021-10-07T02:59:41.776Z", "updated_at": "2023-02-16T02:48:14.950Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678350, "name": "aws"}, {"id": 2678351, "name": "gcp"}], "url": "https://kb.databricks.com/sql/wrong-data-read-from-snowflake"}, {"id": 1383897, "name": "Inner join drops records in result", "views": 7956, "accessibility": 1, "description": "Avoid dropped records when performing an inner join.", "codename": "inner-join-drops-records-in-result", "created_at": "2022-05-23T23:15:48.305Z", "updated_at": "2022-05-23T23:35:15.088Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTg4RG5ka2Qrd1NiVTBGM2xBL2ZkVDQzdG5ubkJsdE53eElodExWVjF5RTc5OGJPZ2l6Cm52SlVPMlFxNjNOV1BxNnowc2xsTVlIanVCV1c5cDByT1JTeG1WYXJrVG5va3B5anZxN1JoNnF3ZXA3bwphdkYwTXBldVhHNHBHSlcvS1d2ckxKTzJ2aGc2Mzl5dWF2YzJiQmFodUUyaGZSTW4zMC9LdklpeFEzYU8KV2dwaWIxWWROOEY4QktCSGFMU3k1a0ZEZnNNbUZpTnBOeStGU2YzdHZubEoyVlVoaXJqdUhUWGY5TW9oCm9IS0pYUHJldHN3SytsVXAzQVV2UlN3VUxOZDJoY3RCKzRNM0hQa09NL3l5RHArWXdtcVAyOG5MTm5QZwp6S1lUOEpiN3FKN1g1SWNjT3FGc2RaNzhCMW1FbHVVQVhPWXpYSDNydVY4U3JyQ2Z0Rm9tQUluYlVyTlAKd2VlNmNPQzcvYnVGZzNHb3lOckM2MWJqK2pPMk1VbHd4Z0QxeG5wTjNwSkgvNzFNYmlnSnk4dXlubGg3CnJTOGpyUzBlWDNvS3dQYUVOWngxRjBLdnNBMXpWanhuWEVVbFlhTWxiMlVhR0NSUjZtcXVyMWpmWXBVNwpveVJMS0xzbHdqYTlGM0JzUDNSUVJOSkVNZ21wbUZzTVFYOHRwdjVQUSt4Qzh0dDNYa1E5d0JCRHliazYKN3pRVzdJdEhqcFJZZC9weWRUNzdqMFlZOGYvOXJ2WVhNaFp6ZHBLZER2T0dIU0c2VmRIenlvOW1LVWF2Ck1PbU1UVjBrVVhCREdyOFEvQnByZEtZenhJdGFBTTdxbHd3YWg5MEhOVEZXZHd2Mm9KNURIaExHQlBSVAoxSFAvajcrTTNLU2xPS1ovUmV6a0VyaE9zdy9qd3dHQlRuc1FYTkVlVHdzR2NqbXZ5dklsSXdSNDJzb2YKbjFQNndiS2VRQlNEdzZIUzBtZXNoSDY2Rjd4dzcxVGQ2aDJoaS9DZXlGWS9uVmtZM3BoWnorNnFEYmJ0ClhSWlRScmZNOFFrVWdxVjZ1N2pQRE9uRWJRTGRLQVRpaVFBUkZ5N2lsb0dVZGplNUxLNmdKSEE1WnZ3WgpuWElQQUFBcUtsTmw2MDF6eThXMVhVQXp2VTBROHBicFN1WnBML2NzWW5pQnFYdUNsVjJXZDRFQ2Y5V0IKRnFRZnZwTXRMb0pwQytld0w3ZUxVZHEzYWZrYmt0elRkZTN1bVdhYmZYeWNydzJ3TTRXb1gwSjNEZWtoCkpPVGg3blFKampQc0I1MjFjWlVMci9NTURSb2tqK1hDV1phWHV4bk02RzlDRVBOSEFROVRjcmFDclVqRApVa29wcndnbTFlUHNkYm1yOSs2K3JlSFNReHkyOGlvSThmOFVmcUlyLzJ6YVJPY0l2TFMwa0E0OCtVeHUKbWtqNk9tY0RES2VUcXU4U3U0eGhkTldsOWtxZEI4ekUvQmFDZmRTb3pnOXNkUHZzZDhJPQo=.0e7bbfc2dff5e1d51c1c3d289c5e4e49\"></div><h1 id=\"isPasted\">Problem</h1><p>You perform an inner join, but the resulting joined table is missing data.</p><p>For example, assume you have two tables, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">orders</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">models</span>.</p><pre>%python\r\n\r\ndf_orders = spark.createDataFrame([('Nissan','Altima','2-door 2.5 S Coupe'), ('Nissan','Altima','4-door 3.5 SE Sedan'), ('Nissan','Altima',''), ('Nissan','Altima', None)], [\"Company\", \"Model\", \"Info\"])</pre><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1653348616887-example-orders-table.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Sample orders table.\"></p><pre>%python\r\n\r\ndf_models = spark.createDataFrame([('Nissan','Altima',''), ('Nissan','Altima','2-door 2.5 S Coupe'), ('Nissan','Altima','2-door 3.5 SE Coupe'), ('Nissan','Altima','4-door 2.5 S Sedan'), ('Nissan','Altima','4-door 3.5 SE Sedan'), ('Nissan','Altima','4-door 3.5 SL Sedan'), ('Nissan','Altima','4-door HYBRID Sedan'), ('Nissan','Altima',None)], [\"Company\", \"Model\", \"Info\"])</pre><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1653348669237-example-models-table.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Sample models table.\"></p><p>You attempt a straight join of the two tables.</p><pre>%python\r\ndf_orders.createOrReplaceTempView(\"Orders\")\r\ndf_models.createOrReplaceTempView(\"Models\")\r\nSQL\r\nCopy to clipboardCopy\r\nSELECT *\r\nMAGIC FROM Orders a\r\nMAGIC INNER JOIN Models b\r\nMAGIC ON a.Company = b.Company\r\nMAGIC AND a.Model = b.Model\r\nMAGIC AND a.Info = b.Info</pre><p>The resulting joined table only includes three of the four records from the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">orders</span> table. The record with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">null</span> value in a column does not appear in the results.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1653348713660-default-spark-inner-join-no-nulls.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Results of a default inner join. Null values are dropped.\"></p><h1>Cause</h1><p>Apache Spark does not consider <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">null</span> values when performing a join operation.</p><p>If you attempt to join tables, and some of the columns contain <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">null</span> values, the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">null</span> records will not be included in the resulting joined table.</p><h1>Solution</h1><p>If your source tables contain <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">null</span> values, you should use the Spark <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">null</span> safe operator (<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;=&gt;</span>).</p><p>When you use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;=&gt;</span> Spark processes <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">null</span> values (instead of dropping them) when performing a join.</p><p>For example, if we modify the sample code with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">&lt;=&gt;</span>, the resulting table does not drop the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">null</span> values.</p><pre>%sql\r\n\r\nSELECT *\r\nMAGIC FROM Orders a\r\nMAGIC INNER JOIN Models b\r\nMAGIC ON a.Company = b.Company\r\nMAGIC AND a.Model = b.Model\r\nMAGIC AND a.Info &lt;=&gt; b.Info</pre><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1653348758451-null-safe-spark-inner-join.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Results of a null safe inner join. Null values are visible.\"></p><h2>Example notebook</h2><p>Review the <a href=\"https://docs.databricks.com/_static/notebooks/kb/sql/inner-join-drops-null-values-example.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Inner join drops null values example notebook</a>.</p>", "body_txt": "Problem You perform an inner join, but the resulting joined table is missing data. For example, assume you have two tables, orders and models. %python df_orders = spark.createDataFrame([('Nissan','Altima','2-door 2.5 S Coupe'), ('Nissan','Altima','4-door 3.5 SE Sedan'), ('Nissan','Altima',''), ('Nissan','Altima', None)], [\"Company\", \"Model\", \"Info\"]) %python df_models = spark.createDataFrame([('Nissan','Altima',''), ('Nissan','Altima','2-door 2.5 S Coupe'), ('Nissan','Altima','2-door 3.5 SE Coupe'), ('Nissan','Altima','4-door 2.5 S Sedan'), ('Nissan','Altima','4-door 3.5 SE Sedan'), ('Nissan','Altima','4-door 3.5 SL Sedan'), ('Nissan','Altima','4-door HYBRID Sedan'), ('Nissan','Altima',None)], [\"Company\", \"Model\", \"Info\"]) You attempt a straight join of the two tables. %python df_orders.createOrReplaceTempView(\"Orders\") df_models.createOrReplaceTempView(\"Models\") SQL Copy to clipboardCopy SELECT * MAGIC FROM Orders a MAGIC INNER JOIN Models b MAGIC ON a.Company = b.Company MAGIC AND a.Model = b.Model MAGIC AND a.Info = b.Info The resulting joined table only includes three of the four records from the orders table. The record with a null value in a column does not appear in the results. Cause Apache Spark does not consider null values when performing a join operation. If you attempt to join tables, and some of the columns contain null values, the null records will not be included in the resulting joined table. Solution If your source tables contain null values, you should use the Spark null safe operator (&lt;=&gt;). When you use &lt;=&gt; Spark processes null values (instead of dropping them) when performing a join. For example, if we modify the sample code with &lt;=&gt;, the resulting table does not drop the null values. %sql SELECT * MAGIC FROM Orders a MAGIC INNER JOIN Models b MAGIC ON a.Company = b.Company MAGIC AND a.Model = b.Model MAGIC AND a.Info &lt;=&gt; b.Info Example notebook Review the Inner join drops null values example notebook.", "format": "html", "updated_at": "2022-05-23T23:35:15.086Z"}, "author": {"id": 860965, "email": "siddharth.panchal@databricks.com", "name": "siddharth.panchal ", "first_name": "siddharth.panchal", "last_name": "", "role_id": "draft_writer", "created_at": "2022-04-27T04:56:07.243Z", "updated_at": "2023-04-18T16:33:31.124Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678311, "name": "aws"}, {"id": 2678312, "name": "azure"}, {"id": 2678313, "name": "gcp"}], "url": "https://kb.databricks.com/sql/inner-join-drops-records-in-result"}, {"id": 1383896, "name": "Find the size of a table", "views": 16679, "accessibility": 1, "description": "How to find the size of a table.", "codename": "find-size-of-table", "created_at": "2022-05-23T23:12:10.093Z", "updated_at": "2022-05-23T23:15:34.636Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMThXeDBBS2plNGF5aG10R1NOQjZPejhCQ1JCNW9OVG5OaHNrTU9XUnlZdVd1eTBzRmIvCnYyTk9MWFZpM0IxNS9XUHBUR3RMV1ZiNzdzV0pNY3JHUC9jNnNxeDJ5WnpDNnZjOUp3dUc0SnJKdWF1NQpFbWhITGVkWmdaMFlpUFJocm90WER1RVRBZ0xUMDlmYTQ5STVyeWp5M0YyejUxR1JteGV4S0lkSkFFZjAKQVNaVnQwMzdTL29vU3pVamtnNGRZVHNPY3EwZnU1ZDBIN2lTZnJqRWlUYk8yU1NGd3JBNDlYeWFLM3FmClZBU3F4WlpLY2hpWmRZRHhBQnQvcWVKQ3puVnlKM3V4YjJGdGZma2NWcWdCaExxYlpGVGxSQXErK2hoLwptM01OWk9yKzh2ZnNUbjhTbWsvNERleXNUYjN1MmVxMnh5aWxWMzNma09VamZ5WXU4TUlKK1FsSnlTNWIKYlNEaFhZMTIwNlVSdU1GVDNTMWx6dmdHcTF1ditkQWtPZXRlNU9oNTQ2Vi9HbzV2U3VKS1hVSWpydUhhCkRXWnE3bjN2RDRTd210b1R1VGlTTHpTbVQrVUdNMEx4cHlCclBrYkNIaXdmemZSSDh0dURRdXR2OHRRTQpnaEVsSkFwdk50RlNnbVNLSzZSekVkZkJkZXd4cVNOMXZkK0tXYVpIYXgxeC9XTXFIOXVGWW5kV2NtSTcKRlFuakkxNTlBS3pMYVh3eUY2TmNxZGtubzFrNTZMV0M0ZmdWeHV1TitJT0htOG11bkxaUEFxaDlSVEEzCkpQaC8xcGM0cXhLVzRISjVMblR5czhac2tlaVZmV3QraDA3c21rSElFRmNMMUpoQzdMZHhYK09zbFVxawppUGtZaGlaajE3cGs5L1NGOThIMU5YT2lyYVVJYTF0dTJnMXJkZkp3NTdDWjdDbHRkamNhRjZoTk4vSGYKa2c2RmxVN2tpVi9XN2JqYko4cG5Zb043cGU4M0FCWDN0WkZFYzQ3OWVhQnN6Vjh5cUNkOENXUmdrTXdVCnN1Vm5Xakp3cFBlVHdsY21GTy9vWllnMythT0pIUkk0Z0ordkhPelNiS21DTjVYaUdoeWpwMkFkYXN1eAo0WTdBbTQ1T1Z1TERKNmhYYndjK29JcWg5Wm9TWlpQZk9kQ2VwM2hKSUxTOS9LcGFqMjRlbUkrRmhqSFoKRXo4aFVnenBhZnZBTmhiN1pDL1F3M2hrUGdBRTRQa2MyMGtOYVZVQmovdEFuZ0JQa2F3dG4rbEpTMTAwCmJhK2hEKzBtemZkUTdtNXVYQVVvano3MEhBZEY1c3gvVTRBTWpaTlBEbGhTT0RCOUFOOWdDb3U3THU4TQpGQm54SG9scGJYMGNKOWx1ZnJoR2hVZ0JrZmdCak16bzFYQi9wR0ZVTnZRazRSZkVZR3Q1ZUk2RThpLzgKbW1nZlAvY3JXaGpqVG9PdVpnUXR1N1hIQjk4blBRaURjY0lCRkhPaHRBYTRubEJGNnhRPQo=.feaa9f2028d46b614f53dde38c240755\"></div><p id=\"isPasted\">This article explains how to find the size of a table.</p><p>The command used depends on if you are trying to find the size of a delta table or a non-delta table.</p><h1>Size of a delta table</h1><p>To find the size of a delta table, you can use a Apache Spark SQL command.</p><pre>%scala\r\n\r\nimport com.databricks.sql.transaction.tahoe._\r\nval deltaLog = DeltaLog.forTable(spark, \"dbfs:/&lt;path-to-delta-table&gt;\")\r\nval snapshot = deltaLog.snapshot \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // the current delta table snapshot\r\nprintln(s\"Total file size (bytes): ${deltaLog.snapshot.sizeInBytes}\")</pre><h1>Size of a non-delta table</h1><p>You can determine the size of a non-delta table by calculating the total sum of the individual files within the underlying directory.</p><p>You can also use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">queryExecution.analyzed.stats</span> to return the size.</p><pre>%scala\r\n\r\nspark.read.table(\"&lt;non-delta-table-name&gt;\").queryExecution.analyzed.stats</pre><p><br></p>", "body_txt": "This article explains how to find the size of a table. The command used depends on if you are trying to find the size of a delta table or a non-delta table. Size of a delta table To find the size of a delta table, you can use a Apache Spark SQL command. %scala import com.databricks.sql.transaction.tahoe._ val deltaLog = DeltaLog.forTable(spark, \"dbfs:/&lt;path-to-delta-table&gt;\") val snapshot = deltaLog.snapshot \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // the current delta table snapshot println(s\"Total file size (bytes): ${deltaLog.snapshot.sizeInBytes}\") Size of a non-delta table You can determine the size of a non-delta table by calculating the total sum of the individual files within the underlying directory. You can also use queryExecution.analyzed.stats to return the size. %scala spark.read.table(\"&lt;non-delta-table-name&gt;\").queryExecution.analyzed.stats", "format": "html", "updated_at": "2022-05-23T23:15:34.634Z"}, "author": {"id": 790229, "email": "mathan.pillai@databricks.com", "name": "mathan.pillai ", "first_name": "mathan.pillai", "last_name": "", "role_id": "admin", "created_at": "2022-01-26T19:28:01.464Z", "updated_at": "2023-04-28T22:28:56.701Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678291, "name": "aws"}, {"id": 2678292, "name": "azure"}, {"id": 2678293, "name": "gcp"}], "url": "https://kb.databricks.com/sql/find-size-of-table"}, {"id": 1383892, "name": "Error when running MSCK REPAIR TABLE in parallel", "views": 8914, "accessibility": 1, "description": "Do not run `MSCK REPAIR` commands in parallel. It results in a read timed out or out of memory error message.", "codename": "error-run-msck-repair-table-parallel", "created_at": "2022-05-23T23:07:08.949Z", "updated_at": "2022-05-23T23:11:59.678Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlJNnhMWVVucTI2eXRTZHBxais5bUdtVjZ6VUphY0d3elNHNGtKUFo4Mm0rNFgyMFIyCmhXMlhINjJQVFl2MFVPQ1lYbUp6VWk2bHdncDgwbGRtZlh6cFI4eHhKQ1RmT2IxeXllejdyWnFjcDZGaAp1M3A2aHJDRzdxZ3Y0ZjhueURQQmdLOFJlQlIxOStZdjNrelJCMCtVQlBtNTJ0WVR6ZHkxd1BWV2M1VlQKaTdoaEV5am4vR3l4TDlLYk4va1lvRG4vYVlYTXpzdnhGbWlWUSswK21rSEpJRUVSWlhlVWlCbWZTVk1BClZ0eElQRnNTZ2dBNENxMW1xNnNiblRnbm13K1hILzdpaFRzbHBBdzZCYStWOUhuYTRBcGtreERENlROdgpFNEJ3MU4zK0RiWktQbk5mdndaMGgrM1YramFHQ0w3UkdWbnoyL1kxRnI5dHY2dTdkOFFPMTBVeUdaMGYKUWR5aVRlbTRVTkd4OXd1OTFpb3pBcWhjY2UrSGhpQlFXU1JPTUFQSDFhTDhqQ01LNXFrcHVBREwxQjFFCnd5Q2crQkkwdEtISzl6VnFXTFNHRXFqZUNEOVdiQlVleWZrRjk1SWdlM09XK0IvRW84cWF1RU1kekJwMgp1S0d3TDVLWXdWeDlya0FOZE5lbDRqZXIwajR1RTlFRGROc3NBNHhYUnhhL1ZmcDl1YjVMRG9vR0hBUEkKQW5jcjBSckEyN0VBUm5qZmtkUEF6ZG9CQjVMSURWR3JYM2xCeGRUQWQ1bmtpYjFHeUtjUlhqL2E2U2lXCkp0eU5NUFJEd1NKT05iVXA0Y3FUWktOd0dmL094STd1TjJ1cHJLbGI0N1FiVFZvb2dUMlMzQmZJTmtvUgpkKzFpN1BnbFRPK2xMd2tTUnlST2tjM2taakFsc3FHalBCSkVCOG1CSDdGb2xLNDZwbHVVMTZPbHRFblAKWkRwbVM2QVkxcURYdVNWbDRXWkcrdHlNR2hLZWtCbkRHU2crS3YvNW1IcExndnB4V3NKZUVqNnNoeEFhCkk1ZmxHa1RXWUZyME40Z25iZW5ML1JLSWVDcTNqZ2w2RDNodFZqc21HWW9tUUEwcDFhdEFya1VGY29HVgpQb0I5S0tjNlN5QVlVT1NUa0NkZlpFWlNMRkZEUTEySGpHMGFrZHRHLzdST0UwZmVsV0c1UW05TFEvTU8KUGlCNUFPMFZQZlNreFNwdUEwbjF5WlZxL2REUW11S09TV0RNNWpLZkJ0blZzNVppaSs0ZXZSckl6ZTBDClZYN0RFbjFjOWZQVmwyMmRtdkJrRjR0WDVsNjFNQkR0MUh0NGNxbStqZkVGWURRa0pkMEE4dnlta2E1eApNOFM0UVp1ZngwOWlpYlNGWm8rZ2N1Z1dQRG5KT21YUFpEcCtBdW82RVJyekhFZmFtM3A3TUE4YUZreW8KWWNLbGpvTzFSWm9zQjI3ZzFYT0FJS0lXaWkwRlF4eEhqdm1lSVJwWWtyV0RXeTJBVXVNPQo=.309447b0b0ca73950c65f3f915567c6e\"></div><h1 id=\"isPasted\">Problem</h1><p>You are trying to run <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">MSCK REPAIR TABLE &lt;table-name&gt;</span> commands for the same table in parallel and are getting <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.net.SocketTimeoutException: Read timed out</span> or out of memory error messages.</p><h1>Cause</h1><p>When you try to add a large number of new partitions to a table with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">MSCK REPAIR</span> in parallel, the Hive metastore becomes a limiting factor, as it can only add a few partitions per second. The greater the number of new partitions, the more likely that a query will fail with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.net.SocketTimeoutException: Read timed out</span> error or an out of memory error message.</p><h1>Solution</h1><p>You should not attempt to run multiple <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">MSCK REPAIR TABLE &lt;table-name&gt;</span> commands in parallel.</p><p>Databricks uses multiple threads for a single <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">MSCK REPAIR</span> by default, which splits <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">createPartitions()</span> into batches. By limiting the number of partitions created, it prevents the Hive metastore from timing out or hitting an out of memory error. It also gathers the fast stats (number of files and the total size of files) in parallel, which avoids the bottleneck of listing the metastore files sequentially. This is controlled by <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.gatherFastStats</span>, which is enabled by default.</p>", "body_txt": "Problem You are trying to run MSCK REPAIR TABLE &lt;table-name&gt; commands for the same table in parallel and are getting java.net.SocketTimeoutException: Read timed out or out of memory error messages. Cause When you try to add a large number of new partitions to a table with MSCK REPAIR in parallel, the Hive metastore becomes a limiting factor, as it can only add a few partitions per second. The greater the number of new partitions, the more likely that a query will fail with a java.net.SocketTimeoutException: Read timed out error or an out of memory error message. Solution You should not attempt to run multiple MSCK REPAIR TABLE &lt;table-name&gt; commands in parallel. Databricks uses multiple threads for a single MSCK REPAIR by default, which splits createPartitions() into batches. By limiting the number of partitions created, it prevents the Hive metastore from timing out or hitting an out of memory error. It also gathers the fast stats (number of files and the total size of files) in parallel, which avoids the bottleneck of listing the metastore files sequentially. This is controlled by spark.sql.gatherFastStats, which is enabled by default.", "format": "html", "updated_at": "2022-05-23T23:11:59.676Z"}, "author": {"id": 821785, "email": "ashritha.laxminarayana@databricks.com", "name": "ashritha.laxminarayana ", "first_name": "ashritha.laxminarayana", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.362Z", "updated_at": "2022-08-05T10:37:27.313Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678288, "name": "aws"}, {"id": 2678289, "name": "azure"}, {"id": 2678290, "name": "gcp"}], "url": "https://kb.databricks.com/sql/error-run-msck-repair-table-parallel"}, {"id": 1383888, "name": "Error when downloading full results after join", "views": 11115, "accessibility": 1, "description": "If you have duplicate columns after a join, you will get an error when trying to download the full results.", "codename": "error-download-full-results", "created_at": "2022-05-23T22:49:54.569Z", "updated_at": "2022-05-23T23:06:56.168Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9NenMvc3JNaHVrcldZRjhJYmlYeDNHWlY2MSs1cXB4TEEwdjIxLzgrQUZHQ1ZXR3hWCkFiaXYvLzNENTN5ODNjbDdLT2REek9FK1lkS3VhbDJKRkkxRms5K2JubWdRN2ZYbXZ4ekZETmRkaG5jawpiQW5mM1VPS3BOaTdTbVNZRXFTTVIvaFltcnVyaEx2VTJLK3hBR0MxVlhJSXF6dXJydFg0ZC82WEE0UzkKY1hZM2kwNTM3TkQwb1o4ak9Yd0ZYdTNhbjVDamxVZDJSd3ZpbzVQSERoY3d6WTNGL2tsc3hOWVFlTXl4CkVnYVNJNm1BZ1Z0L0dCY2s3aUZvcFBac0kvTGJDYVY0QjcxQlJqUDJlKzZTU2xVeGdRMlUvTFBESWY4agpITExoZG04TjBXR3NWMVIwN0piaHRROUQwRGJJanBPdnhEbVFWNXhIWjNYcEJCSDZ0KzFUenorb0ZoaGkKV3Bjam1DS3U5U0dlcWw2Tit3UXZtOGt2bm1KTVRKTStVUWZFSnVtUjB0SzZQVFFDZzZhSEhEYnVlZU9mCm9yY2dJZkc3Y1duWVo5UEEwMzR3TkZEV0VWakt0aHlQMXBlY0xJWStzdzZGTW1PTTdLdzBkWlFleUN6Vgpob3Y2L1ZHNjZ4N3dpSzdiM2ZmSXh2cnZlbFJadDE1dDZIdm51RU0vWXdudFVCajJ2RmJ0VVRaUGFqcnQKUTJqTDNpMTc4S0VqYzZrQStIaGpybE5BR0syWTRBYUNVL1lYeHkyUENheWxIQlJoalRMcmhVcUtVcmU3CjRETGtUTEdMMWErZy92WFRkcHRzU2U2Tk5Fam5waFpuUTRyMkorYVFGK3VIc3lGNGo5SFZ1bm40aUtHdAoyVmN5ZThkTHEvVHdFRFNhb25FVlAvRnhmTXhIQ2JWc0RPTHBwMzdjVUx5clN3THRaM0gzZG9TWXNMN3AKMDBCOFZ1ZTZnUmMrVnVaOWpsL3k1bmc2NXBJMEQwLzQyR1JHa0hLTW5tb0ZsVDNMODVxS0xkN3BNbENXCk9QWk82Zzh5a0FLZnFlR0tFNFp4UkoxYnM0T0VHUEt0MUc2OHkzbnVUTXRtYXZ6dDZWVkg1V3ZyT0pFNApKKzdqZ05zSWhnK1Jqd2RZbzRwWWtWM2dYc2Z1dURyOHc0Q0QrVk1Wa1F3VE5RbUcybXVZRWRNYnlGNTIKZURvT2ZKZXRRaUdIR2UraElVaXBaYkFlNGlVRERWcXNPMGxIa3RvYmNvVkMzYlJSSDhXZXNBSHd1aHE0ClFHMkg5bXF4OFZNWlg3R2RRbzRxRGsvUlZSekthYWgvTkNmM1BDdnR6UFVySGdCTE9odVNNK01PVlpSYwpnSThtandpVUFxajI0UGdNT1BWcytCVDU2d2dpYlh4ZjBWeUZ3UENKTDZOcVk5OXUrSEtBUkFFT1FnTTgKaTY5ajhOZDd3MVF1SEtscVdVdGF3M0oyUllLTlZBPT0K.0e10b72dc1eaf3e65615173ec11752db\"></div><h1 id=\"isPasted\">Problem</h1><p>You are working with two tables in a notebook. You perform a join. You can preview the output, but when you try to <strong>Download full results</strong> you get an error.</p><pre>Error in SQL statement: AnalysisException: Found duplicate column(s) when inserting into dbfs:/databricks-results/</pre><h2>Reproduce error</h2><ol>\n<li>Create two tables.<pre>%python\r\n\r\nfrom pyspark.sql.functions import *\r\n\r\ndf = spark.range(12000)\r\ndf = df.withColumn(\"col2\",lit(\"test\"))\r\ndf.createOrReplaceTempView(\"table1\")\r\n\r\ndf1 = spark.range(5)\r\ndf1.createOrReplaceTempView(\"table2\")</pre>\n</li>\n<li>Perform left outer join on the tables.<pre>%sql\r\n\r\nselect * from table1 t1 left join table2 t2 on t1.id = t2.id</pre>\n</li>\n<li>Click <strong>Download preview</strong>. A CSV file downloads.</li>\n<li>Click <strong>Download full results</strong>. An error is generated.</li>\n</ol><h1>Cause</h1><ul>\n<li>\n<strong>Download preview</strong> works because this is a frontend only operation that runs in the browser. No constraints are checked and only 1000 rows are included in the CSV file.</li>\n<li>\n<strong>Download full results</strong> re-executes the query in Apache Spark and writes the CSV file internally. The error occurs when duplicate columns are found after a join operation.</li>\n</ul><h1>Solution</h1><h2>Option 1</h2><p>If you select all the required columns, and avoid duplicate columns after the join operation, you will not get the error and can download the full result.</p><pre>%sql\r\n\r\nselect t1.id, t1.col2 from table1 t1 left join table2 t2 on t1.id = t2.id</pre><h2>Option 2</h2><p>You can use DataFrames to prevent duplicated columns. If there are no duplicated columns after the join operation, you will not get the error and can download the full result.</p><pre>%python\r\n\r\nresult_df = df.join(df1, [\"id\"],\"left\")\r\ndisplay(result_df)</pre><p><br></p>", "body_txt": "Problem You are working with two tables in a notebook. You perform a join. You can preview the output, but when you try to Download full results you get an error. Error in SQL statement: AnalysisException: Found duplicate column(s) when inserting into dbfs:/databricks-results/ Reproduce error Create two tables.%python from pyspark.sql.functions import * df = spark.range(12000) df = df.withColumn(\"col2\",lit(\"test\")) df.createOrReplaceTempView(\"table1\") df1 = spark.range(5) df1.createOrReplaceTempView(\"table2\") Perform left outer join on the tables.%sql select * from table1 t1 left join table2 t2 on t1.id = t2.id Click Download preview. A CSV file downloads.\nClick Download full results. An error is generated. Cause Download preview works because this is a frontend only operation that runs in the browser. No constraints are checked and only 1000 rows are included in the CSV file. Download full results re-executes the query in Apache Spark and writes the CSV file internally. The error occurs when duplicate columns are found after a join operation. Solution Option 1 If you select all the required columns, and avoid duplicate columns after the join operation, you will not get the error and can download the full result. %sql select t1.id, t1.col2 from table1 t1 left join table2 t2 on t1.id = t2.id Option 2 You can use DataFrames to prevent duplicated columns. If there are no duplicated columns after the join operation, you will not get the error and can download the full result. %python result_df = df.join(df1, [\"id\"],\"left\") display(result_df)", "format": "html", "updated_at": "2022-05-23T23:06:56.163Z"}, "author": {"id": 831506, "email": "manjunath.swamy@databricks.com", "name": "manjunath.swamy ", "first_name": "manjunath.swamy", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T05:55:43.385Z", "updated_at": "2023-03-24T10:04:17.174Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678276, "name": "aws"}, {"id": 2678277, "name": "azure"}, {"id": 2678278, "name": "gcp"}], "url": "https://kb.databricks.com/sql/error-download-full-results"}, {"id": 1383887, "name": "Error in SQL statement: AnalysisException: Table or view not found", "views": 17438, "accessibility": 1, "description": "Learn how to resolve the AnalysisException SQL error \"Table or view not found\".", "codename": "global-temp-view-not-found", "created_at": "2022-05-23T22:41:15.604Z", "updated_at": "2022-05-23T22:49:24.566Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStCTUx4YzkrNVhZdTduQnJZTDlQUEJrd0pxYkIyZmsyZUFQRVpIS1NTVEtRdWFvL2FRCmJlTWkrdlFabGNvSTFSenFVcUJsU09EdVFLZ0c0NUpma0I1aVRRYng5UW0vby94WnJKdTgxR3F0VURlOApPVjlTbzFEKzByRVk5OWc4L3VxRjk0Z3NvMjczSjU5SkM0MnNkQkkyUnRLTWJYcGRLcFU3M0ROV2w2OGwKR0VybXdzY0NWY3hqemZUOWphR3VleXRlK2dKYjRJZWhsLzY5dlIyK21zb09sU1pRM2ZCT0tqdXNyNzlaCk83aEhaK2dEaHBvcjV6d2lTVTVES0FpemMzV05HMlBUNUhWZ2hpTlRWZndObkhDNDMvdTJKZFZWOFZvNgp2a3ZXbENaM213aDVPZWNWcU9paENVVCtwckEyeW9Tb2FjcFFnbDNicHVSc05DQ2hFOGNKTEYvWGtRR2cKdHJ1OVIvTUFFNThhS21LcUE3Mk1mV3YzalVnSVQwZVpHV3NaS01QemY1OUZuVnBrMjNWbm1ZZjV0MFBWCmwwK3EzUTBuVEMxeURlMDNBVjhCTUt3OE5QSFlGdE43eHhpcjZEZVBoR2NSdDBDemFkc21XRk5FZTI4ZApxMjdaV2h3VW1ZeC8vWGtBaVpNc1UwWFJGZEJXVTFBSlFwenZmSjRhL3pQNjRjNCtaZmtlREFmOVZKNi8KOG9RUnhzSU5aa0VDNzh4eC9zVFZxOUtyNHZBRUZvWXVvbFFmcmZYcGRBczUzUzhoaWJON25UR3NOVlQ0ClloRDN6UWdYL1dRWmo0R0NOTXhOcGgyL2wwQ0RYZGRUOGZ0T3ljTmR6V3ZoZmpobzJUZEFlQ2YzK3lKZgpKQzhzcEdmdjQvREJSN0pLV0RMaWRoeWdDUmlBazFWc1dZcFJ5VW8yOUpWaDJwU2VXWjhTREtxZVVaalkKb21EMC9lRkdQbzhpQkhZQnFsQ1FxcnVTYkF4Z1lYN25NWjJ0cWFsaVRrZTlrdWtBM0VLd1VpQU5kNlVFClVmdkU0K3hkaFdMTjdNL0dyQjJ3YytScU80bkZ1RlpoSTRNbG1zZ2JtOGN0ZDZJZXQwUEpqUXVhMFhBZgp5UUFZNWtVa29YdkJLRmpVTkV0R1R6UHZZUmlIaW05ZGJ0WVgzNjI5K0tYbTl4UkkwUDdKZ1dXNk1GdFMKNU01VUxQY2hEM0JIdFIyQ3BLTmc1QUUybFBiN1JBWVN6MUlWMzF5by9YRnljU0NqV05Xb2J5U0gydm1aCg==.558cc241285390e7641482ec2301f45d\"></div><h1 id=\"isPasted\">Problem</h1><p>When you try to query a table or view, you get this error:</p><pre>AnalysisException:Table or view not found when trying to query a global temp view</pre><h1>Cause</h1><p>You typically create global <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">temp</span> views so they can be accessed from different sessions and kept alive until the application ends. You can create a global <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">temp</span> view with the following statement:</p><pre>%scala\r\n\r\ndf.createOrReplaceGlobalTempView(\"&lt;global-view-name&gt;\")</pre><p>Here, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">df</span> is the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DataFrame</span>. Another way to create the view is with:</p><pre>%sql\r\n\r\nCREATE GLOBAL TEMP VIEW &lt;global-view-name&gt;</pre><p>All global temporary views are tied to a system temporary database named <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">global_temp</span>. If you query the global table or view without explicitly mentioning the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">global_temp</span> database, then the error occurs.</p><h1>Solution</h1><p>Always use the qualified table name with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">global_temp</span> database, so that you can query the global view data successfully.</p><p>For example:</p><pre>%sql\r\n\r\nselect * from global_temp.&lt;global-view-name&gt;;</pre><p><br></p>", "body_txt": "Problem When you try to query a table or view, you get this error: AnalysisException:Table or view not found when trying to query a global temp view Cause You typically create global temp views so they can be accessed from different sessions and kept alive until the application ends. You can create a global temp view with the following statement: %scala df.createOrReplaceGlobalTempView(\"&lt;global-view-name&gt;\") Here, df is the DataFrame. Another way to create the view is with: %sql CREATE GLOBAL TEMP VIEW &lt;global-view-name&gt; All global temporary views are tied to a system temporary database named global_temp. If you query the global table or view without explicitly mentioning the global_temp database, then the error occurs. Solution Always use the qualified table name with the global_temp database, so that you can query the global view data successfully. For example: %sql select * from global_temp.&lt;global-view-name&gt;;", "format": "html", "updated_at": "2022-05-23T22:49:24.565Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:30.391Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678266, "name": "aws"}, {"id": 2678267, "name": "azure"}, {"id": 2678268, "name": "gcp"}], "url": "https://kb.databricks.com/sql/global-temp-view-not-found"}, {"id": 1383875, "name": "Generate unique increasing numeric values", "views": 33306, "accessibility": 1, "description": "Use Apache Spark functions to generate unique and increasing numbers in a column in a table in a file or DataFrame.", "codename": "gen-unique-increasing-values", "created_at": "2022-05-23T22:22:03.153Z", "updated_at": "2022-05-23T22:41:00.945Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9ocDZCWDJmcXN0SXdMbUV4OW9yNjg5MCtnMG5pelJIWGo5eDRWd0V4S0dqNnBCUGNhCkxBQm9mOEd2VjNrNEJZL1FzZ01jYmdCWHQ3S0dxeEVQWC9NK3ZiczE0YTJXUEZBMU1ZNlU3R2tlL05sMQpzUW9jQkRldURVUm5KZVZySkp3Q2Ywc0dRdFZpMmNOS01RRzhuWWdqM3lNdVpUYnJFWTU2aGpMMWFjbFUKWlpEc0liN1k4SU5PNlRyRUZEdXVsWFBIRWFYYkdLSG5VYVo2QWxodkYrYW4wWS9OVm9DclM1WXZ3TjNUCmVOaUlXd0ZQaGt1QnZiMXNTYnVJVzZLeW9FNGgzVHpnVitGeGxtOHp6SkJXTmtVZC9jTlFTWjd6aHhkcwp0RDhHYlFhaEFXczgrOUFPVHR2d3Q2ejlUcUkyS2dLdzdHMzNxWmQvNzlVYXRjenB3U3dmbmRTeFFNdXMKUFVuQi9zMThwSlhrUDZ4SG5oOEl5ZzVLMkFBTFArU2VJLytHSW1qcHZiL0hwWFVwK3F0cWNWWEIvU29YCk43VWdITHFOcmsyb3NBZnZoSHVvdlRBVXhWN3p5c1Y5YjdGMFYrVk5HRlZuRE1zMWJKUW5lbUNFdDFScwpydlRCK2ttZGZ5bTlqYWh1dmhta1Btb3Ura1lVMVVESzNmODlDb2Z1Vktpc1kyRUJLRDRmTWthWHhMbFUKL2J3VDV5VHN2S1NuWXFQczVGaHhCU1h1VGRYeGhKM2p3THdQVnlKSmtSSzlrSHBMZzhscTFwVERVUjdpCjltb2M2ZVZqbUk5NGtCRk1rbE5NdHlIaEkyWUNmK0Y1cGhxTHM3Z0dZRk54R0VqZi8rd3dvNnp4OVprSAowWFRiNlByUSsyd3VGWmNTTWZ1eEZXSkZkWXFOd1AvTEZDeG5sYTlpWkdNK2d4UktTS1IyaUZnN1NObjcKenE2Z2ZqV1ZCM3ZydFpWR1FQUzZZaXpLaWJwVHhDMXdkZjFnVW1Qb1RLRWs0S2tQaUFOVEhyTlV2ZG1NCnRuOE9hOHh0YU5NaCt3Nm9uR1l3Q05BTlFOZW1OTG9nWkhHbzAvdHh5UitmS1pyWkdHL1lzMzBFZWthdwpQMVNPdkoyUFV3ZGI2cjNvTm1naU1RVGpNTW45UjBDZ2ZQUVdRYnQ0aUpCVzVEa2lvUkdGZkpQR1NnWUIKK0R2R1NNbThZK3h1bS9wVHU1U2hQRW1vSjVyYXRoeHJwMkw0ZEhtcXo4VWFSYlZ0WXJWQWd6QzVmSGVYCg==.29383e6f3070bbdbe9ab2755fe5cc2b0\"></div><p id=\"isPasted\">This article shows you how to use Apache Spark functions to generate unique increasing numeric values in a column.</p><p>We review three different methods to use. You should select the method that works best with your use case.</p><h1>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">zipWithIndex()</span> in a Resilient Distributed Dataset (RDD)</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">zipWithIndex()</span> function is only available within RDDs. You cannot use it directly on a DataFrame.</p><p>Convert your DataFrame to a RDD, apply <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">zipWithIndex()</span> to your data, and then convert the RDD back to a DataFrame.</p><p>We are going to use the following example code to add unique id numbers to a basic table with two entries.</p><pre>%python\r\n\r\ndf = spark.createDataFrame(\r\n\u00a0 \u00a0 [\r\n\u00a0 \u00a0 \u00a0 \u00a0 ('Alice','10'),('Susan','12')\r\n\u00a0 \u00a0 ],\r\n\u00a0 \u00a0 ['Name','Age']\r\n)\r\n\r\n\r\ndf1=df.rdd.zipWithIndex().toDF()\r\ndf2=df1.select(col(\"_1.*\"),col(\"_2\").alias('increasing_id'))\r\ndf2.show()</pre><p>Run the example code and we get the following results:</p><pre>+-----+---+-------------+\r\n| Name|Age|increasing_id|\r\n+-----+---+-------------+\r\n|Alice| 10| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a00|\r\n|Susan| 12| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01|\r\n+-----+---+-------------+</pre><h1>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">monotonically_increasing_id()</span> for unique, but not consecutive numbers</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">monotonically_increasing_id()</span> function generates monotonically increasing 64-bit integers.</p><p>The generated id numbers are guaranteed to be increasing and unique, but they are not guaranteed to be consecutive.</p><p>We are going to use the following example code to add monotonically increasing id numbers to a basic table with two entries.</p><pre>%python\r\n\r\nfrom pyspark.sql.functions import *\r\n\r\ndf_with_increasing_id = df.withColumn(\"monotonically_increasing_id\", monotonically_increasing_id())\r\ndf_with_increasing_id.show()</pre><p>Run the example code and we get the following results:</p><pre>+-----+---+---------------------------+\r\n| Name|Age|monotonically_increasing_id|\r\n+-----+---+---------------------------+\r\n|Alice| 10| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8589934592|\r\n|Susan| 12| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a025769803776|\r\n+-----+---+---------------------------+</pre><h1>Combine <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">monotonically_increasing_id()</span> with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">row_number()</span> for two columns</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">row_number()</span> function generates numbers that are consecutive.</p><p>Combine this with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">monotonically_increasing_id()</span> to generate two columns of numbers that can be used to identify data entries.</p><p>We are going to use the following example code to add monotonically increasing id numbers and row numbers to a basic table with two entries.</p><pre>%python\r\n\r\nfrom pyspark.sql.functions import *\r\nfrom pyspark.sql.window import *\r\n\r\nwindow = Window.orderBy(col('monotonically_increasing_id'))\r\ndf_with_consecutive_increasing_id = df_with_increasing_id.withColumn('increasing_id', row_number().over(window))\r\ndf_with_consecutive_increasing_id.show()</pre><p>Run the example code and we get the following results:</p><pre>+-----+---+---------------------------+-------------+\r\n| Name|Age|monotonically_increasing_id|increasing_id|\r\n+-----+---+---------------------------+-------------+\r\n|Alice| 10| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8589934592| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01|\r\n|Susan| 12| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a025769803776| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02|\r\n+-----+---+---------------------------+-------------+</pre><p>If you need to increment based on the last updated maximum value, you can define a previous maximum value and then start counting from there.</p><p>We\u2019re going to build on the example code that we just ran.</p><p>First, we need to define the value of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">previous_max_value</span>. You would normally do this by fetching the value from your existing output table. For this example, we are going to define it as 1000.</p><pre>%python\r\n\r\nprevious_max_value = 1000\r\ndf_with_consecutive_increasing_id.withColumn(\"cnsecutiv_increase\", col(\"increasing_id\") + lit(previous_max_value)).show()</pre><p>When this is combined with the previous example code and run, we get the following results:</p><pre>+-----+---+---------------------------+-------------+------------------+\r\n| Name|Age|monotonically_increasing_id|increasing_id|cnsecutiv_increase|\r\n+-----+---+---------------------------+-------------+------------------+\r\n|Alice| 10| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8589934592| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01001|\r\n|Susan| 12| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a025769803776| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01002|\r\n+-----+---+---------------------------+-------------+------------------+</pre><p><br></p>", "body_txt": "This article shows you how to use Apache Spark functions to generate unique increasing numeric values in a column. We review three different methods to use. You should select the method that works best with your use case. Use zipWithIndex() in a Resilient Distributed Dataset (RDD) The zipWithIndex() function is only available within RDDs. You cannot use it directly on a DataFrame. Convert your DataFrame to a RDD, apply zipWithIndex() to your data, and then convert the RDD back to a DataFrame. We are going to use the following example code to add unique id numbers to a basic table with two entries. %python df = spark.createDataFrame( \u00a0 \u00a0 [ \u00a0 \u00a0 \u00a0 \u00a0 ('Alice','10'),('Susan','12') \u00a0 \u00a0 ], \u00a0 \u00a0 ['Name','Age'] ) df1=df.rdd.zipWithIndex().toDF() df2=df1.select(col(\"_1.*\"),col(\"_2\").alias('increasing_id')) df2.show() Run the example code and we get the following results: +-----+---+-------------+ | Name|Age|increasing_id| +-----+---+-------------+ |Alice| 10| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a00| |Susan| 12| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01| +-----+---+-------------+ Use monotonically_increasing_id() for unique, but not consecutive numbers The monotonically_increasing_id() function generates monotonically increasing 64-bit integers. The generated id numbers are guaranteed to be increasing and unique, but they are not guaranteed to be consecutive. We are going to use the following example code to add monotonically increasing id numbers to a basic table with two entries. %python from pyspark.sql.functions import * df_with_increasing_id = df.withColumn(\"monotonically_increasing_id\", monotonically_increasing_id()) df_with_increasing_id.show() Run the example code and we get the following results: +-----+---+---------------------------+ | Name|Age|monotonically_increasing_id| +-----+---+---------------------------+ |Alice| 10| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8589934592| |Susan| 12| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a025769803776| +-----+---+---------------------------+ Combine monotonically_increasing_id() with row_number() for two columns The row_number() function generates numbers that are consecutive. Combine this with monotonically_increasing_id() to generate two columns of numbers that can be used to identify data entries. We are going to use the following example code to add monotonically increasing id numbers and row numbers to a basic table with two entries. %python from pyspark.sql.functions import * from pyspark.sql.window import * window = Window.orderBy(col('monotonically_increasing_id')) df_with_consecutive_increasing_id = df_with_increasing_id.withColumn('increasing_id', row_number().over(window)) df_with_consecutive_increasing_id.show() Run the example code and we get the following results: +-----+---+---------------------------+-------------+ | Name|Age|monotonically_increasing_id|increasing_id| +-----+---+---------------------------+-------------+ |Alice| 10| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8589934592| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01| |Susan| 12| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a025769803776| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02| +-----+---+---------------------------+-------------+ If you need to increment based on the last updated maximum value, you can define a previous maximum value and then start counting from there. We\u2019re going to build on the example code that we just ran. First, we need to define the value of previous_max_value. You would normally do this by fetching the value from your existing output table. For this example, we are going to define it as 1000. %python previous_max_value = 1000 df_with_consecutive_increasing_id.withColumn(\"cnsecutiv_increase\", col(\"increasing_id\") + lit(previous_max_value)).show() When this is combined with the previous example code and run, we get the following results: +-----+---+---------------------------+-------------+------------------+ | Name|Age|monotonically_increasing_id|increasing_id|cnsecutiv_increase| +-----+---+---------------------------+-------------+------------------+ |Alice| 10| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8589934592| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01001| |Susan| 12| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a025769803776| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02| \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01002| +-----+---+---------------------------+-------------+------------------+", "format": "html", "updated_at": "2022-05-23T22:41:00.943Z"}, "author": {"id": 791511, "email": "ram.sankarasubramanian@databricks.com", "name": "ram.sankarasubramanian ", "first_name": "ram.sankarasubramanian", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T01:51:47.575Z", "updated_at": "2023-04-10T05:38:44.201Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678260, "name": "aws"}, {"id": 2678261, "name": "azure"}, {"id": 2678262, "name": "gcp"}], "url": "https://kb.databricks.com/sql/gen-unique-increasing-values"}, {"id": 1383868, "name": "Duplicate columns in the metadata error", "views": 11888, "accessibility": 1, "description": "Spark job fails while processing a Delta table with org.apache.spark.sql.AnalysisException Found duplicate column(s) in the metadata error.", "codename": "dupe-column-in-metadata", "created_at": "2022-05-23T22:12:52.910Z", "updated_at": "2022-05-23T22:16:07.403Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS83UmgxTkZDdFNRNzVjN3ZRbTlmNzJ4K0dHdGlzeUlHL2lFZlBOUFE1ZzJUekJpalN1CkNlUDRTTmRleE9YWWpwd2EzalBDUS91eEZicU9wOHZjSVBscUllcEZoTThLc0E3cE94RVRBamQwY21qagpidXNMWFZ0ek1yN3EwNlkwWFl4RzFrNWJUTGVjZ0tKSnlPZW80a3g1djcrVnRXUzVMT21FcUk4MmNPVFIKUkdPSDRYT2IzU09IQmhBSEZRWThSRUlSQXRsMUt6MjduSlJIWVFNLy95aUppL1BCVktnV05XTlBma092ClNrZEY0QTd5bGhkV0s1UCswdnRxVStHRklNOWt1WVQ2QmYxWEN5TmJrekE0djJtWklCYjluMWRCSEU0NApRcTdBTzV1bFJRczZ6emNVMmpIY2pOaVRVaWpKdnVpa0U2MHNJTFhCQk04OGZLRHVuYmVMSG5rMmpHcUYKWUVONmJKTlJoOFI0anN2WWlQYU8vcENJWnEydExLem9pVVcyOEtHZkVRQnlIT2JTOEJUVWJJcjFHOGNZCmJseG9VRGFYOWQ0cHVZRjliR29PL2gyYkx3b0YxbU01MG9QM2RLaVpuRFZYNE81OFpGQksvT3duS3pvTQpLZStXUGJPZ2pZeVVCSVk0MkxrZXp5aURJZWtjOWRnYWYvSjZ1MFQ5cVcrQ2x5bVRUb3k0c25IVi9mYk4KQlpjcjhjZUptVVJEL1lhZGM3RndqK1BwVjBJTW84c1M5Z0Y0YWRQOE5NaFk2dFFHYU16UjFQM2VQRTYyCkhJaGU5aTBUTUYvc0p1R1pNVExoVlZKZHBDS3k1bDVhbUs1RHRiMm9sTTF2emdaM0RqbWQxNnhTZVE5dAp2NFVTNlIzb0dKRW5laGp1NnF0cERLci9BQzZSa2FoUC85enMxQnFGWHptM0t0UlFQV0phRHVMenFQTXIKWDQwT28vZ1VONzArZHFkbFVBSzZqcG9KUHRWVzNiclpVYTJKUGg5SDk4RGJKU2NwRTBVNENXRlFMcmxUCkdNSG9CQXlHMk5YaHJ1czFoL0pYYTJZbWY5Wm0waDFBMmVaT1gwTW9VdlNZQ2k1U2UvRkRrMGRISkxGYworZEh1b1FTY3E5R3VZL1I1SkNQOTltTzNvNFcxdlRlU1hycz0K.0580af45c0cffffab9fd3826ec59aab6\"></div><h1 id=\"isPasted\">Problem</h1><p>Your Apache Spark job is processing a Delta table when the job fails with an error message.</p><pre>org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the metadata update: col1, col2...</pre><h1>Cause</h1><p>There are duplicate column names in the Delta table. Column names that differ only by case are considered duplicate.</p><p>Delta Lake is case preserving, but case insensitive, when storing a schema.</p><p>Parquet is case sensitive when storing and returning column information.</p><p>Spark can be case sensitive, but it is case insensitive by default.</p><p>In order to avoid potential data corruption or data loss, duplicate column names are not allowed.</p><h1>Solution</h1><p>Delta tables must not contain duplicate column names.</p><p>Ensure that all column names are unique.</p>", "body_txt": "Problem Your Apache Spark job is processing a Delta table when the job fails with an error message. org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the metadata update: col1, col2... Cause There are duplicate column names in the Delta table. Column names that differ only by case are considered duplicate. Delta Lake is case preserving, but case insensitive, when storing a schema. Parquet is case sensitive when storing and returning column information. Spark can be case sensitive, but it is case insensitive by default. In order to avoid potential data corruption or data loss, duplicate column names are not allowed. Solution Delta tables must not contain duplicate column names. Ensure that all column names are unique.", "format": "html", "updated_at": "2022-05-23T22:16:07.401Z"}, "author": {"id": 867531, "email": "vikas.yadav@databricks.com", "name": "vikas.yadav ", "first_name": "vikas.yadav", "last_name": "", "role_id": "draft_writer", "created_at": "2022-05-10T10:04:06.319Z", "updated_at": "2022-05-11T20:22:24.392Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678224, "name": "aws"}, {"id": 2678225, "name": "azure"}, {"id": 2678226, "name": "sql"}], "url": "https://kb.databricks.com/sql/dupe-column-in-metadata"}, {"id": 1383834, "name": "Disable broadcast when query plan has BroadcastNestedLoopJoin", "views": 13263, "accessibility": 1, "description": "How to disable broadcast when the query plan has BroadcastNestedLoopJoin.", "codename": "disable-broadcast-when-broadcastnestedloopjoin", "created_at": "2022-05-23T21:35:33.930Z", "updated_at": "2022-05-23T22:12:41.806Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlUc2ZQdnloQlUxZFVremxHQnk2eTNoZktNY3ZlNk1Icy8rMGgzMEtwdGZwUXpteWFCCnJVaVBqN0lCRzBldGxkY1BqUmE1TXdwMU5sOUd2TU5EcWlxNXZKU2tCMGhQTFlZd01wR05lZ2VubGxRWQpjSjVkeG1QT2ZscFhYODZobU5yQVZDVGxjL2U1Y0FCMkpidmlZNEJHVTRTd0xwcHV1SEpNMm0wY2cvUGcKRHhLUTFaSEdQU3VDcXBPVjliUkwrbVRUNXRqRDVlM3dnQ3VUQXdGalNxUmg5WWtjYjdSSUdWUVpSVHB3ClBZMldFbTlyRVFMb2V1eEJxazdEazkram85azlYZ3QxclFTZkx0Z1ZCRjJEdFRleTRjd1V5cWZIZ0drZApxLzhvM2txdzZvNGpBMjN2OGN5RlNjQmI5eEF6VHBPaFVKRUhUclBzZ1A0dzMzeHpRSTU0MFF6d3BCdVAKK3UwOElhQldwOWdqSW1XVzI1RXJmdG12MzZqd3I4R3FqRHd6YnJSM3BFV3ZYbGk2c1E4ZUhaKy9CRS9TCk5VUTVrL3BLTDIyZlkydU5udGhxd2NpNDduOFVSUGhadDhnT2JOSnhEL21FRFlvSmt4YjcyQW5NTHpVWgpFakwzZ3lrQ3BTbmt2UTZaM1Z6SmZTenQwWnhQRzFyNnMwbFNIK0FMNXFucjZxTzBPVGdiWlZBZVJuanoKY0JWZ05md2tJeDd1NDBITFRoS25HWHA4U3lkNktYMFNCa0VNRGg3aDcyNFpuTEZNa2xSaThSVkRoUkJ3CjdBTmtWa0dvZzc0RUtmYnRybmpVWWQweS9HYUVDYUpuUk5DbnZDVmoxc2dPeE10TWxtZHlMczdZSFBjNgpWV2Qyd0NnM2lSZ2pUYkNLM0RnTUhSS3FHZDdEbE5FTi9CUnhzc01CbHFFVDhXZE5FZTlEblN5OStJVm4KL3F5UW15SzV1Nk9xcTZ1aE45VjlyUU1RY0lBelNvc1VMYVgrKy81dytqWC9Idk1IcXgxbTBFUDlXbVBFCkgvR3NuS2F6VGdvRlZmMWxKYlJ3Y2kwUmh4V2piQitlSmdoU2JoQStTek5uTG5yZXpiL3cxN3lLdGxhRQpPWWZwdjI2c08xWWxCbWFVQW1JQThwdUl5emlpdDErYkhIcVhla1ZCaUtqUVZLcklVOW9tSTdhanplWHQKNFhvWGxYci9sbXNwWmd1cVlneXpreWg2eTJ3WTFERTNNU0lIU1I0PQo=.dc1f354720584e584522bb20d797a88b\"></div><p id=\"isPasted\">This article explains how to disable broadcast when the query plan has <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastNestedLoopJoin</span> in the physical plan.</p><p>You expect the broadcast to stop after you disable the broadcast threshold, by setting <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.autoBroadcastJoinThreshold</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">-1</span>, but Apache Spark tries to broadcast the bigger table and fails with a broadcast error.</p><p>This behavior is NOT a bug, however it can be unexpected. We are going to review the expected behavior and provide a mitigation option for this issue.</p><h1>Create tables</h1><p>Start by creating two tables, one with null values <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">table_withNull</span> and the other without null values <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">tblA_NoNull</span>.</p><pre>%sql\r\n\r\nsql(\"SELECT id FROM RANGE(10)\").write.mode(\"overwrite\").saveAsTable(\"tblA_NoNull\")\r\nsql(\"SELECT id FROM RANGE(50) UNION SELECT NULL\").write.mode(\"overwrite\").saveAsTable(\"table_withNull\")</pre><h1>Attempt to disable broadcast</h1><p>We attempt to disable broadcast by setting <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.autoBroadcastJoinThreshold</span> for the query, which has a sub-query with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">in</span> clause.</p><pre>%sql\r\n\r\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\r\nsql(\"select * from table_withNull where id not in (select id from tblA_NoNull)\").explain(true)</pre><p>If you review the query plan, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastNestedLoopJoin</span> is the last possible fallback in this situation. It appears even after attempting to disable the broadcast.</p><pre>== Physical Plan ==\r\n*(2) BroadcastNestedLoopJoin BuildRight, LeftAnti, ((id#2482L = id#2483L) || isnull((id#2482L = id#2483L)))\r\n:- *(2) FileScan parquet default.table_withnull[id#2482L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/table_withnull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\r\n+- BroadcastExchange IdentityBroadcastMode, [id=#2586]\r\n\u00a0 \u00a0+- *(1) FileScan parquet default.tbla_nonull[id#2483L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/tbla_nonull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;</pre><p>If the data being processed is large enough, this results in broadcast errors when Spark attempts to broadcast the table.</p><h1>Rewrite query using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">not exists</span> instead of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">in</span>\n</h1><p>You can resolve the issue by rewriting the query with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">not exists</span> instead of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">in</span>.</p><pre>%sql\r\n\r\n// It can be rewritten into a NOT EXISTS, which will become a regular join:\r\nsql(\"select * from table_withNull where not exists (select 1 from tblA_NoNull where table_withNull.id = tblA_NoNull.id)\").explain(true)</pre><p>By using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">not exists</span>, the query runs with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SortMergeJoin</span>.</p><pre>== Physical Plan ==\r\nSortMergeJoin [id#2482L], [id#2483L], LeftAnti\r\n:- Sort [id#2482L ASC NULLS FIRST], false, 0\r\n: \u00a0+- Exchange hashpartitioning(id#2482L, 200), [id=#2653]\r\n: \u00a0 \u00a0 +- *(1) FileScan parquet default.table_withnull[id#2482L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/table_withnull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;\r\n+- Sort [id#2483L ASC NULLS FIRST], false, 0\r\n\u00a0 \u00a0+- Exchange hashpartitioning(id#2483L, 200), [id=#2656]\r\n\u00a0 \u00a0 \u00a0 +- *(2) Project [id#2483L]\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0+- *(2) Filter isnotnull(id#2483L)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 +- *(2) FileScan parquet default.tbla_nonull[id#2483L] Batched: true, DataFilters: [isnotnull(id#2483L)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/tbla_nonull], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct&lt;id:bigint&gt;</pre><h1>Explanation</h1><p>Spark doesn\u2019t do this automatically, because Spark and SQL have slightly different semantics for null handling.</p><p>In SQL, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">not in</span> means that if there is any null value in the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">not in</span> values, the result is empty. This is why it can only be executed with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastNestedLoopJoin</span>. All <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">not in</span> values must be known in order to ensure there is no null value in the set.</p><h1>Example notebook</h1><p>This notebook has a complete example, showing why Spark does not automatically switch <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastNestedLoopJoin</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SortMergeJoin</span>.</p><p>Review the <a href=\"https://docs.databricks.com/_static/notebooks/kb/sql/broadcastnestedloopjoin-example.html\" title=\"\" id=\"\" target=\"_blank\" rel=\"noopener noreferrer\"></a><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\"><a href=\"https://docs.databricks.com/_static/notebooks/kb/sql/broadcastnestedloopjoin-example.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">BroadcastNestedLoopJoin</a></span><a href=\"https://docs.databricks.com/_static/notebooks/kb/sql/broadcastnestedloopjoin-example.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">\u00a0example notebook</a>.</p><p><br></p><p><br></p>", "body_txt": "This article explains how to disable broadcast when the query plan has BroadcastNestedLoopJoin in the physical plan. You expect the broadcast to stop after you disable the broadcast threshold, by setting spark.sql.autoBroadcastJoinThreshold to -1, but Apache Spark tries to broadcast the bigger table and fails with a broadcast error. This behavior is NOT a bug, however it can be unexpected. We are going to review the expected behavior and provide a mitigation option for this issue. Create tables Start by creating two tables, one with null values table_withNull and the other without null values tblA_NoNull. %sql sql(\"SELECT id FROM RANGE(10)\").write.mode(\"overwrite\").saveAsTable(\"tblA_NoNull\") sql(\"SELECT id FROM RANGE(50) UNION SELECT NULL\").write.mode(\"overwrite\").saveAsTable(\"table_withNull\") Attempt to disable broadcast We attempt to disable broadcast by setting spark.sql.autoBroadcastJoinThreshold for the query, which has a sub-query with an in clause. %sql spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) sql(\"select * from table_withNull where id not in (select id from tblA_NoNull)\").explain(true) If you review the query plan, BroadcastNestedLoopJoin is the last possible fallback in this situation. It appears even after attempting to disable the broadcast. == Physical Plan == *(2) BroadcastNestedLoopJoin BuildRight, LeftAnti, ((id#2482L = id#2483L) || isnull((id#2482L = id#2483L))) :- *(2) FileScan parquet default.table_withnull[id#2482L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/table_withnull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt; +- BroadcastExchange IdentityBroadcastMode, [id=#2586] \u00a0 \u00a0+- *(1) FileScan parquet default.tbla_nonull[id#2483L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/tbla_nonull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt; If the data being processed is large enough, this results in broadcast errors when Spark attempts to broadcast the table. Rewrite query using not exists instead of in You can resolve the issue by rewriting the query with not exists instead of in. %sql // It can be rewritten into a NOT EXISTS, which will become a regular join: sql(\"select * from table_withNull where not exists (select 1 from tblA_NoNull where table_withNull.id = tblA_NoNull.id)\").explain(true) By using not exists, the query runs with SortMergeJoin. == Physical Plan == SortMergeJoin [id#2482L], [id#2483L], LeftAnti :- Sort [id#2482L ASC NULLS FIRST], false, 0 : \u00a0+- Exchange hashpartitioning(id#2482L, 200), [id=#2653] : \u00a0 \u00a0 +- *(1) FileScan parquet default.table_withnull[id#2482L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/table_withnull], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt; +- Sort [id#2483L ASC NULLS FIRST], false, 0 \u00a0 \u00a0+- Exchange hashpartitioning(id#2483L, 200), [id=#2656] \u00a0 \u00a0 \u00a0 +- *(2) Project [id#2483L] \u00a0 \u00a0 \u00a0 \u00a0 \u00a0+- *(2) Filter isnotnull(id#2483L) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 +- *(2) FileScan parquet default.tbla_nonull[id#2483L] Batched: true, DataFilters: [isnotnull(id#2483L)], Format: Parquet, Location: InMemoryFileIndex[dbfs:/user/hive/warehouse/tbla_nonull], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct&lt;id:bigint&gt; Explanation Spark doesn\u2019t do this automatically, because Spark and SQL have slightly different semantics for null handling. In SQL, not in means that if there is any null value in the not in values, the result is empty. This is why it can only be executed with BroadcastNestedLoopJoin. All not in values must be known in order to ensure there is no null value in the set. Example notebook This notebook has a complete example, showing why Spark does not automatically switch BroadcastNestedLoopJoin to SortMergeJoin. Review the BroadcastNestedLoopJoin \u00a0example notebook.", "format": "html", "updated_at": "2022-05-23T22:12:41.801Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:30.391Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678220, "name": "aws"}, {"id": 2678221, "name": "azure"}], "url": "https://kb.databricks.com/sql/disable-broadcast-when-broadcastnestedloopjoin"}, {"id": 1383831, "name": "Date functions only accept int values in Apache Spark 3.0", "views": 7717, "accessibility": 1, "description": "Date functions only accept int values in Apache Spark 3.0; fractional and string values return AnalysisException error.", "codename": "date-int-only-spark-30", "created_at": "2022-05-23T21:29:04.385Z", "updated_at": "2023-02-28T00:26:20.274Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTluZExha2hHWHNWYStQVm1qbndsOGFpYW5zYmczMXE3YjExVXNqNTl1NzVpOUkxTFQrCkxCbjhwdGUxcFJWVTYrdFJJOTI4T29XZFgzMTBpL3YzdWdRc0l1YjRLcWpsTE9rdDBrZFJadXJ3Ym9NYwp5K0JsMUc5eFBhM1FGeFF1bWxJWFRDUmZVYjlqVzk2VEplMElaTXREU1hFcE1ya1VEbXlhRlRrOU01MEcKclJGT1oyK05XTlVxYm9LZm9HQ0g4eW5KWHViR1ZVclhybVlVRUZxQUxSZjRJM21hR3kwaXN0bE1qOGI4Clc4VGNCNThIbTJZU09sVXBGNHZvajNtd2kyVzFkeXhmQmYrTVhWSkkrSXlqMit6TldJTkQ3YmNnQ0I3MgpUSlpIM1FlZ05pc1ZjbHRqcmVnVy92Nzl5VDU2NCtwZjNvbDlIR2xZc0E2eDRRVUtacWlLd0gxQUJ4L3MKWTdiLzhvcnZHaUlqdHYxdGYvVkhpZDJnOU9PeG45NElOUXkwbzd5d21uVFNiZkNGRnJjTkRhUHZhUmR3Cm5kbVdGOE5JVlRhYWQ2WU9CSGxTdnpjdGtMTXc0UzY2OWhvVzlWTktXY0R6TGxJRW13SVJQcHFKOWFxegozV2VaM21kUWpNb2NnTW1GUHRyTThhNXB6aklFM1duSC95aENVSE9RVU1LZW5tQTdwS2l4R2FUQ0NUNm8KZzN6aHorVFN5aGZjUGtLMnorQTJJdXlIRXQzdXpzN1ZDY3dMM1hwYWptamxSVVdJdTFqRm5udjdOSXpDCkNNTVFOOVErc2ZuRzlMU3NMLzlONHRjV3ppT05jeHFIVFRpN2pVdCtuTjV4Z0MyVVJhWEdmY2FGZHFsNwpHdjJHVFNuNll6V3RvTTZ6bzN3SVUwa2JsZkc4NXFNZEt1SUJHdDI5T1JZb2hMcDhYR1FHdlZvazFsaWgKdlFuMTkxVUdLd2djNnNwU1poSnBvQ1RmNFlIOUI5TFQwQU9hZXUra05NZkNzcGYwckhKak14TVNBaGprCmN3aDVMcHVCZGpVSlV5cHgzZEwwTFMwRG9MbVg2TEUrR0pSS1MxTkxtOTQ3enpKRkNoQ3l2dTdPOG14ZAo0L0dIaSs0bWdvSFVXNm85S2lsajJIeERSam96TzVqdUJNRUhqLzR1SkxFQ1lnelRNNlBNcHVoVkI4clYKV09DZWpYSjQ2V2YxanNFOE13eUMxOVNjWHdPbG9mWnFtdXd5OGZSR3BJRzVUYkVuSWFVQ0NtdTNZbU8vCg==.e4661f5eb29aefec65471d3aba1a1d11\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>You are attempting to use the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date_add()</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date_sub()</span> functions in Spark 3.0, but they are returning an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Error in SQL statement: AnalysisException</span> error message.</p><p>In Spark 2.4 and below, both functions work as normal.</p><pre>%sql\r\n\r\nselect date_add(cast('1964-05-23' as date), '12.34')</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>You are attempting to use a fractional or string value as the second argument.</p><p>In Spark 2.4 and below, if the second argument is a fractional or string value, it is coerced to an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">int</span> value before <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date_add()</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date_sub()</span> is evaluated.</p><p>Using the example code listed above, the value 12.34 is converted to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">12</span> before <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date_add()</span> is evaluated.</p><p>In Spark 3.0, if the second argument is a fractional or string value, it returns an error.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">int</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">smallint</span>, or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">tinyint</span> values as the second argument for the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date_add()</span> or <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">date_sub()</span> functions in Spark 3.0.</p><pre>%sql\r\n\r\nselect date_add(cast('1964-05-23' as date), '12')</pre><pre>%sql\r\n\r\nselect date_add(cast('1964-05-23' as date), 12)</pre><p>Both of these examples work properly in Spark 3.0.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"info-3\">Info</h3>\n<p class=\"hj-alert-text\">If you are importing this data from another source, you should create a routine to sanitize the values and ensure the data is in integer form before passing it to one of the date functions.</p>\n</div>\n</div><p><br></p>", "body_txt": "Problem You are attempting to use the date_add() or date_sub() functions in Spark 3.0, but they are returning an Error in SQL statement: AnalysisException error message. In Spark 2.4 and below, both functions work as normal. %sql select date_add(cast('1964-05-23' as date), '12.34') Cause You are attempting to use a fractional or string value as the second argument. In Spark 2.4 and below, if the second argument is a fractional or string value, it is coerced to an int value before date_add() or date_sub() is evaluated. Using the example code listed above, the value 12.34 is converted to 12 before date_add() is evaluated. In Spark 3.0, if the second argument is a fractional or string value, it returns an error. Solution Use int, smallint, or tinyint values as the second argument for the date_add() or date_sub() functions in Spark 3.0. %sql select date_add(cast('1964-05-23' as date), '12') %sql select date_add(cast('1964-05-23' as date), 12) Both of these examples work properly in Spark 3.0. Info\nIf you are importing this data from another source, you should create a routine to sanitize the values and ensure the data is in integer form before passing it to one of the date functions.", "format": "html", "updated_at": "2023-02-28T00:26:20.244Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:30.391Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678205, "name": "aws"}, {"id": 2678206, "name": "azure"}, {"id": 2678207, "name": "gcp"}], "url": "https://kb.databricks.com/sql/date-int-only-spark-30"}, {"id": 1383826, "name": "Cannot grow BufferHolder; exceeds size limitation", "views": 11657, "accessibility": 1, "description": "Cannot grow BufferHolder by size because the size after growing exceeds limitation; java.lang.IllegalArgumentException error.", "codename": "cannot-grow-bufferholder-exceeds-size", "created_at": "2022-05-23T21:19:08.376Z", "updated_at": "2022-05-23T21:28:51.208Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlRdm1pZTNnaVNxMkxlTU5TazgzN0RVRkl3Z2txTGhnSGpuSDJOaVJLVCtzMXAyYy84CkRQTWtRSjRjRnNpYlBSQU9Hd0FhMy90RDJqeEQyd0NZeFdxWTB0VERTVnVRMk9sWDRGVG5ZS3o4enc2QQpENSswemRPVmxIUkl0bWt2SnIzSmpnTjl0UStwNmlKYVlXcTJ6WDNjRndNVWNDQm13VTN3aGFaWXNBdUQKUXJCZmFNTFhheXVRYVQrOFdkY0s0eW5SUFhaRkNtN20vTU9VTzRuaHluMWVWTXgyWWNLakd4L3ErZU50Ck9iZHpBQjhSOWxrd01GM1VUdG8xQXlCaENsSEQwY1JLMWJYWkJEMXZPVm9yMjRON1JZWitTZ1VUR1BzeQpSOEUzYzZwWFY4MWp0VmtNaVBHYUVxa1ZMM0xvWHJsS09xTnJoY2RFSXBtU3dxUmtBSkNoemMwdzVpaTAKWXA1MExMcE0zTG5uN0NSNTM1RG1MVFM2V0VZcWV4UG5mLys0TlFLV1lUM1VTYWtTQzBEb3ZISytOQmlKCkNBdnl6K1EwTi9hS25uTlZuOG42Qmt2QlUwc2tMWUlHVlo1bEpFSktGYmV4K2JwN3VjVlVWYzVycnBCbwpvSzFEaTdNNTVvWVk1OUJmUldwd0piM3pRSndXckJsYmNZR08vd0d0cVR2ZWZsM1AwOC9BcXhzQmpVRW0Kc0tZM3FSQWk5VlBwLzhVcXlnVHJaRW80U0c3am9YRzlMc1BIalFWSmE0dEgyRWZ4SmZ5R016T01zUmRUCmpqNmsvMWRTYkNFYlBRbGd5ZVNoQ1VCQ0dYN1NpWnl0cy93ODFkaTBGeGxxSnBqTUZzY2tQaE1sWmZCbwpTSlc5N0JhVG0yM2Q4NWQvYXRybjF3K2tJdkt0d2tUdEsrbFRrYWZFZWJxWVFwN2VUb25xbUZLTHllVk4KSllVcmswSUV6dkRFeVJKOVljRVAwT2swaXZGNXAvZFlMYndzWk1GQUJ0WkNGR0hLeERpL1c2VEhWUEMxCnprNlprSEE0SHpJcGVFeE5uaFBBNFVZWjlNSjhyczdZMHpUNVhJL2twZERDdERUUkJ2b1lUTE1nN01pbQpqbHNMUGlrVEN2d0Z4NXRxWUV1NFlJTEk1bFgxVFNNR0ZGU2JvWS9RZ1o2TFRNQXJIY0tISXZBZFNpYUwKaGVzZmdrZTdUOE5kbjVjaVY3c3NOa0xnQTQ4MzBwQ29rU2Jrc0NqREhxV3Z3bkdIdzFpK2pzYnVhT3lSClFidUpYZ3piUWtpNytBcFgvbitwNXZIQk4rTmxOdjJ1dmlLUDFITWc3M3c9Cg==.561c840145cabd956dfe9e14ce4a8802\"></div><h1 id=\"isPasted\">Problem</h1><p>Your Apache Spark job fails with an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IllegalArgumentException: Cannot grow BufferHolder</span> error.</p><pre>java.lang.IllegalArgumentException: Cannot grow BufferHolder by size XXXXXXXXX because the size after growing exceeds size limitation 2147483632</pre><h1>Cause</h1><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BufferHolder</span> has a maximum size of 2147483632 bytes (approximately 2 GB).</p><p>If a column value exceeds this size, Spark returns the exception.</p><p>This can happen when using aggregates like <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">collect_list</span>.</p><p>This example code generates duplicates in the column values which exceed the maximum size of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BufferHolder</span>. As a result, it returns an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IllegalArgumentException: Cannot grow BufferHolder</span> error when run in a notebook.</p><pre>%sql\r\n\r\nimport org.apache.spark.sql.functions._\r\nspark.range(10000000).withColumn(\"id1\",lit(\"jkdhdbjasdshdjkqgdkdkasldksashjckabacbaskcbakshckjasbc$%^^&amp;&amp;&amp;&amp;&amp;*jxcfdkwbfkjwdqndlkjqslkndskbndkjqbdjkbqwjkdbxnsa xckqjwbdxsabvnxbaskxqbhwdhqjskdjxbqsjdhqkjsdbkqsjdkjqdhkjqsabcxns ckqjdkqsbcxnsab ckjqwbdjckqscx ns csjhdjkqsdhjkqshdjsdhqksjdhxqkjshjkshdjkqsdhkjqsdhjqskxb kqscbxkjqsc\")).groupBy(\"id1\").\r\nagg(collect_list(\"id1\").alias(\"days\")).\r\nshow()</pre><h1>Solution</h1><p>You must ensure that column values do not exceed 2147483632 bytes. This may require you to adjust how you process data in your notebook.</p><p><br></p><p>Looking at our example code, using collect_set instead of collect_list, resolves the issue and allows the example to run to completion. This single change works because the example data set contains a large number of duplicate entries.</p><p><br></p><pre>%sql\r\n\r\nimport org.apache.spark.sql.functions._\r\nspark.range(10000000).withColumn(\"id1\",lit(\"jkdhdbjasdshdjkqgdkdkasldksashjckabacbaskcbakshckjasbc$%^^&amp;&amp;&amp;&amp;&amp;*jxcfdkwbfkjwdqndlkjqslkndskbndkjqbdjkbqwjkdbxnsa xckqjwbdxsabvnxbaskxqbhwdhqjskdjxbqsjdhqkjsdbkqsjdkjqdhkjqsabcxns ckqjdkqsbcxnsab ckjqwbdjckqscx ns csjhdjkqsdhjkqshdjsdhqksjdhxqkjshjkshdjkqsdhkjqsdhjqskxb kqscbxkjqsc\")).groupBy(\"id1\").\r\nagg(collect_set(\"id1\").alias(\"days\")).\r\nshow()</pre><p>If using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">collect_set</span> does not keep the size of the column below the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BufferHolder</span> limit of 2147483632 bytes, the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">IllegalArgumentException: Cannot grow BufferHolder</span> error still occurs. In this case, we would have to split the list into multiple DataFrames and write it out as separate files.</p>", "body_txt": "Problem Your Apache Spark job fails with an IllegalArgumentException: Cannot grow BufferHolder error. java.lang.IllegalArgumentException: Cannot grow BufferHolder by size XXXXXXXXX because the size after growing exceeds size limitation 2147483632 Cause BufferHolder has a maximum size of 2147483632 bytes (approximately 2 GB). If a column value exceeds this size, Spark returns the exception. This can happen when using aggregates like collect_list. This example code generates duplicates in the column values which exceed the maximum size of BufferHolder. As a result, it returns an IllegalArgumentException: Cannot grow BufferHolder error when run in a notebook. %sql import org.apache.spark.sql.functions._ spark.range(10000000).withColumn(\"id1\",lit(\"jkdhdbjasdshdjkqgdkdkasldksashjckabacbaskcbakshckjasbc$%^^&amp;&amp;&amp;&amp;&amp;*jxcfdkwbfkjwdqndlkjqslkndskbndkjqbdjkbqwjkdbxnsa xckqjwbdxsabvnxbaskxqbhwdhqjskdjxbqsjdhqkjsdbkqsjdkjqdhkjqsabcxns ckqjdkqsbcxnsab ckjqwbdjckqscx ns csjhdjkqsdhjkqshdjsdhqksjdhxqkjshjkshdjkqsdhkjqsdhjqskxb kqscbxkjqsc\")).groupBy(\"id1\"). agg(collect_list(\"id1\").alias(\"days\")). show() Solution You must ensure that column values do not exceed 2147483632 bytes. This may require you to adjust how you process data in your notebook. Looking at our example code, using collect_set instead of collect_list, resolves the issue and allows the example to run to completion. This single change works because the example data set contains a large number of duplicate entries. %sql import org.apache.spark.sql.functions._ spark.range(10000000).withColumn(\"id1\",lit(\"jkdhdbjasdshdjkqgdkdkasldksashjckabacbaskcbakshckjasbc$%^^&amp;&amp;&amp;&amp;&amp;*jxcfdkwbfkjwdqndlkjqslkndskbndkjqbdjkbqwjkdbxnsa xckqjwbdxsabvnxbaskxqbhwdhqjskdjxbqsjdhqkjsdbkqsjdkjqdhkjqsabcxns ckqjdkqsbcxnsab ckjqwbdjckqscx ns csjhdjkqsdhjkqshdjsdhqksjdhxqkjshjkshdjkqsdhkjqsdhjqskxb kqscbxkjqsc\")).groupBy(\"id1\"). agg(collect_set(\"id1\").alias(\"days\")). show() If using collect_set does not keep the size of the column below the BufferHolder limit of 2147483632 bytes, the IllegalArgumentException: Cannot grow BufferHolder error still occurs. In this case, we would have to split the list into multiple DataFrames and write it out as separate files.", "format": "html", "updated_at": "2022-05-23T21:28:51.205Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:30.391Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678203, "name": "aws"}, {"id": 2678204, "name": "azure"}], "url": "https://kb.databricks.com/sql/cannot-grow-bufferholder-exceeds-size"}, {"id": 1383793, "name": "Broadcast join exceeds threshold, returns out of memory error", "views": 13272, "accessibility": 1, "description": "Resolve an Apache Spark OutOfMemorySparkException error that occurs when a table using BroadcastHashJoin exceeds the BroadcastJoinThreshold.", "codename": "bchashjoin-exceeds-bcjointhreshold-oom", "created_at": "2022-05-23T19:57:57.013Z", "updated_at": "2022-05-23T21:18:59.211Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSthNHZ0U08rL1JHVEZrUmVXVDhQWHNPajlyZ04xQjdzSlFTckk2SlEzcXdwS2xXNi9kCjgrT2RSd2MveFIrNVlrUmIxdkU3bUVBVTI0Y3RHNlFBNUkzcEhWdFo5ZGVPMHpZSURiSEppUzRSamk1TwoxM3NSa0Jlb0QyU1A0d2ZieStJb2dVTkFDWVlVOGxJTHNNUlJDZUlSa3ZUd3dPNTNnMUovKzJhbkFFK2QKK3dvYmlLVytnV25XNG1jOGs4K2lHUnpBbENKMkNoVmhyNCthNFpmdk1SbG9xankxZWthRzNMazI0NlVmCk9aMnZqc2pleC9NZzNweUxqUE5wY2t5THBzMG5VQ1JnVU9KVGpsd0xVKzB2YzBPcDZlSXJuY1dlZk1DNQpYeXpld3N0Zk9xQ1BzbVlQdHZKcXNNN1NFZms1UXdZcTl4eHN5N2xyUlFlcDVHd1EzUEJUZnF6WkR1UHcKTEV0M0pQOGdnNXdHZkdtL3ZkRXlyKzJCTnBJWTN3RnAxRzBkUG16eW1OdmtBSkdwVUhrK2NyVTNURTgyCnByL2hMRW5IVmxOdkxkRi8yQ1drY0NQOTg3d04wQWdHV09nb1dOTVYvZUtuUitGcTFLYXFkcHBuaVphRApKVG56NFppNHpjWUtwcTFKSVc1VE9pbDFSZzhYNzB1dVl6RWtOZERNcjZGZ3BvVFB2UnJMWkU2bVE5aUUKNU9GVldINkNoN3lhSURFdTZRc1d5RHNGOEVhMzJ6cjZKVndPeXRXTDhPL0t2dC9RSUU5bVFSZTIzdXljCmZNWGs2bHBjTFhmY2NpSDE4UloyYTN1T09TRXNNVG9LSEsrcWREWEovdkFRaWFQcGFwMWtnSjdNaU5qeAphbXp0VUUzVGpaajVMOHQzOXoyc3VzSkluanRmS3FFbWw2SnR4N285ejVNZ1lpZjZ4c1FYU1Z5SzNVeGQKSmRzbmpNd295a2VKaWoweE9nTU8zb2xlazR4OUx0UkNaTkxUNHR4WHRWYVNhNEsvakRuRzZhNTdLaWNxCnM3MENmY0hOK3gwcFRKZ2NoWGJyNFRLUHFNSkFvU015WmJHRVVRNGN6Umt1V0ZKMjlYdExyMDZnVWZGRAp0eDVjZ1RxSjg2bHNnYXJORG1iYmMyUDNEMFBKZGxkTGZ5ZnBERDJObUVEZ2VJUUhENDYxOWFMNDQ2YWEKOGh2V0RVOHljVTZvVVlBd3hMOWV4QXlLSHkzak5rMm8vODd3ZjU5ellFMzZVNnVNK0JteEluNkpGMXhRCg==.7a4e1f3bcc9aa4b7f8bcd4347175c6ce\"></div><h1 id=\"isPasted\">Problem</h1><p>You are attempting to join two large tables, projecting selected columns from the first table and all columns from the second table.</p><p>Despite the total size exceeding the limit set by <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.autoBroadcastJoinThreshold</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastHashJoin</span> is used and Apache Spark returns an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OutOfMemorySparkException</span> error.</p><pre>org.apache.spark.sql.execution.OutOfMemorySparkException: Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=1073741824. You can disable broadcasts for this query using set spark.sql.autoBroadcastJoinThreshold=-1</pre><h1>Cause</h1><p>This is due to a limitation with Spark\u2019s size estimator.</p><p>If the estimated size of one of the DataFrames is less than the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">autoBroadcastJoinThreshold</span>, Spark may use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">BroadcastHashJoin</span> to perform the join. If the available nodes do not have enough resources to accommodate the broadcast DataFrame, your job fails due to an out of memory error.</p><h1>Solution</h1><p>There are three different ways to mitigate this issue.</p><ul>\n<li>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ANALYZE TABLE</span> (<a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/analyze-table.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"ANALYZE TABLE\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/spark/latest/spark-sql/language-manual/analyze-table\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"ANALYZE TABLE\">Azure</a>) to collect details and compute statistics about the DataFrames before attempting a join.</li>\n<li>Cache the table (<a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/cache-table.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Cache the table\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/spark/latest/spark-sql/language-manual/cache-table\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Cache the table\">Azure</a>) you are broadcasting.<ol>\n<li>Run\u00a0<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">explain\u00a0</span>on your join command to return the physical plan.<pre>%sql\r\nexplain(&lt;join command&gt;)</pre>\n</li>\n<li>Review the physical plan. If the broadcast join returns <em>BuildLeft</em>, cache the left side table. If the broadcast join returns <em>BuildRight</em>, cache the right side table.</li>\n</ol>\n</li>\n<li>In Databricks Runtime 7.0 and above, set the join type to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">SortMergeJoin</span> with join hints enabled.</li>\n</ul>", "body_txt": "Problem You are attempting to join two large tables, projecting selected columns from the first table and all columns from the second table. Despite the total size exceeding the limit set by spark.sql.autoBroadcastJoinThreshold, BroadcastHashJoin is used and Apache Spark returns an OutOfMemorySparkException error. org.apache.spark.sql.execution.OutOfMemorySparkException: Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=1073741824. You can disable broadcasts for this query using set spark.sql.autoBroadcastJoinThreshold=-1 Cause This is due to a limitation with Spark\u2019s size estimator. If the estimated size of one of the DataFrames is less than the autoBroadcastJoinThreshold, Spark may use BroadcastHashJoin to perform the join. If the available nodes do not have enough resources to accommodate the broadcast DataFrame, your job fails due to an out of memory error. Solution There are three different ways to mitigate this issue. Use ANALYZE TABLE (AWS | Azure) to collect details and compute statistics about the DataFrames before attempting a join.\nCache the table (AWS | Azure) you are broadcasting.\nRun\u00a0explain\u00a0on your join command to return the physical plan.%sql explain(&lt;join command&gt;) Review the physical plan. If the broadcast join returns BuildLeft, cache the left side table. If the broadcast join returns BuildRight, cache the right side table. In Databricks Runtime 7.0 and above, set the join type to SortMergeJoin with join hints enabled.", "format": "html", "updated_at": "2022-05-23T21:18:59.209Z"}, "author": {"id": 821129, "email": "sandeep.chandran@databricks.com", "name": "sandeep.chandran ", "first_name": "sandeep.chandran", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-09T13:12:40.679Z", "updated_at": "2023-04-20T07:19:44.218Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256871, "name": "SQL with Apache Spark", "codename": "sql", "accessibility": 1, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678201, "name": "aws"}, {"id": 2678202, "name": "azure"}], "url": "https://kb.databricks.com/sql/bchashjoin-exceeds-bcjointhreshold-oom"}, {"id": 1383780, "name": "Write a DataFrame with missing columns to a Redshift table", "views": 6215, "accessibility": 1, "description": "Use `FILLRECORD` to populate missing columns when writing a DataFrame to a Redshift table.", "codename": "df-missing-columns-to-redshift", "created_at": "2022-05-23T19:47:29.342Z", "updated_at": "2022-05-23T19:54:01.344Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlBK3JRc0dsNjRzV1F4amVSVHV4UzU5elZYSnB0TDR3WHVPa3VhUUNhd0NET1Q1ei9wCmlRRE1nM05aZVdmUytvU2pxbVJ1M2wvNXhSVEZPaDVLZEp1QTVadTNoMHBrTVRHVHdOYTJPZG1DRWpMMApZbXcwWDJmTWMraWwra3Z0YVFpbjdUTE9EWWNGUUIwYzd3dkFZVTk2VTlEUVJidW92U010YnlBVWNZRC8KazA1QXN6akhNNk9QSnhHWU4zaFVHdEo0ckdhbUFGdjN0SWtpckh4TkdxcUJtNnJCWkMva2xqTG41R3owCkVJYUkvdzJaM0o0NUxsVUdjSXlIUEpXeUZqSC9yQnhpT2ltTUdTZ3dLYWVwa3AxNmRJVWRkcmk1ekQ4cwptQmtUY3BGSytrblRYVzNKTThlVUJjTlcrNEJuaG1tNFhxYlJWQVUvSHBxU2p5c0ovaFc2WUQ2aGZXdVMKVU40NnliUzlEZkhXWnM1ZGFHU3hPZjgreEZ5TWkvSnpMYW45NzZ4dmRkY1U1bmxDS1R2NTVMam5GRVNZCnRFYUpTVG5sbmJYWHpMMWVqb2pUS283dFluTktVZ2pxRDVFaFEzeTdJL093NUhDRC9CYkZOM0s4TXFsQQpkN3NrYkVaYVkraHNTODRkS28ydzBMMVF1bFJTNVlhZDR6M0NiMVZrZDQ0RGJZOUxUa0RQckZCV1VKSVEKcTFvaFV6Zzhta1M4VVh3eldMWm9aM2pwUUpQRnNuSjFSaHlXbDJ4dzZMNnVEcnJLUjVLYUx0QTQ2TGxFCkVodFZkdEF3UU82MGh2c3dDNjJJYkZUV3Z0aFNyMUp3QWVRWkVhY2lmQnBuQXlvWHFKZ0FhcmVNaFRGYwpzNUIycndRUHgwdE5hcld6TEdaTG42OE9YMnJoWFI4VlA5VlJTemFnWjNzSGRYTzVBeDk1THhKaHJFVVAKMGJDLzhBMVlGekNJeVZwbmdvYTgwT1BKVFJvRUZsK1ZNQnpTU3lVZUxqMlVKZWprRDFKTUZDc25rcGxuCjh2ZEZldnpOM0pQTkRxQXlkaG5BTmFsYVZLcEMvazVKekg5U0NmNmx5UUx4dk9ZY3g3ckRrWitPcG9TRgoyaWFUbkhDN2dERnN3MTNNRUc0aUgyOVBWdGtvUS9BUklKVHdTa1VsVWM5SGsvK291c2thdTZsWkt0Q3IKNUhPeW5GYS9iSU9ONzM1UXJ0VHhxRzZYZnRnME16cmRMYndmZ2lMaE1aWGVjWXNsd3NqNmUveWZ1b3RwCkpYTnUrSHNNWnpnWVErWGw3eVVFa3A5STFETU5tMHhWdDlOY3RadThHdWlsc0JxaC9xNWVZaEUzTGRlNwpyd3U5a05uR2wzNzV4cHUvUnFtTHhoZkVabnZ1dW1SaXh2UDAreGwxaVdCZnFxdytnTUdUd3ZwN1VEdjUKaEMyOVYza0pOUC9JMUpBM0RMd2I3K3dnUXhIQlNBPT0K.be67cdc286f10a42ace794bfef8620f6\"></div><h1 id=\"isPasted\">Problem</h1><p>When writing to Redshift tables, if the target table has more columns than the source Apache Spark DataFrame you may get a copy error.</p><pre>The COPY failed with error: [Amazon][Amazon Redshift] (1203) Error occurred while trying to execute a query: ERROR: Load into table table-name failed. Check the 'stl_load_errors' system table for details. \u201c1203 - Input data had fewer columns than were defined in the DDL\u201d</pre><h1>Cause</h1><p>The source Spark DataFrame and the target Redshift table need to have the same number of columns.</p><h1>Solution</h1><p><strong>Option 1</strong>: Update the notebook or job operation to add the missing columns in the spark DataFrame.</p><p>You can populate the new columns with null values if there is no data, or with actual values if there is new data that needs to be written to the target Redshift table.</p><p>This option requires manual intervention and can become time consuming if there are a large number of notebooks or jobs that need to be modified, or if new columns are added to the target on a regular basis.</p><p><strong>Option 2</strong>: Use the AWS Redshift data conversion parameter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">FILLRECORD</span>.</p><p>When <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">FILLRECORD</span> is used, it allows data files to be loaded when contiguous columns are missing at the end of some of the records. The missing columns are filled with either zero-length strings or null values, as appropriate for the data types of the columns in question.</p><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">FILLRECORD</span> can be specified using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">extracopyoptions</span> while performing the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">df.write</span> operation.</p><pre>%scala\r\n\r\ndf.write \\\r\n\u00a0 .format(\"com.databricks.spark.redshift\") \\\r\n\u00a0 .option(\"url\", \"jdbc:redshift://redshifthost:5439/database?user=username&amp;password=pass\") \\\r\n\u00a0 .option(\"dbtable\", \"my_table_copy\") \\\r\n\u00a0 .option(\"tempdir\", \"s3n://path/for/temp/data\") \\\r\n\u00a0 .option(\"extracopyoptions\", \"FILLRECORD\") \\\r\n\u00a0 .save()</pre><p><br></p>", "body_txt": "Problem When writing to Redshift tables, if the target table has more columns than the source Apache Spark DataFrame you may get a copy error. The COPY failed with error: [Amazon][Amazon Redshift] (1203) Error occurred while trying to execute a query: ERROR: Load into table table-name failed. Check the 'stl_load_errors' system table for details. \u201c1203 - Input data had fewer columns than were defined in the DDL\u201d Cause The source Spark DataFrame and the target Redshift table need to have the same number of columns. Solution Option 1: Update the notebook or job operation to add the missing columns in the spark DataFrame. You can populate the new columns with null values if there is no data, or with actual values if there is new data that needs to be written to the target Redshift table. This option requires manual intervention and can become time consuming if there are a large number of notebooks or jobs that need to be modified, or if new columns are added to the target on a regular basis. Option 2: Use the AWS Redshift data conversion parameter FILLRECORD. When FILLRECORD is used, it allows data files to be loaded when contiguous columns are missing at the end of some of the records. The missing columns are filled with either zero-length strings or null values, as appropriate for the data types of the columns in question. FILLRECORD can be specified using extracopyoptions while performing the df.write operation. %scala df.write \\ \u00a0 .format(\"com.databricks.spark.redshift\") \\ \u00a0 .option(\"url\", \"jdbc:redshift://redshifthost:5439/database?user=username&amp;password=pass\") \\ \u00a0 .option(\"dbtable\", \"my_table_copy\") \\ \u00a0 .option(\"tempdir\", \"s3n://path/for/temp/data\") \\ \u00a0 .option(\"extracopyoptions\", \"FILLRECORD\") \\ \u00a0 .save()", "format": "html", "updated_at": "2022-05-23T19:54:01.341Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678140, "name": "aws"}], "url": "https://kb.databricks.com/scala/df-missing-columns-to-redshift"}, {"id": 1383777, "name": "Multiple Apache Spark JAR jobs fail when run concurrently", "views": 5817, "accessibility": 1, "description": "Apache Spark JAR jobs failing with an AnalysisException error when run concurrently.", "codename": "spark-jar-job-error", "created_at": "2022-05-23T19:42:59.310Z", "updated_at": "2023-02-28T00:18:40.419Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTloY2plSjFFUkM4OEtOaGVUTlpiRWk5K29aeGlTTHVRWHQ2M01INmFXSkVmbHpQckYyCmVPbHM1UW4yWkw1Rk1ueklNdWRFbjIxUGNiT0w5RVpxTjFVSHliaWcvT1FzRGlqNlhLVzRvNnpjTE1ubgpVY3pxd25JbVp6eXd5bFBIdmREQUloQVBxM0VEUHZZVCtxRTZKdm5MeXBGaHVoR050VlVvWEVZRCs3bm4KZGJTM0JMRGtjbU16a1pJWFBlL1crL3ZQZ2NxU3dabmsyWGx6UXF1Y2ZPL1ZYeUFoYTN3RHNuQnJDaVlEClQzZHN0b1l5N2U3NVh6ZkJuSzFwNkJJSlNub3BSZS95WGRsOEp4WlJxdlRHK1ptK2wzSlNVZ21LdHdXeQprRlpkaUJCREFqblZkd3BxZXVheEJQaE0wTlVUbEF6QTVmTzl1QjJIWEJqbG9vL2RFVE5KbXROWDEweC8KeU56bUFSeFUyRU8xM0Rsd3YxeDM1Y3RlUkxHZU81NzJBTDlUb0o4M2dCV2wzZGFRd3RDWXRlZDBHK1ZZCmh4OWxCVUlyS256ek5QUVdBZk45ektKTjdtZDQ2QnRqNWdjM2gzZ2xNbm0xNVorZG9nMTE2SzJRTVRMdApJQTFWK1phZzc2TXh0dzlMUlh6eFVLRGxZV2t6RlpVZFZZSkRGcW53bmd1cTlMSk5kaDZSRzF1eTIzTk4KNmNnd3owWko1cGRDK3BnS0VSOEI5K3hpTmJQTXJvUk1UY0ZVWjQxOCtvNGVXUEV3MkNpb0xrb3NLbXRPCmp1aFhiZnQ3MEx2NGRwdlRNUWhXaUhscGlBZWY5d3NlS0dyVU5EV25aNjVDZVkvaTZxUzQ4bUZvQUJFcgpLY1JjY0pPTjdncHB5TTNrbUs0WDkyMFROOTMrT05RU2VZdVVQUHpCTHN2bVFaaUJIUUtISHBYNnByeVcKbUxxTVdKcGIxR1Yxakt2T0NNZm9ha0RRZGJlSUpMK0lxSHQvUk1PMm5RanJYb2UvNU1uQlVLaUVIcEMxCkREQXQ5VmtsK09OS00rdmEvbWY3MkZmOXhnS2wwWGpqSXVONmR0cjJQemJEczkvTzdvSGJnbEd6ZUFUUQpVSVRqVW4rNUZIbjVVc29DbzI1S3pkM09pRmZ5djNCY3JOMkpIV2ZvSEk2N3k3NHNQRzZ0UkZFVnZDTEUKcHE1TWhodk5aWkZpamVSSHJ0NEZFd0J1U0ZzWWxocklUcHJyMlpCTW1PbDNONTJwVVZrc1ZSVGpMbzhCCi9yN21iaitTSVlsLzcxVjg3aXprOWtsRFk5U2JGb2Q4SVRWWTRzblJ6dHc9Cg==.5d7ae71b46b8eb63e3b0551baf52a79b\"></div><h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p>If you run multiple Apache Spark JAR jobs concurrently, some of the runs might fail with the error:</p><pre>org.apache.spark.sql.AnalysisException: Table or view not found: xxxxxxx; line 1 pos 48</pre><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p>This error occurs due to a bug in Scala. When an object extends <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">App</span>, its <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">val</span> fields are no longer immutable and they can be changed when the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">main</span> method is called. If you run JAR jobs multiple times, a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">val</span> field containing a DataFrame can be changed inadvertently.</p><p>As a result, when any one of the concurrent runs finishes, it wipes out the temporary views of the other runs. <a href=\"https://github.com/scala/bug/issues/11576\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Scala issue 11576</a> provides more detail.</p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p>To work around this bug, call the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">main()</span> method explicitly. As an example, if you have code similar to this:</p><pre>%scala\r\n\r\n\u00a0 object MainTest extends App {\r\n\u00a0 \u00a0 ...\r\n\u00a0 }</pre><p>You can replace it with code that does not extend <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">App</span>:</p><pre>%scala\r\n\r\n\u00a0 object MainTest {\r\n\u00a0 \u00a0 def main(args: Array[String]) {\r\n\u00a0 \u00a0 ......\r\n\u00a0 \u00a0 }\r\n\u00a0 }</pre><p><br></p>", "body_txt": "Problem If you run multiple Apache Spark JAR jobs concurrently, some of the runs might fail with the error: org.apache.spark.sql.AnalysisException: Table or view not found: xxxxxxx; line 1 pos 48 Cause This error occurs due to a bug in Scala. When an object extends App, its val fields are no longer immutable and they can be changed when the main method is called. If you run JAR jobs multiple times, a val field containing a DataFrame can be changed inadvertently. As a result, when any one of the concurrent runs finishes, it wipes out the temporary views of the other runs. Scala issue 11576 provides more detail. Solution To work around this bug, call the main() method explicitly. As an example, if you have code similar to this: %scala \u00a0 object MainTest extends App { \u00a0 \u00a0 ... \u00a0 } You can replace it with code that does not extend App: %scala \u00a0 object MainTest { \u00a0 \u00a0 def main(args: Array[String]) { \u00a0 \u00a0 ...... \u00a0 \u00a0 } \u00a0 }", "format": "html", "updated_at": "2023-02-28T00:18:40.406Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:30.391Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678128, "name": "aws"}, {"id": 2678130, "name": "azure"}], "url": "https://kb.databricks.com/scala/spark-jar-job-error"}, {"id": 1383762, "name": "Select files using a pattern match", "views": 16665, "accessibility": 1, "description": "Use a glob pattern match to select specific files in a folder.", "codename": "pattern-match-files-in-path", "created_at": "2022-05-23T19:22:12.101Z", "updated_at": "2022-05-23T19:39:39.536Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9qTWpSTlROTXl1anNNR2luMGkvOE9ZbG9rZ2pOQmh2SC9GSGcvMFpyTmdrRnB4ejN0CmlDcEFDM1lYNTJQWkpYcCtHaG9zamRnYjJldnZQV2owQ1Z3c0N6NFNBRUJCSFZxdmNEVEdUeXNTRTE3cQoxd1dTblhnVXZwVTFXMU9zRjd2ei9PbGlDc0E4enJjRTlGNlVURC9aQ1k0aHp1U3B0UkE2V3pUOFBFbFEKNWUvUTlDcUdDRUw2VlZydDRrcEp2a21kb01HTzdtOWI0enlZTHVhYUJObGRBQ2x3cVFmeCtHNXRSdHNoCmU1SVRsZEs0ZUdhSGNQb2JNS3pBaWUvaDROR2JMMDQyMTFVS0pDc0VXYitnei91c051UXExc3NlcmREOApoYkZHSUl5TW5OMTBFV1RNUllqK05xWmpjdmNuSmR0MHNldnk0MU1NS2pBQSt5UVg5VS8vZGlMbXYvYUQKczF5d0VVTXdFRGVrT3IweEpWSzJadDN6Ynl1SzN2TWRGNFBwQUZ3UVdZOFdnMlBvbGp2NHpBVjlsZ2FpCkZCOWJRNzBCRG1MWldPay9lakh6OUw0YVFhcktTVzN1ZHZNaHFreDFsUk5zVkhSMHFNYWdYM0hCWXFGTwptcW9OT1duODNoUFQ1WDlMRzN0VmQwU0x5ZTFSTm1OQW9rT0JCNE5SVjU0c09oUmhGWk1ET2lDTGxUaEgKL0FJMHdyamxNNVBZWWxDYStob1ozL2x1anBscHp1SGppcGZYU2xaaVIvcXFhWXFsNjdKaGs5QTVvTlpOCitEbWVzNkNxeUIwRnMzaXdvSDd0U1R1V1ExNmt4REIwS0NtMUxteURSK252bFhrYU1OejdySzV0eU5LUQpZOTNiek5sbVZDeXFmcnR2bDU0cjVQMUtGZFJhUzVqQnFlYWVLNHc0QmlqVUFqdCt0U2F1YnVhM1haZysKMnhaV1pXa2ZxeS8zRHJ6blgrZTQwZ2lpb1dvRWNjUlduV0pDL3VSNlVCWnhSNDl3MFBLdG1XNzZYOGxVClhuTHY5eTBvVWVDVzRPOGdVbDNmQStnTllrS2tpY3RHMHRGRTFKQnA1R0Q4N0toSFZOc2lKZ1dJaUkyUAoyQmFyYjIvNmVXUmRFZFBDQVJYZy9HUVVxd1NNK05UeHEvT011MkI4aE94KzBQanZubmcybm12RmJlakYKNDVPRTY2ZU5zNWk1WDB6SmlvYWI0RUJaaXR6OWNDNG1mdGFsQ1Q0OTcvVFZhbzFlQ3c3TG1OeEo5QTV3CmJnekk2aGI4Z1pRNURXYk8zZ21mOFhscEw5SkdBN1VZU21wMzAyaXdPdENhNzF3ZFpuVVloRWxFdVkxegpSQTVRV2x5Mk4reUpUMFp5ek5BT0ZENTc2SkplUllFc2x4T0ZvMG9LZ0JPNHpzTCthRlowMEhTUlVhbjAKbDFrMUJBNUQK.aba79d4dcd157e239cae4eea28398074\"></div><p id=\"isPasted\">When selecting files, a common requirement is to only read specific files from a folder.</p><p>For example, if you are processing logs, you may want to read files from a specific month. Instead of enumerating each file and folder to find the desired files, you can use a glob pattern to match multiple files with a single expression.</p><p>This article uses example patterns to show you how to read specific files from a sample list.</p><h1>Sample files</h1><p>Assume that the following files are located in the root folder.</p><pre>//root/1999.txt\r\n//root/2000.txt\r\n//root/2001.txt\r\n//root/2002.txt\r\n//root/2003.txt\r\n//root/2004.txt\r\n//root/2005.txt\r\n//root/2020/04.txt\r\n//root/2020/05.txt</pre><h1>Glob patterns</h1><h2>Asterisk</h2><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">*</span> - The asterisk matches one or more characters. It is a wild card for multiple characters.</p><p>This example matches all files with a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">.txt</span> extension</p><pre>%scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/*.txt\"))</pre><h2>Question mark</h2><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">?</span> - The question mark matches a single character. It is a wild card that is limited to replacing a single character.</p><p>This example matches all files from the root folder, except 1999.txt. It does not search the contents of the 2020 folder.</p><pre>%scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200?.txt\"))</pre><h2>Character class</h2><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">[ab]</span> - The character class matches a single character from the set. It is represented by the characters you want to match inside a set of brackets.</p><p>This example matches all files with a 2 or 3 in place of the matched character. It returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2002.txt</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2003.txt</span> from the sample files.</p><pre>%scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200[23].txt\"))</pre><h2>Negated character class</h2><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">[^ab]</span> - The negated character class matches a single character that is not in the set. It is represented by the characters you want to exclude inside a set of brackets.</p><p>This example matches all files except those with a 2 or 3 in place of the matched character. It returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2000.txt</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2001.txt</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2004.txt</span>, and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2005.txt</span> from the sample files.</p><pre>%scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200[^23].txt\"))</pre><h2>Character range</h2><p><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard;'>[</span><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">a-b]</span> - The character class matches a single character in the range of values. It is represented by the range of characters you want to match inside a set of brackets.</p><p>This example matches all files with a character within the search range in place of the matched character. It returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2002.txt</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2003.txt</span>, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2004.txt</span>, and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2005.txt</span> from the sample files.</p><pre>%scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200[2-5].txt\"))</pre><h2>Negated character range</h2><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">[^a-b]</span> - The negated character class matches a single character that is not in the range of values. It is represented by the range of characters you want to exclude inside a set of brackets.</p><p>This example matches all files with a character outside the search range in place of the matched character. It returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2000.txt</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2001.txt</span> from the sample files.</p><pre>%scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/200[^2-5].txt\"))</pre><h2>Alternation</h2><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">{a,b}</span> - Alternation matches either expression. It is represented by the expressions you want to match inside a set of curly brackets.</p><p>This example matches all files with an expression that matches one of the two selected expressions. It returns <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2004.txt</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">2005.txt</span> from the sample files.</p><pre>%scala\r\n\r\ndisplay(spark.read.format(\"text\").load(\"//root/20{04, 05}.txt\"))</pre><p><br></p>", "body_txt": "When selecting files, a common requirement is to only read specific files from a folder. For example, if you are processing logs, you may want to read files from a specific month. Instead of enumerating each file and folder to find the desired files, you can use a glob pattern to match multiple files with a single expression. This article uses example patterns to show you how to read specific files from a sample list. Sample files Assume that the following files are located in the root folder. //root/1999.txt //root/2000.txt //root/2001.txt //root/2002.txt //root/2003.txt //root/2004.txt //root/2005.txt //root/2020/04.txt //root/2020/05.txt Glob patterns Asterisk * - The asterisk matches one or more characters. It is a wild card for multiple characters. This example matches all files with a .txt extension %scala display(spark.read.format(\"text\").load(\"//root/*.txt\")) Question mark ? - The question mark matches a single character. It is a wild card that is limited to replacing a single character. This example matches all files from the root folder, except 1999.txt. It does not search the contents of the 2020 folder. %scala display(spark.read.format(\"text\").load(\"//root/200?.txt\")) Character class [ab] - The character class matches a single character from the set. It is represented by the characters you want to match inside a set of brackets. This example matches all files with a 2 or 3 in place of the matched character. It returns 2002.txt and 2003.txt from the sample files. %scala display(spark.read.format(\"text\").load(\"//root/200[23].txt\")) Negated character class [^ab] - The negated character class matches a single character that is not in the set. It is represented by the characters you want to exclude inside a set of brackets. This example matches all files except those with a 2 or 3 in place of the matched character. It returns 2000.txt, 2001.txt, 2004.txt, and 2005.txt from the sample files. %scala display(spark.read.format(\"text\").load(\"//root/200[^23].txt\")) Character range [ a-b] - The character class matches a single character in the range of values. It is represented by the range of characters you want to match inside a set of brackets. This example matches all files with a character within the search range in place of the matched character. It returns 2002.txt, 2003.txt, 2004.txt, and 2005.txt from the sample files. %scala display(spark.read.format(\"text\").load(\"//root/200[2-5].txt\")) Negated character range [^a-b] - The negated character class matches a single character that is not in the range of values. It is represented by the range of characters you want to exclude inside a set of brackets. This example matches all files with a character outside the search range in place of the matched character. It returns 2000.txt and 2001.txt from the sample files. %scala display(spark.read.format(\"text\").load(\"//root/200[^2-5].txt\")) Alternation {a,b} - Alternation matches either expression. It is represented by the expressions you want to match inside a set of curly brackets. This example matches all files with an expression that matches one of the two selected expressions. It returns 2004.txt and 2005.txt from the sample files. %scala display(spark.read.format(\"text\").load(\"//root/20{04, 05}.txt\"))", "format": "html", "updated_at": "2022-05-23T19:39:39.529Z"}, "author": {"id": 790229, "email": "mathan.pillai@databricks.com", "name": "mathan.pillai ", "first_name": "mathan.pillai", "last_name": "", "role_id": "admin", "created_at": "2022-01-26T19:28:01.464Z", "updated_at": "2023-04-28T22:28:56.701Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678106, "name": "aws"}, {"id": 2678107, "name": "azure"}, {"id": 2678108, "name": "gcp"}], "url": "https://kb.databricks.com/scala/pattern-match-files-in-path"}, {"id": 1383747, "name": "Running C++ code in Scala", "views": 6077, "accessibility": 1, "description": "Learn how to run C++ code in Scala with this example notebook.", "codename": "running-c-plus-plus-code-scala", "created_at": "2022-05-23T19:15:26.742Z", "updated_at": "2022-05-23T19:21:55.847Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9hNUpEZDZSOXBrdDhJdlN6UitNNmxYdysyNW9WdGNmSytFaGJqRjZzR0Jaa0lQcG9kCmNRUmVrSytOaDlpWFZQSUJTb3B6dmo1TFZKZDkxNkpjK0lodWsrUi9FeE40Qk5yLzFIOFBoVFMxWXdvMApXaVhnWXlwdm91S0NZUFd5WmlrQ0pHbmlqRXR2RmxPYW1ObEdaSHlTTUxlTWZDczhNVURnRlhNcU9scnQKZkoyS0tqOXJiM2hHL0w0TzFINjk3WkNKWGEvMHlSTjNNU2draW9OeTYrOS8vNWJoU2dpc3d2cGovMlZkCk9TbEVlV1N2cTNLMFRDdGJLWDR3d0lrLzJGaUFqZy9Bc0V0RXFyem1xTXlKNENyVHpiUS82SDRPMGZiRQpKaHpGbUx6QzNGbks1OURYL1hqaThmLytyYmNTcWpQandNTmhyNlFpSGdCMTQwU0JaN2JJTFFaZS91ZUUKNXdlU09qRkNYZ3dDRnNMRWVyR2d0TmlKVElZeU5kTmg2ZEdHazZvNXhVeXpROFZ3Y1RZOEljZGRjM0t0CkV3OU5yWkt0Zm9zL0RSZGo0d01NTE1URDl2RWc5bDBGVXRpdjdWc01zTkQ4bHRaa3B1cUNDR01jMURUOQpmc2RoR1B1U2FPNnRTZnAwRit3bVpBVXhOdFQ5MVZRcmxlSEhmU3NCWXptajBIcG9aQlpRM0lrNkNZaXoKVnJMV0hHVHE5eUhMbjNkbjhkSWY5Znd0OHJOWS94WGNENEdzZ0NjMWNKYjY5UUp5K3o3UGVDQStlQVNPCm43N3Jva3BPVUxLYnNUb0daUjRQK1h2V1JGWVdJL3NLYlRaSGViZ3FIUXN1bXM2cVRNeGxPNU95QjFBMgpmbjltdjYxK29MdmlvNG1VQlNnMGRTTlJqaHVGS1lhTG5Ydys4LzZCa3VtMHIvK0wyS1U3SjcwOGlVQm0KSDZJUVp3c2h3ek5YSnBhd3J1aUdiVlFIQ0sxUjdvVmlaTHlUaDNZQWVFOTZyODVsbm1sL2Y0aGN1ZXJkCmFRR3l3T2Q5VitZWUpTOFg4bVlKTVNiOGM0VHN4dW5KNFlJaVJRcnpsNVA5N09YcStZQU1CRkhqOVExRQpvbE9Jd3VqZjFrMFhjYnBtb3J1SyswN0tIWmhleHo5T1lCZThKYU9KU1d0VTRNWGlXWWhTU3hsRXFJZTUKMGIyZlArSjA4N3poNkVwNGs3ZlYxdFU0UzZZa0pIZnFNa04vQ0JnRWRZWng1VkZQNnRrcHNPb054a1pHCg==.165c40b3cc57b22a962db6b54fb06b16\"></div><h1 id=\"isPasted\">Run C++ from Scala notebook</h1><p>Review the <a href=\"https://docs.databricks.com/_static/notebooks/kb/scala/run-c-plus-plus-scala.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Run C++ from Scala notebook</a>.</p>", "body_txt": "Run C++ from Scala notebook Review the Run C++ from Scala notebook.", "format": "html", "updated_at": "2022-05-23T19:21:55.845Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:30.391Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678091, "name": "aws"}, {"id": 2678092, "name": "azure"}], "url": "https://kb.databricks.com/scala/running-c-plus-plus-code-scala"}, {"id": 1383664, "name": "Trouble reading external JDBC tables after upgrading from Databricks Runtime 5.5", "views": 5969, "accessibility": 1, "description": "Fail to read external JDBC tables after upgrading from Databricks Runtime 5.5 to 6.0 and above.", "codename": "read-fail-jdbc-dbr6x", "created_at": "2022-05-23T19:09:04.907Z", "updated_at": "2022-05-23T19:14:06.372Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9vQUo2aGpnU0J5Ty9yVmJiSnVYb3lvUUVCbWlOVzZaQ1Yvbkdadk1lU0haUUFSbUdYCmYrWkpMbGN1RXBZQzRyK1ZIcWFia1M1cHRiLzRMNmd0Zk5haXhlOTlxMlJYNTBmQkFnTmVrdWF5NEdoWAoxQ0E5T0l5WW10TUhaekFPYmVTbHhqUUpSMTF1YW9Pd24vTzBneVB2WFBDQWNjbGVJcGFpdmVET1p5cTkKZktRZ3RqYWpGdlE0SUNoOUNJZ3ZSNENqU0EvVTVzNkVDRmc5TGpnbjNTQTYxS00wS2QyWEpQbERKclhPCjJrZ2t3RUZNYXBQQThiZndySnZBL1VRSjQwSExjby9jMGRLNTRSVmQvSENRQWNkejhGaHhENEp4bWVncQpFUERyOGNkd0d4Wk9rWUdQV29TQWRUNmxuMDFUeThIS2dxZCtsb29iWVBZdEQvNnhmWHVWYS9GVTUvaWUKUDFKYVF5b3pGUGpuV0dZL24ybzBQOGM3dS91UWoveEoybFJiYmlJZkhvckxPSUd5ZE5sMEQwMU9qU2FYCnorR1dqNFI2T0RsSjVlUC93bGg0OW9iekcyMmFmZXZDVlNtS2FMY3BYWUkvbFlOZU05QzVJK0J1L3VNZgpQckhBU0kxSVV5SW0zTFBJSURBbis4bDlmSWtWb3RqMzhVQm1kUVozMGpLWk56a3ZVTkFlTWZYbjNYdjUKaVBCUlp4WUdwbUZzc25uUFVVZ0RYQ2VYTkQ3c25zUmgzRU94eXNvTUFiS21KSXlhOXk3NVVOQlNQR2dXCm80YXpJMHNScXFKT1NRM0djdEhZQ3owY0VkdVVIQjF0Sm1HengrcjVpTnUvaUMxSU5PSERwYTZ4YUFySAp0S0d6QTMrUTMrUkxKakNOdlV3VUhJS040bjBQRm1oZVphWGEwbEVENjFzRXVwRDJMTHhXV1Y5S2VzTEEKQm1TUzFLMWpaRDNkbGRjQkY3b1A5Rm4ya09zWGpoWklxQm1HRmtVU1g5anZQRlpOQTNQWnhDUDdRZXdTCkJwMEZXa3ZnYXlHNi9qaHZlTmdsMDJBaEtzN1ROUE9DTkw2RE1IczlWVEZvbk9YUVg1R01RalVzTW1pQgpzM1JJZXNPY2VUSnZrSmZEaUQxM1BROEhzWXN5WHM5OE9vZ2lpSnpQRk5XbnJ1YnZsK2lTS2ZsV2VFOTcKV2lIc0kySzJWdzNmOUFOU0dNdXJ1Z3ZIRVZjV3k4RENTenpQVXgzd0RiV2hraUVpUzI4VkE2SzIrYTIvCg==.673dd09116d47f3d11904903a5d86a0d\"></div><h1 id=\"isPasted\">Problem</h1><p>Attempting to read external tables via JDBC works fine on Databricks Runtime 5.5, but the same table reads fail on Databricks Runtime 6.0 and above.</p><p>You see an error similar to the following:</p><pre>com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: java.util.concurrent.ExecutionException: org.apache.spark.sql.AnalysisException: org.apache.spark.sql.jdbc does not allow user-specified schemas.\r\nat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\r\nat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\r\nat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\r\nat java.lang.Thread.run(Thread.java:748)\r\n.\r\nCaused by: org.apache.spark.sql.AnalysisException: org.apache.spark.sql.jdbc does not allow user-specified schemas.;\r\n\r\nat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)</pre><h1>Cause</h1><p>Databricks Runtime 5.5 and below infers the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">session_id</span> attribute as a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">smallint</span>. Databricks Runtime 6.0 and above infers the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">session_id</span> attribute as an <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">int</span>.</p><p>This change to the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">session_id</span> attribute causes queries to fail with a schema issue.</p><h1>Solution</h1><p>If you are using external tables that were created in Databricks Runtime 5.5 and below in Databricks Runtime 6.0 and above, you must set the Apache Spark configuration <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.legacy.mssqlserver.numericMapping.enabled</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">true</span>. This ensures that Databricks Runtime 6.0 and above infers the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">session_id</span> attribute as a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">smallint</span>.</p><ol>\n<li>Open the <strong>Clusters</strong> page.</li>\n<li>Select a cluster.</li>\n<li>Click <strong>Edit</strong>.</li>\n<li>Click <strong>Advanced Options</strong>.</li>\n<li>Click <strong>Spark</strong>.</li>\n<li>In the <strong>Spark config</strong> field, enter <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.legacy.mssqlserver.numericMapping.enabled true</span>.</li>\n<li>Save the change and start, or restart, the cluster.</li>\n</ol>", "body_txt": "Problem Attempting to read external tables via JDBC works fine on Databricks Runtime 5.5, but the same table reads fail on Databricks Runtime 6.0 and above. You see an error similar to the following: com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: java.util.concurrent.ExecutionException: org.apache.spark.sql.AnalysisException: org.apache.spark.sql.jdbc does not allow user-specified schemas. at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299) at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286) at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) at java.lang.Thread.run(Thread.java:748) . Caused by: org.apache.spark.sql.AnalysisException: org.apache.spark.sql.jdbc does not allow user-specified schemas.; at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350) Cause Databricks Runtime 5.5 and below infers the session_id attribute as a smallint. Databricks Runtime 6.0 and above infers the session_id attribute as an int. This change to the session_id attribute causes queries to fail with a schema issue. Solution If you are using external tables that were created in Databricks Runtime 5.5 and below in Databricks Runtime 6.0 and above, you must set the Apache Spark configuration spark.sql.legacy.mssqlserver.numericMapping.enabled to true. This ensures that Databricks Runtime 6.0 and above infers the session_id attribute as a smallint. Open the Clusters page.\nSelect a cluster.\nClick Edit.\nClick Advanced Options.\nClick Spark.\nIn the Spark config field, enter spark.sql.legacy.mssqlserver.numericMapping.enabled true.\nSave the change and start, or restart, the cluster.", "format": "html", "updated_at": "2022-05-23T19:14:06.364Z"}, "author": {"id": 791650, "email": "mohammed.haseeb@databricks.com", "name": "Mohammed.Haseeb ", "first_name": "Mohammed.Haseeb", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T09:36:19.563Z", "updated_at": "2022-03-08T02:15:25.476Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678078, "name": "aws"}, {"id": 2678079, "name": "azure"}], "url": "https://kb.databricks.com/scala/read-fail-jdbc-dbr6x"}, {"id": 1383569, "name": "Manage the size of Delta tables", "views": 6467, "accessibility": 1, "description": "Recommendations that can help you manage the size of your Delta tables.", "codename": "manage-size-delta-table", "created_at": "2022-05-23T16:33:25.578Z", "updated_at": "2022-05-23T18:56:58.953Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTkxV1VRWlhqcTRCQkZGUHlwVzVWUjMxUk0rYkxodVNaUEF2TG1wZUdhemsxSGduZ3AvCjVFV2xxVDlHOFptNmgwMEk0QUU5TTNFNUgzQUcvYkNtN1dZcHcyL3I5ejFvMWt2NHQxUHZ4b0xuUTdGVgpvZXNsQlVYamJzK3NsQlFJQ1UyVWlrQ2NBb2x5d0lEU2NuK2Q0VjlpdVpPU3Q5NXRBbFhCbUJ4WXdmOXkKUFI2TDNhNzF4WkF0Qm1VL2dlTzBITXdSS3NKdGhlWXZrVXRSR1VhWEZ1djVNa3QrblI5YWxmUld6S05ZCjFud1gvR1lVVnRKa01KQkI5SHQ5VGQ5cldMU0NpODVDbFBSU1BoOHBTdHBtUEZPWUV3aThkMEFBN0Z0ZQpSREl6K1VmekVuWGNaemNZekh6Tzc2SFZycVUwTXc5UUNWV05QUXo1eUtvL29tY1hiWUpQeW85VklKKysKcnRxR2ZHNVNtUk1MMXNoVytEVkNoQW9zcWs3VU9HTDlmbW9SZnF1MVJGd0JpWXZqR0hPaVlsVlR1bzNVCk9DY202NVBoNUEwZERGQTFGdzBMVDRtS1BPNlBvd0IxWnhtVlNTT1dSbEVjck12SmNEM291cGZETzlEawpwNmRUVFA2Y0k4enNiVVFrK1BERm9WZW5yM0JZUTNaK1dDNVVZZ3FNZXZlZ1RKQlp5ajAvaE0vZUNiVTUKMEltaVltanFvQVVCR0xrUzVtSGY3eFpWRXFNVXU5MVRaNFg3dzRtSnM2T3NuWkJEVUtsYWttaHB1Q3ZjCkJWOGpyQmxhdm5KcTMzWjZYYkxwaFN1QzQwVU82cE85K1p5WDhQM1oyT2F5M0JXR0JiVlRKc3Z3ODRVcwo5bjlWWGNiVzNaQVMxK2ZYamhLZk9lbU4yUlIrdDhRMEo3U2p1aC9XTk5ZaEY2Y0tiU083ekxiV0pMc2MKN0lzbmREL2hvTWgvdU90anB4RCtKaW1PNUIrUmtmek9JTnROV3BSWGxsdFJtSUVZbGhKVzJaaGJTZ1NvCkJDNER2aW5tb3pkMkVuWmo0c2V1SDAxM1p4eFVKMk13azJxY2p6SDIzaWVjMWx4Wm1oQUpUT3AwYlNTMApZbVZKLzdsV1AwMkFoRFpxQ1FsYzQ2Sk9kTGZ5Ymw0SzNqUHNNNDVkUFpIMFk1UVZyZkZTUTBhMUZUZDIKa3dNcENRaFZBS1FsWk4xRnJEYWpaclArQndYYThlbk5HZ0dkTjZZa3phZEp5VGRXOGFERWNVbitWWlVBCnZUcjRXa2Z1emVZOTlVVVhLVEhLRHlJRDRCWTE4QW9CVmNLY1FYTGtucmZNVjB4aU1ENzJ3Yk9aSmt5egphUWQ0aFlNVFhtYTJnclBJa0M5dCt4eGdFVldOVTZPbW91aE9ZTVFPRThHSUJXOC93S1NqbEhVNmhqUmYKb28ybjE1ZHFNNVJSb1dYb0hXNFMyZSt6Z214VzBBPT0K.8490cd675f66b8c8a078cd5138ad2450\"></div><p id=\"isPasted\">Delta tables are different than traditional tables. Delta tables include ACID transactions and time travel features, which means they maintain transaction logs and stale data files. These additional features require storage space.</p><p>In this article we discuss recommendations that can help you manage the size of your Delta tables.</p><h1>Enable file system versioning</h1><p>When you enable file system versioning, you keep multiple variants of your data in the same storage bucket. The file system creates versions of your data, instead of deleting items, which increases the storage space available for your Delta table.</p><h1>Enable bloom filters</h1><p>A Bloom filter index (<a href=\"https://docs.databricks.com/delta/optimizations/bloom-filters.html#bloom-filter-indexes\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Bloom filter index\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/delta/optimizations/bloom-filters#bloom-filter-indexes\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Bloom filter index\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/delta/optimizations/bloom-filters.html#bloom-filter-indexes\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Bloom filter index\">GCP</a>) is a space-efficient data structure that enables data skipping on chosen columns, particularly for fields containing arbitrary text. Databricks supports file level Bloom filters; each data file can have a single Bloom filter index file associated with it. Before reading a file Databricks checks the index file and the file is read only if the index indicates that the file might match a data filter.</p><p>The size of a Bloom filter depends on the number elements in the set for which the Bloom filter has been created and the required false positive probability (FPP). The lower the FPP, the higher the number of used bits per element and the more accurate it will be, at the cost of more storage space.</p><h1>Review your Delta <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">logRetentionDuration</span> policy</h1><p>Log files are retained for 30 days by default. This value is configurable through the delta.logRetentionDuration property. You can set a value for this property with the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ALTER TABLE SET TBLPROPERTIES</span> SQL method. The more days you retain, the more storage space you consume. For example if you set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">delta.logRetentionDuration = '365 days'</span> it keeps the log files for 365 days instead of the default of 30 days.</p><h1>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> your Delta table</h1><p>VACUUM (<a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-vacuum.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"VACUUM\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/spark/latest/spark-sql/language-manual/delta-vacuum\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"VACUUM\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/spark/latest/spark-sql/language-manual/delta-vacuum.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"VACUUM\">GCP</a>) removes data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. Files are deleted according to the time they have been logically removed from Delta\u2019s transaction log + retention hours, not their modification timestamps on the storage system. The default threshold is 7 days. Databricks does not automatically trigger <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> operations on Delta tables. You must run this command manually. <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">VACUUM</span> helps you delete obsolete files that are no longer needed.</p><h1>\n<span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> your Delta table</h1><p>The OPTIMIZE (<a href=\"https://docs.databricks.com/delta/optimizations/file-mgmt.html\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/delta/optimizations/file-mgmt\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">Azure</a> | <a href=\"https://docs.gcp.databricks.com/delta/optimizations/file-mgmt.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"OPTIMIZE\">GCP</a>) command compacts multiple Delta files into large single files. This improves the overall query speed and performance of your Delta table by helping you avoid having too many small files around. By default, <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">OPTIMIZE</span> creates 1GB files.</p>", "body_txt": "Delta tables are different than traditional tables. Delta tables include ACID transactions and time travel features, which means they maintain transaction logs and stale data files. These additional features require storage space. In this article we discuss recommendations that can help you manage the size of your Delta tables. Enable file system versioning When you enable file system versioning, you keep multiple variants of your data in the same storage bucket. The file system creates versions of your data, instead of deleting items, which increases the storage space available for your Delta table. Enable bloom filters A Bloom filter index (AWS | Azure | GCP) is a space-efficient data structure that enables data skipping on chosen columns, particularly for fields containing arbitrary text. Databricks supports file level Bloom filters; each data file can have a single Bloom filter index file associated with it. Before reading a file Databricks checks the index file and the file is read only if the index indicates that the file might match a data filter. The size of a Bloom filter depends on the number elements in the set for which the Bloom filter has been created and the required false positive probability (FPP). The lower the FPP, the higher the number of used bits per element and the more accurate it will be, at the cost of more storage space. Review your Delta logRetentionDuration policy Log files are retained for 30 days by default. This value is configurable through the delta.logRetentionDuration property. You can set a value for this property with the ALTER TABLE SET TBLPROPERTIES SQL method. The more days you retain, the more storage space you consume. For example if you set delta.logRetentionDuration = '365 days' it keeps the log files for 365 days instead of the default of 30 days. VACUUM your Delta table VACUUM (AWS | Azure | GCP) removes data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. Files are deleted according to the time they have been logically removed from Delta\u2019s transaction log + retention hours, not their modification timestamps on the storage system. The default threshold is 7 days. Databricks does not automatically trigger VACUUM operations on Delta tables. You must run this command manually. VACUUM helps you delete obsolete files that are no longer needed. OPTIMIZE your Delta table The OPTIMIZE (AWS | Azure | GCP) command compacts multiple Delta files into large single files. This improves the overall query speed and performance of your Delta table by helping you avoid having too many small files around. By default, OPTIMIZE creates 1GB files.", "format": "html", "updated_at": "2022-05-23T18:56:58.949Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2678074, "name": "aws"}, {"id": 2678075, "name": "azure"}, {"id": 2678076, "name": "gcp"}], "url": "https://kb.databricks.com/scala/manage-size-delta-table"}, {"id": 1383562, "name": "Intermittent NullPointerException when AQE is enabled", "views": 6958, "accessibility": 1, "description": "When adaptive query execution (AQE) is enabled, and cluster scales down and loses shuffle data, you can get a `NullPointerException` error.", "codename": "intermittent-nullpointerexception-aqe-enabled", "created_at": "2022-05-23T16:24:29.680Z", "updated_at": "2022-05-23T16:33:08.685Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSsycTNoVXFqd3NmbFJtcTcxRWM4V0daNXVtQzhHdTRENTR3VzZKSlZZck9kODBNNGNSCnZ2aTJvREZITitsanBFWlYzRkx5YjRUd2ZTdlBuamdEN3pSQm5YNnZzYXZTblR6UkY5TXh5NE9RdFlONwpPRngxQ05WcmlHd0hLTVFwTEZhSEoxWFpxajNMWjExalRPaHdBTFdTZ2Y0V05TMXY5VFVDTUVsSVlsTnQKeTlMSVQ3bzArWDBPQ2tuRWN2TTFsYzF1YkJlS2NYeko0YTd1MkhUSStBTjR4UWJUdjNaQ0hocDc1c1pLCldLWGZtVFg1cWVTc0dRd3VWSGF0QTllVkpZTUVLeElpbnlsOXBBZVFwTG9ORUFnMjEvSExEWlhlTDBidQplUHUwYlVJOUV3dURkQTdpUWs4Z21TQm9yYXh2eEdxUFpvSndJUE9EQ29RckFxNTZIdngybExaMnRkOCsKSFhLeWRLcSsxclhWd0dTRUhmcDV3MGV0RzhjYS93RDZBL29JdjYraG1UNnFXQ0FoN0JtMlJpdUVOU2JQCk5OZW96OWJyQmhUSUs5c2E5YXlwSSszaUc3RFh1STJMa1pLRmtLNXVvdThXMTdVSVpSQnNrT2ZxckpHaAp3SjcvaGRTUEpDVDZVdk9SYTNtNG8wTmlhR3Y3TGF5UFBOSGwvVWdIaUxMay92b1RRTGs5LzFtVytYeFMKbFRWeEpXVk01eXVyUGUyMUJBengvcnVjNFgrM2lzSE1kMlhzT2tUUG5HYnM3UlJnZFEyb0haNUFUTEduCkNwQmRQOVc0SnhPY0tqb3lrdDUxTjdoWm56MVBPbzJFemlTbE9YV01yVkl0dStJcDVmbU9ZSTNlVitDQQp6SFJvMkhMemh3cVVNUE9CbVhzcGg1amVsajhTUnlCNlRaSlc3NVF5dVR2ZGJJQ3FRVWpKaVVMWk9PVDEKdDluazBQS1BRTGh2anZWcGxDTmhZWWt6b2tuVkwxaVJjcE9zMVFpOFkwSy94OXdwcUxXNG1xTENodWJZCjFVTFR6S2hudDhuV0ZNU0JIbVBxNndaYkc0SWF4Uit0Q3dxVy9SbklRb0J0WDlqMkRhTnZYdVF5STl0bwpZV3pmZHppQmR6eW9QVHNEVG9FbDhsSUE5TDhwUDVSUk1Pcm9LSzBGRFloYW5YT0l5VXNxT3pCTzJWcjQKcVE2c1dUVzdXbjdVRHQzMEE3Z2tTV3FQSEV3aEhQUmNtNmR2dVdVPQo=.fe14ae62382bd396e6c37c5076f9f6a9\"></div><h1 id=\"isPasted\">Problem</h1><p>You get an intermittent <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">NullPointerException</span> error when saving your data.</p><pre>Py4JJavaError: An error occurred while calling o2892.save.\r\n: java.lang.NullPointerException\r\n\u00a0 \u00a0 at org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin.$anonfun$getMapSizesForReduceId$1(OptimizeSkewedJoin.scala:167)\r\n\u00a0 \u00a0 at org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin.$anonfun$getMapSizesForReduceId$1$adapted(OptimizeSkewedJoin.scala:167)\r\n\u00a0 \u00a0 at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\u00a0 \u00a0 at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\u00a0 \u00a0 ....\r\n\u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u00a0 \u00a0 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u00a0 \u00a0 at java.lang.reflect.Method.invoke(Method.java:498)\r\n\u00a0 \u00a0 at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\u00a0 \u00a0 at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\r\n\u00a0 \u00a0 at py4j.Gateway.invoke(Gateway.java:295)\r\n\u00a0 \u00a0 at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\u00a0 \u00a0 at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\u00a0 \u00a0 at py4j.GatewayConnection.run(GatewayConnection.java:251)\r\n\u00a0 \u00a0 at java.lang.Thread.run(Thread.java:748)</pre><h1>Cause</h1><p>This error can occur if adaptive query execution (AQE) (<a href=\"https://docs.databricks.com/spark/latest/spark-sql/aqe.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"adaptive query execution\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/spark/latest/spark-sql/aqe\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"adaptive query execution\">Azure</a>) is enabled and you are joining data. If AQE is enabled, skew join is also enabled.</p><p>If any of the shuffle data fails due to a cluster scaling down event it generates a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">NullPointerException</span> error.</p><h1>Solution</h1><p>Set <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.sql.adaptive.skewJoin.enabled</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">false</span> in your <strong>Spark config</strong> (<a href=\"https://docs.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">AWS</a> | <a href=\"https://docs.microsoft.com/azure/databricks/clusters/configure#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\">Azure</a><a href=\"https://docs.gcp.databricks.com/clusters/configure.html#spark-configuration\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Spark config\"></a>).</p>", "body_txt": "Problem You get an intermittent NullPointerException error when saving your data. Py4JJavaError: An error occurred while calling o2892.save. : java.lang.NullPointerException \u00a0 \u00a0 at org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin.$anonfun$getMapSizesForReduceId$1(OptimizeSkewedJoin.scala:167) \u00a0 \u00a0 at org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin.$anonfun$getMapSizesForReduceId$1$adapted(OptimizeSkewedJoin.scala:167) \u00a0 \u00a0 at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238) \u00a0 \u00a0 at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) \u00a0 \u00a0 .... \u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) \u00a0 \u00a0 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \u00a0 \u00a0 at java.lang.reflect.Method.invoke(Method.java:498) \u00a0 \u00a0 at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) \u00a0 \u00a0 at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380) \u00a0 \u00a0 at py4j.Gateway.invoke(Gateway.java:295) \u00a0 \u00a0 at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) \u00a0 \u00a0 at py4j.commands.CallCommand.execute(CallCommand.java:79) \u00a0 \u00a0 at py4j.GatewayConnection.run(GatewayConnection.java:251) \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:748) Cause This error can occur if adaptive query execution (AQE) (AWS | Azure) is enabled and you are joining data. If AQE is enabled, skew join is also enabled. If any of the shuffle data fails due to a cluster scaling down event it generates a NullPointerException error. Solution Set spark.sql.adaptive.skewJoin.enabled to false in your Spark config (AWS | Azure ).", "format": "html", "updated_at": "2022-05-23T16:33:08.667Z"}, "author": {"id": 790229, "email": "mathan.pillai@databricks.com", "name": "mathan.pillai ", "first_name": "mathan.pillai", "last_name": "", "role_id": "admin", "created_at": "2022-01-26T19:28:01.464Z", "updated_at": "2023-04-28T22:28:56.701Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2677400, "name": "aws"}, {"id": 2677417, "name": "azure"}], "url": "https://kb.databricks.com/scala/intermittent-nullpointerexception-aqe-enabled"}, {"id": 1383543, "name": "from_json returns null in Apache Spark 3.0", "views": 9568, "accessibility": 1, "description": "Spark 3.0 and above cannot parse JSON arrays as structs; from_json returns null.", "codename": "from-json-null-spark3", "created_at": "2022-05-23T16:06:44.266Z", "updated_at": "2022-05-23T16:24:09.281Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMSt0WXkzRFZMN0o1WnVBT2F6QzYra2wwNHM1SlZnT0NkMk4yWXVqamtNbXc3dHAwUm54CnJoMWdvbmNGVDd0cTJUenBJY3pSeGgzbHl0Y3IzWU9lSXN2MUNha3pmanJpblduL2F2TmZKWVVicXhGTQpqdWNrS2ZvcHJndGlEMmFJY2VKYnBJYmdoOG1xbS9xaXFXSXl5YU85NUZvbkhQdHlaOVY3a1E1Nm1yUm0KL3F0TklQV1lYSFNyeEVCQmlzVjIzSjRTWnN5U1FzZERpNkg3TDRZSEVYYjNQS2N1RDMrTDF5d3JZZHpzCnRrUG5KbzVvdXZaelZVMUlNTFNVMHZOdkJjVTFDcndoQ1BtN3o0M09DUm5GbHRtRFpoTUU3RkxiTFoyMApmWjFLaW50SmMzQUIwbGMySEZGWU8xQUpkbWFWY0pzT2NHd2FKeWlvZ1FxKzU1cUMrM00rN0d4YXVNc3cKZExFMjdsdy9tajhYTDUrMGZhWFdvU0VPK0pWbGR5aHlDZ1FOOGJKOGtFRHU2K0hzVDRjQ2VzUEpsY2pqClhWMElmU2FCS2JWMzY0Y1FnMDcrZSsyQk9pRXpWa2xTZlRMWWNqYWdIZzR5U0VtQkE5Q1hrT0gwb3hEMApoNHhxSUU2UHQ0ZDFLdDQyQXBMN2M1RVltbnMvMXEyVnQ3R1VubHgyc05jQWN1MVJRYVIyeHF5MDdtNHYKQjR5eEx0RUdOa0E5aytzcXZOWnlyVlM2a0dTMHVsbmxDamJRVlgvRndMQVp3cWJnM241R2pERE5FNGVDClJxblF2TXE1VjY0b1JTSll5b0dWZjljSGNrcnB3ODNTR3RaeHE5K01hay9xZUx5bU5mL0Q4OWYvZjVKcQo4OUcxWkY5SmFYQkdBMUt0ZnpCTWdSbEI3ZEY3cFBOYWlkTGM1Zm53OEZIYU0yZXdOa21qWUJRS21wVCsKNjhhR01ybTZhdEJacWNOcjNVMVpqMFZrV1JEMXN5VVZxZTkvL2o2Ump5ZlE5V1JxbEdYUVFIenZLUDRZClgyN0wwWGhQSXg5RnVOVUFtajdsS3RGSXNkVVpYR0JjdFV3RGRicFBZWkhPMUtkMjlFdkRDQkpNWnlDNgpkZEsxQm9uOXJONUM1NEJzWGg3dFRPUUtYVXZrREd1NDRnMFN3aEFQdTN6eXY3SmtHRVZ2YWVYQ0Q2NXgKZ2J3U1dFRVM0d044dC9Cak1JYmcrRzdBUnVzTWNLOWZJYm1IQmtGVGdMSSs1aDJSdWFmVWxiU2tRNmFxCkFycnJvZkhVdnh2S0FMcXdYdGdvNUVKczdlaTQvcEwwdmJmUHpYSHRPUDRLY3ZjdE9lZ2VpcVVKMFMxUQovYjBvQmw1RkdvYVNIV2Mwa1YvZ2hRbFhyRktyVDVpQ2hxTDZrUjdhWGVuaUpiL1lqa0ttbnpWQm1wVDIKdEt5NGNIY2hjOEw0VXY1SXZQWVI0TjR5V08xUzJncGt4b2sxRjNFU1IyUkMzNmJLbUpBPQo=.8768d5cfb0a861429419c74dd65984cf\"></div><h1 id=\"isPasted\">Problem</h1><p>The <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">from_json</span> function is used to parse a JSON string and return a struct of values.</p><p>For example, if you have the JSON string <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">[{\"id\":\"001\",\"name\":\"peter\"}]</span>, you can pass it to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">from_json</span> with a schema and get parsed struct values in return.</p><pre>%python\r\n\r\nfrom pyspark.sql.functions import col, from_json\r\ndisplay(\r\n\u00a0 df.select(col('value'), from_json(col('value'), json_df_schema, {\"mode\" : \"PERMISSIVE\"}))\r\n)</pre><p>In this example, the dataframe contains a column \u201cvalue\u201d, with the contents <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">[{\u201cid\u201d:\u201d001\u201d,\u201dname\u201d:\u201dpeter\u201d}]</span> and the schema is <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">StructType(List(StructField(id,StringType,true),StructField(name,StringType,true)))</span>.</p><p>This works correctly on Spark 2.4 and below (Databricks Runtime 6.4 ES and below).</p><pre>* id:\r\n\u00a0 \"001\"\r\n* name:\r\n\u00a0 \"peter\"</pre><p>This returns null values on Spark 3.0 and above (Databricks Runtime 7.3 LTS and above).</p><pre>* id:\r\n\u00a0 null\r\n* name:\r\n\u00a0 null</pre><h1>Cause</h1><p>This occurs because Spark 3.0 and above cannot parse JSON arrays as structs.</p><p>You can confirm this by running <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">from_json</span> in <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">FAILFAST</span> mode.</p><pre>%python\r\n\r\nfrom pyspark.sql.functions import col, from_json\r\ndisplay(\r\n\u00a0 df.select(col('value'), from_json(col('value'), json_df_schema, {\"mode\" : \"FAILFAST\"}))\r\n)</pre><p>This returns an error message that defines the root cause.</p><p><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">Caused by: RuntimeException: Parsing JSON arrays as structs is forbidden</span></p><h1>Solution</h1><p>You must pass the schema as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ArrayType</span> instead of <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">StructType</span> in Databricks Runtime 7.3 LTS and above.</p><pre>%python\r\n\r\nfrom pyspark.sql.types import StringType, ArrayType, StructType, StructField\r\nschema_spark_3 = ArrayType(StructType([StructField(\"id\",StringType(),True),StructField(\"name\",StringType(),True)]))\r\n\r\n\r\nfrom pyspark.sql.functions import col, from_json\r\ndisplay(\r\n\u00a0 df.select(col('value'), from_json(col('value'), schema_spark_3, {\"mode\" : \"PERMISSIVE\"}))\r\n)</pre><p>In this example code, the previous <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">StructType</span> schema is enclosed in <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">ArrayType</span> and the new schema is used with <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">from_json</span>.</p><p>This parses the JSON string correctly and returns the expected values.</p>", "body_txt": "Problem The from_json function is used to parse a JSON string and return a struct of values. For example, if you have the JSON string [{\"id\":\"001\",\"name\":\"peter\"}], you can pass it to from_json with a schema and get parsed struct values in return. %python from pyspark.sql.functions import col, from_json display( \u00a0 df.select(col('value'), from_json(col('value'), json_df_schema, {\"mode\" : \"PERMISSIVE\"})) ) In this example, the dataframe contains a column \u201cvalue\u201d, with the contents [{\u201cid\u201d:\u201d001\u201d,\u201dname\u201d:\u201dpeter\u201d}] and the schema is StructType(List(StructField(id,StringType,true),StructField(name,StringType,true))). This works correctly on Spark 2.4 and below (Databricks Runtime 6.4 ES and below). * id: \u00a0 \"001\" * name: \u00a0 \"peter\" This returns null values on Spark 3.0 and above (Databricks Runtime 7.3 LTS and above). * id: \u00a0 null * name: \u00a0 null Cause This occurs because Spark 3.0 and above cannot parse JSON arrays as structs. You can confirm this by running from_json in FAILFAST mode. %python from pyspark.sql.functions import col, from_json display( \u00a0 df.select(col('value'), from_json(col('value'), json_df_schema, {\"mode\" : \"FAILFAST\"})) ) This returns an error message that defines the root cause. Caused by: RuntimeException: Parsing JSON arrays as structs is forbidden Solution You must pass the schema as ArrayType instead of StructType in Databricks Runtime 7.3 LTS and above. %python from pyspark.sql.types import StringType, ArrayType, StructType, StructField schema_spark_3 = ArrayType(StructType([StructField(\"id\",StringType(),True),StructField(\"name\",StringType(),True)])) from pyspark.sql.functions import col, from_json display( \u00a0 df.select(col('value'), from_json(col('value'), schema_spark_3, {\"mode\" : \"PERMISSIVE\"})) ) In this example code, the previous StructType schema is enclosed in ArrayType and the new schema is used with from_json. This parses the JSON string correctly and returns the expected values.", "format": "html", "updated_at": "2022-05-23T16:24:09.276Z"}, "author": {"id": 790360, "email": "shanmugavel.chandrakasu@databricks.com", "name": "shanmugavel.chandrakasu ", "first_name": "shanmugavel.chandrakasu", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T21:14:15.541Z", "updated_at": "2023-04-20T21:51:08.608Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2677354, "name": "aws"}, {"id": 2677355, "name": "azure"}, {"id": 2677356, "name": "gcp"}], "url": "https://kb.databricks.com/scala/from-json-null-spark3"}, {"id": 1383068, "name": "Decimal$DecimalIsFractional assertion error", "views": 8370, "accessibility": 1, "description": "Using `round()` or casing a double to decimal results in a `Decimal$DecimalIsFractional` assertion error. java.lang.AssertionError assertion failed", "codename": "decimal-is-fractional-error", "created_at": "2022-05-23T07:30:48.653Z", "updated_at": "2022-05-23T07:36:01.387Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9VdnRkNG9iNmpJbHlNdFpMRkhvSHdVaTBvRyt2WXJCVjFWYUdqNER1SXpRU1VFdE5iCjRIanR0VndIUmRKLzNpQ0IxNVBjMUdJM2Fja29jWXNjaTl2dUtkMi9NWlFxYjY5M0pac2h4elE4UjJVNApGUG45V1BIdDlsNmFTeVZYRC9vbXVqS0F2N2Y3S2FreWRlWmxnMk9jbkRWekFlK1ZEaFZDRXRycWlEanMKSEJRbUhTM0lBMSsxL2s0MG5RVXFudG5jcUl0WHF5djNWZ29zcFNrdFVObGk2UnBWZHB3VTZQN1dDMXFKCllTTUVVWDhOeCtocC9rWUN3R2JudmpycXhNTnIrZkVKbEFDVkd3enBGNGN0N1Z4K3lQMGdCU0pPaStSQgp5VWdQMFJRZXVvZHg0NW9TM2diZmlBUnMxM0RKMkxCMElXek9zb3ljbGlyZ3BYYjNpRWhHTlpFQklCUUsKWWhuUEVMd1dYTWt6dW1UWFJ4MnkzMm5yVzN3b1lGemorQ2FxSzBlR0F2YzVudDErb3hXdE5qSHlCSkxwCjBJbzViZERyL0VScjg2TEZvMWUvNmM4dCtEZWtGOFQ5REtkNndwTnJTVGpWYm1rVU84QUFRRlc3cVRBYwpGeXl2aktWZFlzaXpDVlI5cmNaWE5tU1FyejNiWkVvSDZ1Lzk5SGt4aFRPbXl1by9ycTJpNkFueVplNmEKVWhzaHVHUmlIYUUvMlBhaEJ3a2JMWG12eGlrbnA5a1NWaGMvcXdvcmVRd0dpcWxuZzZwSnhvWWRYUEw1Cm1ucGw0VVpucUxqTHVCTHBYdFJvaTUyRHpVR0FFak1lU0xUdGRTdjJwM1pxUGhxUmcwTU9ETS8ydXQ4WQpweWtCRHZkaXcrNGdDb0pNVGFKd0pBbzFTbGtTSm1uOGhFZ2daN2xlNHNFNHpGRkt1NjZYNElsaDFkaUwKT2x0L1JXM3VBQjJ4eVJLdStHS2hrb2l5bXdBSlcwWDJ1REQzOTdhK1FibUJ5YkNTekE1VjdISjRra3MxCmt0dzFpRnJIOFJ2WjRxd0tYc2o1Yk92cWsyN3FrL2IyV3NsV3d0TkVqUm5rMk5GcmFTSzl5YmYzbUpUcApRVjZNNklVWnZWTzgwQW1BWGprUUMvSGFXOVNGVzFPRkc2SDlLSmw3bGdiTFQ5M1NCYVdKODVJWGNWUkgKRTZnTHRKS2Z0YmlzaXdGTk9oRXU0akNiWUFaVEV1eTlBeFkybFNxUFNPSW1YdENyeXdxb2RkV1NKTFBqCjJiZDhTSXh5NkhGYUk2bkhjU2hxRHdCeUxZZWJITUVONkoyYjFkRVU5VjBKTlJlRTk0SHRLQW1WemRFZQoyVDNlQmF5bUhqaisyTlRQMS80dFQ2NXBZYWFZVFVOZmJveGQ1VStIZ0hjdDRzQU0yVXlUZWJtWnhyZHgKellDaDg2NWkyd0VsK1piNzI3WkdNczZRQTZPWHpIVTkrZDFxN3labEdqQW1pbDFmYUxRPQo=.77b86521de498c98adf913d8d30bcd5d\"></div><h1 id=\"isPasted\">Problem</h1><p>You are running a job on Databricks Runtime 7.x or above when you get a <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">java.lang.AssertionError: assertion failed: Decimal$DecimalIsFractional</span> error message.</p><p>Example stack trace:</p><pre>java.lang.AssertionError: assertion failed:\r\n\u00a0Decimal$DecimalIsFractional\r\n\u00a0 while compiling: &lt;notebook&gt;\r\n\u00a0 \u00a0during phase: globalPhase=terminal, enteringPhase=jvm\r\n\u00a0 library version: version 2.12.10\r\n\u00a0compiler version: version 2.12.10\r\nreconstructed args: -deprecation -classpath .....\r\n*** WARNING: skipped 126593 bytes of output ***</pre><p>This error message only occurs on the first run of your notebook. Subsequent runs complete without error.</p><h1>Cause</h1><p>There are two common use cases that can trigger this error message.</p><ul>\n<li>\n<strong>Cause 1</strong>: You are trying to use the round() function on a decimal column that contains null values in a notebook.</li>\n<li>\n<strong>Cause 2</strong>: You are casting a double column to a decimal column in a notebook.</li>\n</ul><p>This example code can be used to reproduce the error:</p><pre>%scala\r\n\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.{DataFrame, SparkSession}\r\nimport org.apache.spark.sql.Column\r\n\r\n//Sample data with decimal values\r\n\r\nval updateData = \u00a0Seq(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Row(BigDecimal.decimal(123.456), 123.456),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Row(BigDecimal.decimal(123.456), 123.456))\r\n\r\nval updateSchema = List(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StructField(\"amt_decimal\", DecimalType(14,3), true),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StructField(\"amt_double\", DoubleType, true))\r\n\r\nval testDF = \u00a0spark.createDataFrame(\r\n\u00a0 spark.sparkContext.parallelize(updateData),\r\n\u00a0 StructType(updateSchema)\r\n)\r\n\r\n// Cause 1:\r\n// round() on the Decimal column reproduces the error\r\n\r\ntestDF.withColumn(\"round_amt_decimal\",round(col(\"amt_decimal\"),2)).show()\r\n\r\n// Cause 2:\r\n// CAST() on the Double column to Decimal reproduces the error\r\n\r\ntestDF.createOrReplaceTempView(\"dec_table\")\r\nspark.sql(\"select CAST(amt_double AS DECIMAL(3,3)) AS dec_col from dec_table\").show()</pre><h1>Solution</h1><p>This is a known issue and can be safely ignored.</p><p><br></p><p>The error message does not halt the notebook run and it should not result in any data loss.</p>", "body_txt": "Problem You are running a job on Databricks Runtime 7.x or above when you get a java.lang.AssertionError: assertion failed: Decimal$DecimalIsFractional error message. Example stack trace: java.lang.AssertionError: assertion failed: \u00a0Decimal$DecimalIsFractional \u00a0 while compiling: &lt;notebook&gt; \u00a0 \u00a0during phase: globalPhase=terminal, enteringPhase=jvm \u00a0 library version: version 2.12.10 \u00a0compiler version: version 2.12.10 reconstructed args: -deprecation -classpath ..... *** WARNING: skipped 126593 bytes of output *** This error message only occurs on the first run of your notebook. Subsequent runs complete without error. Cause There are two common use cases that can trigger this error message. Cause 1: You are trying to use the round() function on a decimal column that contains null values in a notebook. Cause 2: You are casting a double column to a decimal column in a notebook. This example code can be used to reproduce the error: %scala import org.apache.spark.sql.functions._ import org.apache.spark.sql.types._ import org.apache.spark.sql.{DataFrame, SparkSession} import org.apache.spark.sql.Column //Sample data with decimal values val updateData = \u00a0Seq( \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Row(BigDecimal.decimal(123.456), 123.456), \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Row(BigDecimal.decimal(123.456), 123.456)) val updateSchema = List( \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StructField(\"amt_decimal\", DecimalType(14,3), true), \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StructField(\"amt_double\", DoubleType, true)) val testDF = \u00a0spark.createDataFrame( \u00a0 spark.sparkContext.parallelize(updateData), \u00a0 StructType(updateSchema) ) // Cause 1: // round() on the Decimal column reproduces the error testDF.withColumn(\"round_amt_decimal\",round(col(\"amt_decimal\"),2)).show() // Cause 2: // CAST() on the Double column to Decimal reproduces the error testDF.createOrReplaceTempView(\"dec_table\") spark.sql(\"select CAST(amt_double AS DECIMAL(3,3)) AS dec_col from dec_table\").show() Solution This is a known issue and can be safely ignored. The error message does not halt the notebook run and it should not result in any data loss.", "format": "html", "updated_at": "2022-05-23T07:36:01.385Z"}, "author": {"id": 790278, "email": "saikrishna.pujari@databricks.com", "name": "saikrishna.pujari ", "first_name": "saikrishna.pujari", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T20:08:51.444Z", "updated_at": "2023-02-20T11:00:57.025Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2677101, "name": "aws"}, {"id": 2677102, "name": "azure"}], "url": "https://kb.databricks.com/scala/decimal-is-fractional-error"}, {"id": 1382248, "name": "Create a DataFrame from a JSON string or Python dictionary", "views": 37859, "accessibility": 1, "description": "Create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary.", "codename": "create-df-from-json-string-python-dictionary", "created_at": "2022-05-20T09:22:47.839Z", "updated_at": "2022-07-01T11:31:59.158Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMTlNMmdLbGcvWW9seTFBRnZpalNxT0hYNVl4NkJiNDR3NXRJSk9aZUpNUFdXL0NQU0p5CmduT2JoNVVPQmRxWURkcFh3TTlWRmEvb0p3TWdMbWMzMlZaeTdaZzVCRWNKeGpPMGs5NXNoWDRKSVdvawpvdUF6UzRFZGU1Z1N6NG9oM3pJN25ZYXJqUTJuTWlTZ0xmdjhFUXF4SENxR2haQTFyR1VtTzAyQU0vVUoKN3FhSTZnbUZqRldpZTBzL283OGRSQXpkNWt5ZEhlb2lBalpEbnQ3SGtjM0ZiTWZDM0lPWnhUcDRuV1FSClQ2NDl1ellZbDk5T2ZxWUtuQTJFRkpxaEVTdEtOa2VLUFMzQ3pyc3pNVTQ0cis3L0h1dGhWVnN0dzJkawo2VzhEUVdlVHhoNE5DbjVGUTBENHRyQXdqbTJBTXF5TUFYc2dTSWVFSXVHOGVnVkNFUW9DcC95NWhVWlIKelZCbWZWakFQS0ZCQm9ZMmxPTUJlNnJyYXliczFaL09WTE5oQjlGNGJIb1lpVlJrRkZUUzc5NjBwMFRrClpSSU5KZ2o3aWZnZzArRnBRNTcvNDkwMHdGQ3R5eXJWODhjZXFVck13dDA0dUZFQ0xRZk5YakQ0Tk9NVApwZ2QwaDQwRTRtQm1Ba21rblVYblV3TFJrT2V2ekszazNBSDJIdjhtV1N2WVhNZHhMUXMyZXN6MHVlV24KVkZNbU1IN2xrUG1PM2dUZ1RCaDI1bmtDbUN2OGVTWnZhd3dkeG9mWjlzWnBQOURsTWJzRWtGOTVIU1IvCjdUVk5aVDVqcHR3QSt1NG02Y2VMN3o5ZVJXY1hVUjltaUhKWjh6Q3lYaTRnRjBQWnk5T2JORXoxTXBsbApPMVJta0lRVzVYSG9UK0pQbW9VZ1JKTW5UaWsvZjNXQmc3UU9aTkU3bityVU9KWWZKVXJ5dHF5bTFBWWUKR1NrMVdXVm9pRVJwRHdteXM5YXFaQmJBRGZRSHZ1VTZqTmJ6dCs2cFhMRVYwTzE1dXVTWlh5TFNLV2lZCm1sOVJ0NlRUYVppMVE0eDVxeE53dFhqanVia1JGRXR4SUdPRVlRRDRFTm5FZC82YmZqVVE3blkwMTl2VgpiRGFZcE9HMFZVQVBmQytwcEZEMng1dVFGNnMyYW5HMnl5eStVQkVabHNKSTJaTFRWcWxEbVJ1T3hKQmYKaVFsb0J3UEtheUE4cG80Z05jSUViUGRvUzhhZERPdU5KUXp1MFlGYTNmc0R0VkVyc0xDR3BkdGQydlRYClByT3B3UHI3a3dYWmpUdGdhQnNvVk1hOUd6NWpUdG1Dc0J1NGpOWXpLTXk4Q2dCazNJQ3lkYzRwQ0FYdAp4TXBRM0FrNTlsMnMwak1TUitiSzNoQ2JnVkxvczZ0QWY2TVlBZ1lWQTVjeksyQjJGQ003dHpvN1J3MXgKVFY0NS9zampOaWRqYTFLWURFMnp1ejU5eWNSUU1Yd1RNY3h0Nng3emJkS2Y3OUpxRU9GQzNMdFVFVWVjCjFOSkI3M2JFL2RnTgo=.df01e16f383c6207fb3a920e9513a51d\"></div><p id=\"isPasted\">In this article we are going to review how you can create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary.</p><h1>Create a Spark DataFrame from a JSON string</h1><ol>\n<li>Add the JSON content from the variable to a list.<pre>%scala\r\n\r\nimport scala.collection.mutable.ListBuffer\r\nval json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\r\nval json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\r\n\r\nvar json_seq = new ListBuffer[String]()\r\njson_seq += json_content1\r\njson_seq += json_content2</pre>\n</li>\n<li>Create a Spark dataset from the list.<pre>%scala\r\n\r\nval json_ds = json_seq.toDS()</pre>\n</li>\n<li>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.read.json\u00a0</span>to parse the Spark dataset.<pre>%scala\r\n\r\nval df= spark.read.json(json_ds)\r\ndisplay(df)</pre>\n</li>\n</ol><h1>Combined sample code</h1><p>These sample code blocks combine the previous steps into individual examples. The Python and Scala samples perform the same tasks.</p><pre>%python\r\n\r\njson_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\r\njson_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\r\n\r\njson_list = []\r\njson_list.append(json_content1)\r\njson_list.append(json_content2)\r\n\r\ndf = spark.read.json(sc.parallelize(json_list))\r\ndisplay(df)</pre><pre>%scala\r\n\r\nimport scala.collection.mutable.ListBuffer\r\nval json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\"\r\nval json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\"\r\n\r\nvar json_seq = new ListBuffer[String]()\r\njson_seq += json_content1\r\njson_seq += json_content2\r\n\r\nval json_ds = json_seq.toDS()\r\nval df= spark.read.json(json_ds)\r\ndisplay(df)</pre><h1>Extract a string column with JSON data from a DataFrame and parse it</h1><ol>\n<li>Select the JSON column from a DataFrame and convert it to an RDD of type <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RDD[Row]</span>.<pre>%scala\r\n\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\n\r\nval test_df = Seq((\"1\", \"{'json_col1': 'hello', 'json_col2': 32}\", \"1.0\"),(\"1\", \"{'json_col1': 'hello', 'json_col2': 'world'}\", \"1.0\")).toDF(\"row_number\", \"json\", \"token\")\r\n\r\n\r\nval row_rdd = test_df.select(col(\"json\")).rdd \u00a0// Selecting just the JSON column and converting it to RDD.</pre>\n</li>\n<li>Convert <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RDD[Row]</span> to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RDD[String]</span>.<pre>%scala\r\n\r\nval string_rdd = row_rdd.map(_.mkString(\",\"))</pre>\n</li>\n<li>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.read.json</span> to parse the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">RDD[String]</span>.<pre>%scala\r\n\r\n\r\nval df1= spark.read.json(string_rdd)\r\n\u00a0display(df1)</pre>\n</li>\n</ol><h2>Combined sample code</h2><p>This sample code block combines the previous steps into a single example.</p><pre>%scala\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\nval test_df = Seq((\"1\", \"{'json_col1': 'hello', 'json_col2': 32}\", \"1.0\"),(\"1\", \"{'json_col1': 'hello', 'json_col2': 'world'}\", \"1.0\")).toDF(\"row_number\", \"json\", \"token\")\r\n\r\nval row_rdd = test_df.select(col(\"json\")).rdd\r\nval string_rdd = row_rdd.map(_.mkString(\",\"))\r\n\r\nval df1= spark.read.json(string_rdd)\r\ndisplay(df1)</pre><h1>Create a Spark DataFrame from a Python dictionary</h1><ol>\n<li>Check the data type and confirm that it is of dictionary type.<pre>%python\r\n\r\njsonDataDict = {\"job_id\":33100,\"run_id\":1048560,\"number_in_job\":1,\"state\":{\"life_cycle_state\":\"PENDING\",\"state_message\":\"Waiting for cluster\"},\"task\":{\"notebook_task\":{\"notebook_path\":\"/Users/user@databricks.com/path/test_notebook\"}},\"cluster_spec\":{\"new_cluster\":{\"spark_version\":\"4.3.x-scala2.11\",\"attributes\":{\"type\":\"fixed_node\",\"memory\":\"8g\"},\"enable_elastic_disk\":\"false\",\"num_workers\":1}},\"cluster_instance\":{\"cluster_id\":\"0000-000000-wares10\"},\"start_time\":1584689872601,\"setup_duration\":0,\"execution_duration\":0,\"cleanup_duration\":0,\"creator_user_name\":\"user@databricks.com\",\"run_name\":\"my test job\",\"run_page_url\":\"https://testurl.databricks.com#job/33100/run/1\",\"run_type\":\"SUBMIT_RUN\"}\r\n\r\ntype(jsonDataDict)</pre>\n</li>\n<li>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">json.dumps</span>to convert the Python dictionary into a JSON string.<pre>%python\r\n\r\nimport json\r\njsonData = json.dumps(jsonDataDict)</pre>\n</li>\n<li>Add the JSON content to a list.<pre>%python\r\n\r\njsonDataList = []\r\njsonDataList.append(jsonData)</pre>\n</li>\n<li>Convert the list to a RDD and parse it using <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.read.json</span>.<pre>%python\r\n\r\njsonRDD = sc.parallelize(jsonDataList)\r\ndf = spark.read.json(jsonRDD)\r\ndisplay(df)</pre>\n</li>\n</ol><h2>Combined sample code</h2><p>These sample code block combines the previous steps into a single example.</p><pre>%python\r\n\r\njsonDataDict = {\"job_id\":33100,\"run_id\":1048560,\"number_in_job\":1,\"state\":{\"life_cycle_state\":\"PENDING\",\"state_message\":\"Waiting for cluster\"},\"task\":{\"notebook_task\":{\"notebook_path\":\"/Users/user@databricks.com/path/test_notebook\"}},\"cluster_spec\":{\"new_cluster\":{\"spark_version\":\"4.3.x-scala2.11\",\"attributes\":{\"type\":\"fixed_node\",\"memory\":\"8g\"},\"enable_elastic_disk\":\"false\",\"num_workers\":1}},\"cluster_instance\":{\"cluster_id\":\"0000-000000-wares10\"},\"start_time\":1584689872601,\"setup_duration\":0,\"execution_duration\":0,\"cleanup_duration\":0,\"creator_user_name\":\"user@databricks.com\",\"run_name\":\"my test job\",\"run_page_url\":\"https://testurl.databricks.com#job/33100/run/1\",\"run_type\":\"SUBMIT_RUN\"}\r\n\r\ntype(jsonDataDict)\r\n\r\nimport json\r\njsonData = json.dumps(jsonDataDict)\r\n\r\njsonDataList = []\r\njsonDataList.append(jsonData)\r\n\r\njsonRDD = sc.parallelize(jsonDataList)\r\ndf = spark.read.json(jsonRDD)\r\ndisplay(df)</pre><h1>Example notebook</h1><p>Review the <a href=\"https://docs.databricks.com/_static/notebooks/kb/scala/parse-json-string-python-dictionary-example.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Parse a JSON string or Python dictionary example notebook</a>.</p><p><br></p>", "body_txt": "In this article we are going to review how you can create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary. Create a Spark DataFrame from a JSON string Add the JSON content from the variable to a list.%scala import scala.collection.mutable.ListBuffer val json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\" val json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\" var json_seq = new ListBuffer[String]() json_seq += json_content1 json_seq += json_content2 Create a Spark dataset from the list.%scala val json_ds = json_seq.toDS() Use spark.read.json\u00a0to parse the Spark dataset.%scala val df= spark.read.json(json_ds) display(df) Combined sample code These sample code blocks combine the previous steps into individual examples. The Python and Scala samples perform the same tasks. %python json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\" json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\" json_list = [] json_list.append(json_content1) json_list.append(json_content2) df = spark.read.json(sc.parallelize(json_list)) display(df) %scala import scala.collection.mutable.ListBuffer val json_content1 = \"{'json_col1': 'hello', 'json_col2': 32}\" val json_content2 = \"{'json_col1': 'hello', 'json_col2': 'world'}\" var json_seq = new ListBuffer[String]() json_seq += json_content1 json_seq += json_content2 val json_ds = json_seq.toDS() val df= spark.read.json(json_ds) display(df) Extract a string column with JSON data from a DataFrame and parse it Select the JSON column from a DataFrame and convert it to an RDD of type RDD[Row].%scala import org.apache.spark.sql.functions._ val test_df = Seq((\"1\", \"{'json_col1': 'hello', 'json_col2': 32}\", \"1.0\"),(\"1\", \"{'json_col1': 'hello', 'json_col2': 'world'}\", \"1.0\")).toDF(\"row_number\", \"json\", \"token\") val row_rdd = test_df.select(col(\"json\")).rdd \u00a0// Selecting just the JSON column and converting it to RDD. Convert RDD[Row] to RDD[String].%scala val string_rdd = row_rdd.map(_.mkString(\",\")) Use spark.read.json to parse the RDD[String].%scala val df1= spark.read.json(string_rdd) \u00a0display(df1) Combined sample code This sample code block combines the previous steps into a single example. %scala import org.apache.spark.sql.functions._ val test_df = Seq((\"1\", \"{'json_col1': 'hello', 'json_col2': 32}\", \"1.0\"),(\"1\", \"{'json_col1': 'hello', 'json_col2': 'world'}\", \"1.0\")).toDF(\"row_number\", \"json\", \"token\") val row_rdd = test_df.select(col(\"json\")).rdd val string_rdd = row_rdd.map(_.mkString(\",\")) val df1= spark.read.json(string_rdd) display(df1) Create a Spark DataFrame from a Python dictionary Check the data type and confirm that it is of dictionary type.%python jsonDataDict = {\"job_id\":33100,\"run_id\":1048560,\"number_in_job\":1,\"state\":{\"life_cycle_state\":\"PENDING\",\"state_message\":\"Waiting for cluster\"},\"task\":{\"notebook_task\":{\"notebook_path\":\"/Users/user@databricks.com/path/test_notebook\"}},\"cluster_spec\":{\"new_cluster\":{\"spark_version\":\"4.3.x-scala2.11\",\"attributes\":{\"type\":\"fixed_node\",\"memory\":\"8g\"},\"enable_elastic_disk\":\"false\",\"num_workers\":1}},\"cluster_instance\":{\"cluster_id\":\"0000-000000-wares10\"},\"start_time\":1584689872601,\"setup_duration\":0,\"execution_duration\":0,\"cleanup_duration\":0,\"creator_user_name\":\"user@databricks.com\",\"run_name\":\"my test job\",\"run_page_url\":\"https://testurl.databricks.com#job/33100/run/1\",\"run_type\":\"SUBMIT_RUN\"} type(jsonDataDict) Use json.dumpsto convert the Python dictionary into a JSON string.%python import json jsonData = json.dumps(jsonDataDict) Add the JSON content to a list.%python jsonDataList = [] jsonDataList.append(jsonData) Convert the list to a RDD and parse it using spark.read.json.%python jsonRDD = sc.parallelize(jsonDataList) df = spark.read.json(jsonRDD) display(df) Combined sample code These sample code block combines the previous steps into a single example. %python jsonDataDict = {\"job_id\":33100,\"run_id\":1048560,\"number_in_job\":1,\"state\":{\"life_cycle_state\":\"PENDING\",\"state_message\":\"Waiting for cluster\"},\"task\":{\"notebook_task\":{\"notebook_path\":\"/Users/user@databricks.com/path/test_notebook\"}},\"cluster_spec\":{\"new_cluster\":{\"spark_version\":\"4.3.x-scala2.11\",\"attributes\":{\"type\":\"fixed_node\",\"memory\":\"8g\"},\"enable_elastic_disk\":\"false\",\"num_workers\":1}},\"cluster_instance\":{\"cluster_id\":\"0000-000000-wares10\"},\"start_time\":1584689872601,\"setup_duration\":0,\"execution_duration\":0,\"cleanup_duration\":0,\"creator_user_name\":\"user@databricks.com\",\"run_name\":\"my test job\",\"run_page_url\":\"https://testurl.databricks.com#job/33100/run/1\",\"run_type\":\"SUBMIT_RUN\"} type(jsonDataDict) import json jsonData = json.dumps(jsonDataDict) jsonDataList = [] jsonDataList.append(jsonData) jsonRDD = sc.parallelize(jsonDataList) df = spark.read.json(jsonRDD) display(df) Example notebook Review the Parse a JSON string or Python dictionary example notebook.", "format": "html", "updated_at": "2022-07-01T11:31:59.155Z"}, "author": {"id": 791511, "email": "ram.sankarasubramanian@databricks.com", "name": "ram.sankarasubramanian ", "first_name": "ram.sankarasubramanian", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T01:51:47.575Z", "updated_at": "2023-04-10T05:38:44.201Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2677098, "name": "aws"}, {"id": 2677099, "name": "azure"}, {"id": 2677100, "name": "gcp"}], "url": "https://kb.databricks.com/scala/create-df-from-json-string-python-dictionary"}, {"id": 1382243, "name": "Convert nested JSON to a flattened DataFrame", "views": 16077, "accessibility": 1, "description": "How to convert a flattened DataFrame to nested JSON using a nested case class.", "codename": "flatten-nested-columns-dynamically", "created_at": "2022-05-20T09:16:01.657Z", "updated_at": "2022-05-20T09:22:29.648Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS8wZDk1SFVQRW92WUxRZ3dMZXNsc09vNGZLTy8xbjZleDJ5K2pHVDJoWm51T250MEdZCkJTWU9kd1lLeklJMkUxMUV6enVQcWZIUklDRGZNQm51VE9PTWdPUzQrNnV6QTA0ek5Jbm80blhhaE44RApaZnUwaGtCSHo0TXIzano3VXkxWkx6Q01DaC9meVEza3NYVDJnamNOVUJNZjFuaXRxTlY3S0theXM4bE8KblhlUllBZmsxRkRRdDdDVHd5djdpVDNoTHo3RzJDMXJyR0lMeHk2bTBuazJSQU45VE1IdFgzLzJGdldrCmN1YVV3d2V2WUVYcHRNaTNVNkcyRW40UDNocW82cWQvQitOV1lEOW40QzkwN1llNXBzZkJ6MHYvYWhicwpUWTdMbGVsWWc2aFUrcEovV2ZzREpMTjRGM0RNODlJR2JBQmNRcnYyZmc5amp4NG01Z1BHemhjTlIrUEQKblRTRWJFWXBKYlBaQXU5WFg0NGxoUUk3UFlYQlc5bWlhb0xJOXlFV3dNSDNZN3BQN2JIMFJqRFJ2RFRnCjI0YmV6Q3VwRVBsUWlTUEx4TnJmbm1nem85Q1hjNW1LZHltYXdBRk9wdkkySXVmU2dzSldZeVBLNkpwUgpwL3liQnUwYWR5K09nN0ZHa2VzS2xwMW54dUgzNUJ5bFcvdDQyOXFlbmliT1NWRlhFQWNTWUhiOHZCQXoKWVRmL1hYNmt5MWxnbFFoK0s5MzBCMXN3NmdteGtpbUw0UTgrMzErOGhxUTNsKzFXMlNNNUUxMVp0UU5xCkxxSWk1aWE4ZzhzZWxnOEdld053azZramt2T1JWSlFjdjFXL085VFBNbzZXMDhXekF5U01TeXg4UlZNaQp6c3FKT0RVMmljTDV1bndHWFJ6cjVMeG1Dbk8zV0RDejNqd2lCM0RmaXRWWWp4ZllrZTlQN0loaGFYaXgKNC8rZ3ZWbDdvUUo4cTJJTkRkaTlNb281UW5QUWhyZ3dwdHJtK05kaDc2dlFHVGtvQjl0MVZ0aTdKbkUyCk1sVFZSVHR5d3g3OUdIdGo4N2svQ1QxZ25lSm0wRXhUY3RzeU5zeXJuOXpFWWpuMWtacDMwaWRrVEJwagpqL2FtQmUvcmNFL2NXUXl2NVJ6R0IwWHJzb0orOGtoUjFBeFNLYjh3a0RFa2ZzZXpqWm5YVkNwc1ZSR0YKTmxycURhanMyL0plMkdBOXcrOC80SVNnTDZFWGdaM3ovbW9uaWRxUlFkbTAvWUJGbEI5UWJldkVvN1ZFCg==.9b9ef2fd866f09ba3526ea04a54c03e8\"></div><p id=\"isPasted\">This article shows you how to flatten nested JSON, using only <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">$\"column.*\"</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">explode</span> methods.</p><h1>Sample JSON file</h1><p>Pass the sample JSON string to the reader.</p><pre>%scala\r\n\r\nval json =\"\"\"\r\n{\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"id\": \"0001\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"donut\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"Cake\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"ppu\": 0.55,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"batters\":\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"batter\":\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"1001\", \"type\": \"Regular\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"1002\", \"type\": \"Chocolate\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"1003\", \"type\": \"Blueberry\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"1004\", \"type\": \"Devil's Food\" }\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"topping\":\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5001\", \"type\": \"None\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5002\", \"type\": \"Glazed\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5005\", \"type\": \"Sugar\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5003\", \"type\": \"Chocolate\" },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5004\", \"type\": \"Maple\" }\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\r\n}\r\n\"\"\"</pre><h1>Convert to DataFrame</h1><p>Add the JSON string as a collection type and pass it as an input to <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">spark.createDataset</span>. This converts it to a DataFrame. The JSON reader infers the schema automatically from the JSON string.</p><p>This sample code uses a list collection type, which is represented as <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">json :: Nil</span>. You can also use other Scala collection types, such as Seq (Scala Sequence).</p><pre>%scala\r\n\r\nimport org.apache.spark.sql.functions._\r\nimport spark.implicits._\r\nval DF= spark.read.json(spark.createDataset(json :: Nil))</pre><h1>Extract and flatten</h1><p>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">$\"column.*\"</span> and <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">explode</span> methods to flatten the struct and array types before displaying the flattened DataFrame.</p><pre>%scala\r\n\r\ndisplay(DF.select($\"id\" as \"main_id\",$\"name\",$\"batters\",$\"ppu\",explode($\"topping\")) // Exploding the topping column using explode as it is an array type\r\n\u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"topping_id\",$\"col.id\") // Extracting topping_id from col using DOT form\r\n\u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"topping_type\",$\"col.type\") // Extracting topping_tytpe from col using DOT form\r\n\u00a0 \u00a0 \u00a0 \u00a0 .drop($\"col\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 .select($\"*\",$\"batters.*\") // Flattened the struct type batters tto array type which is batter\r\n\u00a0 \u00a0 \u00a0 \u00a0 .drop($\"batters\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 .select($\"*\",explode($\"batter\"))\r\n\u00a0 \u00a0 \u00a0 \u00a0 .drop($\"batter\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"batter_id\",$\"col.id\") // Extracting batter_id from col using DOT form\r\n\u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"battter_type\",$\"col.type\") // Extracting battter_type from col using DOT form\r\n\u00a0 \u00a0 \u00a0 \u00a0 .drop($\"col\")\r\n\u00a0 \u00a0 \u00a0 \u00a0)</pre><div class=\"hj-alert-block hj-warning-block\" data-controller=\"alert-block\">\n<a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\">\n<h3 class=\"hj-alert-heading\">Warning</h3>\n<p class=\"hj-alert-text\">Make sure to use $ for all column names, otherwise you may get an error message: <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">overloaded method value select with alternatives</span>.</p>\n</div>\n</div><h1>Example notebook</h1><p>Run the <a href=\"https://docs.databricks.com/_static/notebooks/kb/scala/nested-json-to-dataframe.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">Nested JSON to DataFrame example notebook</a> to view the sample code and results.</p>", "body_txt": "This article shows you how to flatten nested JSON, using only $\"column.*\" and explode methods. Sample JSON file Pass the sample JSON string to the reader. %scala val json =\"\"\" { \u00a0 \u00a0 \u00a0 \u00a0 \"id\": \"0001\", \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"donut\", \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"Cake\", \u00a0 \u00a0 \u00a0 \u00a0 \"ppu\": 0.55, \u00a0 \u00a0 \u00a0 \u00a0 \"batters\": \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"batter\": \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"1001\", \"type\": \"Regular\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"1002\", \"type\": \"Chocolate\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"1003\", \"type\": \"Blueberry\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"1004\", \"type\": \"Devil's Food\" } \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ] \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }, \u00a0 \u00a0 \u00a0 \u00a0 \"topping\": \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5001\", \"type\": \"None\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5002\", \"type\": \"Glazed\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5005\", \"type\": \"Sugar\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5007\", \"type\": \"Powdered Sugar\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5003\", \"type\": \"Chocolate\" }, \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"id\": \"5004\", \"type\": \"Maple\" } \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ] } \"\"\" Convert to DataFrame Add the JSON string as a collection type and pass it as an input to spark.createDataset. This converts it to a DataFrame. The JSON reader infers the schema automatically from the JSON string. This sample code uses a list collection type, which is represented as json :: Nil. You can also use other Scala collection types, such as Seq (Scala Sequence). %scala import org.apache.spark.sql.functions._ import spark.implicits._ val DF= spark.read.json(spark.createDataset(json :: Nil)) Extract and flatten Use $\"column.*\" and explode methods to flatten the struct and array types before displaying the flattened DataFrame. %scala display(DF.select($\"id\" as \"main_id\",$\"name\",$\"batters\",$\"ppu\",explode($\"topping\")) // Exploding the topping column using explode as it is an array type \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"topping_id\",$\"col.id\") // Extracting topping_id from col using DOT form \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"topping_type\",$\"col.type\") // Extracting topping_tytpe from col using DOT form \u00a0 \u00a0 \u00a0 \u00a0 .drop($\"col\") \u00a0 \u00a0 \u00a0 \u00a0 .select($\"*\",$\"batters.*\") // Flattened the struct type batters tto array type which is batter \u00a0 \u00a0 \u00a0 \u00a0 .drop($\"batters\") \u00a0 \u00a0 \u00a0 \u00a0 .select($\"*\",explode($\"batter\")) \u00a0 \u00a0 \u00a0 \u00a0 .drop($\"batter\") \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"batter_id\",$\"col.id\") // Extracting batter_id from col using DOT form \u00a0 \u00a0 \u00a0 \u00a0 .withColumn(\"battter_type\",$\"col.type\") // Extracting battter_type from col using DOT form \u00a0 \u00a0 \u00a0 \u00a0 .drop($\"col\") \u00a0 \u00a0 \u00a0 \u00a0) Warning\nMake sure to use $ for all column names, otherwise you may get an error message: overloaded method value select with alternatives. Example notebook Run the Nested JSON to DataFrame example notebook to view the sample code and results.", "format": "html", "updated_at": "2022-05-20T09:22:29.646Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:30.391Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2676634, "name": "aws"}, {"id": 2676635, "name": "azure"}, {"id": 2676636, "name": "gcp"}], "url": "https://kb.databricks.com/scala/flatten-nested-columns-dynamically"}, {"id": 1382239, "name": "Convert flattened DataFrame to nested JSON", "views": 10504, "accessibility": 1, "description": "How to convert a flattened DataFrame to nested JSON using a nested case class.", "codename": "convert-flat-df-to-nested-json", "created_at": "2022-05-20T09:07:36.646Z", "updated_at": "2022-05-20T09:15:43.001Z", "next_expiration_on": null, "published": true, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMStpb0Z6aDdGdHNOUE1zbGNqbXBTamN5WDdYTXJKbG5Vd1lEaldvQlZtTVRaall3T0dGCmJLSS9ad2J4cCtKbUV5QW1meEhlSlFVYVRFMDRLOExFY1hHcFlaby9iaDNoN1J6MHhyUHlOSzRvRVhEdwpFUWVyVHpHRW1tVnZOYVVET3RZRmYzSFkxc2p0RGFCMUFRcWpRYmRnR3I0RkU5RmVKUFA0elVLQi9PZWgKdklFL081bkE4R1J2Mnl2ckZWaERuK2RmK3J5Q3RIem45ZkR5blFRMVZXSzZmWFI0ZnM5YnJtOW5sdmd3CktiWXFJRDlmZjJuRHJ5aG1ucGdBRGxkSnJVWWJWbDYydG9SZEFyekJkT3Ixank5RXdETWVPVHlZMFZTQgp1YUUwUlFyeGVyUjZjdUQvZFJCYUs5YzdvZjZjSzI3SVdLZ1RhRE9nS09EZk4zclVRUmEyOFNZSU5PemsKV3ByVHdYdVpwRHpwaGd1RnZ3M09PTGd3V1BvTkRCU3dkekFJWVFxWnNXZ2dOUjJZcEVvR3pBdHBWaUMzCmVtOEVvK3V5T3NuZTJSak9qMnNIRGxWR2d5UVg0UUVUYWgrMXVNN0JUWkppRzkwZmI3aGV5cDZqRDQwZAo3QURXTVB6UkQxZ0lQOHcxeUFZcWx2R0dIY2s0UG5tVWNFeXh6OHc1YzZ6eFh2TFhRcCtYbjdGWk9SYWgKZGFHTGg0QmVhNUIrOHpycmhJWW0zemhLVWlYeXpwRk5PQ1J1WXlXUzVoVWQybXRDbzgvYnhISlZXd2FGCk1XT0U5OTFwQkNYRklYUVBMakJ6TU9hb0NvQitJdEZPMmNsYytiZHUzaXg2N2RFYVNNS05KV1pCSitoYgoxUStFWmkwc0JaLzBPVzNick1ENkhRN0FCQUMyeUNSVFlXZmkwQnd3NFZWdThVMGxjQnEzSDk4TENYRzIKUTJOcVNZeVF4cktVQ2FXK0JnS0NveGpKZmhpdy9mUW8yQjFaMjNvcHpkd3B6U0tqcVVhSUFmQ0ZneHlSCmdzZ3M0TmJUeEFJZGZZVWptY1Z2K0dzNmhqK3U2czBKU3JFSjJ6MFNaWHcvNFFYMTFPZE5tbEFTRlloWgpuaEdMb1QzNTFmM2tmWVo0STV1bE8zVXE2VWVUcDF3bDhHZHYrWkxLSExBTE52MHRtaUVhdG5jRVlqeWwKK2ZTT3ZSSzdTcUc0T1Q4dkdFWU1VZldOdHBBTUdrdnBSZTBSTEl0K0RBL1dmbzU3THlmNUN2clVyNjFaCg==.2ae1f6ad034114efe6be459871c4000c\"></div><p id=\"isPasted\">This article explains how to convert a flattened DataFrame to a nested structure, by nesting a case class within another case class.</p><p>You can use this technique to build a JSON file, that can then be sent to an external API.</p><h1>Define nested schema</h1><p>We\u2019ll start with a flattened DataFrame.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1653037929389-flat-DataFrame.jpg\" style=\"width: truepx;\" class=\"fr-fic fr-dib\" alt=\"Example flattened DataFrame.\"></p><p>Using this example DataFrame, we define a custom nested schema using case classes.</p><pre>%scala\r\n\r\ncase class empId(id:String)\r\ncase class depId(dep_id:String)\r\ncase class details(id:empId,name:String,position:String,depId:depId)\r\ncase class code(manager_id:String)\r\ncase class reporting(reporting:Array[code])\r\ncase class hireDate(hire_date:String)\r\ncase class emp_record(emp_details:details,incrementDate:String,commission:String,country:String,hireDate:hireDate,reports_to:reporting)</pre><p>You can see that the case classes nest different data types within one another.</p><h1>Convert flattened DataFrame to a nested structure</h1><p>Use <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">DF.map</span> to pass every row object to the corresponding case class.</p><pre>%scala\r\n\r\nimport spark.implicits._\r\nval nestedDF= DF.map(r=&gt;{\r\nval empID_1= empId(r.getString(0))\r\nval depId_1 = depId(r.getString(7))\r\nval details_1=details(empID_1,r.getString(1),r.getString(2),depId_1)\r\nval code_1=code(r.getString(3))\r\nval reporting_1 = reporting(Array(code_1))\r\nval hireDate_1 = hireDate(r.getString(4))\r\nemp_record(details_1,r.getString(8),r.getString(6),r.getString(9),hireDate_1,reporting_1)\r\n\r\n}\r\n)</pre><p>This creates a nested DataFrame.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1653037962100-nested-DataFrame.jpg\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\" alt=\"Example nested DataFrame.\"></p><h1>Write out nested DataFrame as a JSON file</h1><p>Use the <span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">repartition().write.option</span> function to write the nested DataFrame to a JSON file.</p><pre>%scala\r\n\r\nnestedDF.repartition(1).write.option(\"multiLine\",\"true\").json(\"dbfs:/tmp/test/json1/\")</pre><h1>Example notebook</h1><p>Review the <a href=\"https://docs.databricks.com/_static/notebooks/kb/scala/df-to-nested-json.html\" id=\"\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"\">DataFrame to nested JSON example notebook</a> to see each of these steps performed.</p>", "body_txt": "This article explains how to convert a flattened DataFrame to a nested structure, by nesting a case class within another case class. You can use this technique to build a JSON file, that can then be sent to an external API. Define nested schema We\u2019ll start with a flattened DataFrame. Using this example DataFrame, we define a custom nested schema using case classes. %scala case class empId(id:String) case class depId(dep_id:String) case class details(id:empId,name:String,position:String,depId:depId) case class code(manager_id:String) case class reporting(reporting:Array[code]) case class hireDate(hire_date:String) case class emp_record(emp_details:details,incrementDate:String,commission:String,country:String,hireDate:hireDate,reports_to:reporting) You can see that the case classes nest different data types within one another. Convert flattened DataFrame to a nested structure Use DF.map to pass every row object to the corresponding case class. %scala import spark.implicits._ val nestedDF= DF.map(r=&gt;{ val empID_1= empId(r.getString(0)) val depId_1 = depId(r.getString(7)) val details_1=details(empID_1,r.getString(1),r.getString(2),depId_1) val code_1=code(r.getString(3)) val reporting_1 = reporting(Array(code_1)) val hireDate_1 = hireDate(r.getString(4)) emp_record(details_1,r.getString(8),r.getString(6),r.getString(9),hireDate_1,reporting_1) } ) This creates a nested DataFrame. Write out nested DataFrame as a JSON file Use the repartition().write.option function to write the nested DataFrame to a JSON file. %scala nestedDF.repartition(1).write.option(\"multiLine\",\"true\").json(\"dbfs:/tmp/test/json1/\") Example notebook Review the DataFrame to nested JSON example notebook to see each of these steps performed.", "format": "html", "updated_at": "2022-05-20T09:15:43.000Z"}, "author": {"id": 488104, "email": "adam.pavlacka@databricks.com", "name": "Adam Pavlacka", "first_name": "Adam", "last_name": "Pavlacka", "role_id": "superadmin", "created_at": "2021-10-07T00:54:10.791Z", "updated_at": "2023-04-30T23:27:30.391Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256870, "name": "Scala with Apache Spark", "codename": "scala", "accessibility": 1, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2676632, "name": "aws"}, {"id": 2676633, "name": "azure"}], "url": "https://kb.databricks.com/scala/convert-flat-df-to-nested-json"}, {"id": 1427758, "name": "How to create a custom docker image with customized Spark configs", "views": 1, "accessibility": 2, "description": "", "codename": "1427758-how-to-create-a-custom-docker-image-with-customized-spark-configs", "created_at": "2022-07-06T05:41:23.332Z", "updated_at": "2022-11-29T15:03:27.663Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p id=\"isPasted\">Category: &lt;list PRIMARY category article should appear in; category list is here: https://databricks.helpjuice.com/admin/categories/all-articles&gt;</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;</p><p>Author: &lt;Databricks email of author&gt;</p><p>Owning Team: &lt;Region + Platform/Spark&gt;</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt;</p></div><h1>How-to Introduction</h1><p>&lt;Enter problem text&gt;</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p>&lt;Enter cause text&gt;</p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCategory: &lt;list PRIMARY category article should appear in; category list is here: https://databricks.helpjuice.com/admin/categories/all-articles&gt;\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;\nAuthor: &lt;Databricks email of author&gt;\nOwning Team: &lt;Region + Platform/Spark&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt; How-to Introduction &lt;Enter problem text&gt; Note\nInsert your text here Instructions &lt;Enter cause text&gt;", "format": "html", "updated_at": "2022-07-06T05:41:23.538Z"}, "author": {"id": 831829, "email": "manoj.hegde@databricks.com", "name": "manoj.hegde ", "first_name": "manoj.hegde", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T15:23:57.894Z", "updated_at": "2023-04-13T05:12:40.616Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 299407, "name": "Needs Work (Plat India)", "codename": "needs-work-plat-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290942, "name": "platform-india-drafts", "codename": "platform-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/needs-work-plat-india/1427758-how-to-create-a-custom-docker-image-with-customized-spark-configs"}, {"id": 1427745, "name": "Empty lines causing browser unresponsive", "views": 10, "accessibility": 2, "description": "", "codename": "1427745-empty-lines-causing-browser-unresponsive", "created_at": "2022-07-06T05:23:39.288Z", "updated_at": "2022-08-09T05:25:32.545Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Category: &lt;list PRIMARY category article should appear in; category list is here: https://databricks.helpjuice.com/admin/categories/all-articles&gt;</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;</p><p>Author: &lt;Databricks email of author&gt;</p><p>Owning Team: &lt;Region + Platform/Spark&gt;</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt;</p></div><h1>Problem</h1><p>&lt;Enter problem text&gt;</p><h1>Cause</h1><p>&lt;Enter cause text&gt;</p><h1>Solution</h1><p>&lt;Enter solution text&gt;</p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCategory: &lt;list PRIMARY category article should appear in; category list is here: https://databricks.helpjuice.com/admin/categories/all-articles&gt;\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;\nAuthor: &lt;Databricks email of author&gt;\nOwning Team: &lt;Region + Platform/Spark&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt; Problem &lt;Enter problem text&gt; Cause &lt;Enter cause text&gt; Solution &lt;Enter solution text&gt;", "format": "html", "updated_at": "2022-07-06T05:23:39.422Z"}, "author": {"id": 789489, "email": "ravirahul.padmanabhan@databricks.com", "name": "ravirahul.padmanabhan ", "first_name": "ravirahul.padmanabhan", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:02:48.540Z", "updated_at": "2023-04-21T14:21:18.115Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 21486, "name": "India-Platform"}]}, "category": {"id": 297319, "name": "Will not publish", "codename": "will-not-publish", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}], "keywords": [{"id": 3298718, "name": "browser unresponsive"}, {"id": 3298719, "name": "lines empty"}], "url": "https://kb.databricks.com/will-not-publish/1427745-empty-lines-causing-browser-unresponsive"}, {"id": 1427740, "name": "REST API to enable Model Serving", "views": 1, "accessibility": 2, "description": "", "codename": "1427740-rest-api-to-enable-model-serving", "created_at": "2022-07-06T05:10:48.317Z", "updated_at": "2022-09-28T06:11:54.499Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p id=\"isPasted\">Category: &lt;list PRIMARY category article should appear in; category list is here: https://databricks.helpjuice.com/admin/categories/all-articles&gt;</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;</p><p>Author: &lt;Databricks email of author&gt;</p><p>Owning Team: &lt;India + Platform&gt;</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;<a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f00000EMD2vAAH/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5003f00000EMD2vAAH/view</a></p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt;</p></div><h1 data-toc=\"true\" id=\"how-to-introduction-0\">How-to Introduction</h1><p>This article explains how to use REST APIs to enable Model-Serving, Get-Status, disable Model-Serving and update instance/node type used for the model serving cluster.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"note-1\">Note</h3><p class=\"hj-alert-text\">Learn about MLflow Model Serving on Databricks&nbsp;</p><p class=\"hj-alert-text\">https://docs.databricks.com/applications/mlflow/model-serving.html https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/model-serving</p></div></div><h1 data-toc=\"true\" id=\"instructions-2\">Instructions</h1><p><br></p><p>This article uses Python for interacting with MLFlow REST endpoints. Any developer-friendly REST API tool such as &nbsp; &nbsp; &nbsp; &nbsp; POSTMAN, or CURL can be leveraged as well. Initialize environment</p><p id=\"isPasted\">Initialize environment</p><pre>token = &quot;dapixxxxxx&quot;\ninstance = &quot;https://&lt;shard&gt;.cloud.databricks.com&quot;&quot; OR https://&lt;adb-xxxxxx.xx&gt;.azuredatabricks.net&quot;&quot;\nheaders = {&#39;Authorization&#39;: f&#39;Bearer {token}&#39;}</pre><p>Enable Model Serving</p><pre>import requests\nurl = f&#39;https://{instance}/api/2.0/mlflow/endpoints/enable&#39;\nrequests.post(url, headers=headers, json={&quot;registered_model_name&quot;: &quot;&lt;model name&gt;&quot;})</pre><p>Get-Status</p><pre id=\"isPasted\">url = f&#39;https://{instance}/api/2.0/mlflow/endpoints/get-status&#39;\nr = requests.get(url, headers=headers, json={&quot;registered_model_name&quot;: &quot;&lt;model name&gt;&quot;})\nr.json()</pre><p>Disable Model Serving</p><pre>url = f&#39;https://{instance}/api/2.0/mlflow/endpoints/disable&#39;\nr = requests.post(url, headers=headers, json={&quot;registered_model_name&quot;: &quot;&lt;model name&gt;&quot;})\nr.json()</pre><p>Update Cluster Config</p><pre>url = f&#39;https://{instance}/api/2.0/mlflow/endpoints/update-cluster-config&#39;\nr = requests.put(url, headers=headers, json={&quot;registered_model_name&quot;: &quot;&lt;model name&gt;&quot;, &quot;desired_cluster_config&quot;:{&quot;node_type_id&quot;: &quot;&lt;instance_type&gt;&quot;}})\nr.json()</pre><p><br></p><div data-aura-class=\"forcePageBlockSection forcePageBlockSectionView\" data-aura-rendered-by=\"442:772;a\" id=\"isPasted\" style=\"box-sizing: border-box; margin-top: var(--lwc-spacingXSmall,0.5rem); margin-bottom: var(--lwc-spacingXSmall,0.5rem);\"><div aria-hidden=\"false\" data-aura-rendered-by=\"458:772;a\" style=\"box-sizing: border-box; overflow: visible; visibility: visible; opacity: 1; height: auto; transition: none 0s ease 0s; padding-top: var(--lwc-spacingXSmall,0.5rem);\"><div data-aura-rendered-by=\"459:772;a\" style=\"box-sizing: border-box;\"><div data-aura-class=\"forcePageBlockSectionRow\" data-aura-rendered-by=\"438:772;a\" style=\"box-sizing: border-box; display: flex; margin-right: 0px; margin-left: 0px;\"><div data-aura-class=\"forcePageBlockItem forcePageBlockItemView\" data-aura-rendered-by=\"425:772;a\" style=\"box-sizing: border-box; display: flex; flex: 1 1 0%; min-width: 0px; padding-right: var(--lwc-spacingSmall,0.75rem); padding-left: var(--lwc-spacingSmall,0.75rem); margin-top: ; margin-right: 0px; margin-bottom: ; margin-left: 0px;\"><div data-aura-rendered-by=\"426:772;a\" style=\"box-sizing: border-box; position: relative; min-width: 0px; flex-basis: 100%; border-bottom: var(--lwc-borderWidthThin,1px) solid var(--lwc-colorBorder,rgb(229, 229, 229)); margin-bottom: 0px; flex-grow: 1; display: block; padding: 0 var(--lwc-spacingXxSmall,0.25rem); width: 1112.67px;\"><div data-aura-rendered-by=\"430:772;a\" style=\"box-sizing: border-box; clear: left; position: relative; display: flex; padding-top: var(--lwc-spacingXxxSmall,0.125rem); padding-bottom: var(--lwc-spacingXxxSmall,0.125rem); border-bottom: 0px; padding-left: 0px; width: 1104.67px; flex-basis: 100%;\"><span data-aura-rendered-by=\"431:772;a\" style=\"box-sizing: border-box; overflow-wrap: break-word; word-break: break-word; display: inline-block; font-size: var(--lwc-inputStaticFontSize,0.875rem); font-weight: var(--lwc-inputStaticFontWeight,400); color: var(--lwc-inputStaticColor,rgb(24, 24, 24)); width: calc(100% - var(--lwc-squareIconSmallBoundary,1.5rem)); flex-grow: 1; padding-top: 0px; padding-bottom: 0px; min-height: calc(var(--lwc-varFontSize7,1.25rem) + 1px);\"><div data-aura-class=\"uiOutputRichText forceOutputRichText forceKnowledgeOutputRichTextForKnowledge\" data-aura-rendered-by=\"422:772;a\" dir=\"ltr\" style=\"box-sizing: border-box; line-height: var(--lwc-lineHeightText,1.5); overflow-wrap: break-word; hyphens: manual; overflow: auto;\"><br></div></span></div></div></div></div></div></div></div><div data-aura-class=\"forcePageBlockSection forcePageBlockSectionView\" data-aura-rendered-by=\"608:772;a\" style=\"box-sizing: border-box; margin-top: var(--lwc-spacingXSmall,0.5rem); margin-bottom: var(--lwc-spacingXSmall,0.5rem);\"><div aria-hidden=\"false\" data-aura-rendered-by=\"624:772;a\" style='box-sizing: border-box; overflow: visible; visibility: visible; opacity: 1; height: auto; transition: none 0s ease 0s; padding-top: var(--lwc-spacingXSmall,0.5rem); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 13px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><div data-aura-rendered-by=\"625:772;a\" style=\"box-sizing: border-box;\"><div data-aura-class=\"forcePageBlockSectionRow\" data-aura-rendered-by=\"499:772;a\" style=\"box-sizing: border-box; display: flex; margin-right: 0px; margin-left: 0px;\"><div data-aura-class=\"forcePageBlockItem forcePageBlockItemView\" data-aura-rendered-by=\"467:772;a\" style=\"box-sizing: border-box; display: flex; flex: 1 1 0%; min-width: 0px; padding-right: var(--lwc-spacingSmall,0.75rem); padding-left: var(--lwc-spacingSmall,0.75rem); margin-top: ; margin-right: var(--lwc-varSpacingMedium,1rem); margin-bottom: ; margin-left: 0px;\"><br></div></div></div></div></div><p><br></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCategory: &lt;list PRIMARY category article should appear in; category list is here: https://databricks.helpjuice.com/admin/categories/all-articles&gt;\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;\nAuthor: &lt;Databricks email of author&gt;\nOwning Team: &lt;India + Platform&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;https://databricks.lightning.force.com/lightning/r/Case/5003f00000EMD2vAAH/view Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt; How-to Introduction This article explains how to use REST APIs to enable Model-Serving, Get-Status, disable Model-Serving and update instance/node type used for the model serving cluster. Note\nLearn about MLflow Model Serving on Databricks\u00a0\nhttps://docs.databricks.com/applications/mlflow/model-serving.html https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/model-serving Instructions This article uses Python for interacting with MLFlow REST endpoints. Any developer-friendly REST API tool such as \u00a0 \u00a0 \u00a0 \u00a0 POSTMAN, or CURL can be leveraged as well. Initialize environment Initialize environment token = \"dapixxxxxx\"\ninstance = \"https://&lt;shard&gt;.cloud.databricks.com\"\" OR https://&lt;adb-xxxxxx.xx&gt;.azuredatabricks.net\"\"\nheaders = {'Authorization': f'Bearer {token}'} Enable Model Serving import requests\nurl = f'https://{instance}/api/2.0/mlflow/endpoints/enable'\nrequests.post(url, headers=headers, json={\"registered_model_name\": \"&lt;model name&gt;\"}) Get-Status url = f'https://{instance}/api/2.0/mlflow/endpoints/get-status'\nr = requests.get(url, headers=headers, json={\"registered_model_name\": \"&lt;model name&gt;\"})\nr.json() Disable Model Serving url = f'https://{instance}/api/2.0/mlflow/endpoints/disable'\nr = requests.post(url, headers=headers, json={\"registered_model_name\": \"&lt;model name&gt;\"})\nr.json() Update Cluster Config url = f'https://{instance}/api/2.0/mlflow/endpoints/update-cluster-config'\nr = requests.put(url, headers=headers, json={\"registered_model_name\": \"&lt;model name&gt;\", \"desired_cluster_config\":{\"node_type_id\": \"&lt;instance_type&gt;\"}})\nr.json()", "format": "html", "updated_at": "2022-07-06T05:15:00.134Z"}, "author": {"id": 789819, "email": "arvind.ravish@databricks.com", "name": "arvind.ravish ", "first_name": "arvind.ravish", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T14:57:00.920Z", "updated_at": "2023-04-12T21:20:46.123Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 7942, "name": "costCenter.711-SupportEngineering"}]}, "category": {"id": 299410, "name": "Tech Review Approved (Plat India)", "codename": "tech-review-approved-plat-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290942, "name": "platform-india-drafts", "codename": "platform-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3269263, "name": "model serving"}, {"id": 3269264, "name": "rest api"}], "url": "https://kb.databricks.com/tech-review-approved-plat-india/1427740-rest-api-to-enable-model-serving"}, {"id": 1425684, "name": "How to update Databricks Repos with AAD token using Git credentials", "views": 153, "accessibility": 2, "description": "", "codename": "update-databricks-repos-with-aad-token-using-git-credentials", "created_at": "2022-07-04T08:39:34.809Z", "updated_at": "2023-03-28T05:40:11.193Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: NA</p><p id=\"isPasted\">Category: Notebooks&nbsp;</p><p>Secondary category: Repos</p><p>Cloud Version: Azure, AWS</p><p>Author: srihasa.akepati@databricks.com</p><p>Owning Team: EMEA + Platform</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5008Y00001vRatCQAS/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5008Y00001vRatCQAS/view</a></p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1 data-toc=\"true\" id=\"how-to-introduction-0\">How-to Introduction</h1><p>Updating Databricks Repos with Re<i class=\"helpjuice-thread\" data-id=\"8224273606-3y80r\">mote Repo using AAD toke</i>n.&nbsp;</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\" data-toc=\"true\" id=\"note-1\">Note</h3><p class=\"hj-alert-text\" style=\"direction: ltr; text-align: left;\">We cannot update Databricks Repos directly with an AAD token. It results in following exception <span id=\"isPasted\"><sub>{&quot;error_code&quot;:&quot;PERMISSION_DENIED&quot;,&quot;message&quot;:&quot;Encountered an error with your Azure Active Directory credentials. Please try logging out of Azure Active Directory (</sub></span><sub><a rel=\"noreferrer noopener\" target=\"_blank\" title=\"https://portal.azure.com%29/\">https://portal.azure.com)</a></sub><span><sub>&nbsp;and logging back in.&quot;}</sub></span></p></div></div><h1 data-toc=\"true\" id=\"instructions-2\">Instructions</h1><p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Step 1 : S</span><span style=\"font-size:11.5pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">et DevOps PAT for SPN using Git Credentials API. When using this API you need to use AAD token of the service principal.</span></p><p><br>Ref - <a href=\"https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/gitcredentials\" id=\"isPasted\">https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/gitcredentials</a></p><p><span id=\"isPasted\" style=\"font-size: 14px;font-style: inherit;font-weight: inherit;\"><em>personal_access_token</em> is PAT token acquired from Azure DevOps.&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><span style=\"border:none;display:inline-block;overflow:hidden;width:569px;height:364px;\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1658141778911-1658141778911.png\" class=\"fr-fic fr-dib\"></span></span></p><p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;margin-top:12pt;margin-bottom:12pt;\"><span style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Step 2 : Use the update Repos API using the same AAD token.</span></p><p dir=\"ltr\" style=\"line-height:1.38;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><span style=\"border:none;display:inline-block;overflow:hidden;width:561px;height:342px;\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1657602922462-1657602922462.png\" width=\"561\" height=\"342\" class=\"fr-fic fr-dii\"></span></span></p><p><br></p>", "body_txt": "Restricted Content DBR Version: NA\nCategory: Notebooks\u00a0\nSecondary category: Repos\nCloud Version: Azure, AWS\nAuthor: srihasa.akepati@databricks.com\nOwning Team: EMEA + Platform\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5008Y00001vRatCQAS/view Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; How-to Introduction Updating Databricks Repos with Remote Repo using AAD token.\u00a0 Note\nWe cannot update Databricks Repos directly with an AAD token. It results in following exception {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"Encountered an error with your Azure Active Directory credentials. Please try logging out of Azure Active Directory ( https://portal.azure.com) \u00a0and logging back in.\"} Instructions Step 1 : S et DevOps PAT for SPN using Git Credentials API. When using this API you need to use AAD token of the service principal. Ref - https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/gitcredentials personal_access_token is PAT token acquired from Azure DevOps.\u00a0 Step 2 : Use the update Repos API using the same AAD token.", "format": "html", "updated_at": "2022-07-22T09:31:12.526Z"}, "author": {"id": 840402, "email": "srihasa.akepati@databricks.com", "name": "srihasa.akepati ", "first_name": "srihasa.akepati", "last_name": "", "role_id": "draft_writer", "created_at": "2022-04-05T17:25:21.038Z", "updated_at": "2023-03-25T11:28:44.527Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 299410, "name": "Tech Review Approved (Plat India)", "codename": "tech-review-approved-plat-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290942, "name": "platform-india-drafts", "codename": "platform-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3099084, "name": "aad token"}, {"id": 3099085, "name": "databricks"}], "url": "https://kb.databricks.com/tech-review-approved-plat-india/update-databricks-repos-with-aad-token-using-git-credentials"}, {"id": 1424753, "name": "Unpivot in spark-sql", "views": 2, "accessibility": 2, "description": "", "codename": "how-to-unpivot-in-spark-sql", "created_at": "2022-07-01T13:34:01.560Z", "updated_at": "2022-12-01T06:19:02.526Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: all DBR &gt; 7.3 LTS</p><p id=\"isPasted\">Category: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>SQL with Spark</span></p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: sunando.bhattacharya@databricks.com</p><p>Owning Team: India + Spark</p><p>Ticket URL: <a data-aura-class=\"uiOutputURL\" data-aura-rendered-by=\"173:772;a\" data-interactive-lib-uid=\"3\" dir=\"ltr\" href=\"https://stackoverflow.com/questions/42465568/unpivot-in-spark-sql-pyspark\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; overflow-wrap: break-word; word-break: break-word; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' title=\"\">https://stackoverflow.com/questions/42465568/unpivot-in-spark-sql-pyspark</a></p><p>Last reviewed date: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>7/30/2021 3:19 PM</span> by <a data-aura-class=\"forceOutputLookup\" data-aura-rendered-by=\"263:772;a\" data-navigable=\"true\" data-ownerid=\"252:772;a\" data-recordid=\"0053f000000pN3QAAU\" data-refid=\"recordId\" data-special-link=\"true\" href=\"https://databricks.lightning.force.com/lightning/r/0053f000000pN3QAAU/view\" id=\"isPasted\" rel=\"noreferrer\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; max-width: 100%; overflow: auto; text-overflow: initial; white-space: normal; display: inline; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\" title=\"Saikrishna Pujari\">Saikrishna Pujari</a></p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, <strong>Ready for Final Review</strong>, Published&gt;</p></div><h1>How-to Introduction</h1><p data-aura-rendered-by=\"371:772;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><u style=\"box-sizing: border-box;\"><span style=\"box-sizing: border-box; font-size: 14px;\"><span style=\"box-sizing: border-box; font-family: Georgia, serif;\"><span style=\"box-sizing: border-box; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; color: rgb(0, 0, 0);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">What is a pivot dataset?</span></span></span></span></span></span></span></u></p><p data-aura-rendered-by=\"371:772;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><span style=\"box-sizing: border-box; font-size: 14px;\"><span style=\"box-sizing: border-box; font-family: Georgia, serif;\"><span style=\"box-sizing: border-box; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; color: rgb(51, 51, 51);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">One of the many new features added in Spark 1.6 was the ability to pivot data, creating pivot tables, with a DataFrame (with Scala, Java, or Python). A pivot is an aggregation where one (or more in the general case) of the grouping columns has its distinct values transposed into individual columns. Pivot tables are an essential part of data analysis and reporting. Many popular data manipulation tools (pandas, reshape2, and Excel) and databases (MS SQL and Oracle 11g) include the ability to pivot data.&nbsp;</span></span></span></span></span></span></span></span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p><p data-aura-rendered-by=\"371:772;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><span style=\"box-sizing: border-box; font-size: 14px;\"><span style=\"box-sizing: border-box; font-family: Georgia, serif;\"><span style=\"box-sizing: border-box; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; color: rgb(51, 51, 51);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">In Spark SQL we only have the ability to pivot a data frame but there is no approach to unpivot a pivot table. In this article, we will discuss the approach to unpivot an existing data frame using org.apache.spark.sql.function.expr and Stack UTDF.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">Note: when using this workaround for unpivot we can only unpivot the data till the aggregate level the original df can&#39;t be fully recovered only the rows and columns will be transposed to original.</span></span></span></span></span></span></span></span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p><span data-aura-rendered-by=\"423:772;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-family: Georgia, serif;\"><span style=\"box-sizing: border-box; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; color: rgb(51, 51, 51);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">First, let&rsquo;s initialize the original data on top of which we will apply pivot aggregation.</span></span></span></span></span></span></span></span></p><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px;'>%scala\nval initial_df=Seq((&quot;G&quot;,&quot;X&quot;,1),(&quot;G&quot;,&quot;Y&quot;,2),(&quot;G&quot;,&quot;X&quot;,3),(&quot;H&quot;,&quot;Y&quot;,4),(&quot;H&quot;,&quot;Z&quot;,5)).toDF(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;)</span></pre><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px;'>%python\ninitial_df=spark.createDataFrame([(&quot;G&quot;,&quot;X&quot;,1),(&quot;G&quot;,&quot;Y&quot;,2),(&quot;G&quot;,&quot;X&quot;,3),(&quot;H&quot;,&quot;Y&quot;,4),(&quot;H&quot;,&quot;Z&quot;,5)],list(&quot;ABC&quot;))</span></pre><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"font-family: Times New Roman,Times,serif,-webkit-standard;\">display(initial_df)</span></pre><p data-aura-rendered-by=\"423:772;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656682827290-Untitled.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\"><span style=\"box-sizing: border-box; font-size: 13pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Arial;\"><span style=\"box-sizing: border-box; color: rgb(51, 51, 51);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\"><span style=\"box-sizing: border-box; font-size: 14px;\"><span style=\"box-sizing: border-box; font-family: Georgia, serif;\">The initial data frame has 3 columns A, B, and C. so to pivot the dataframe we will first group data based on column A then we will pivot column B and apply a sum on column C.</span></span></span></span></span></span></span></span></span></p><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px;'>%scala\nval pivot_df=initial_df.groupBy(&quot;A&quot;).pivot(&quot;B&quot;).sum(&quot;C&quot;)</span></pre><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px;'>%python\npivot_df=initial_df.groupBy(&quot;A&quot;).pivot(&quot;B&quot;).sum(&quot;C&quot;)</span></pre><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px;'>display(pivot_df)</span></pre><p data-aura-rendered-by=\"423:772;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656682890863-Untitled%202.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\"><span style=\"box-sizing: border-box; font-size: 14px;\"><span style=\"box-sizing: border-box; font-family: Georgia, serif;\"><span style=\"box-sizing: border-box; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; color: rgb(51, 51, 51);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">Once we have the pivot data we will now try to unpivot the data using stacks inside org.apache.spark.sql.functions.expr. The stack(values) is part of the Built-in table generation function(UDTF). Normal user-defined functions, such as concat(), take in a single input row and output a single output row. In contrast, table-generating functions transform a single input row into multiple output rows.</span></span></span></span></span></span></span></span></p><p data-aura-rendered-by=\"423:772;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><span style=\"box-sizing: border-box; font-size: 14px;\"><span style=\"box-sizing: border-box; font-family: Georgia, serif;\"><span style=\"box-sizing: border-box; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; color: rgb(51, 51, 51);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">Syntax:</span></span></span></span></span></span></span></span></p><p data-aura-rendered-by=\"423:772;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><span style=\"box-sizing: border-box; font-size: 10.5pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">stack(int r,T</span></span></span></span></span></span></span><span style=\"box-sizing: border-box; font-size: 13pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\"><sub style=\"box-sizing: border-box; font-size: 0.6em; line-height: 0; position: relative; vertical-align: baseline; bottom: -0.25em;\">1&nbsp;</sub></span></span></span></span></span></span></span><span style=\"box-sizing: border-box; font-size: 10.5pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">V</span></span></span></span></span></span></span><span style=\"box-sizing: border-box; font-size: 13pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\"><sub style=\"box-sizing: border-box; font-size: 0.6em; line-height: 0; position: relative; vertical-align: baseline; bottom: -0.25em;\">1</sub></span></span></span></span></span></span></span><span style=\"box-sizing: border-box; font-size: 10.5pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">,...,T</span></span></span></span></span></span></span><span style=\"box-sizing: border-box; font-size: 13pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\"><sub style=\"box-sizing: border-box; font-size: 0.6em; line-height: 0; position: relative; vertical-align: baseline; bottom: -0.25em;\">n/r&nbsp;</sub></span></span></span></span></span></span></span><span style=\"box-sizing: border-box; font-size: 10.5pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">V</span></span></span></span></span></span></span><span style=\"box-sizing: border-box; font-size: 13pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\"><sub style=\"box-sizing: border-box; font-size: 0.6em; line-height: 0; position: relative; vertical-align: baseline; bottom: -0.25em;\">n</sub></span></span></span></span></span></span></span><span style=\"box-sizing: border-box; font-size: 10.5pt; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; font-family: Roboto, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(23, 43, 77);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">) Breaks up&nbsp;</span></span><span style=\"box-sizing: border-box; font-style: italic;\"><span style=\"box-sizing: border-box; text-decoration: none;\">n</span></span><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">&nbsp;values V<sub style=\"box-sizing: border-box; font-size: 0.6em; line-height: 0; position: relative; vertical-align: baseline; bottom: -0.25em;\">1</sub>,...,V<sub style=\"box-sizing: border-box; font-size: 0.6em; line-height: 0; position: relative; vertical-align: baseline; bottom: -0.25em;\">n</sub>&nbsp;into&nbsp;</span></span><span style=\"box-sizing: border-box; font-style: italic;\"><span style=\"box-sizing: border-box; text-decoration: none;\">r&nbsp;</span></span><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">rows. Each row will have&nbsp;</span></span><span style=\"box-sizing: border-box; font-style: italic;\"><span style=\"box-sizing: border-box; text-decoration: none;\">n/r&nbsp;</span></span><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">columns.&nbsp;</span></span><span style=\"box-sizing: border-box; font-style: italic;\"><span style=\"box-sizing: border-box; text-decoration: none;\">r&nbsp;</span></span><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">must be constant.</span></span></span></span></span></span></span></p><p data-aura-rendered-by=\"423:772;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><span style=\"box-sizing: border-box; font-size: 14px;\"><span style=\"box-sizing: border-box; font-family: Georgia, serif;\"><span style=\"box-sizing: border-box; font-variant: normal; white-space: pre-wrap;\"><span style=\"box-sizing: border-box; color: rgb(51, 51, 51);\"><span style=\"box-sizing: border-box; background-color: rgb(255, 255, 255);\"><span style=\"box-sizing: border-box; font-weight: 400;\"><span style=\"box-sizing: border-box; font-style: normal;\"><span style=\"box-sizing: border-box; text-decoration: none;\">So using stack UDTF we will try to get the aggregated unpivot data.</span></span></span></span></span></span></span></span></p><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px;'>%scala\nimport org.apache.spark.sql.functions._\nval unpivot_df=pivot_df.select($&quot;A&quot;, expr(&quot;stack(3, &#39;X&#39;, X, &#39;Y&#39;, Y, &#39;Z&#39;, Z) as (B, C)&quot;)).where(&quot;C is not null&quot;)</span></pre><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px;'>%python\nunpivot_df=pivot_df.selectExpr(&quot;A&quot;, &quot;stack(3, &#39;X&#39;, X, &#39;Y&#39;, Y, &#39;Z&#39;, Z) as (B, C)&quot;).where(&quot;C is not null&quot;)</span></pre><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 16px;'>display(unpivot_df)</span></pre><p data-aura-rendered-by=\"423:772;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; line-height: 1.38;'><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656682932324-Untitled%203.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib fr-fil\"></p><p><br></p>", "body_txt": "Restricted Content DBR Version: all DBR &gt; 7.3 LTS\nCategory: SQL with Spark Secondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: AWS, Azure, GCP\nAuthor: sunando.bhattacharya@databricks.com\nOwning Team: India + Spark\nTicket URL: https://stackoverflow.com/questions/42465568/unpivot-in-spark-sql-pyspark Last reviewed date: 7/30/2021 3:19 PM by Saikrishna Pujari Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt; How-to Introduction What is a pivot dataset? One of the many new features added in Spark 1.6 was the ability to pivot data, creating pivot tables, with a DataFrame (with Scala, Java, or Python). A pivot is an aggregation where one (or more in the general case) of the grouping columns has its distinct values transposed into individual columns. Pivot tables are an essential part of data analysis and reporting. Many popular data manipulation tools (pandas, reshape2, and Excel) and databases (MS SQL and Oracle 11g) include the ability to pivot data.\u00a0 \u00a0 In Spark SQL we only have the ability to pivot a data frame but there is no approach to unpivot a pivot table. In this article, we will discuss the approach to unpivot an existing data frame using org.apache.spark.sql.function.expr and Stack UTDF. Note: when using this workaround for unpivot we can only unpivot the data till the aggregate level the original df can't be fully recovered only the rows and columns will be transposed to original. \u00a0 Note\nInsert your text here Instructions First, let\u2019s initialize the original data on top of which we will apply pivot aggregation. %scala\nval initial_df=Seq((\"G\",\"X\",1),(\"G\",\"Y\",2),(\"G\",\"X\",3),(\"H\",\"Y\",4),(\"H\",\"Z\",5)).toDF(\"A\",\"B\",\"C\") %python\ninitial_df=spark.createDataFrame([(\"G\",\"X\",1),(\"G\",\"Y\",2),(\"G\",\"X\",3),(\"H\",\"Y\",4),(\"H\",\"Z\",5)],list(\"ABC\")) display(initial_df) The initial data frame has 3 columns A, B, and C. so to pivot the dataframe we will first group data based on column A then we will pivot column B and apply a sum on column C. %scala\nval pivot_df=initial_df.groupBy(\"A\").pivot(\"B\").sum(\"C\") %python\npivot_df=initial_df.groupBy(\"A\").pivot(\"B\").sum(\"C\") display(pivot_df) Once we have the pivot data we will now try to unpivot the data using stacks inside org.apache.spark.sql.functions.expr. The stack(values) is part of the Built-in table generation function(UDTF). Normal user-defined functions, such as concat(), take in a single input row and output a single output row. In contrast, table-generating functions transform a single input row into multiple output rows. Syntax: stack(int r,T 1\u00a0 V 1 ,...,T n/r\u00a0 V n ) Breaks up\u00a0 n \u00a0values V1,...,Vn\u00a0into\u00a0 r\u00a0 rows. Each row will have\u00a0 n/r\u00a0 columns.\u00a0 r\u00a0 must be constant. So using stack UDTF we will try to get the aggregated unpivot data. %scala\nimport org.apache.spark.sql.functions._\nval unpivot_df=pivot_df.select($\"A\", expr(\"stack(3, 'X', X, 'Y', Y, 'Z', Z) as (B, C)\")).where(\"C is not null\") %python\nunpivot_df=pivot_df.selectExpr(\"A\", \"stack(3, 'X', X, 'Y', Y, 'Z', Z) as (B, C)\").where(\"C is not null\") display(unpivot_df)", "format": "html", "updated_at": "2022-07-01T13:44:58.533Z"}, "author": {"id": 885174, "email": "sunando.bhattacharya@databricks.com", "name": "sunando.bhattacharya ", "first_name": "sunando.bhattacharya", "last_name": "", "role_id": "draft_writer", "created_at": "2022-06-14T21:15:21.303Z", "updated_at": "2023-01-16T13:43:49.037Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 307309, "name": "Needs Work (Spark India)", "codename": "needs-work-spark-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290939, "name": "spark-india-drafts", "codename": "spark-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2781450, "name": "aws"}, {"id": 2781451, "name": "azure"}, {"id": 2781452, "name": "gcp"}, {"id": 2956044, "name": "pivot"}, {"id": 2781453, "name": "spark"}, {"id": 2781454, "name": "spark-sql"}, {"id": 2781455, "name": "sql"}, {"id": 2956043, "name": "stack"}, {"id": 2956045, "name": "unpivot"}], "url": "https://kb.databricks.com/needs-work-spark-india/how-to-unpivot-in-spark-sql"}, {"id": 1424674, "name": "Grouping attribute column is named as 'key' instead of 'value' in spark 3.x", "views": 1, "accessibility": 2, "description": "", "codename": "grouping-attribute-column-is-named-as-key-instead-of-value-in-spark-3x", "created_at": "2022-07-01T11:48:04.771Z", "updated_at": "2022-11-25T05:21:01.354Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All DBR versions &gt; 7.3</p><p id=\"isPasted\">Category: SQL with Spark</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: Shashank Sharma</p><p>Owning Team: India + Spark</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>8/17/2021 11:49 PM</span> by Vikas Reddy</p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, <strong>Ready for Final Review</strong>, Published&gt;</p></div><h1>How-to Introduction</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>If a user used the column name as value in 2.x, the same code will fail in 3.x with an error &quot;value column will not exist&quot;</span></p><h1>Instructions</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>In Spark 2.4 and below, Dataset.groupByKey results to a grouped dataset with key attribute is wrongly named as &ldquo;value&rdquo;, if the key is non-struct type, for example, int, string, array, etc. This is counterintuitive and makes the schema of aggregation queries unexpected. For example, the schema of ds.groupByKey(...).count() is (value, count). Since Spark 3.0, we name the grouping attribute to &ldquo;key&rdquo;. The old behavior is preserved under a newly added configuration spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue with a default value of false.</span></p><p><span id=\"isPasted\"><img src=\"https://s3.amazonaws.com/helpjuice-static/helpjuice_production%2Fuploads%2Fupload%2Fimage%2F10723%2Fdirect%2F1656676407453-Screenshot+2022-07-01+at+5.23.03+PM.png\" style=\"width: truepx;\" class=\"fr-fic fr-dib\"></span><br></p>", "body_txt": "Restricted Content DBR Version: All DBR versions &gt; 7.3\nCategory: SQL with Spark\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: AWS, Azure, GCP\nAuthor: Shashank Sharma\nOwning Team: India + Spark\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: 8/17/2021 11:49 PM by Vikas Reddy\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt; How-to Introduction If a user used the column name as value in 2.x, the same code will fail in 3.x with an error \"value column will not exist\" Instructions In Spark 2.4 and below, Dataset.groupByKey results to a grouped dataset with key attribute is wrongly named as \u201cvalue\u201d, if the key is non-struct type, for example, int, string, array, etc. This is counterintuitive and makes the schema of aggregation queries unexpected. For example, the schema of ds.groupByKey(...).count() is (value, count). Since Spark 3.0, we name the grouping attribute to \u201ckey\u201d. The old behavior is preserved under a newly added configuration spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue with a default value of false.", "format": "html", "updated_at": "2022-07-01T11:53:36.368Z"}, "author": {"id": 840937, "email": "deepak.bhutada@databricks.com", "name": "deepak.bhutada ", "first_name": "deepak.bhutada", "last_name": "", "role_id": "draft_writer", "created_at": "2022-04-06T11:01:04.397Z", "updated_at": "2023-04-21T14:02:10.764Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 307309, "name": "Needs Work (Spark India)", "codename": "needs-work-spark-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290939, "name": "spark-india-drafts", "codename": "spark-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3115108, "name": "attribute grouping"}, {"id": 3115107, "name": "spark 3.x"}], "url": "https://kb.databricks.com/needs-work-spark-india/grouping-attribute-column-is-named-as-key-instead-of-value-in-spark-3x"}, {"id": 1424656, "name": "Configuring application name with spark jdbc for Azure sql server", "views": 1, "accessibility": 2, "description": "", "codename": "configuring-application-name-with-spark-jdbc-for-azure-sql-server", "created_at": "2022-07-01T11:27:14.305Z", "updated_at": "2022-11-28T02:46:51.851Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version:&nbsp;</p><p id=\"isPasted\">Category: <span id=\"isPasted\" style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Scala with Spark</span></p><p>Secondary category: <span id=\"isPasted\" style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Notebooks;SQL with Spark</span></p><p>Cloud Version: Azure</p><p>Author: <span id=\"isPasted\" style=\"font-size:10pt;font-family:Roboto,sans-serif;color:#dca10d;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">manjunath.swamy@databricks.com</span></p><p>Owning Team: India + Spark</p><p>Ticket URL:&nbsp;</p><p>Last reviewed date: &nbsp;09-28-2021 and DD Sharma</p><p>Review status:&nbsp;</p></div><h1>How-to Introduction</h1><p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">You can use Databricks to query many SQL databases using JDBC drivers. Databricks Runtime contains JDBC drivers for Microsoft SQL Server and Azure SQL Database.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Below document provide detail information of how to connect and read data from spark to SQL databases using JDBC drivers.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><a href=\"https://docs.databricks.com/data/data-sources/sql-databases.html\" style=\"text-decoration:none;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#1155cc;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">https://docs.databricks.com/data/data-sources/sql-databases.html</span></a></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Generally while configuring the spark jdbc, we will not specify the application name which is connecting to sql server. Some times when we have 100&rsquo;s of databricks/spark jobs connecting to SQL server, if any issue/information we need to filter out based on each spark/databricks apllication then it will be difficult as all the applications will be providing the default name.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">So in these kind of situation while configuring the spark jdbc in spark apllications, we can specify the application name to make it unique.</span></p><h1>Instructions</h1><p data-aura-rendered-by=\"49:2441;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">Connect by specifying a customized application name:</b></p><p data-aura-rendered-by=\"49:2441;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>For MSFT/Azure sql server : <a href=\"https://docs.microsoft.com/en-us/sql/connect/jdbc/building-the-connection-url?view=sql-server-ver15\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://docs.microsoft.com/en-us/sql/connect/jdbc/building-the-connection-url?view=sql-server-ver15</a></p><pre data-aura-rendered-by=\"49:2441;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Class.forName(&quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;)\n\nval jdbcHostname = &quot;&quot;\nval jdbcPort = 1433\nval jdbcDatabase = &quot;&quot;\n\n// Create the JDBC URL without passing in the user and password parameters.\nval jdbcUrl = s&quot;jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase};applicationName=&lt;your_app_name&gt;&quot;\n\n// Create a Properties() object to hold the parameters.\nimport java.util.Properties\nval connectionProperties = new Properties()\n\nconnectionProperties.put(&quot;user&quot;, &quot;&quot;)\nconnectionProperties.put(&quot;password&quot;, &quot;&quot;)\n\n\nval driverClass = &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;\nconnectionProperties.setProperty(&quot;Driver&quot;, driverClass)\n\n\nval employees_table = spark.read.jdbc(jdbcUrl, &quot;SalesLT.Address&quot;, connectionProperties)\nemployees_table.show()</pre><p><span id=\"isPasted\" style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:#ffffff;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\"><span style=\"border:none;display:inline-block;overflow:hidden;width:624px;height:244px;\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1656675118655-1656675118655.png\" width=\"624\" height=\"244\" class=\"fr-fic fr-dii\"></span></span><br></p><p><br></p>", "body_txt": "Restricted Content DBR Version:\u00a0\nCategory: Scala with Spark Secondary category: Notebooks;SQL with Spark Cloud Version: Azure\nAuthor: manjunath.swamy@databricks.com Owning Team: India + Spark\nTicket URL:\u00a0\nLast reviewed date: \u00a009-28-2021 and DD Sharma\nReview status:\u00a0 How-to Introduction You can use Databricks to query many SQL databases using JDBC drivers. Databricks Runtime contains JDBC drivers for Microsoft SQL Server and Azure SQL Database. \u00a0 \u00a0 Below document provide detail information of how to connect and read data from spark to SQL databases using JDBC drivers. https://docs.databricks.com/data/data-sources/sql-databases.html \u00a0 \u00a0 Generally while configuring the spark jdbc, we will not specify the application name which is connecting to sql server. Some times when we have 100\u2019s of databricks/spark jobs connecting to SQL server, if any issue/information we need to filter out based on each spark/databricks apllication then it will be difficult as all the applications will be providing the default name. \u00a0 So in these kind of situation while configuring the spark jdbc in spark apllications, we can specify the application name to make it unique. Instructions Connect by specifying a customized application name: For MSFT/Azure sql server : https://docs.microsoft.com/en-us/sql/connect/jdbc/building-the-connection-url?view=sql-server-ver15 Class.forName(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\") val jdbcHostname = \"\"\nval jdbcPort = 1433\nval jdbcDatabase = \"\" // Create the JDBC URL without passing in the user and password parameters.\nval jdbcUrl = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase};applicationName=&lt;your_app_name&gt;\" // Create a Properties() object to hold the parameters.\nimport java.util.Properties\nval connectionProperties = new Properties() connectionProperties.put(\"user\", \"\")\nconnectionProperties.put(\"password\", \"\") val driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nconnectionProperties.setProperty(\"Driver\", driverClass) val employees_table = spark.read.jdbc(jdbcUrl, \"SalesLT.Address\", connectionProperties)\nemployees_table.show()", "format": "html", "updated_at": "2022-07-01T11:32:48.969Z"}, "author": {"id": 831506, "email": "manjunath.swamy@databricks.com", "name": "manjunath.swamy ", "first_name": "manjunath.swamy", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T05:55:43.385Z", "updated_at": "2023-03-24T10:04:17.174Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 316339, "name": "Ready for Tech Review (Spark India)", "codename": "ready-for-tech-review-spark-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290939, "name": "spark-india-drafts", "codename": "spark-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3115086, "name": "azure sql"}, {"id": 3115087, "name": "spark jdbc"}], "url": "https://kb.databricks.com/ready-for-tech-review-spark-india/configuring-application-name-with-spark-jdbc-for-azure-sql-server"}, {"id": 1424545, "name": "Downloading the InterpretML model graphs as html/ pdf to local in Databricks", "views": 6, "accessibility": 2, "description": "", "codename": "downloading-the-interpretml-model-graphs-as-html-pdf-to-local-in-databricks", "created_at": "2022-07-01T07:11:04.014Z", "updated_at": "2022-11-25T05:06:26.167Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000Ce5J&feoid=00N3f000000m2eR&refid=0EM3f000000NzkX\" class=\"fr-fic fr-dii\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version:&nbsp;</p><p id=\"isPasted\">Category:&nbsp;</p><p>Secondary category:&nbsp;</p><p>Cloud Version: &nbsp;AWS, Azure, GCP</p><p>Author: <a aria-expanded=\"false\" aria-haspopup=\"menu\" href=\"mailto:manjunath.swamy@databricks.com\" id=\"isPasted\" rel=\"noopener noreferrer\" style=\"box-sizing: inherit; color: rgba(var(--sk_highlight,18,100,163),1); text-decoration: none; font-family: Slack-Lato, Slack-Fractions, appleLogo, sans-serif; font-size: 15px; font-style: normal; font-variant-ligatures: common-ligatures; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);\" target=\"_blank\">manjunath.swamy@databricks.com</a></p><p>Owning Team: India + Spark</p><p>Ticket URL: <a data-aura-class=\"uiOutputURL\" data-aura-rendered-by=\"173:987;a\" data-interactive-lib-uid=\"3\" dir=\"ltr\" href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f00000DNyrQAAT/view\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; overflow-wrap: break-word; word-break: break-word; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' title=\"\">https://databricks.lightning.force.com/lightning/r/Case/5003f00000DNyrQAAT/view</a></p><p>Last reviewed date: 08-09-2021 by Pradeep Kumar</p></div><h1>How-to Introduction</h1><p dir=\"ltr\" id=\"isPasted\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">InterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model&#39;s global behavior, or understand the reasons behind individual predictions.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><a href=\"https://github.com/interpretml/interpret/\" style=\"text-decoration:none;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">https://github.com/interpretml/interpret/</span></a></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Interpretability is essential for:</span></p><ul style=\"margin-top:0;margin-bottom:0;padding-inline-start:48px;\"><li aria-level=\"1\" dir=\"ltr\" style=\"list-style-type:disc;font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Model debugging - Why did my model make this mistake?</span></p></li><li aria-level=\"1\" dir=\"ltr\" style=\"list-style-type:disc;font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Feature Engineering - How can I improve my model?</span></p></li><li aria-level=\"1\" dir=\"ltr\" style=\"list-style-type:disc;font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Detecting fairness issues - Does my model discriminate?</span></p></li><li aria-level=\"1\" dir=\"ltr\" style=\"list-style-type:disc;font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Human-AI cooperation - How can I understand and trust the model&#39;s decisions?</span></p></li><li aria-level=\"1\" dir=\"ltr\" style=\"list-style-type:disc;font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Regulatory compliance - Does my model satisfy legal requirements?</span></p></li><li aria-level=\"1\" dir=\"ltr\" style=\"list-style-type:disc;font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;\"><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">High-risk applications - Healthcare, finance, judicial, ...</span></p></li></ul><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&gt; InterpretML provides the show() function which produces the graphs which helps to understand model&#39;s global behavior, or understand individual predictions.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&gt; Sometimes this graphs can be very huge and when showing in Databricks notebook it results in below error. In Databricks, by design, there is a hard limit on the output emitted to stdout (notebook output) and the limit is 20MB. Hence the reason the show command throws the below exception.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">com.databricks.rpc.RPCResponseTooLarge: rpc response (of 20974766 bytes) exceeds limit of 20971520 bytes</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&gt; &nbsp;As an alternative method we can try saving the the graphs generated by show() as image or html file in /FileStore of databricks path which can be download to local and can open in browser.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&gt; FileStore is a special folder within&nbsp;</span><a href=\"https://docs.databricks.com/data/databricks-file-system.html\" style=\"text-decoration:none;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Databricks File System (DBFS)</span></a><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#181818;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">&nbsp;where you can save files and have them accessible to your web browser.</span></p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\">&nbsp;</p><p dir=\"ltr\" style=\"line-height:1.38;background-color:#ffffff;margin-top:0pt;margin-bottom:0pt;\"><a href=\"https://docs.databricks.com/data/filestore.html#:~:text=Files%20stored%20in%20/FileStore%20are%20accessible%20in%20your%20web%20browser%20at%20https://%3Cdatabricks-instance%3E/files/.%20For%20example%2C%20the%20file%20you%20stored%20in%20/FileStore/my-stuff/my-file.txt%20is%20accessible%20at%20https://%3Cdatabricks-instance%3E/files/my-stuff/my-file.txt\" style=\"text-decoration:none;\"><span style=\"font-size:10.5pt;font-family:Roboto,sans-serif;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">https://docs.databricks.com/data/filestore.html#:~:text=Files%20stored%20in%20/FileStore%20are%20accessible%20in%20your%20web%20browser%20at%20https://%3Cdatabricks-instance%3E/files/.%20For%20example%2C%20the%20file%20you%20stored%20in%20/FileStore/my-stuff/my-file.txt%20is%20accessible%20at%20https://%3Cdatabricks-instance%3E/files/my-stuff/my-file.txt</span></a></p><h1>Instructions</h1><p data-aura-rendered-by=\"49:2236;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">&gt; Install interpretML library and kaleido</b></p><pre data-aura-rendered-by=\"49:2236;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">%pip install interpret\n\n%pip install kaleido</pre><p data-aura-rendered-by=\"49:2236;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">Example&nbsp;</b>1<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">&gt; Execute simple ML code by train and testing the model</b></p><pre data-aura-rendered-by=\"49:2236;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">import pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\nboston = load_boston()\nfeature_names = list(boston.feature_names)\ndf = pd.DataFrame(boston.data, columns=feature_names)\ndf[&quot;target&quot;] = boston.target\n# df = df.sample(frac=0.1, random_state=1)\ntrain_cols = df.columns[0:-1]\nlabel = df.columns[-1]\nX = df[train_cols]\ny = df[label]\n\nseed = 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed</pre><p data-aura-rendered-by=\"49:2236;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>&nbsp;</p><pre data-aura-rendered-by=\"49:2236;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n#Blackbox system can include preprocessing, not just a regressor!\npca = PCA()\nrf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n\nblackbox_model = Pipeline([(&#39;pca&#39;, pca), (&#39;rf&#39;, rf)])\nblackbox_model.fit(X_train, y_train)</pre><p data-aura-rendered-by=\"49:2236;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">&gt; Generate Graphs using show() for the model to understand</b></p><pre data-aura-rendered-by=\"49:2236;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">from interpret import show\nfrom interpret.perf import RegressionPerf\n\nblackbox_perf = RegressionPerf(blackbox_model.predict).explain_perf(X_test, y_test, name=&#39;Blackbox&#39;)\nshow(blackbox_perf)</pre><p><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000Ce5J&feoid=00N3f000000m2eR&refid=0EM3f000000Nzn7\" class=\"fr-fic fr-dib\"></p><p data-aura-rendered-by=\"49:2236;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>&gt; Writing a graphs as html file to download using <b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">visualize() function.</b></p><pre data-aura-rendered-by=\"49:2236;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">fig_black_box = blackbox_perf.visualize()\nfig_black_box.write_html(&quot;/dbfs/FileStore/test_black_box_image.html&quot;)\n\ndisplayHTML(f&quot;&quot;&quot;\n  &lt;a href=&quot;<a href=\"https://cust-success.cloud.databricks.com/files/test_black_box_image.html\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://cust-success.cloud.databricks.com/files/test_black_box_image.html&quot;</a>&quot; download&gt;Download a html file &lt;/a&gt;\n&quot;&quot;&quot;)</pre><p><br></p><p><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000Ce5J&feoid=00N3f000000m2eR&refid=0EM3f000000NzkX\" class=\"fr-fic fr-dib\"><br></p><pre id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">from interpret.blackbox import LimeTabular\nfrom interpret import show\n\n#Blackbox explainers need a predict function, and optionally a dataset\nlime = LimeTabular(predict_fn=blackbox_model.predict, data=X_train, random_state=1)\n\n#Pick the instances to explain, optionally pass in labels if you have them\nlime_local = lime.explain_local(X_test[:5], y_test[:5], name=&#39;LIME&#39;)\n\nshow(lime_local)</pre><p><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000Ce5J&feoid=00N3f000000m2eR&refid=0EM3f000000Nzl6\" class=\"fr-fic fr-dib\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000Ce5J&feoid=00N3f000000m2eR&refid=0EM3f000000NzlL\" class=\"fr-fic fr-dib\"><br></p><p><br></p>", "body_txt": "Restricted Content DBR Version:\u00a0\nCategory:\u00a0\nSecondary category:\u00a0\nCloud Version: \u00a0AWS, Azure, GCP\nAuthor: manjunath.swamy@databricks.com Owning Team: India + Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f00000DNyrQAAT/view Last reviewed date: 08-09-2021 by Pradeep Kumar How-to Introduction InterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model's global behavior, or understand the reasons behind individual predictions. \u00a0 https://github.com/interpretml/interpret/ \u00a0 Interpretability is essential for: Model debugging - Why did my model make this mistake? Feature Engineering - How can I improve my model? Detecting fairness issues - Does my model discriminate? Human-AI cooperation - How can I understand and trust the model's decisions? Regulatory compliance - Does my model satisfy legal requirements? High-risk applications - Healthcare, finance, judicial, ... \u00a0 \u00a0 &gt; InterpretML provides the show() function which produces the graphs which helps to understand model's global behavior, or understand individual predictions. \u00a0 &gt; Sometimes this graphs can be very huge and when showing in Databricks notebook it results in below error. In Databricks, by design, there is a hard limit on the output emitted to stdout (notebook output) and the limit is 20MB. Hence the reason the show command throws the below exception. \u00a0 com.databricks.rpc.RPCResponseTooLarge: rpc response (of 20974766 bytes) exceeds limit of 20971520 bytes \u00a0 &gt; \u00a0As an alternative method we can try saving the the graphs generated by show() as image or html file in /FileStore of databricks path which can be download to local and can open in browser. \u00a0 &gt; FileStore is a special folder within\u00a0 Databricks File System (DBFS) \u00a0where you can save files and have them accessible to your web browser. \u00a0 https://docs.databricks.com/data/filestore.html#:~:text=Files%20stored%20in%20/FileStore%20are%20accessible%20in%20your%20web%20browser%20at%20https://%3Cdatabricks-instance%3E/files/.%20For%20example%2C%20the%20file%20you%20stored%20in%20/FileStore/my-stuff/my-file.txt%20is%20accessible%20at%20https://%3Cdatabricks-instance%3E/files/my-stuff/my-file.txt Instructions &gt; Install interpretML library and kaleido %pip install interpret %pip install kaleido Example\u00a01 &gt; Execute simple ML code by train and testing the model import pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split boston = load_boston()\nfeature_names = list(boston.feature_names)\ndf = pd.DataFrame(boston.data, columns=feature_names)\ndf[\"target\"] = boston.target\n# df = df.sample(frac=0.1, random_state=1)\ntrain_cols = df.columns[0:-1]\nlabel = df.columns[-1]\nX = df[train_cols]\ny = df[label] seed = 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed \u00a0 from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline #Blackbox system can include preprocessing, not just a regressor!\npca = PCA()\nrf = RandomForestRegressor(n_estimators=100, n_jobs=-1) blackbox_model = Pipeline([('pca', pca), ('rf', rf)])\nblackbox_model.fit(X_train, y_train) &gt; Generate Graphs using show() for the model to understand from interpret import show\nfrom interpret.perf import RegressionPerf blackbox_perf = RegressionPerf(blackbox_model.predict).explain_perf(X_test, y_test, name='Blackbox')\nshow(blackbox_perf) &gt; Writing a graphs as html file to download using visualize() function. fig_black_box = blackbox_perf.visualize()\nfig_black_box.write_html(\"/dbfs/FileStore/test_black_box_image.html\") displayHTML(f\"\"\" &lt;a href=\"https://cust-success.cloud.databricks.com/files/test_black_box_image.html\"\" download&gt;Download a html file &lt;/a&gt;\n\"\"\") from interpret.blackbox import LimeTabular\nfrom interpret import show #Blackbox explainers need a predict function, and optionally a dataset\nlime = LimeTabular(predict_fn=blackbox_model.predict, data=X_train, random_state=1) #Pick the instances to explain, optionally pass in labels if you have them\nlime_local = lime.explain_local(X_test[:5], y_test[:5], name='LIME') show(lime_local)", "format": "html", "updated_at": "2022-07-01T07:25:26.970Z"}, "author": {"id": 831506, "email": "manjunath.swamy@databricks.com", "name": "manjunath.swamy ", "first_name": "manjunath.swamy", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-24T05:55:43.385Z", "updated_at": "2023-03-24T10:04:17.174Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 307309, "name": "Needs Work (Spark India)", "codename": "needs-work-spark-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290939, "name": "spark-india-drafts", "codename": "spark-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3115111, "name": "databricks"}, {"id": 3115112, "name": "downloading"}], "url": "https://kb.databricks.com/needs-work-spark-india/downloading-the-interpretml-model-graphs-as-html-pdf-to-local-in-databricks"}, {"id": 1423829, "name": "Extract Job ID and Run ID from Job Name to use it in the code", "views": 257, "accessibility": 2, "description": "", "codename": "1423829-extract-job-id-and-run-id-from-job-name-to-use-it-in-the-code", "created_at": "2022-06-30T10:48:55.744Z", "updated_at": "2022-07-14T14:23:45.547Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Category: &lt;list PRIMARY category article should appear in; category list is here: https://databricks.helpjuice.com/admin/categories/all-articles&gt;</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;</p><p>Author: Mohit</p><p>Owning Team: &lt;Region + Platform/Spark&gt;</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1>Problem</h1><p>We are having Databricks Job running with main class and JAR file in it. Our JAR file code base is in Scala (written in IntelliJ IDEA). Now, when our job starts running, we need to log Job ID and Run ID into our SQL database for future purpose. Our Databricks Jobs are scheduled job and are suppose to automatically trigger. How can we get Job ID from Job Name?&nbsp;</p><h1>Cause</h1><p>&lt;Enter cause text&gt;</p><h1>Solution</h1><p id=\"isPasted\">You can make use of the task parameters, and you will be able to print the job id and run ID.</p><p><span style=\"color: rgb(64, 64, 64); font-family: DMSans, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">You can pass templated variables into a job task as part of the task&rsquo;s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job&rsquo;s start time.</span></p><p><span style=\"color: rgb(64, 64, 64); font-family: DMSans, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named&nbsp;</span><code style=\"box-sizing: border-box; font-family: Menlo, monospace; font-size: 13.76px; white-space: nowrap; max-width: 100%; padding: 0px 2px; color: rgb(5, 34, 71); overflow-x: auto; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box;\">MyJobId</span></code><span style=\"color: rgb(64, 64, 64); font-family: DMSans, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">&nbsp;with a value of&nbsp;</span><code style=\"box-sizing: border-box; font-family: Menlo, monospace; font-size: 13.76px; white-space: nowrap; max-width: 100%; padding: 0px 2px; color: rgb(5, 34, 71); overflow-x: auto; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box;\">my-job-6</span></code><span style=\"color: rgb(64, 64, 64); font-family: DMSans, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">&nbsp;for any run of job ID 6, add the following task parameter:</span></p><pre style=\"box-sizing: border-box; font-family: Menlo, monospace; font-size: 12.8px; white-space: pre; position: relative; z-index: 1; margin: 0px; padding: 12px; line-height: 1.5; display: block; overflow: auto; color: rgb(5, 34, 71); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(242, 242, 242); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box;\">{</span>\n<span style=\"box-sizing: border-box; color: rgb(187, 187, 187);\">  </span><span style=\"box-sizing: border-box; color: rgb(127, 0, 85); font-weight: bold;\">&quot;MyJobID&quot;</span><span style=\"box-sizing: border-box;\">:</span><span style=\"box-sizing: border-box; color: rgb(187, 187, 187);\"> </span><span style=\"box-sizing: border-box; color: rgb(42, 0, 255);\">&quot;my-job-{{job_id}}&quot;</span>\n<span style=\"box-sizing: border-box;\">}</span></pre><p><br></p><p>Similarly for <span style=\"color: rgb(5, 34, 71); font-family: Menlo, monospace; font-size: 13.76px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">{{run_id}}</span></p><p>Below docs are for reference:</p><p><a href=\"https://docs.databricks.com/data-engineering/jobs/jobs.html#task-parameter-variables\">https://docs.databricks.com/data-engineering/jobs/jobs.html#task-parameter-variables</a></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCategory: &lt;list PRIMARY category article should appear in; category list is here: https://databricks.helpjuice.com/admin/categories/all-articles&gt;\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;\nAuthor: Mohit\nOwning Team: &lt;Region + Platform/Spark&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; Problem We are having Databricks Job running with main class and JAR file in it. Our JAR file code base is in Scala (written in IntelliJ IDEA). Now, when our job starts running, we need to log Job ID and Run ID into our SQL database for future purpose. Our Databricks Jobs are scheduled job and are suppose to automatically trigger. How can we get Job ID from Job Name?\u00a0 Cause &lt;Enter cause text&gt; Solution You can make use of the task parameters, and you will be able to print the job id and run ID. You can pass templated variables into a job task as part of the task\u2019s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job\u2019s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named\u00a0 MyJobId \u00a0with a value of\u00a0 my-job-6 \u00a0for any run of job ID 6, add the following task parameter: { \"MyJobID\" : \"my-job-{{job_id}}\"\n} Similarly for {{run_id}} Below docs are for reference: https://docs.databricks.com/data-engineering/jobs/jobs.html#task-parameter-variables", "format": "html", "updated_at": "2022-06-30T10:50:47.181Z"}, "author": {"id": 789494, "email": "prabakar.ammeappin@databricks.com", "name": "prabakar.ammeappin ", "first_name": "prabakar.ammeappin", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:54:57.352Z", "updated_at": "2023-04-05T07:45:06.002Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 300496, "name": "Needs Work (Plat EMEA)", "codename": "needs-work-plat-emea", "accessibility": 2, "description": "", "icon": "far fa-hammer", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290944, "name": "platform-emea-drafts", "codename": "platform-emea-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3252640, "name": "extract id"}, {"id": 3252639, "name": "job name"}], "url": "https://kb.databricks.com/needs-work-plat-emea/1423829-extract-job-id-and-run-id-from-job-name-to-use-it-in-the-code"}, {"id": 1423358, "name": "Cannot install R library leafpop", "views": 137, "accessibility": 2, "description": "", "codename": "cannot-install-r-library-leafpop", "created_at": "2022-06-29T15:43:12.262Z", "updated_at": "2023-04-24T07:00:55.153Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: NA</p><p>Category: R library</p><p>Secondary category:&nbsp;</p><p>Cloud Version: &nbsp;AWS, Azure, GCP</p><p>Author: Abishek.Subramanian@databricks.com</p><p>Owning Team: <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">EMEA Platform</span></p><p>Ticket URL: NA</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1>Problem</h1><p>&lt;Enter problem text&gt;</p><h1>Cause</h1><p>&lt;Enter cause text&gt;</p><h1>Solution</h1><pre><span id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>Step</span><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>\u00a01</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>%python</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>dbutils.fs.put(&quot;dbfs:/FileStore/XXXXX/leafpop_init.sh&quot;,&quot;&quot;&quot;</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>#!/bin/bash</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>set -euxo pipefail</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>sudo apt-get -y update &amp;&amp; apt-get install -y libudunits2-dev libgdal-dev libgeos-dev libproj-dev</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>sudo apt -y install libfontconfig1-dev</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>&quot;&quot;&quot;,True)</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>Step 2</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Using a notebook run the below command</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>#\u00a0</span><span style='box-sizing: border-box; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: -apple-system, system-ui, \"Segoe UI\", system-ui, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Web\", sans-serif; font-size: 14px;'>install.packages(c(&quot;leafpop&quot;))</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Step 3</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>After successfully installing the package to validate the status please run the below command</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>library(&quot;leafpop&quot;)</span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 12px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 243, 243); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>installed.packages()</span></pre><p><br></p>", "body_txt": "Restricted Content DBR Version: NA\nCategory: R library\nSecondary category:\u00a0\nCloud Version: \u00a0AWS, Azure, GCP\nAuthor: Abishek.Subramanian@databricks.com\nOwning Team: EMEA Platform Ticket URL: NA\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; Problem &lt;Enter problem text&gt; Cause &lt;Enter cause text&gt; Solution Step \u00a01 %python dbutils.fs.put(\"dbfs:/FileStore/XXXXX/leafpop_init.sh\",\"\"\" #!/bin/bash set -euxo pipefail sudo apt-get -y update &amp;&amp; apt-get install -y libudunits2-dev libgdal-dev libgeos-dev libproj-dev sudo apt -y install libfontconfig1-dev \"\"\",True) Step 2 Using a notebook run the below command #\u00a0 install.packages(c(\"leafpop\")) Step 3 After successfully installing the package to validate the status please run the below command library(\"leafpop\") installed.packages()", "format": "html", "updated_at": "2022-06-30T11:10:25.660Z"}, "author": {"id": 789494, "email": "prabakar.ammeappin@databricks.com", "name": "prabakar.ammeappin ", "first_name": "prabakar.ammeappin", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:54:57.352Z", "updated_at": "2023-04-05T07:45:06.002Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 300201, "name": "Ready for Final Review (Plat EMEA)", "codename": "ready-for-final-review-plat-emea", "accessibility": 2, "description": "", "icon": "far fa-file-certificate", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290944, "name": "platform-emea-drafts", "codename": "platform-emea-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3114439, "name": "installation"}, {"id": 3114440, "name": "r library"}], "url": "https://kb.databricks.com/ready-for-final-review-plat-emea/cannot-install-r-library-leafpop"}, {"id": 1423114, "name": "Unable to infer schema for Parquet", "views": 1, "accessibility": 2, "description": "", "codename": "unable-to-infer-schema-for-parquet", "created_at": "2022-06-29T11:15:23.590Z", "updated_at": "2023-04-13T13:07:41.640Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>Category: Data Management&nbsp;</p><p>Secondary category: NA</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: Manisha Jena</p><p>Owning Team: India Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f000000CeLHAA0/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f000000CeLHAA0/view</a></p><p>Last reviewed date: October 21st, 2021 by Ram <span dir=\"ltr\" style=\"box-sizing: border-box; border-bottom: var(--lwc-borderWidthThin,1px) dotted var(--lwc-colorBorderInfo,rgb(116, 116, 116));\">Sankarasubramanian</span></p><p>Review status: Tech Review Approved</p></div><h1>Problem</h1><ul data-aura-rendered-by=\"371:796;a\" id=\"isPasted\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box;\">Unable to infer the schema for Parquet files when reading from a&nbsp;directory.</li><li style=\"box-sizing: border-box;\">We will get the following error:</li></ul><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>org.apache.spark.sql.AnalysisException: Unable to infer the schema for Parquet. It must be specified manually.</span></p><h1>Cause</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Generally&nbsp;</span><b data-aura-rendered-by=\"421:796;a\" style='box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>Unable to infer the schema for Parquet</b><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;occurs when spark fails to infer the schema from the given directory.</span></p><p data-aura-rendered-by=\"421:796;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>When we don&#39;t specify schema, the spark can either not find any files(in case of an empty directory) or find a different schema from other files and, it will throw the exception <b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">Unable to infer the schema for Parquet</b>.</p><p data-aura-rendered-by=\"421:796;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>Below are the two scenarios when the error is thrown:</p><ol data-aura-rendered-by=\"421:796;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: decimal; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box;\">Empty directory</li><li style=\"box-sizing: border-box;\">When the base path is given instead of the complete path, while reading the Parquet file and there are multiple subfolders containing Parquet files, it will throw the error: <b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">Unable to infer the schema for Parquet</b></li></ol><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>E.g. let&#39;s consider the below sample repo:</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Scenario - 1</span></p><ul data-aura-rendered-by=\"421:796;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box;\">Create empty directory <b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">&quot;/dbfs/tmp/testparquet_empty&quot;</b></li></ul><p><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeLH&feoid=00N3f000000m2eP&refid=0EM3f000001dcQx\" alt=\"image.png\" data-aura-rendered-by=\"421:796;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\" class=\"fr-fic fr-dii\"></p><ul data-aura-rendered-by=\"421:796;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box;\">The below code will throw an error</li></ul><pre data-aura-rendered-by=\"421:796;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">     val df = spark.read.parquet(&quot;dbfs:/tmp/testparquet_empty&quot;)</pre><p><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeLH&feoid=00N3f000000m2eP&refid=0EM3f000001dcRg\" alt=\"Screenshot 2021-10-21 at 11.07.50 AM.png\" data-aura-rendered-by=\"421:796;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\" class=\"fr-fic fr-dii\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Scenario - 2</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>When the base path is given instead of the complete path, while reading the Parquet file and there are multiple subfolders containing Parquet files, it will throw the error:&nbsp;</span><b data-aura-rendered-by=\"421:796;a\" style='box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>Unable to infer the schema for Parquet</b></p><pre data-aura-rendered-by=\"421:796;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">import org.apache.hadoop.fs.Path\nval basePath = &quot;dbfs:/tmp/testparquet&quot;\nspark.range(1).toDF(&quot;a&quot;).write.parquet(new Path(basePath, &quot;first&quot;).toString)\nspark.range(1,2).toDF(&quot;a&quot;).write.parquet(new Path(basePath, &quot;second&quot;).toString)\nspark.range(2,3).toDF(&quot;a&quot;).write.parquet(new Path(basePath, &quot;third&quot;).toString)</pre><pre data-aura-rendered-by=\"421:796;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">val df = spark.read.parquet(basePath)</pre><p><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeLH&feoid=00N3f000000m2eP&refid=0EM3f000001dcRl\" alt=\"image.png\" data-aura-rendered-by=\"421:796;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\" class=\"fr-fic fr-dii\"></p><p><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeLH&feoid=00N3f000000m2eP&refid=0EM3f000001dcRv\" alt=\"Screenshot 2021-10-21 at 11.37.15 AM.png\" data-aura-rendered-by=\"421:796;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\" class=\"fr-fic fr-dii\"></p><h1>Solution</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Please find below the solution for both the scenarios:</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Scenario - 1</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>The below code will not throw any error or exception because we have provided the schema and the complete path instead of the base path:</span></p><pre data-aura-rendered-by=\"471:796;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">val df_schema = spark.read.schema(&quot;a int&quot;).parquet(&quot;dbfs:/tmp/testparquet_empty&quot;)\u00a0</pre><p data-aura-rendered-by=\"471:796;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeLH&feoid=00N3f000000m2eY&refid=0EM3f000001dcS0\" alt=\"image.png\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer;\" class=\"fr-fic fr-dii\"><br style=\"box-sizing: border-box;\">Scenario - 2:<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">The below code will not throw any error or exception because we have provided the schema and the complete path instead of the base path:</p><pre data-aura-rendered-by=\"471:796;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">import org.apache.hadoop.fs.Path\nval basePath = &quot;dbfs:/tmp/testparquet&quot;\nspark.range(1).toDF(&quot;a&quot;).write.parquet(new Path(basePath, &quot;first1&quot;).toString)\nspark.range(1,2).toDF(&quot;a&quot;).write.parquet(new Path(basePath, &quot;second2&quot;).toString)\nspark.range(2,3).toDF(&quot;a&quot;).write.parquet(new Path(basePath, &quot;third3&quot;).toString)</pre><pre data-aura-rendered-by=\"471:796;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">val dfWithSchema = spark.read.schema(&quot;a long&quot;).parquet(basePath + &quot;/third3/&quot;)</pre><p data-aura-rendered-by=\"471:796;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style=\"box-sizing: border-box;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeLH&feoid=00N3f000000m2eY&refid=0EM3f000001dcSA\" alt=\"image.png\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer;\" class=\"fr-fic fr-dii\"><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeLH&feoid=00N3f000000m2eY&refid=0EM3f000001dcSF\" alt=\"image.png\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer;\" class=\"fr-fic fr-dii\"><br style=\"box-sizing: border-box;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeLH&feoid=00N3f000000m2eY&refid=0EM3f000000P7sv\" alt=\"image.png\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer;\" class=\"fr-fic fr-dii\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Repro Notebook URL -&nbsp;</span><a data-aura-rendered-by=\"471:796;a\" href=\"https://cust-success.cloud.databricks.com/#notebook/1969510/command/1969511\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://cust-success.cloud.databricks.com/#notebook/2065048/command/2065049</a></p>", "body_txt": "Restricted Content Category: Data Management\u00a0\nSecondary category: NA\nCloud Version: AWS, Azure, GCP\nAuthor: Manisha Jena\nOwning Team: India Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f000000CeLHAA0/view Last reviewed date: October 21st, 2021 by Ram Sankarasubramanian Review status: Tech Review Approved Problem Unable to infer the schema for Parquet files when reading from a\u00a0directory.\nWe will get the following error: org.apache.spark.sql.AnalysisException: Unable to infer the schema for Parquet. It must be specified manually. Cause Generally\u00a0 Unable to infer the schema for Parquet \u00a0occurs when spark fails to infer the schema from the given directory. When we don't specify schema, the spark can either not find any files(in case of an empty directory) or find a different schema from other files and, it will throw the exception Unable to infer the schema for Parquet. Below are the two scenarios when the error is thrown: Empty directory\nWhen the base path is given instead of the complete path, while reading the Parquet file and there are multiple subfolders containing Parquet files, it will throw the error: Unable to infer the schema for Parquet E.g. let's consider the below sample repo: Scenario - 1 Create empty directory \"/dbfs/tmp/testparquet_empty\" The below code will throw an error val df = spark.read.parquet(\"dbfs:/tmp/testparquet_empty\") Scenario - 2 When the base path is given instead of the complete path, while reading the Parquet file and there are multiple subfolders containing Parquet files, it will throw the error:\u00a0 Unable to infer the schema for Parquet import org.apache.hadoop.fs.Path\nval basePath = \"dbfs:/tmp/testparquet\"\nspark.range(1).toDF(\"a\").write.parquet(new Path(basePath, \"first\").toString)\nspark.range(1,2).toDF(\"a\").write.parquet(new Path(basePath, \"second\").toString)\nspark.range(2,3).toDF(\"a\").write.parquet(new Path(basePath, \"third\").toString) val df = spark.read.parquet(basePath) Solution Please find below the solution for both the scenarios: Scenario - 1 The below code will not throw any error or exception because we have provided the schema and the complete path instead of the base path: val df_schema = spark.read.schema(\"a int\").parquet(\"dbfs:/tmp/testparquet_empty\")\u00a0 Scenario - 2: The below code will not throw any error or exception because we have provided the schema and the complete path instead of the base path: import org.apache.hadoop.fs.Path\nval basePath = \"dbfs:/tmp/testparquet\"\nspark.range(1).toDF(\"a\").write.parquet(new Path(basePath, \"first1\").toString)\nspark.range(1,2).toDF(\"a\").write.parquet(new Path(basePath, \"second2\").toString)\nspark.range(2,3).toDF(\"a\").write.parquet(new Path(basePath, \"third3\").toString) val dfWithSchema = spark.read.schema(\"a long\").parquet(basePath + \"/third3/\") Repro Notebook URL -\u00a0 https://cust-success.cloud.databricks.com/#notebook/2065048/command/2065049", "format": "html", "updated_at": "2022-06-29T11:18:33.833Z"}, "author": {"id": 838921, "email": "chandana.koppal@databricks.com", "name": "chandana.koppal ", "first_name": "chandana.koppal", "last_name": "", "role_id": "draft_writer", "created_at": "2022-04-04T10:55:10.687Z", "updated_at": "2023-03-20T04:23:14.357Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 316341, "name": "Ready for Final Review (Spark India)", "codename": "ready-for-final-review-spark-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290939, "name": "spark-india-drafts", "codename": "spark-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3115078, "name": "inferring"}, {"id": 2951092, "name": "parquet"}], "url": "https://kb.databricks.com/ready-for-final-review-spark-india/unable-to-infer-schema-for-parquet"}, {"id": 1420252, "name": "How to solve dependencies in jar files", "views": 2, "accessibility": 2, "description": "", "codename": "1420252-how-to-solve-dependencies-in-jar-files", "created_at": "2022-06-27T13:28:30.262Z", "updated_at": "2022-09-05T11:03:20.017Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: DBR 7.3 LTS - DBR 10.5 LTS</p><p id=\"isPasted\">Category: &lt;list PRIMARY category article should appear in&gt; Library&nbsp;</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt; ALL</p><p>Author: kavya.parag@databricks.com</p><p>Owning Team: &lt;India + Platform&gt;</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1>How-to Introduction</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>How to solve dependencies in jar files</span></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Pre-requisites: You will need to have maven installed.</span><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>1. Create&nbsp;pom.xml, replace it with the correct dependency block:</span><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span data-aura-rendered-by=\"423:773;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: \"Courier New\", Courier, monospace;'>&lt;project&gt;<br style=\"box-sizing: border-box;\">&nbsp;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;<br style=\"box-sizing: border-box;\">&nbsp;&lt;groupId&gt;com.databricks.support&lt;/groupId&gt;<br style=\"box-sizing: border-box;\">&nbsp;&lt;artifactId&gt;example&lt;/artifactId&gt;<br style=\"box-sizing: border-box;\">&nbsp;&lt;version&gt;1&lt;/version&gt;<br style=\"box-sizing: border-box;\">&nbsp;&lt;dependencies&gt;<br style=\"box-sizing: border-box;\">&nbsp; &nbsp;&lt;dependency&gt;<br style=\"box-sizing: border-box;\"> &lt;groupId&gt;###<a href=\"https://group.id/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">group.id</a>###&lt;/groupId&gt;<br style=\"box-sizing: border-box;\"> &lt;artifactId&gt;###<a href=\"https://artifact.id/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">artifact.id</a>###&lt;/artifactId&gt;<br style=\"box-sizing: border-box;\">&nbsp; &nbsp; &nbsp;&lt;version&gt;###version###&lt;/version&gt;<br style=\"box-sizing: border-box;\">&nbsp; &nbsp;&lt;/dependency&gt;<br style=\"box-sizing: border-box;\">&nbsp;&lt;/dependencies&gt;<br style=\"box-sizing: border-box;\">&lt;/project&gt;</span><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>2. Clean up the local repo first:</span><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span data-aura-rendered-by=\"423:773;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: \"Courier New\", Courier, monospace;'>mvn dependency:purge-local-repository -DactTransitively=false -DreResolve=false</span><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>3. Pull in all the required JARs:</span><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span data-aura-rendered-by=\"423:773;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: \"Courier New\", Courier, monospace;'>mvn org.apache.maven.plugins:maven-dependency-plugin:2.10:tree -Dverbose=true</span><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>4.Find all the JARs:</span><br data-aura-rendered-by=\"423:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span data-aura-rendered-by=\"423:773;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: \"Courier New\", Courier, monospace;'>mkdir -p&nbsp;$(pwd)/jars<br style=\"box-sizing: border-box;\">cp $(find $HOME/.m2 -type f -iname&nbsp;&lsquo;*###package_name###*.jar&rsquo;)&nbsp;$(pwd)/jars/<br style=\"box-sizing: border-box;\">mvn dependency:copy-dependencies -f $(find $HOME/.m2 -type f -iname &lsquo;*###package_name###*.pom&#39;) -DoutputDirectory=$(pwd)/jars/</span></p>", "body_txt": "Restricted Content DBR Version: DBR 7.3 LTS - DBR 10.5 LTS\nCategory: &lt;list PRIMARY category article should appear in&gt; Library\u00a0\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt; ALL\nAuthor: kavya.parag@databricks.com\nOwning Team: &lt;India + Platform&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; How-to Introduction How to solve dependencies in jar files Note\nInsert your text here Instructions Pre-requisites: You will need to have maven installed. 1. Create\u00a0pom.xml, replace it with the correct dependency block: &lt;project&gt;\u00a0&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\u00a0&lt;groupId&gt;com.databricks.support&lt;/groupId&gt;\u00a0&lt;artifactId&gt;example&lt;/artifactId&gt;\u00a0&lt;version&gt;1&lt;/version&gt;\u00a0&lt;dependencies&gt;\u00a0 \u00a0&lt;dependency&gt; &lt;groupId&gt;###group.id###&lt;/groupId&gt; &lt;artifactId&gt;###artifact.id###&lt;/artifactId&gt;\u00a0 \u00a0 \u00a0&lt;version&gt;###version###&lt;/version&gt;\u00a0 \u00a0&lt;/dependency&gt;\u00a0&lt;/dependencies&gt;&lt;/project&gt; 2. Clean up the local repo first: mvn dependency:purge-local-repository -DactTransitively=false -DreResolve=false 3. Pull in all the required JARs: mvn org.apache.maven.plugins:maven-dependency-plugin:2.10:tree -Dverbose=true 4.Find all the JARs: mkdir -p\u00a0$(pwd)/jarscp $(find $HOME/.m2 -type f -iname\u00a0\u2018*###package_name###*.jar\u2019)\u00a0$(pwd)/jars/mvn dependency:copy-dependencies -f $(find $HOME/.m2 -type f -iname \u2018*###package_name###*.pom') -DoutputDirectory=$(pwd)/jars/", "format": "html", "updated_at": "2022-06-27T13:31:34.894Z"}, "author": {"id": 789487, "email": "kavya.parag@databricks.com", "name": "kavya.parag ", "first_name": "kavya.parag", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T06:35:45.179Z", "updated_at": "2023-03-29T14:10:28.525Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 299407, "name": "Needs Work (Plat India)", "codename": "needs-work-plat-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290942, "name": "platform-india-drafts", "codename": "platform-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3282506, "name": "dependencies"}, {"id": 3282507, "name": "jar files"}], "url": "https://kb.databricks.com/needs-work-plat-india/1420252-how-to-solve-dependencies-in-jar-files"}, {"id": 1420243, "name": "How to do thread dump collection.", "views": 1, "accessibility": 2, "description": "", "codename": "1420243-how-to-do-thread-dump-collection", "created_at": "2022-06-27T13:06:29.320Z", "updated_at": "2022-09-14T06:22:15.904Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p id=\"isPasted\">Category: &lt;list PRIMARY category article should appear in&gt;</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt; All</p><p>Author: venkatasai.vanaparthi@databricks.com</p><p>Owning Team: &lt;India + Platform&gt;</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1>How-to Introduction</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>In scenarios where clusters are running for long period without any activity. Most of the times some hung spark job/jobs might be causing the issue.</span></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\"><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: pre-line; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Thread dump has to be taken during the problem state. (live debugging)</span></p></div></div><h1>Instructions</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Please find the Spark configs for hang detection and for thread dump collection. The value has to be put in milliseconds. The configuration has to be done 1/4 of the expected job time. That is if a job&#39;s expected time is 60 mins, then the value should be set to 15 minutes. In milliseconds it would be 900000</span><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>spark.databricks.hangingTaskDetector.enabled</span><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>spark.databricks.hangingTaskDetector.hangingTaskThreshold 900000</span><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>When there is no progress for 15 mins, it means -&nbsp;the thread stack trace did not change for 15 mins</span><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>so now that we identified a thread which is stuck for 15 mins. If I want to take the thread dump of the thread once in 3 mins, we can set the below value accordingly.</span><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>spark.databricks.hangingTaskDetector.threadDumpThreshold</span><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>We can view the thread dump from Cluster -&gt; Spark UI -&gt; &nbsp;Executors -&gt; Thread Dump</span></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCategory: &lt;list PRIMARY category article should appear in&gt;\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt; All\nAuthor: venkatasai.vanaparthi@databricks.com\nOwning Team: &lt;India + Platform&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; How-to Introduction In scenarios where clusters are running for long period without any activity. Most of the times some hung spark job/jobs might be causing the issue. Note Thread dump has to be taken during the problem state. (live debugging) Instructions Please find the Spark configs for hang detection and for thread dump collection. The value has to be put in milliseconds. The configuration has to be done 1/4 of the expected job time. That is if a job's expected time is 60 mins, then the value should be set to 15 minutes. In milliseconds it would be 900000 spark.databricks.hangingTaskDetector.enabled spark.databricks.hangingTaskDetector.hangingTaskThreshold 900000 When there is no progress for 15 mins, it means -\u00a0the thread stack trace did not change for 15 mins so now that we identified a thread which is stuck for 15 mins. If I want to take the thread dump of the thread once in 3 mins, we can set the below value accordingly. spark.databricks.hangingTaskDetector.threadDumpThreshold We can view the thread dump from Cluster -&gt; Spark UI -&gt; \u00a0Executors -&gt; Thread Dump", "format": "html", "updated_at": "2022-06-27T13:13:44.156Z"}, "author": {"id": 789491, "email": "pavan.kumarchalamcharla@databricks.com", "name": "pavan.kumarchalamcharla ", "first_name": "pavan.kumarchalamcharla", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T07:11:25.774Z", "updated_at": "2023-03-28T06:33:31.123Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 299407, "name": "Needs Work (Plat India)", "codename": "needs-work-plat-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290942, "name": "platform-india-drafts", "codename": "platform-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/needs-work-plat-india/1420243-how-to-do-thread-dump-collection"}, {"id": 1420231, "name": "Extract Job ID and Run ID from Job Name to use it in the code", "views": 82, "accessibility": 2, "description": "", "codename": "extract-job-id-and-run-id-from-job-name-to-use-it-in-the-code", "created_at": "2022-06-27T12:52:52.945Z", "updated_at": "2022-06-27T12:57:42.054Z", "next_expiration_on": null, "published": false, "answer": {"body": "<p id=\"isPasted\"><strong>Problem:</strong></p><p>We are having Databricks Job running with main class and JAR file in it. Our JAR file code base is in Scala (written in IntelliJ IDEA). Now, when our job starts running, we need to log Job ID and Run ID into our SQL database for future purpose. Our Databricks Jobs are scheduled job and are suppose to automatically trigger. How can we get Job ID from Job Name? Job Name will be unique.</p><p><br></p><p><strong>Solution:</strong></p><p id=\"isPasted\">You can make use of the task parameters, and you will be able to print the job id and run ID.</p><p><span id=\"isPasted\" style=\"color: rgb(64, 64, 64); font-family: DMSans, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">You can pass templated variables into a job task as part of the task&rsquo;s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job&rsquo;s start time.</span></p><p><span id=\"isPasted\"><span id=\"isPasted\" style=\"color: rgb(64, 64, 64); font-family: DMSans, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named&nbsp;</span><code style=\"box-sizing: border-box; font-family: Menlo, monospace; font-size: 13.76px; white-space: nowrap; max-width: 100%; padding: 0px 2px; color: rgb(5, 34, 71); overflow-x: auto; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box;\">MyJobId</span></code><span style=\"color: rgb(64, 64, 64); font-family: DMSans, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">&nbsp;with a value of&nbsp;</span><code style=\"box-sizing: border-box; font-family: Menlo, monospace; font-size: 13.76px; white-space: nowrap; max-width: 100%; padding: 0px 2px; color: rgb(5, 34, 71); overflow-x: auto; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box;\">my-job-6</span></code><span style=\"color: rgb(64, 64, 64); font-family: DMSans, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">&nbsp;for any run of job ID 6, add the following task parameter:</span></span></p><pre id=\"isPasted\" style=\"box-sizing: border-box; font-family: Menlo, monospace; font-size: 12.8px; white-space: pre; position: relative; z-index: 1; margin: 0px; padding: 12px; line-height: 1.5; display: block; overflow: auto; color: rgb(5, 34, 71); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(242, 242, 242); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box;\">{</span>\n<span style=\"box-sizing: border-box; color: rgb(187, 187, 187);\">  </span><span style=\"box-sizing: border-box; color: rgb(127, 0, 85); font-weight: bold;\">&quot;MyJobID&quot;</span><span style=\"box-sizing: border-box;\">:</span><span style=\"box-sizing: border-box; color: rgb(187, 187, 187);\"> </span><span style=\"box-sizing: border-box; color: rgb(42, 0, 255);\">&quot;my-job-{{job_id}}&quot;</span>\n<span style=\"box-sizing: border-box;\">}</span></pre><p><br></p><p>Similarly for&nbsp;<span id=\"isPasted\" style=\"color: rgb(5, 34, 71); font-family: Menlo, monospace; font-size: 13.76px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">{{run_id}}</span></p><p>Below docs are for reference:</p><p><a href=\"https://docs.databricks.com/data-engineering/jobs/jobs.html#task-parameter-variables\">https://docs.databricks.com/data-engineering/jobs/jobs.html#task-parameter-variables</a></p>", "body_txt": "Problem: We are having Databricks Job running with main class and JAR file in it. Our JAR file code base is in Scala (written in IntelliJ IDEA). Now, when our job starts running, we need to log Job ID and Run ID into our SQL database for future purpose. Our Databricks Jobs are scheduled job and are suppose to automatically trigger. How can we get Job ID from Job Name? Job Name will be unique. Solution: You can make use of the task parameters, and you will be able to print the job id and run ID. You can pass templated variables into a job task as part of the task\u2019s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job\u2019s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named\u00a0 MyJobId \u00a0with a value of\u00a0 my-job-6 \u00a0for any run of job ID 6, add the following task parameter: { \"MyJobID\" : \"my-job-{{job_id}}\"\n} Similarly for\u00a0{{run_id}} Below docs are for reference: https://docs.databricks.com/data-engineering/jobs/jobs.html#task-parameter-variables", "format": "html", "updated_at": "2022-06-27T12:56:19.408Z"}, "author": {"id": 790032, "email": "mohit.miglani@databricks.com", "name": "mohit.miglani ", "first_name": "mohit.miglani", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T17:05:54.364Z", "updated_at": "2023-01-31T15:40:10.983Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 300196, "name": "Duplicate (Plat EMEA)", "codename": "duplicate-plat-emea", "accessibility": 2, "description": "", "icon": "far fa-trash-alt", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290944, "name": "platform-emea-drafts", "codename": "platform-emea-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2769813, "name": "databricks"}, {"id": 2769815, "name": "databricksjob"}, {"id": 2769812, "name": "extract"}, {"id": 2769808, "name": "job"}, {"id": 2769806, "name": "jobid"}, {"id": 2769811, "name": "jobname"}, {"id": 2769807, "name": "runid"}, {"id": 2769810, "name": "task parameters"}], "url": "https://kb.databricks.com/duplicate-plat-emea/extract-job-id-and-run-id-from-job-name-to-use-it-in-the-code"}, {"id": 1419202, "name": "Problem Cause Solution", "views": 222, "accessibility": 2, "description": "", "codename": "hc-cluster-on-unity-catalog", "created_at": "2022-06-24T09:53:49.230Z", "updated_at": "2022-06-24T11:39:25.404Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: NA</p><p>Category: Clusters</p><p>Secondary category: NA</p><p>Cloud Version: Azure</p><p>Author: sajesh@databricks.com</p><p>Owning Team: EMEA Platform</p><p>Ticket URL:&nbsp;<a href=\"https://databricks.lightning.force.com/lightning/r/Case/5008Y00001vRM8zQAG/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5008Y00001vRM8zQAG/view</a></p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1>Problem</h1><p><span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Unable to create a high concurrency cluster on Unity Catalog enabled workspace</span></p><h1>Cause</h1><p id=\"isPasted\">It is expected behavior. If your workspace is enabled for Unity Catalog, High Concurrency clusters are not available.</p><p>Refer: https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-mode</p><h1>Solution</h1><p><span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">I</span><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">nstead, you</span><span style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">&nbsp;use security mode to ensure the integrity of access controls and enforce strong isolation guarantees.</span></p>", "body_txt": "Restricted Content DBR Version: NA\nCategory: Clusters\nSecondary category: NA\nCloud Version: Azure\nAuthor: sajesh@databricks.com\nOwning Team: EMEA Platform\nTicket URL:\u00a0https://databricks.lightning.force.com/lightning/r/Case/5008Y00001vRM8zQAG/view Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; Problem Unable to create a high concurrency cluster on Unity Catalog enabled workspace Cause It is expected behavior. If your workspace is enabled for Unity Catalog, High Concurrency clusters are not available. Refer: https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-mode Solution I nstead, you \u00a0use security mode to ensure the integrity of access controls and enforce strong isolation guarantees.", "format": "html", "updated_at": "2022-06-24T10:00:12.363Z"}, "author": {"id": 885492, "email": "sajesh@databricks.com", "name": "Sajesh S", "first_name": "Sajesh", "last_name": "S", "role_id": "draft_writer", "created_at": "2022-06-15T12:19:31.333Z", "updated_at": "2023-02-17T13:05:43.117Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 300196, "name": "Duplicate (Plat EMEA)", "codename": "duplicate-plat-emea", "accessibility": 2, "description": "", "icon": "far fa-trash-alt", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290944, "name": "platform-emea-drafts", "codename": "platform-emea-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3057487, "name": "cluster concurrency"}, {"id": 3057486, "name": "unity catalog"}], "url": "https://kb.databricks.com/duplicate-plat-emea/hc-cluster-on-unity-catalog"}, {"id": 1419194, "name": "Hyperopt fails with Exception: Method maxNumConcurrentTasks([]) does not exist", "views": null, "accessibility": 2, "description": "", "codename": "hyperopt-fails-with-exception-method-maxnumconcurrenttasks[]-does-not-exist", "created_at": "2022-06-24T09:28:41.477Z", "updated_at": "2022-06-30T10:43:34.261Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All</p><p>Category: Machine Lerning</p><p>Secondary category: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Python with Spark</span></p><p>Cloud Version: All</p><p>Author: <a data-aura-class=\"forceOutputLookup\" data-aura-rendered-by=\"38:773;a\" data-navigable=\"true\" data-ownerid=\"27:773;a\" data-recordid=\"0053f000000tu0mAAA\" data-refid=\"recordId\" data-special-link=\"true\" href=\"https://databricks.lightning.force.com/lightning/r/0053f000000tu0mAAA/view\" id=\"isPasted\" rel=\"noreferrer\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; max-width: 100%; overflow: auto; text-overflow: initial; white-space: normal; display: inline; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\" title=\"Chetan Kardekar\">Chetan Kardekar</a></p><p>Owning Team: India Spark</p><p>Ticket URL: <a data-aura-class=\"uiOutputURL\" data-aura-rendered-by=\"173:773;a\" data-interactive-lib-uid=\"3\" dir=\"ltr\" href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f00000EKbSPAA1/view\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; overflow-wrap: break-word; word-break: break-word; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' title=\"\">https://databricks.lightning.force.com/lightning/r/Case/5003f00000EKbSPAA1/view</a></p><p>Last reviewed date: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>10/4/2021 11:34 AM&nbsp;</span>by <a data-aura-class=\"forceOutputLookup\" data-aura-rendered-by=\"263:773;a\" data-navigable=\"true\" data-ownerid=\"252:773;a\" data-recordid=\"0054N000003pJrFQAU\" data-refid=\"recordId\" data-special-link=\"true\" href=\"https://databricks.lightning.force.com/lightning/r/0054N000003pJrFQAU/view\" id=\"isPasted\" rel=\"noreferrer\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; max-width: 100%; overflow: auto; text-overflow: initial; white-space: normal; display: inline; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\" title=\"Pradeep Kumar Palaniswamy\">Pradeep Kumar Palaniswamy</a></p><p>Review status: Tech Review Approved</p></div><h1>Problem</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Databricks job fails with py4j.Py4JException: Method maxNumConcurrentTasks([]) does not exist, while tuning machine learning hyperparameters using hyperopt on</span><br data-aura-rendered-by=\"371:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Databricks ML runtime cluster.</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f000000CeF4&feoid=00N3f000000m2eW&refid=0EM3f000001clE5\" class=\"fr-fic fr-dib\"></span><br></p><h1>Cause</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Databricks ML runtime already comes pre-installed with hyperopt package.</span><br data-aura-rendered-by=\"421:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"421:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><a data-aura-rendered-by=\"421:773;a\" href=\"https://docs.databricks.com/release-notes/runtime/8.4ml.html#python-libraries\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://docs.databricks.com/release-notes/runtime/8.4ml.html#python-libraries</a><br data-aura-rendered-by=\"421:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"421:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>You need not install the hyperopt package manually using</span><b data-aura-rendered-by=\"421:773;a\" style='box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><i style=\"box-sizing: border-box;\">&nbsp;%pip install hyperopt</i></b><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp; in the notebook. &nbsp;</span><br data-aura-rendered-by=\"421:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"421:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>If you are installing hyperopt package manually you would get into&nbsp;</span><i data-aura-rendered-by=\"421:773;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>&quot;py4j.Py4JException: Method maxNumConcurrentTasks([]) does not exist&quot; error.</i><br data-aura-rendered-by=\"421:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p><h1>Solution</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Utilise the pre-installed&nbsp;hyperopt package available in Databricks ML runtime instead of installing it again</span><br data-aura-rendered-by=\"471:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"471:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Below is the repro notebook for future reference :</span><br data-aura-rendered-by=\"471:773;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><a data-aura-rendered-by=\"471:773;a\" href=\"https://cust-success.cloud.databricks.com/#notebook/1730965/command/1730966\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://cust-success.cloud.databricks.com/#notebook/1730965/command/1730966</a></p>", "body_txt": "Restricted Content DBR Version: All\nCategory: Machine Lerning\nSecondary category: Python with Spark Cloud Version: All\nAuthor: Chetan Kardekar Owning Team: India Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f00000EKbSPAA1/view Last reviewed date: 10/4/2021 11:34 AM\u00a0by Pradeep Kumar Palaniswamy Review status: Tech Review Approved Problem Databricks job fails with py4j.Py4JException: Method maxNumConcurrentTasks([]) does not exist, while tuning machine learning hyperparameters using hyperopt on Databricks ML runtime cluster. Cause Databricks ML runtime already comes pre-installed with hyperopt package. https://docs.databricks.com/release-notes/runtime/8.4ml.html#python-libraries You need not install the hyperopt package manually using \u00a0%pip install hyperopt \u00a0 in the notebook. \u00a0 If you are installing hyperopt package manually you would get into\u00a0 \"py4j.Py4JException: Method maxNumConcurrentTasks([]) does not exist\" error. \u00a0 Solution Utilise the pre-installed\u00a0hyperopt package available in Databricks ML runtime instead of installing it again Below is the repro notebook for future reference : https://cust-success.cloud.databricks.com/#notebook/1730965/command/1730966", "format": "html", "updated_at": "2022-06-24T09:35:08.875Z"}, "author": {"id": 821786, "email": "chetan.kardekar@databricks.com", "name": "chetan.kardekar ", "first_name": "chetan.kardekar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.705Z", "updated_at": "2022-07-01T13:13:21.129Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 297319, "name": "Will not publish", "codename": "will-not-publish", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/will-not-publish/hyperopt-fails-with-exception-method-maxnumconcurrenttasks%5B%5D-does-not-exist"}, {"id": 1419193, "name": "Untitled Article", "views": null, "accessibility": 1, "description": null, "codename": "1419193-untitled-article", "created_at": "2022-06-24T09:28:17.648Z", "updated_at": "2022-06-24T09:28:17.666Z", "next_expiration_on": null, "published": false, "answer": {"body": "", "body_txt": "", "format": "html", "updated_at": "2022-06-24T09:28:17.661Z"}, "author": {"id": 821786, "email": "chetan.kardekar@databricks.com", "name": "chetan.kardekar ", "first_name": "chetan.kardekar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.705Z", "updated_at": "2022-07-01T13:13:21.129Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 256866, "name": "Streaming", "codename": "streaming", "accessibility": 1, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 257132, "name": "All articles", "codename": "all-articles", "accessibility": 1, "description": "", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/streaming/1419193-untitled-article"}, {"id": 1413740, "name": "parquet to delta stream - how to make fields as optional", "views": 1, "accessibility": 2, "description": "", "codename": "parquet-to-delta-stream-how-to-make-fields-as-optional", "created_at": "2022-06-20T10:27:20.113Z", "updated_at": "2022-11-29T07:55:54.780Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All DBR versions</p><p id=\"isPasted\">Category: Delta Lake</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: deepak.bhutada@databricks.com</p><p>Owning Team: India + Spark</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: 8/20/2021 by Ram Sankarsubramanian</p><p>Review status: Ready for Tech Review</p></div><h1>How-to Introduction</h1><p><span id=\"isPasted\" style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Let&rsquo;s assume we have input data with three columns such as transactionId, customerId, and itemId. Among these three columns, itemId is optional which means some of the parquet files will come without this column. When we read the same data using batch, we can enable mergeSchema which will take care of schema evolution but with respect to streaming we have to specify the schema in advance.</span></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p><span id=\"isPasted\" style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How we can define the schema to make itemId optional :</span></p><p><span id=\"isPasted\" style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Create a schema like below where we are mentioning itemId nullability is True which means if values don&rsquo;t come it will place null values</span></p><pre data-aura-rendered-by=\"423:763;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">val schema = StructType(Seq(StructField(&quot;transactionId&quot;, IntegerType, false),\n                            StructField(&quot;customerId&quot;, IntegerType, false),\n                            StructField(&quot;itemId&quot;, IntegerType, true)))</pre><p><b id=\"isPasted\" style='box-sizing: border-box; font-weight: normal; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Example to demonstrate :&nbsp;</span></b></p><p><b id=\"isPasted\"><span style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span id=\"isPasted\" style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Let&rsquo;s add the first set of input which has all three columns:</span></span></b></p><pre data-aura-rendered-by=\"423:763;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">\u00a0Seq((111,1,1),\n     (112,2,2),\n     (115,1,2),\n     (121,1,1))\n     .toDF(&quot;transactionId&quot;, &quot;customerId&quot;,&quot;itemId&quot;).write.mode(&quot;append&quot;).parquet(BASE_DIR)</pre><p><span id=\"isPasted\" style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now stream the parquet data from &ldquo;BASE_DIR&rdquo; and write the data to delta table as like below, where the stream is initialized which means the target delta table is created with three columns.</span></p><pre data-aura-rendered-by=\"423:763;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">val parquetToDelta_DF= spark.readStream.schema(schema).parquet(BASE_DIR)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.writeStream.format(&quot;delta&quot;)\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.option(&quot;path&quot;, DELTA_DIR)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.option(&quot;checkpointLocation&quot;, CHECKPOINT_DIR)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.outputMode(&quot;append&quot;)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.trigger(Trigger.ProcessingTime(&quot;5 seconds&quot;))\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.start</pre><p><span id=\"isPasted\" style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now, let&#39;s read the target delta path. The output data will look like below,</span></p><pre data-aura-rendered-by=\"423:763;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">display(spark.read.format(&quot;delta&quot;).load(DELTA_DIR)) </pre><p><br id=\"isPasted\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Ocz&feoid=00N4N00000IuyXZ&refid=0EM3f000000O45K\" alt=\"image.png\" data-aura-rendered-by=\"423:763;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\" class=\"fr-fic fr-dii\"></p><p><span id=\"isPasted\" style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Let&rsquo;s add the second set of input to BASE_DIR which has only two columns with no itemId,</span></p><pre data-aura-rendered-by=\"423:763;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Seq((111,1),(112,2)).toDF(&quot;transactionId&quot;,&quot;customerId&quot;).write.mode(&quot;append&quot;).parquet(BASE_DIR)</pre><p><span id=\"isPasted\" style=\"box-sizing: border-box; font-size: 11pt; font-family: Arial; color: rgb(64, 64, 64); background-color: rgb(255, 255, 255); font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Since the streaming query is already initialized and we have made the itemId nullable as True if we read the same delta sink. The output will be like below when there is no data for the third column we have placed null values by following this approach</span></p><pre data-aura-rendered-by=\"423:763;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">display(spark.read.format(&quot;delta&quot;).load(DELTA_DIR))</pre><p><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Ocz&feoid=00N4N00000IuyXZ&refid=0EM3f000000O463\" alt=\"image.png\" data-aura-rendered-by=\"423:763;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\" id=\"isPasted\" class=\"fr-fic fr-dii\"></p><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>You can use below notebook to reproduce the results for demo:</span><br data-aura-rendered-by=\"423:763;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><a data-aura-rendered-by=\"423:763;a\" href=\"https://cust-success.cloud.databricks.com/#notebook/2007521/command/2007562\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://cust-success.cloud.databricks.com/#notebook/2007521/command/2007562</a></p>", "body_txt": "Restricted Content DBR Version: All DBR versions\nCategory: Delta Lake\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: AWS, Azure, GCP\nAuthor: deepak.bhutada@databricks.com\nOwning Team: India + Spark\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: 8/20/2021 by Ram Sankarsubramanian\nReview status: Ready for Tech Review How-to Introduction Let\u2019s assume we have input data with three columns such as transactionId, customerId, and itemId. Among these three columns, itemId is optional which means some of the parquet files will come without this column. When we read the same data using batch, we can enable mergeSchema which will take care of schema evolution but with respect to streaming we have to specify the schema in advance. Note\nInsert your text here Instructions How we can define the schema to make itemId optional : Create a schema like below where we are mentioning itemId nullability is True which means if values don\u2019t come it will place null values val schema = StructType(Seq(StructField(\"transactionId\", IntegerType, false), StructField(\"customerId\", IntegerType, false), StructField(\"itemId\", IntegerType, true))) Example to demonstrate :\u00a0 Let\u2019s add the first set of input which has all three columns: \u00a0Seq((111,1,1), (112,2,2), (115,1,2), (121,1,1)) .toDF(\"transactionId\", \"customerId\",\"itemId\").write.mode(\"append\").parquet(BASE_DIR) Now stream the parquet data from \u201cBASE_DIR\u201d and write the data to delta table as like below, where the stream is initialized which means the target delta table is created with three columns. val parquetToDelta_DF= spark.readStream.schema(schema).parquet(BASE_DIR)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.writeStream.format(\"delta\")\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.option(\"path\", DELTA_DIR)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.option(\"checkpointLocation\", CHECKPOINT_DIR)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.outputMode(\"append\")\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.trigger(Trigger.ProcessingTime(\"5 seconds\"))\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0.start Now, let's read the target delta path. The output data will look like below, display(spark.read.format(\"delta\").load(DELTA_DIR)) Let\u2019s add the second set of input to BASE_DIR which has only two columns with no itemId, Seq((111,1),(112,2)).toDF(\"transactionId\",\"customerId\").write.mode(\"append\").parquet(BASE_DIR) Since the streaming query is already initialized and we have made the itemId nullable as True if we read the same delta sink. The output will be like below when there is no data for the third column we have placed null values by following this approach display(spark.read.format(\"delta\").load(DELTA_DIR)) You can use below notebook to reproduce the results for demo: https://cust-success.cloud.databricks.com/#notebook/2007521/command/2007562", "format": "html", "updated_at": "2022-06-20T10:47:15.587Z"}, "author": {"id": 840937, "email": "deepak.bhutada@databricks.com", "name": "deepak.bhutada ", "first_name": "deepak.bhutada", "last_name": "", "role_id": "draft_writer", "created_at": "2022-04-06T11:01:04.397Z", "updated_at": "2023-04-21T14:02:10.764Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 307309, "name": "Needs Work (Spark India)", "codename": "needs-work-spark-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290939, "name": "spark-india-drafts", "codename": "spark-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2838664, "name": "aws"}, {"id": 2838665, "name": "azure"}, {"id": 2838666, "name": "gcp"}], "url": "https://kb.databricks.com/needs-work-spark-india/parquet-to-delta-stream-how-to-make-fields-as-optional"}, {"id": 1413548, "name": "How to handle the exception - org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere", "views": 1, "accessibility": 2, "description": "", "codename": "how-to-handle-the-exception-orgapachesparksqlanalysisexception-data-written-out-does-not-match-replacewhere", "created_at": "2022-06-20T06:04:52.332Z", "updated_at": "2022-10-07T09:49:59.740Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p id=\"isPasted\" style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">DBR Version: All</p><p style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Category: Delta Lake</p><p style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Secondary category: NA</p><p style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Cloud Version: All</p><p style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Author: ashritha.laxminarayana@databricks.com</p><p style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Owning Team: India Spark team</p><p style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Ticket URL: NA</p><p style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Last reviewed date: 19/11/2020 by Pradeep Kumar</p><p style=\"box-sizing: border-box; color: rgb(56, 76, 96); margin: 0px 0px 20px; font-size: 16px; font-weight: 400; line-height: 1.5; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Review status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1>How-to Introduction</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>This article explains on how to mitigate the error &quot;org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere&quot; while using replaceWhere in Delta with overwrite mode.</span></p><p><br data-aura-rendered-by=\"49:2205;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Error Stack Trace -&nbsp;</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p><pre data-aura-rendered-by=\"49:2205;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere &#39;date = &#39;2018-01-30&#39; AND action=&#39;close&#39;&#39;.\nInvalid data would be written to partitions action=Open/date=2018-01-30.;\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.DeltaErrors$.replaceWhereMismatchException(DeltaErrors.scala:505)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:126)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2(WriteIntoDelta.scala:71)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2$adapted(WriteIntoDelta.scala:70)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:203)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:70)</pre><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>This error is an expected behaviour and it can happen in two different scenarios,</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>1. No data is matched the predicate condition mentioned in replaceWhere</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>2. &nbsp;No of partitions data to be written mismatch with the number of partitions mentioned in the predicate condition</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>For example, we have a delta table &quot;events_partitioned_twoLevel&quot;&nbsp;with two level partitions action and date,</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><b data-aura-rendered-by=\"49:2211;a\" style='box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Oyg&feoid=00N4N00000IuyXZ&refid=0EM3f000000O4rY\" alt=\"image.png\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: 203px; width: 588px;\" class=\"fr-fic fr-dii\"><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">Scenario 1&nbsp;</b><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>: Here, no data is matching with the predicate value mentioned in the query. Hence predicate doesn&#39;t pull any data, so replace where write will thrown an exception as like below :</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Oyg&feoid=00N4N00000IuyXZ&refid=0EM3f000000O4rd\" alt=\"image.png\" data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: 217px; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; width: 564px;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Scenario 2 : &nbsp;Here, the condition what we mentioned in the predicate is about one partition but the source dataframe is having data on other partitions as well. The below simulation explains the same, here incrementalData (Source Dataframe) is having data of two partitions but in predicate we talk only about one partition. This error is expected in this case :</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Oyg&feoid=00N4N00000IuyXZ&refid=0EM3f000000O4ri\" alt=\"image.png\" data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: 89px; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; width: 561px;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Adding proper predicate will address the above error as shown below :</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Oyg&feoid=00N4N00000IuyXZ&refid=0EM3f000000O4rn\" alt=\"image.png\" data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: 77px; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; width: 560px;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Working Example -&nbsp;</span><a data-aura-rendered-by=\"49:2211;a\" href=\"https://cust-success.cloud.databricks.com/#notebook/2008086/command/2008105\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://cust-success.cloud.databricks.com/#notebook/2008086/command/2008105</a></p>", "body_txt": "Restricted Content DBR Version: All\nCategory: Delta Lake\nSecondary category: NA\nCloud Version: All\nAuthor: ashritha.laxminarayana@databricks.com\nOwning Team: India Spark team\nTicket URL: NA\nLast reviewed date: 19/11/2020 by Pradeep Kumar\nReview status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; How-to Introduction This article explains on how to mitigate the error \"org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere\" while using replaceWhere in Delta with overwrite mode. Error Stack Trace -\u00a0 \u00a0 org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere 'date = '2018-01-30' AND action='close''.\nInvalid data would be written to partitions action=Open/date=2018-01-30.;\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.DeltaErrors$.replaceWhereMismatchException(DeltaErrors.scala:505)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:126)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2(WriteIntoDelta.scala:71)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2$adapted(WriteIntoDelta.scala:70)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:203)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:70) Note\nInsert your text here Instructions This error is an expected behaviour and it can happen in two different scenarios, 1. No data is matched the predicate condition mentioned in replaceWhere 2. \u00a0No of partitions data to be written mismatch with the number of partitions mentioned in the predicate condition For example, we have a delta table \"events_partitioned_twoLevel\"\u00a0with two level partitions action and date, Scenario 1\u00a0 : Here, no data is matching with the predicate value mentioned in the query. Hence predicate doesn't pull any data, so replace where write will thrown an exception as like below : Scenario 2 : \u00a0Here, the condition what we mentioned in the predicate is about one partition but the source dataframe is having data on other partitions as well. The below simulation explains the same, here incrementalData (Source Dataframe) is having data of two partitions but in predicate we talk only about one partition. This error is expected in this case : Adding proper predicate will address the above error as shown below : Working Example -\u00a0 https://cust-success.cloud.databricks.com/#notebook/2008086/command/2008105", "format": "html", "updated_at": "2022-06-20T06:06:16.922Z"}, "author": {"id": 821785, "email": "ashritha.laxminarayana@databricks.com", "name": "ashritha.laxminarayana ", "first_name": "ashritha.laxminarayana", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.362Z", "updated_at": "2022-08-05T10:37:27.313Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 307309, "name": "Needs Work (Spark India)", "codename": "needs-work-spark-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290939, "name": "spark-india-drafts", "codename": "spark-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 2758661, "name": "aws"}, {"id": 2758662, "name": "azure"}, {"id": 2758664, "name": "delta lake"}, {"id": 2758663, "name": "gcp"}], "url": "https://kb.databricks.com/needs-work-spark-india/how-to-handle-the-exception-orgapachesparksqlanalysisexception-data-written-out-does-not-match-replacewhere"}, {"id": 1413544, "name": "How-to Template", "views": 1, "accessibility": 0, "description": "", "codename": "1413544-how-to-template", "created_at": "2022-06-20T05:42:52.258Z", "updated_at": "2022-06-20T05:47:35.806Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All</p><p id=\"isPasted\">Category: Delta Lake</p><p>Secondary category: NA</p><p>Cloud Version: All</p><p>Author: ashritha.laxminarayana@databricks.com</p><p>Owning Team: India Spark team</p><p>Ticket URL: NA</p><p>Last reviewed date: 19/11/2020 by Pradeep Kumar</p><p>Review status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1>How-to Introduction</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>This article explains on how to mitigate the error &quot;org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere&quot; while using replaceWhere in Delta with overwrite mode.</span></p><p><br data-aura-rendered-by=\"49:2205;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Error Stack Trace -&nbsp;</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p><pre data-aura-rendered-by=\"49:2205;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-spacingXSmall,0.5rem); border: var(--lwc-borderWidthThin,1px) solid #ccc; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere &#39;date = &#39;2018-01-30&#39; AND action=&#39;close&#39;&#39;.\nInvalid data would be written to partitions action=Open/date=2018-01-30.;\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.DeltaErrors$.replaceWhereMismatchException(DeltaErrors.scala:505)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:126)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2(WriteIntoDelta.scala:71)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2$adapted(WriteIntoDelta.scala:70)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:203)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:70)</pre><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>This error is an expected behaviour and it can happen in two different scenarios,</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>1. No data is matched the predicate condition mentioned in replaceWhere</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>2. &nbsp;No of partitions data to be written mismatch with the number of partitions mentioned in the predicate condition</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>For example, we have a delta table &quot;events_partitioned_twoLevel&quot;&nbsp;with two level partitions action and date,</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><b data-aura-rendered-by=\"49:2211;a\" style='box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Oyg&feoid=00N4N00000IuyXZ&refid=0EM3f000000O4rY\" alt=\"image.png\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: 203px; width: 588px;\" class=\"fr-fic fr-dii\"><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">Scenario 1&nbsp;</b><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>: Here, no data is matching with the predicate value mentioned in the query. Hence predicate doesn&#39;t pull any data, so replace where write will thrown an exception as like below :</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Oyg&feoid=00N4N00000IuyXZ&refid=0EM3f000000O4rd\" alt=\"image.png\" data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: 217px; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; width: 564px;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Scenario 2 : &nbsp;Here, the condition what we mentioned in the predicate is about one partition but the source dataframe is having data on other partitions as well. The below simulation explains the same, here incrementalData (Source Dataframe) is having data of two partitions but in predicate we talk only about one partition. This error is expected in this case :</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Oyg&feoid=00N4N00000IuyXZ&refid=0EM3f000000O4ri\" alt=\"image.png\" data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: 89px; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; width: 561px;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Adding proper predicate will address the above error as shown below :</span><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na209.content.force.com/servlet/rtaImage?eid=ka03f0000008Oyg&feoid=00N4N00000IuyXZ&refid=0EM3f000000O4rn\" alt=\"image.png\" data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: 77px; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; width: 560px;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"49:2211;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Working Example -&nbsp;</span><a data-aura-rendered-by=\"49:2211;a\" href=\"https://cust-success.cloud.databricks.com/#notebook/2008086/command/2008105\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://cust-success.cloud.databricks.com/#notebook/2008086/command/2008105</a></p>", "body_txt": "Restricted Content DBR Version: All\nCategory: Delta Lake\nSecondary category: NA\nCloud Version: All\nAuthor: ashritha.laxminarayana@databricks.com\nOwning Team: India Spark team\nTicket URL: NA\nLast reviewed date: 19/11/2020 by Pradeep Kumar\nReview status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; How-to Introduction This article explains on how to mitigate the error \"org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere\" while using replaceWhere in Delta with overwrite mode. Error Stack Trace -\u00a0 \u00a0 org.apache.spark.sql.AnalysisException: Data written out does not match replaceWhere 'date = '2018-01-30' AND action='close''.\nInvalid data would be written to partitions action=Open/date=2018-01-30.;\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.DeltaErrors$.replaceWhereMismatchException(DeltaErrors.scala:505)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:126)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2(WriteIntoDelta.scala:71)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2$adapted(WriteIntoDelta.scala:70)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:203)\n\u00a0\u00a0\u00a0\u00a0at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:70) Note\nInsert your text here Instructions This error is an expected behaviour and it can happen in two different scenarios, 1. No data is matched the predicate condition mentioned in replaceWhere 2. \u00a0No of partitions data to be written mismatch with the number of partitions mentioned in the predicate condition For example, we have a delta table \"events_partitioned_twoLevel\"\u00a0with two level partitions action and date, Scenario 1\u00a0 : Here, no data is matching with the predicate value mentioned in the query. Hence predicate doesn't pull any data, so replace where write will thrown an exception as like below : Scenario 2 : \u00a0Here, the condition what we mentioned in the predicate is about one partition but the source dataframe is having data on other partitions as well. The below simulation explains the same, here incrementalData (Source Dataframe) is having data of two partitions but in predicate we talk only about one partition. This error is expected in this case : Adding proper predicate will address the above error as shown below : Working Example -\u00a0 https://cust-success.cloud.databricks.com/#notebook/2008086/command/2008105", "format": "html", "updated_at": "2022-06-20T05:47:15.185Z"}, "author": {"id": 821785, "email": "ashritha.laxminarayana@databricks.com", "name": "ashritha.laxminarayana ", "first_name": "ashritha.laxminarayana", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.362Z", "updated_at": "2022-08-05T10:37:27.313Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264396, "name": "Delta Lake (Internal)", "codename": "delta-internal", "accessibility": 0, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/delta-internal/1413544-how-to-template"}, {"id": 1413084, "name": "how to mitigate the out of order columns while using databricks csv reader", "views": 1, "accessibility": 2, "description": "", "codename": "how-to-mitigate-the-out-of-order-columns-while-using-databricks-csv-reader", "created_at": "2022-06-17T20:50:56.125Z", "updated_at": "2022-11-29T07:58:02.386Z", "next_expiration_on": null, "published": false, "answer": {"body": "<p>Restricted Content</p><p>DBR Version: <i class=\"helpjuice-thread\" data-id=\"9061209818-skfuy\">7.3 LTS,9.1 LTS, and 10.4 LTS</i></p><p>Category: Python Spark\u00a0</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: mahesh.h@databricks.com</p><p>Owning Team: India + Spark</p><p>Ticket URL: <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f0000018yNBAAY/view\">https://databricks.lightning.force.com/lightning/r/Case/5003f0000018yNBAAY/view</a></p><p>Last reviewed date: <span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-size:14px;\">7-21-2021</span> by Sandeep Katta</p><p>Review status: &lt;In-Progress, <strong>Needs Work</strong>, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt;</p><p><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><i class=\"helpjuice-thread\" data-id=\"9061300150-y8uf1\"><span>This will be helpful when people like to process text data, product-related reviews data, etc contains extra quotes. Let's take an example. The address is represented with door num, area, city, zip code. While representing this, we may end up with some unwanted charters in the column as shown below.</span></i></span></p><p><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><i><span>%py</span></i></span><br><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><i><span>dbutils.fs.put(\"/FileStore/tables/test_mahesh/Employees.csv\",\"\"\"Id,Name,Department,Address,Gender</span></i></span><br><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><i><span>1,Rakesh,cust success,\"RR Nagar,Bengaluru\"\",Karnataka\",Male</span></i></span><br><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><i><span>2,Lohit,cust success,\"Sampige road,Malleshwaram\"\",Bengaluru,Karnataka\",Male</span></i></span><br><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><i><span>3,Deepak,cust success,\"Shivaji Nagar,Bengaluru,karnataka\",Male</span></i></span><br><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><i><span>\"\"\",True)</span></i></span></p><p><br><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><span>While handling these types of data, they might come across data that has extra quotes in the file that lead to column shift.</span></span></p><p><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;\"><span>In the below image, the address column is the file that leads to column shift.</span></span></p><figure class=\"image\"><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/2187709/image.png\"></figure><p><a href=\"#\">Delete</a></p><h3 id=\"note-0\" data-toc=\"true\"><span>Note</span></h3><p><span>Insert your text here</span></p><p><span style=\"background-color:rgb(255,255,255);color:rgba(0,0,0,0.8);font-size:16px;\">we can fix this is by escaping the quotes and treating them as literal by setting .option(\"escape\", \"\"\") in the Dataframe reader . Now we can witness the Adress column has the complete result without value shifting to next column</span><br><br><i class=\"helpjuice-thread\" data-id=\"9061693564-b7mz1\"><span><strong><u>Example Using CSV Dataframe Reader -\u00a0</u></strong></span></i></p><p><span>spark.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"escape\", \"\\\"\").load(\"/FileStore/tables/test_mahesh/Employees.csv\")</span></p><p><i class=\"helpjuice-thread\" data-id=\"9062924009-r3pym\"><span><strong><u>Example Using Create table syntax using CSV reader -\u00a0</u></strong></span></i></p><p><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;font-size:14px;\">The below syntax explains how we can set the options in SQL API</span></p><p><span>spark.sql(\n\u00a0 \u00a0 \u00a0 s\"\"\" CREATE TABLE testDB.test1 (\n\u00a0 \u00a0 \u00a0 \u00a0 |Id string,\n\u00a0 \u00a0 \u00a0 \u00a0 |Name string,\n\u00a0 \u00a0 \u00a0 \u00a0 |Department string,\n\u00a0 \u00a0 \u00a0 \u00a0 |Address string,\n\u00a0 \u00a0 \u00a0 \u00a0 |Gender string)\n\u00a0 \u00a0 \u00a0 \u00a0 |USING com.databricks.spark.csv\n \u00a0 \u00a0 \u00a0 \u00a0|OPTIONS (path \"/FileStore/tables/test_mahesh/Employees.csv\" , header \"true\", escape '\\\"') \"\"\".stripMargin)\u00a0</span></p><p>\u00a0</p><p>\u00a0</p><p>\u00a0</p><p><span style=\"background-color:rgb(255,255,255);color:rgb(24,24,24);font-family:Arial, Helvetica, sans-serif;font-size:14px;\">Demo Notebook -\u00a0</span><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cust-success.cloud.databricks.com/#notebook/1970967/command/1971000\"><span>https://cust-success.cloud.databricks.com/#notebook/1970967/command/1971000</span></a><br>\u00a0</p>", "body_txt": "Restricted Content DBR Version: 7.3 LTS,9.1 LTS, and 10.4 LTS Category: Python Spark\u00a0 Secondary category: &lt;list secondary category, if applicable&gt; Cloud Version: AWS, Azure, GCP Author: mahesh.h@databricks.com Owning Team: India + Spark Ticket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f0000018yNBAAY/view Last reviewed date: 7-21-2021 by Sandeep Katta Review status: &lt;In-Progress, Needs Work, Ready for Tech Review, Tech Review Approved, Ready for Final Review, Published&gt; This will be helpful when people like to process text data, product-related reviews data, etc contains extra quotes. Let's take an example. The address is represented with door num, area, city, zip code. While representing this, we may end up with some unwanted charters in the column as shown below. %py dbutils.fs.put(\"/FileStore/tables/test_mahesh/Employees.csv\",\"\"\"Id,Name,Department,Address,Gender 1,Rakesh,cust success,\"RR Nagar,Bengaluru\"\",Karnataka\",Male 2,Lohit,cust success,\"Sampige road,Malleshwaram\"\",Bengaluru,Karnataka\",Male 3,Deepak,cust success,\"Shivaji Nagar,Bengaluru,karnataka\",Male \"\"\",True) While handling these types of data, they might come across data that has extra quotes in the file that lead to column shift. In the below image, the address column is the file that leads to column shift. Delete Note Insert your text here we can fix this is by escaping the quotes and treating them as literal by setting .option(\"escape\", \"\"\") in the Dataframe reader . Now we can witness the Adress column has the complete result without value shifting to next column Example Using CSV Dataframe Reader -\u00a0 spark.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"escape\", \"\\\"\").load(\"/FileStore/tables/test_mahesh/Employees.csv\") Example Using Create table syntax using CSV reader -\u00a0 The below syntax explains how we can set the options in SQL API spark.sql(\n\u00a0 \u00a0 \u00a0 s\"\"\" CREATE TABLE testDB.test1 (\n\u00a0 \u00a0 \u00a0 \u00a0 |Id string,\n\u00a0 \u00a0 \u00a0 \u00a0 |Name string,\n\u00a0 \u00a0 \u00a0 \u00a0 |Department string,\n\u00a0 \u00a0 \u00a0 \u00a0 |Address string,\n\u00a0 \u00a0 \u00a0 \u00a0 |Gender string)\n\u00a0 \u00a0 \u00a0 \u00a0 |USING com.databricks.spark.csv \u00a0 \u00a0 \u00a0 \u00a0|OPTIONS (path \"/FileStore/tables/test_mahesh/Employees.csv\" , header \"true\", escape '\\\"') \"\"\".stripMargin)\u00a0 \u00a0 \u00a0 \u00a0 Demo Notebook -\u00a0 https://cust-success.cloud.databricks.com/#notebook/1970967/command/1971000 \u00a0", "format": "html", "updated_at": "2022-08-01T15:44:47.398Z"}, "author": {"id": 790832, "email": "mahesh.h@databricks.com", "name": "mahesh.h ", "first_name": "mahesh.h", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T14:08:29.594Z", "updated_at": "2022-12-06T13:28:21.188Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 307309, "name": "Needs Work (Spark India)", "codename": "needs-work-spark-india", "accessibility": 2, "description": "", "icon": "", "image_url": null}, "hierarchy": [{"id": 265000, "name": "Support Article Drafts", "codename": "article-drafts-support-only", "accessibility": 2, "description": "Support Drafts", "icon": "", "image_url": null}, {"id": 290939, "name": "spark-india-drafts", "codename": "spark-india-drafts", "accessibility": 2, "description": "", "icon": "", "image_url": null}], "keywords": [{"id": 3115103, "name": "csv"}, {"id": 3115104, "name": "databricks"}], "url": "https://kb.databricks.com/needs-work-spark-india/how-to-mitigate-the-out-of-order-columns-while-using-databricks-csv-reader"}, {"id": 1413083, "name": "Untitled Article", "views": null, "accessibility": 0, "description": null, "codename": "1413083-untitled-article", "created_at": "2022-06-17T20:50:24.248Z", "updated_at": "2022-06-17T20:50:24.265Z", "next_expiration_on": null, "published": false, "answer": {"body": "", "body_txt": "", "format": "html", "updated_at": "2022-06-17T20:50:24.262Z"}, "author": {"id": 790832, "email": "mahesh.h@databricks.com", "name": "mahesh.h ", "first_name": "mahesh.h", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T14:08:29.594Z", "updated_at": "2022-12-06T13:28:21.188Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264407, "name": "Python with Apache Spark (Internal)", "codename": "python-internal", "accessibility": 0, "description": "These articles can help you to use Python with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/python-internal/1413083-untitled-article"}, {"id": 1413082, "name": "How-to Template", "views": null, "accessibility": 0, "description": "", "codename": "1413082-how-to-template", "created_at": "2022-06-17T20:49:37.789Z", "updated_at": "2022-06-17T20:49:37.937Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p id=\"isPasted\">Category: &lt;list PRIMARY category article should appear in&gt;</p><p>Secondary category: &lt;list secondary category, if applicable&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;</p><p>Author: &lt;Databricks email of author&gt;</p><p>Owning Team: &lt;Region + Platform/Spark&gt;</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p><p>Review status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt;</p></div><h1>How-to Introduction</h1><p>&lt;Enter problem text&gt;</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p>&lt;Enter cause text&gt;</p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCategory: &lt;list PRIMARY category article should appear in&gt;\nSecondary category: &lt;list secondary category, if applicable&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;\nAuthor: &lt;Databricks email of author&gt;\nOwning Team: &lt;Region + Platform/Spark&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;\nReview status: &lt;In-Progress, Ready for Tech Review, Tech Review Approved, Ready for Final Review&gt; How-to Introduction &lt;Enter problem text&gt; Note\nInsert your text here Instructions &lt;Enter cause text&gt;", "format": "html", "updated_at": "2022-06-17T20:49:37.933Z"}, "author": {"id": 790832, "email": "mahesh.h@databricks.com", "name": "mahesh.h ", "first_name": "mahesh.h", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T14:08:29.594Z", "updated_at": "2022-12-06T13:28:21.188Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264407, "name": "Python with Apache Spark (Internal)", "codename": "python-internal", "accessibility": 0, "description": "These articles can help you to use Python with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/python-internal/1413082-how-to-template"}, {"id": 1413081, "name": "Untitled Article", "views": null, "accessibility": 0, "description": null, "codename": "1413081-untitled-article", "created_at": "2022-06-17T20:49:22.981Z", "updated_at": "2022-06-17T20:49:23.014Z", "next_expiration_on": null, "published": false, "answer": {"body": "", "body_txt": "", "format": "html", "updated_at": "2022-06-17T20:49:23.010Z"}, "author": {"id": 790832, "email": "mahesh.h@databricks.com", "name": "mahesh.h ", "first_name": "mahesh.h", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T14:08:29.594Z", "updated_at": "2022-12-06T13:28:21.188Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264407, "name": "Python with Apache Spark (Internal)", "codename": "python-internal", "accessibility": 0, "description": "These articles can help you to use Python with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/python-internal/1413081-untitled-article"}, {"id": 1413080, "name": "Untitled Article", "views": null, "accessibility": 0, "description": null, "codename": "1413080-untitled-article", "created_at": "2022-06-17T20:49:10.785Z", "updated_at": "2022-06-17T20:49:10.831Z", "next_expiration_on": null, "published": false, "answer": {"body": "", "body_txt": "", "format": "html", "updated_at": "2022-06-17T20:49:10.815Z"}, "author": {"id": 790832, "email": "mahesh.h@databricks.com", "name": "mahesh.h ", "first_name": "mahesh.h", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T14:08:29.594Z", "updated_at": "2022-12-06T13:28:21.188Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264407, "name": "Python with Apache Spark (Internal)", "codename": "python-internal", "accessibility": 0, "description": "These articles can help you to use Python with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/python-internal/1413080-untitled-article"}, {"id": 1333750, "name": "Application is not working as per maxOffsetsPerTrigger", "views": 2, "accessibility": 0, "description": "", "codename": "application-is-not-working-as-per-maxoffsetspertrigger", "created_at": "2022-04-06T06:47:41.178Z", "updated_at": "2022-04-06T06:59:31.158Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All</p><p>Cloud Version: All</p><p>Author: vikas.aravabhumi@databricks.com</p><p>Owning Team: India + Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f0000008Ov8AAE/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f0000008Ov8AAE/view</a></p><p>Last reviewed date: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>12/30/2020</span> by Pradeep Ravi</p></div><h1>Problem</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>After restarting the streaming application with maxOffsetsPerTrigger, 1st batch of the restart is loaded more number of records than the &nbsp;maxOffsetsPerTrigger value. But the 2nd batch onwards, it is loading the data as per maxOffsetsPerTrigger value.</span><br data-aura-rendered-by=\"370:771;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p><h1>Cause</h1><div data-aura-class=\"uiOutputRichText forceOutputRichText forceKnowledgeOutputRichTextForKnowledge\" data-aura-rendered-by=\"419:771;a\" dir=\"ltr\" id=\"isPasted\" style=\"box-sizing: border-box; line-height: var(--lwc-lineHeightText,1.5); overflow-wrap: break-word; hyphens: manual; overflow: auto;\"><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">In the restarted application, structured streaming API will list the latest commit file and latest offset file in the checkpoint folder, if the offsets latest file is greater than the commits latest file then API will understand that at the time of processing the previous batch, the application got terminated. So, 1st batch in the restart will be processed based on the existing latest offset file. From the second batch onwards, maxOffsetPerTrigger value will be calculated and data will be fetched as per maxOffsetPerTrigger value.</span></p></div><div data-aura-class=\"uiOutputRichText forceOutputRichText forceKnowledgeOutputRichTextForKnowledge\" data-aura-rendered-by=\"419:771;a\" dir=\"ltr\" style=\"box-sizing: border-box; line-height: var(--lwc-lineHeightText,1.5); overflow-wrap: break-word; hyphens: manual; overflow: auto;\"><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">For example, while processing the 19th batch, the stream was terminated. It means 18 batches are committed. In offsets/18 file, data was committed up to &quot;en&quot;:{&quot;0&quot;:189999}. In the next run, offsets will be fetched from &quot;en&quot;:{&quot;0&quot;:190000} onwards. In the offset/19 file, end offset is showing as &quot;en&quot;:{&quot;0&quot;:199999}. So, the difference between previously committed offset(18) and the latest offset(19) is 10000.</span></p></div><div data-aura-class=\"uiOutputRichText forceOutputRichText forceKnowledgeOutputRichTextForKnowledge\" data-aura-rendered-by=\"419:771;a\" dir=\"ltr\" style=\"box-sizing: border-box; line-height: var(--lwc-lineHeightText,1.5); overflow-wrap: break-word; hyphens: manual; overflow: auto;\"><p style=\"line-height: 2;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">As part of batch 19th, it is fetched 10000 records because, in the restarted application, the spark will compare the latest commit and offsets folder in checkpoint path, in this case, offset(19) is more than commits(18). So, the spark will understand that while processing the offsets(19), the previous stream was terminated, so, it will fetch the data based on offsets(19) file without further calculation of maxOffsetsPerTrigger. From the next batch(20) onwards, maxOffsetsPerTrigger will be considered. So, we can see 1000 records from batch 20 onwards.</span></p></div><div data-aura-class=\"forcePageBlockSection forcePageBlockSectionView\" data-aura-rendered-by=\"439:771;a\" id=\"isPasted\" style=\"box-sizing: border-box; margin-top: var(--lwc-spacingXSmall,0.5rem); margin-bottom: var(--lwc-spacingXSmall,0.5rem);\"><div aria-hidden=\"false\" data-aura-rendered-by=\"455:771;a\" style=\"box-sizing: border-box; overflow: visible; visibility: visible; opacity: 1; height: auto; transition: none 0s ease 0s; padding-top: var(--lwc-spacingXSmall,0.5rem);\"><div data-aura-rendered-by=\"456:771;a\" style=\"box-sizing: border-box;\"><div data-aura-class=\"forcePageBlockSectionRow\" data-aura-rendered-by=\"435:771;a\" style=\"box-sizing: border-box; display: flex; margin-right: 0px; margin-left: 0px;\"><div data-aura-class=\"forcePageBlockItem forcePageBlockItemView\" data-aura-rendered-by=\"422:771;a\" style=\"box-sizing: border-box; display: flex; flex: 1 1 0%; min-width: 0px; padding-right: var(--lwc-spacingSmall,0.75rem); padding-left: var(--lwc-spacingSmall,0.75rem); margin-top: ; margin-right: 0px; margin-bottom: ; margin-left: 0px;\"><div data-aura-rendered-by=\"423:771;a\" style=\"box-sizing: border-box; position: relative; min-width: 0px; flex-basis: 100%; border-bottom: var(--lwc-borderWidthThin,1px) solid var(--lwc-colorBorder,rgb(229, 229, 229)); margin-bottom: 0px; flex-grow: 1; display: block; padding: 0 var(--lwc-spacingXxSmall,0.25rem); width: 1112.67px;\"><div data-aura-rendered-by=\"427:771;a\" style=\"box-sizing: border-box; clear: left; position: relative; display: flex; padding-top: var(--lwc-spacingXxxSmall,0.125rem); padding-bottom: var(--lwc-spacingXxxSmall,0.125rem); border-bottom: 0px; padding-left: 0px; width: 1104.67px; flex-basis: 100%;\"><span data-aura-rendered-by=\"428:771;a\" style=\"box-sizing: border-box; overflow-wrap: break-word; word-break: break-word; display: inline-block; font-size: 14px; font-weight: var(--lwc-inputStaticFontWeight,400); color: var(--lwc-inputStaticColor,rgb(24, 24, 24)); width: 1104.67px; flex-grow: 1; padding-top: 0px; padding-bottom: 0px; min-height: calc(var(--lwc-varFontSize7,1.25rem) + 1px);\"><div data-aura-class=\"uiOutputRichText forceOutputRichText forceKnowledgeOutputRichTextForKnowledge\" data-aura-rendered-by=\"419:771;a\" dir=\"ltr\" style=\"box-sizing: border-box; line-height: var(--lwc-lineHeightText,1.5); overflow-wrap: break-word; hyphens: manual; overflow: auto;\"><br></div></span></div></div></div></div></div></div></div><div data-aura-class=\"forcePageBlockSection forcePageBlockSectionView\" data-aura-rendered-by=\"489:771;a\" style=\"box-sizing: border-box; margin-top: var(--lwc-spacingXSmall,0.5rem); margin-bottom: var(--lwc-spacingXSmall,0.5rem);\"><div aria-hidden=\"false\" data-aura-rendered-by=\"505:771;a\" style='box-sizing: border-box; overflow: visible; visibility: visible; opacity: 1; height: auto; transition: none 0s ease 0s; padding-top: var(--lwc-spacingXSmall,0.5rem); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 13px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><div data-aura-rendered-by=\"506:771;a\" style=\"box-sizing: border-box;\"><div data-aura-class=\"forcePageBlockSectionRow\" data-aura-rendered-by=\"485:771;a\" style=\"box-sizing: border-box; display: flex; margin-right: 0px; margin-left: 0px;\"><div data-aura-class=\"forcePageBlockItem forcePageBlockItemView\" data-aura-rendered-by=\"472:771;a\" style=\"box-sizing: border-box; display: flex; flex: 1 1 0%; min-width: 0px; padding-right: var(--lwc-spacingSmall,0.75rem); padding-left: var(--lwc-spacingSmall,0.75rem); margin-right: 0px; margin-left: 240px;\"><span style=\"font-family: Arial,Helvetica,sans-serif;\"><br></span></div></div></div></div></div><h1><span style=\"font-family: Arial,Helvetica,sans-serif;\">Solution</span></h1><p data-aura-rendered-by=\"470:771;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif, FontAwesome; line-height: 1.6; color: rgb(51, 51, 51); font-size: 16px; background-color: rgb(255, 255, 255);'><span style=\"font-family: Arial,Helvetica,sans-serif;\">Before restarting the application, check the latest offsets and commits folder, if offsets latest file value is greater than commits file value then delete the latest offsets value file or else do nothing.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"><strong style=\"box-sizing: border-box; font-weight: bold;\">Scenario:1</strong><br style=\"box-sizing: border-box;\">If offsets and commits latest value is same then do nothing.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">For example, commits and offsets both are equal to 34th batch. So, if we change the maxOffsetsPerTrigger from 1000 to 500. The app should fetch 500 records from 1st batch(35th batch as per checkpoint) onwards.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"><strong style=\"box-sizing: border-box; font-weight: bold;\">Scenario:2</strong><br style=\"box-sizing: border-box;\">If offsets latest file is greater than the commits latest value then delete the latest offset file in the checkpoint path.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">For example, commits latest value is 62 whereas offsets latest value is 63. After deleting the latest offset(63) value file. In the&nbsp;restart stream, if we change the maxOffsetsPerTrigger from 500 to 100. The restarted stream&nbsp;should fetch 100 records from 1st batch(means 63rd batch as per checkpoint) onwards.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">In the below notebook, I reproduce the issue and provided the steps to resolve the issue in the notebook with examples.</span></p><p><span style=\"font-family: Arial,Helvetica,sans-serif;\"><a data-aura-rendered-by=\"470:771;a\" href=\"https://cust-success.cloud.databricks.com/#notebook/1508197/command/1508198\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://cust-success.cloud.databricks.com/#notebook/1508197/command/1508198</a></span></p>", "body_txt": "Restricted Content DBR Version: All\nCloud Version: All\nAuthor: vikas.aravabhumi@databricks.com\nOwning Team: India + Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f0000008Ov8AAE/view Last reviewed date: 12/30/2020 by Pradeep Ravi Problem After restarting the streaming application with maxOffsetsPerTrigger, 1st batch of the restart is loaded more number of records than the \u00a0maxOffsetsPerTrigger value. But the 2nd batch onwards, it is loading the data as per maxOffsetsPerTrigger value. \u00a0 Cause In the restarted application, structured streaming API will list the latest commit file and latest offset file in the checkpoint folder, if the offsets latest file is greater than the commits latest file then API will understand that at the time of processing the previous batch, the application got terminated. So, 1st batch in the restart will be processed based on the existing latest offset file. From the second batch onwards, maxOffsetPerTrigger value will be calculated and data will be fetched as per maxOffsetPerTrigger value. For example, while processing the 19th batch, the stream was terminated. It means 18 batches are committed. In offsets/18 file, data was committed up to \"en\":{\"0\":189999}. In the next run, offsets will be fetched from \"en\":{\"0\":190000} onwards. In the offset/19 file, end offset is showing as \"en\":{\"0\":199999}. So, the difference between previously committed offset(18) and the latest offset(19) is 10000. As part of batch 19th, it is fetched 10000 records because, in the restarted application, the spark will compare the latest commit and offsets folder in checkpoint path, in this case, offset(19) is more than commits(18). So, the spark will understand that while processing the offsets(19), the previous stream was terminated, so, it will fetch the data based on offsets(19) file without further calculation of maxOffsetsPerTrigger. From the next batch(20) onwards, maxOffsetsPerTrigger will be considered. So, we can see 1000 records from batch 20 onwards. Solution Before restarting the application, check the latest offsets and commits folder, if offsets latest file value is greater than commits file value then delete the latest offsets value file or else do nothing. Scenario:1 If offsets and commits latest value is same then do nothing. For example, commits and offsets both are equal to 34th batch. So, if we change the maxOffsetsPerTrigger from 1000 to 500. The app should fetch 500 records from 1st batch(35th batch as per checkpoint) onwards. Scenario:2 If offsets latest file is greater than the commits latest value then delete the latest offset file in the checkpoint path. For example, commits latest value is 62 whereas offsets latest value is 63. After deleting the latest offset(63) value file. In the\u00a0restart stream, if we change the maxOffsetsPerTrigger from 500 to 100. The restarted stream\u00a0should fetch 100 records from 1st batch(means 63rd batch as per checkpoint) onwards. In the below notebook, I reproduce the issue and provided the steps to resolve the issue in the notebook with examples. https://cust-success.cloud.databricks.com/#notebook/1508197/command/1508198", "format": "html", "updated_at": "2022-04-06T06:58:42.369Z"}, "author": {"id": 798433, "email": "pallavi.gowdar@databricks.com", "name": "pallavi.gowdar ", "first_name": "pallavi.gowdar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-04T12:27:36.050Z", "updated_at": "2023-02-06T13:31:44.617Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264405, "name": "Streaming (Internal)", "codename": "streaming-internal", "accessibility": 0, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/streaming-internal/application-is-not-working-as-per-maxoffsetspertrigger"}, {"id": 1333735, "name": "Arrow is not supported when using file-based collect", "views": null, "accessibility": 0, "description": "", "codename": "arrow-is-not-supported-when-using-file-based-collect", "created_at": "2022-04-06T06:22:47.683Z", "updated_at": "2022-04-06T07:55:10.740Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All</p><p>Cloud Version: AWS</p><p>Author: dan.zafar@databricks.com</p><p>Owning Team: US Spark team</p><p>Ticket URL: <span data-sheets-hyperlink=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000006NAnv/view\" data-sheets-userformat='{\"2\":269053,\"3\":{\"1\":0},\"5\":{\"1\":[{\"1\":2,\"2\":0,\"5\":{\"1\":2,\"2\":0}},{\"1\":0,\"2\":0,\"3\":3},{\"1\":1,\"2\":0,\"4\":1}]},\"6\":{\"1\":[{\"1\":2,\"2\":0,\"5\":{\"1\":2,\"2\":0}},{\"1\":0,\"2\":0,\"3\":3},{\"1\":1,\"2\":0,\"4\":1}]},\"7\":{\"1\":[{\"1\":2,\"2\":0,\"5\":{\"1\":2,\"2\":0}},{\"1\":0,\"2\":0,\"3\":3},{\"1\":1,\"2\":0,\"4\":1}]},\"8\":{\"1\":[{\"1\":2,\"2\":0,\"5\":{\"1\":2,\"2\":0}},{\"1\":0,\"2\":0,\"3\":3},{\"1\":1,\"2\":0,\"4\":1}]},\"9\":0,\"10\":2,\"12\":0,\"14\":{\"1\":2,\"2\":1136076},\"15\":\"Arial\",\"21\":1}' data-sheets-value='{\"1\":2,\"2\":\"https://databricks.lightning.force.com/lightning/r/Case/5003f000006NAnv/view\"}' id=\"isPasted\" style=\"font-size:10pt;font-family:Arial;font-style:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;color:#1155cc;\"><a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000006NAnv/view\" target=\"_blank\">https://databricks.lightning.force.com/lightning/r/Case/5003f000006NAnv/view</a></span></p><p>Last reviewed date: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>11/9/2020</span> by Dan Zafar</p></div><h1>Problem</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Customer started getting an error in the last few days. &#39;Arrow is not supported when using file-based collect&#39;. Customer started getting this error all of a sudden.&nbsp;</span></p><h1>Cause</h1><p data-aura-rendered-by=\"421:784;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='box-sizing: border-box; background-color: var(--lwc-colorBackgroundInput,rgb(255, 255, 255)); color: var(--lwc-colorTextWeak,rgb(62, 62, 60)); font-family: var(--lwc-fontFamily,\"Salesforce Sans\", Arial, sans-serif); font-size: 14px;'>Customer edited the cluster (to HC cluster) which added the following configurations: </span></p><ul data-aura-rendered-by=\"421:784;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">&quot;spark.databricks.pyspark.enableProcessIsolation&quot;: &quot;true&quot;</b></li><li style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">&quot;spark.databricks.passthrough.enabled&quot;: &quot;true&quot;</b></li></ul><p data-aura-rendered-by=\"421:784;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>It just so happens that the <b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">spark.databricks.pyspark.useFileBasedCollect</b>s default value falls back to <b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">spark.databricks.pyspark.enableProcessIsolation</b>. Credential passthrough needs to use the file based collect method to enforce security, and therefore currently doesn&rsquo;t support arrow.</p><h1>Solution</h1><p data-aura-rendered-by=\"471:784;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>If the user made the cluster a High Concurrency cluster at that time, that would explain this mis-match. The customer can easily get around the issue by adding:</p><ul data-aura-rendered-by=\"471:784;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">spark.sql.execution.arrow.pyspark.fallback.enabled true</b></li></ul><p data-aura-rendered-by=\"471:784;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700); font-size: 12px;\"><span style=\"font-size: 14px;\">&nbsp;</span></b><span style=\"box-sizing: border-box; font-size: 14px;\">To the Spark configurations. Otherwise, this is expected behavior for HC clusters.</span></p>", "body_txt": "Restricted Content DBR Version: All\nCloud Version: AWS\nAuthor: dan.zafar@databricks.com\nOwning Team: US Spark team\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000006NAnv/view Last reviewed date: 11/9/2020 by Dan Zafar Problem Customer started getting an error in the last few days. 'Arrow is not supported when using file-based collect'. Customer started getting this error all of a sudden.\u00a0 Cause Customer edited the cluster (to HC cluster) which added the following configurations: \"spark.databricks.pyspark.enableProcessIsolation\": \"true\" \"spark.databricks.passthrough.enabled\": \"true\" It just so happens that the spark.databricks.pyspark.useFileBasedCollects default value falls back to spark.databricks.pyspark.enableProcessIsolation. Credential passthrough needs to use the file based collect method to enforce security, and therefore currently doesn\u2019t support arrow. Solution If the user made the cluster a High Concurrency cluster at that time, that would explain this mis-match. The customer can easily get around the issue by adding: spark.sql.execution.arrow.pyspark.fallback.enabled true \u00a0 To the Spark configurations. Otherwise, this is expected behavior for HC clusters.", "format": "html", "updated_at": "2022-04-06T07:54:52.544Z"}, "author": {"id": 821785, "email": "ashritha.laxminarayana@databricks.com", "name": "ashritha.laxminarayana ", "first_name": "ashritha.laxminarayana", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.362Z", "updated_at": "2022-08-05T10:37:27.313Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264407, "name": "Python with Apache Spark (Internal)", "codename": "python-internal", "accessibility": 0, "description": "These articles can help you to use Python with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2629327, "name": "aws"}, {"id": 2629328, "name": "pyspark"}], "url": "https://kb.databricks.com/python-internal/arrow-is-not-supported-when-using-file-based-collect"}, {"id": 1333733, "name": "Customer lacking permissions for model serving: button greyed out", "views": null, "accessibility": 0, "description": "", "codename": "customer-lacking-permissions-for-model-serving-button-greyed-out", "created_at": "2022-04-06T06:11:47.768Z", "updated_at": "2022-04-06T06:18:44.965Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All</p><p>Cloud Version: AWS</p><p>Author: &nbsp;dan.zafar@databricks.com</p><p>Owning Team: US Spark team</p><p>Ticket URL: <span data-sheets-hyperlink=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000006MJm9/view\" data-sheets-userformat='{\"2\":269053,\"3\":{\"1\":0},\"5\":{\"1\":[{\"1\":2,\"2\":0,\"5\":{\"1\":2,\"2\":0}},{\"1\":0,\"2\":0,\"3\":3},{\"1\":1,\"2\":0,\"4\":1}]},\"6\":{\"1\":[{\"1\":2,\"2\":0,\"5\":{\"1\":2,\"2\":0}},{\"1\":0,\"2\":0,\"3\":3},{\"1\":1,\"2\":0,\"4\":1}]},\"7\":{\"1\":[{\"1\":2,\"2\":0,\"5\":{\"1\":2,\"2\":0}},{\"1\":0,\"2\":0,\"3\":3},{\"1\":1,\"2\":0,\"4\":1}]},\"8\":{\"1\":[{\"1\":2,\"2\":0,\"5\":{\"1\":2,\"2\":0}},{\"1\":0,\"2\":0,\"3\":3},{\"1\":1,\"2\":0,\"4\":1}]},\"9\":0,\"10\":2,\"12\":0,\"14\":{\"1\":2,\"2\":1136076},\"15\":\"Arial\",\"21\":1}' data-sheets-value='{\"1\":2,\"2\":\"https://databricks.lightning.force.com/lightning/r/Case/5003f000006MJm9/view\"}' id=\"isPasted\" style=\"font-size:10pt;font-family:Arial;font-style:normal;text-decoration:underline;-webkit-text-decoration-skip:none;text-decoration-skip-ink:none;color:#1155cc;\"><a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000006MJm9/view\" target=\"_blank\">https://databricks.lightning.force.com/lightning/r/Case/5003f000006MJm9/view</a></span></p><p>Last reviewed date: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>11/4/2020</span> by Dan Zafar</p></div><h1>Problem</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Customer needs to provide model serve permission to one of the user, whenever he hit the serving tab, he see the `enable serving` button is greyed out.</span></p><h1>Cause</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>User does not have permission to create clusters.</span></p><h1>Solution</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;'>the user must have both Model Permissions and Cluster Creation permissions to serve models.</span></p>", "body_txt": "Restricted Content DBR Version: All\nCloud Version: AWS\nAuthor: \u00a0dan.zafar@databricks.com\nOwning Team: US Spark team\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000006MJm9/view Last reviewed date: 11/4/2020 by Dan Zafar Problem Customer needs to provide model serve permission to one of the user, whenever he hit the serving tab, he see the `enable serving` button is greyed out. Cause User does not have permission to create clusters. Solution the user must have both Model Permissions and Cluster Creation permissions to serve models.", "format": "html", "updated_at": "2022-04-06T06:18:23.084Z"}, "author": {"id": 821785, "email": "ashritha.laxminarayana@databricks.com", "name": "ashritha.laxminarayana ", "first_name": "ashritha.laxminarayana", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-10T06:34:53.362Z", "updated_at": "2022-08-05T10:37:27.313Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264400, "name": "Machine learning (Internal)", "codename": "machine-learning-internal", "accessibility": 0, "description": "These articles can help you with your machine learning, deep learning, and other data science workflows in Databricks.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2629272, "name": "aws"}, {"id": 2629273, "name": "ml"}], "url": "https://kb.databricks.com/machine-learning-internal/customer-lacking-permissions-for-model-serving-button-greyed-out"}, {"id": 1333714, "name": "How to install specific JDK build version in Databricks clusters\r\n", "views": null, "accessibility": 0, "description": "", "codename": "how-to-install-specific-jdk-build-version-in-databricks-clusters-", "created_at": "2022-04-06T05:01:25.178Z", "updated_at": "2022-08-30T04:16:11.220Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.a6b16e8ecffa9caac96b8ab0bd46048b\"></div><h1>How-to Introduction</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Databricks runtime supports JDK 8 and there may be minor version updates for JDK across different DBR releases/upgrades. If there are any JDK features/bugs that causes issues with your workloads and requires to revert or fix JDK version to a previous version, you can follow this article instructions to freeze JDK version in Databricks cluster using init script approache.</span>Note</p><h1>Instructions</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Below is a init script to freeze Java version to Open JDK version jdk_1.8_232 in a cluster. \u00a0You would need to find package specific to required JDK build. The init script given below is specific to Ubuntu 16.04 and openjdk 1.8.232. \u00a0Based on runtime OS version, you can install it in a test VM/cluster and verify the functionality.</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>\u00a0</span></p><pre data-aura-rendered-by=\"423:763;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">dbutils.fs.put(\"dbfs:/databricks/init-scripts/install_openjdk_1.8_232.sh\",\"\"\"#!/usr/bin/bash\r\n \r\nsleep 10s\r\n \r\npackage_deb_dbfs_dir=\"/dbfs/databricks/jdk_1.8_232_build\"\r\npackage_deb_local_dir=\"/tmp/jdk_1.8_232_build\"\r\n \r\ncount=`ls -1 ${package_deb_dbfs_dir}/openjdk-8*deb 2&gt;/dev/null | wc -l`\r\n \r\nif [ $count == 0 ]\r\nthen\r\n mkdir -p ${package_deb_dbfs_dir}\r\n cd ${package_deb_dbfs_dir}\r\n wget <a href=\"https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-dbg_8u232-b09-0ubuntu1~16.04.1_amd64.deb\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-dbg_8u232-b09-0ubuntu1~16.04.1_amd64.deb</a>\r\n wget <a href=\"https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jdk_8u232-b09-0ubuntu1~16.04.1_amd64.deb\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jdk_8u232-b09-0ubuntu1~16.04.1_amd64.deb</a>\r\n wget <a href=\"https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jdk-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jdk-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb</a>\r\n wget <a href=\"https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb</a>\r\n wget <a href=\"https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb</a>\r\n wget <a href=\"https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre_8u232-b09-0ubuntu1~16.04.1_amd64.deb\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre_8u232-b09-0ubuntu1~16.04.1_amd64.deb</a>\r\n wget <a href=\"https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-jamvm_8u232-b09-0ubuntu1~16.04.1_amd64.deb\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-jamvm_8u232-b09-0ubuntu1~16.04.1_amd64.deb</a>\r\nfi\r\n \r\njava_version=`java -version 2&gt;&amp;1 | grep -i version`\r\necho \"Before update, java version : $java_version\"\r\n \r\nmkdir -p ${package_deb_local_dir}\r\ncp -r ${package_deb_dbfs_dir}/openjdk-8*deb ${package_deb_local_dir}/\r\ncd ${package_deb_local_dir}\r\n \r\ndpkg -i openjdk-8-dbg_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jdk_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jdk-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jre_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb.1 openjdk-8-jre-jamvm_8u232-b09-0ubuntu1~16.04.1_amd64.deb\r\n \r\njava_version=`java -version 2&gt;&amp;1 | grep -i version`\r\necho \"After update, java version : $java_version\"\r\n \r\necho 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' &gt;&gt; ~/.bashrc\r\n\"\"\", True)</pre>", "body_txt": "How-to Introduction Databricks runtime supports JDK 8 and there may be minor version updates for JDK across different DBR releases/upgrades. If there are any JDK features/bugs that causes issues with your workloads and requires to revert or fix JDK version to a previous version, you can follow this article instructions to freeze JDK version in Databricks cluster using init script approache.Note Instructions Below is a init script to freeze Java version to Open JDK version jdk_1.8_232 in a cluster. \u00a0You would need to find package specific to required JDK build. The init script given below is specific to Ubuntu 16.04 and openjdk 1.8.232. \u00a0Based on runtime OS version, you can install it in a test VM/cluster and verify the functionality. \u00a0 dbutils.fs.put(\"dbfs:/databricks/init-scripts/install_openjdk_1.8_232.sh\",\"\"\"#!/usr/bin/bash sleep 10s package_deb_dbfs_dir=\"/dbfs/databricks/jdk_1.8_232_build\" package_deb_local_dir=\"/tmp/jdk_1.8_232_build\" count=`ls -1 ${package_deb_dbfs_dir}/openjdk-8*deb 2&gt;/dev/null | wc -l` if [ $count == 0 ] then mkdir -p ${package_deb_dbfs_dir} cd ${package_deb_dbfs_dir} wget https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-dbg_8u232-b09-0ubuntu1~16.04.1_amd64.deb wget https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jdk_8u232-b09-0ubuntu1~16.04.1_amd64.deb wget https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jdk-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb wget https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb wget https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb wget https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre_8u232-b09-0ubuntu1~16.04.1_amd64.deb wget https://launchpad.net/~openjdk-security/+archive/ubuntu/ppa/+build/17897390/+files/openjdk-8-jre-jamvm_8u232-b09-0ubuntu1~16.04.1_amd64.deb fi java_version=`java -version 2&gt;&amp;1 | grep -i version` echo \"Before update, java version : $java_version\" mkdir -p ${package_deb_local_dir} cp -r ${package_deb_dbfs_dir}/openjdk-8*deb ${package_deb_local_dir}/ cd ${package_deb_local_dir} dpkg -i openjdk-8-dbg_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jdk_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jdk-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jre_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb openjdk-8-jre-headless_8u232-b09-0ubuntu1~16.04.1_amd64.deb.1 openjdk-8-jre-jamvm_8u232-b09-0ubuntu1~16.04.1_amd64.deb java_version=`java -version 2&gt;&amp;1 | grep -i version` echo \"After update, java version : $java_version\" echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' &gt;&gt; ~/.bashrc \"\"\", True)", "format": "html", "updated_at": "2022-04-06T08:42:13.943Z"}, "author": {"id": 789476, "email": "arjun.kaimaparambilrajan@databricks.com", "name": "arjun.kaimaparambilrajan ", "first_name": "arjun.kaimaparambilrajan", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-26T05:43:28.921Z", "updated_at": "2023-04-03T15:32:29.365Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}, {"id": 7942, "name": "costCenter.711-SupportEngineering"}]}, "category": {"id": 291926, "name": "Uncategorized (Internal)", "codename": "uncategorized-internal", "accessibility": 0, "description": "Articles that need to have a category assigned.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/uncategorized-internal/how-to-install-specific-jdk-build-version-in-databricks-clusters-"}, {"id": 1333710, "name": "How-to Template", "views": 1, "accessibility": 0, "description": "", "codename": "how-to-connect-to-kerberos-enabled-kafka-cluster", "created_at": "2022-04-06T04:47:15.591Z", "updated_at": "2022-08-29T21:08:55.883Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;</p><p>Author:&nbsp;<a data-aura-class=\"emailuiFormattedEmail\" data-aura-rendered-by=\"100:778;a\" href=\"mailto:arjun.kaimaparambilrajan@databricks.com\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;'>arjun.kaimaparambilrajan@databricks.com</a></p><p>Owning Team: <span id=\"isPasted\" style=\"color: rgb(56, 76, 96); font-family: sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">US + Platform</span></p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p></div><h1>How-to Introduction</h1><p data-aura-rendered-by=\"371:772;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 12px 0px 0px; padding: 0px; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(9, 30, 66); font-family: -apple-system, system-ui, \"Segoe UI\", Roboto, \"Noto Sans\", Ubuntu, \"Droid Sans\", \"Helvetica Neue\", sans-serif; background-color: rgb(255, 255, 255);'>Kerberos is widely used authentication mechanism in systems like HDFS,Hive,Kafka etc.The article illustrate&nbsp;steps to&nbsp;connect to Kerberos enabled Kafka clusters specifically. The instructions would be more over similar for others systems as well.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">Below are the requirements to enable Spark connectivity to Kerberos enables systems.<br style=\"box-sizing: border-box;\">&nbsp;</p><ul data-aura-rendered-by=\"371:772;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style='box-sizing: border-box; margin: 12px 0px 0px; padding: 0px; color: rgb(9, 30, 66); font-family: -apple-system, system-ui, \"Segoe UI\", Roboto, \"Noto Sans\", Ubuntu, \"Droid Sans\", \"Helvetica Neue\", sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;'>Driver and Executors need to be configured with JAAS configuration file that has Kerberos authentication details&nbsp;</li><li style='box-sizing: border-box; margin: 12px 0px 0px; padding: 0px; color: rgb(9, 30, 66); font-family: -apple-system, system-ui, \"Segoe UI\", Roboto, \"Noto Sans\", Ubuntu, \"Droid Sans\", \"Helvetica Neue\", sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;'>Kerberos configuration file krb5.conf from the target environment.</li><li style='box-sizing: border-box; margin: 12px 0px 0px; padding: 0px; color: rgb(9, 30, 66); font-family: -apple-system, system-ui, \"Segoe UI\", Roboto, \"Noto Sans\", Ubuntu, \"Droid Sans\", \"Helvetica Neue\", sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-style: initial; text-decoration-color: initial;'>Kerberos keytab file to be used for authentication made available in location accessible to Driver/Executors</li></ul><p>Note</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><div class=\"alert-message\"><p class=\"hj-alert-text\">You may need to set host entries corresponding to Kafka servers/Kerberos servers in /etc/hosts. You can copy /etc/hosts entries from Kafka servers and set it using init script if you are running into issues related to it.Instructions</p></div></div><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Below are the step involved.</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>1) Copy jaas.config, krb5.conf and keytab from target environment to DBFS location /dbfs/databricks/kafka-setup/. We will use init script to copy these files from DBFS to local file system within Driver/Workers.&nbsp;</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>%sh ls -1 &nbsp;/dbfs/databricks/kafka-setup/</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>krb5.conf</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>kafka_jaas.config</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>kafka-reader.keytab</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Make to change keyTab location in &nbsp;jaas.config to point to local filesystem location of the keytab. Sample JAAS config content is shown below.</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>KafkaClient {</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp; &nbsp; com.sun.security.auth.module.Krb5LoginModule required</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp; &nbsp; useKeyTab=true</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp; &nbsp; storeKey=true</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp; &nbsp; keyTab=&quot;/etc/security/keytabs/kafka-reader.keytab&quot;</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp; &nbsp; useTicketCache=false</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp; &nbsp; serviceName=&quot;kafka&quot;</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp; &nbsp; principal=&quot;</span><a data-aura-rendered-by=\"423:772;a\" href=\"mailto:kafkareader@KAFKA.SECURE\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;'>kafkareader@KAFKA.SECURE</a><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&quot;;</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>};</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>2) Created Init script that copies these files from DBFS to corresponding local directories.</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>%scala</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>dbutils.fs.put(&quot;dbfs:/databricks/scripts/set_kafka_connectivity.sh&quot;, &quot;&quot;&quot;#!/bin/bash</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>sleep 10s</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>echo &quot;setting up kafka kerberos files&quot;</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>mkdir -p /databricks/kafka-setup</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>cat /dbfs/databricks/kafka-setup/hosts &gt;&gt; /etc/hosts</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>cp /dbfs/databricks/kafka-setup/krb5.conf /etc/krb5.conf</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>cp /dbfs/databricks/kafka-setup/kafka_jaas.config /databricks/kafka-setup/kafka_jaas.config</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>cp /dbfs/databricks/kafka-setup/cos-soudas3.keytab /etc/security/keytabs/kafka-reader.keytab</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>echo &quot;Done setting up kafka kerberos files&quot;</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&quot;&quot;&quot;, true)</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>3) Set the init script as cluster scoped init script. Reference :&nbsp;</span><a data-aura-rendered-by=\"423:772;a\" href=\"https://docs.microsoft.com/en-us/azure/databricks/clusters/init-scripts#--cluster-scoped-init-scripts\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://docs.microsoft.com/en-us/azure/databricks/clusters/init-scripts#--cluster-scoped-init-scripts</a></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>4) Set below driver/executor jvm option to use the jaas file copied using init script.</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>spark.driver.extraJavaOptions -Djava.security.auth.login.config=/databricks/kafka-setup/kafka_jaas.config</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>spark.executor.extraJavaOptions -Djava.security.auth.login.config=/databricks/kafka-setup/kafka_jaas.config</span></p><p><br data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>5) Set kafka.security.protocol to SASL option configured with Kafka Server and kafka.bootstrap.servers to broker FQDN as shown below with spark streaming code.</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p><pre data-aura-rendered-by=\"423:772;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">import org.apache.spark.sql.functions.{explode, split}\n// Setup connection to Kafka\nval kafka = spark.readStream\n\u00a0 .format(&quot;kafka&quot;)\n\u00a0 .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka-server01.kafka.secure:9092&quot;) \u00a0 // comma separated list of broker:host\n\u00a0 .option(&quot;subscribe&quot;, &quot;KafkaTest&quot;) \u00a0 \u00a0// comma separated list of topics\n\u00a0 .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) // read data from the end of the stream\n\u00a0 .option(&quot;kafka.security.protocol&quot;,&quot;SASL_PLAINTEXT&quot;)\n\u00a0 .load()\nval df = kafka.select(explode(split($&quot;value&quot;.cast(&quot;string&quot;), &quot;\\\\s+&quot;)).as(&quot;word&quot;))\n\u00a0 .groupBy($&quot;word&quot;)\n\u00a0 .count\n\u00a0\u00a0\ndisplay(df.select($&quot;word&quot;, $&quot;count&quot;))</pre>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;\nAuthor:\u00a0arjun.kaimaparambilrajan@databricks.com Owning Team: US + Platform Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt; How-to Introduction Kerberos is widely used authentication mechanism in systems like HDFS,Hive,Kafka etc.The article illustrate\u00a0steps to\u00a0connect to Kerberos enabled Kafka clusters specifically. The instructions would be more over similar for others systems as well. Below are the requirements to enable Spark connectivity to Kerberos enables systems.\u00a0 Driver and Executors need to be configured with JAAS configuration file that has Kerberos authentication details\u00a0\nKerberos configuration file krb5.conf from the target environment.\nKerberos keytab file to be used for authentication made available in location accessible to Driver/Executors Note You may need to set host entries corresponding to Kafka servers/Kerberos servers in /etc/hosts. You can copy /etc/hosts entries from Kafka servers and set it using init script if you are running into issues related to it.Instructions Below are the step involved. 1) Copy jaas.config, krb5.conf and keytab from target environment to DBFS location /dbfs/databricks/kafka-setup/. We will use init script to copy these files from DBFS to local file system within Driver/Workers.\u00a0 %sh ls -1 \u00a0/dbfs/databricks/kafka-setup/ krb5.conf kafka_jaas.config kafka-reader.keytab Make to change keyTab location in \u00a0jaas.config to point to local filesystem location of the keytab. Sample JAAS config content is shown below. KafkaClient { \u00a0 \u00a0 com.sun.security.auth.module.Krb5LoginModule required \u00a0 \u00a0 useKeyTab=true \u00a0 \u00a0 storeKey=true \u00a0 \u00a0 keyTab=\"/etc/security/keytabs/kafka-reader.keytab\" \u00a0 \u00a0 useTicketCache=false \u00a0 \u00a0 serviceName=\"kafka\" \u00a0 \u00a0 principal=\" kafkareader@KAFKA.SECURE \"; }; 2) Created Init script that copies these files from DBFS to corresponding local directories. %scala dbutils.fs.put(\"dbfs:/databricks/scripts/set_kafka_connectivity.sh\", \"\"\"#!/bin/bash \u00a0 sleep 10s echo \"setting up kafka kerberos files\" mkdir -p /databricks/kafka-setup cat /dbfs/databricks/kafka-setup/hosts &gt;&gt; /etc/hosts cp /dbfs/databricks/kafka-setup/krb5.conf /etc/krb5.conf cp /dbfs/databricks/kafka-setup/kafka_jaas.config /databricks/kafka-setup/kafka_jaas.config cp /dbfs/databricks/kafka-setup/cos-soudas3.keytab /etc/security/keytabs/kafka-reader.keytab echo \"Done setting up kafka kerberos files\" \"\"\", true) 3) Set the init script as cluster scoped init script. Reference :\u00a0 https://docs.microsoft.com/en-us/azure/databricks/clusters/init-scripts#--cluster-scoped-init-scripts 4) Set below driver/executor jvm option to use the jaas file copied using init script. spark.driver.extraJavaOptions -Djava.security.auth.login.config=/databricks/kafka-setup/kafka_jaas.config spark.executor.extraJavaOptions -Djava.security.auth.login.config=/databricks/kafka-setup/kafka_jaas.config 5) Set kafka.security.protocol to SASL option configured with Kafka Server and kafka.bootstrap.servers to broker FQDN as shown below with spark streaming code. \u00a0 import org.apache.spark.sql.functions.{explode, split}\n// Setup connection to Kafka\nval kafka = spark.readStream\n\u00a0 .format(\"kafka\")\n\u00a0 .option(\"kafka.bootstrap.servers\", \"kafka-server01.kafka.secure:9092\") \u00a0 // comma separated list of broker:host\n\u00a0 .option(\"subscribe\", \"KafkaTest\") \u00a0 \u00a0// comma separated list of topics\n\u00a0 .option(\"startingOffsets\", \"earliest\") // read data from the end of the stream\n\u00a0 .option(\"kafka.security.protocol\",\"SASL_PLAINTEXT\")\n\u00a0 .load()\nval df = kafka.select(explode(split($\"value\".cast(\"string\"), \"\\\\s+\")).as(\"word\"))\n\u00a0 .groupBy($\"word\")\n\u00a0 .count\n\u00a0\u00a0\ndisplay(df.select($\"word\", $\"count\"))", "format": "html", "updated_at": "2022-04-06T04:53:44.006Z"}, "author": {"id": 791607, "email": "pratik.bhawsar@databricks.com", "name": "pratik.bhawsar ", "first_name": "pratik.bhawsar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T06:27:23.823Z", "updated_at": "2022-08-25T06:06:42.590Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264405, "name": "Streaming (Internal)", "codename": "streaming-internal", "accessibility": 0, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/streaming-internal/how-to-connect-to-kerberos-enabled-kafka-cluster"}, {"id": 1333355, "name": "Problem Cause Solution", "views": null, "accessibility": 0, "description": "", "codename": "1333355-problem-cause-solution", "created_at": "2022-04-05T18:45:59.480Z", "updated_at": "2022-06-01T21:26:53.679Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"VTJGc2RHVmtYMS9MV0NFZmhXaVk4azBMK01hemQrQm9mY2tNQmZDak1mNnJkNk8xd3NVcXdyWjdFTkJwClVVS2I2djRMaExFSnZrbEZDS1IxY29pRUR5RFF4MUFYZHRtVDN3bEV3NXdoeW9uSWZzdURjdzI5N1BBLwpNeVdJNGYyc0dhTk5HYVdIS0xXeitrcGRIc2dOc3VlcENiRkVEVTdBZkNhRVNtRTlRRGw0WHRqdkJhSEYKdTF1WHIvM0ttS1I0aTZ5TjdrZ0drVWloSGdIcEd4TG1ZNkp2ZmJFNHk5bkFSSWppR1pNN1BqUG52RUE1CmRIc09GcFZxNVJOSEpiTCtOTzVqZFRWUGRGNnFQKzlibzJsSVVNYlRFZnN6WEFHWERCM0dRTHBvbXYrbwprYWlzY0JrZ1U0L3VGZDl4NGpxUjBDTUdGU0RpQTJDL1lJNkhXTWcwNjZFQVZnSlhqNnlwbnkrRzdEZksKVVlJQ1F5UTVkWDB5WEl2UDJZWW1HMk9vRHNzMC9ZUzVGbzI3MlVsalhHQkpTMXpjQ01WYVdHenNHU2JPCk9qNlVqbTA5cm40YnBSTGVrUXQ5WU5VY09PRG1pVEFHYW9xdkZ1bzJ2U2lRcEgwYWJsNkRrSjIrelRyeAp2ZFg5NTlDdWkvd3dlN3BvbnVpVTcrMUZONUs3cm9FR3I0VWo5UXF2K0xzR3JDd1BqaU5BOWtnVCs0RHoKSE1OdnQyQUhuNmdDcnNqZE13dVRPb2dQbDBnZXlrVnRSaVpFV0ZmMVdaU3ZoMDA1SXFZTFpJbW9ISVZICmRxaEs0UmYyOFZ2NEJpcXBhcFdoWm9PTmMwRmdXNkNOaktTYW5RRGc5bUJVcm1ycU9PUklUM3JpQnF2VgpDNzVQdkRpZ1hIQWxIWGIxTVJWY3pZZHhnQ2RnYlJsR0VsamJneXNzdVFTSW1yNkVwN04yUXlIWGYxSFYKcTUyVTFPeHFVcnkwU2Q4UHl1enFwWUVqRW42eG45cDFrU2g2aUUrV1BpbjIyYWxmOE84cmtNQWE4YVpKCmJ6MTBSenF2RUZweGVEenlQeWxrYmNSZ2xrRFVCYTZPQmEySGJ1R2dERU5GVCs0MmFZK1dRWG1UN1Y5MgpsZ0U5Mkc3aUJURi9pdVNweVVueHpxY29kend1emU2Q3B0NkdOZ2pIWXoxd2VWUG12bnZ2bTFvcFpjZEEKUkhVTEhtUXZBcjJJRC85QVRtbmVvQ0ovWHZleC9xbm5lSkJoS2dKSFZXVXRSU3NnbG5BSDlhdjVqb0puCkRmeDl0ZW83ckpjZ21JSmVzMit4OFF3d2dGd0VtR3VqNEJkSmZCWDhOUFU9Cg==.f3596361413a3dde4d44665882a1b308\"></div><h1>Problem</h1><p id=\"isPasted\">When we ran the below codes in the notebook to get the users details, we are getting a SCIM API timeout issue.</p><pre id=\"isPasted\">import requests\r\nimport json\r\nAPI_URL = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\r\nTOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\r\nurl = API_URL + '/api/2.0/preview/scim/v2/Users\r\nresponse = requests.get(url,headers={'Authorization': \"Bearer \" + TOKEN}, timeout=900)\r\nprint(\"took: {} seconds\".format(response.elapsed.total_seconds()))\r\nuser_list = list({'name': i['userName'], 'roles': i['roles'], 'id': i['id']} for i in response.json()['Resources'])\r\n# Try to get the user details with attributes=userName,userId,roles</pre><pre>\r\n# Try to get the user details with attributes=userName,userId,roles\r\n\r\nimport requests\r\nimport json\r\nAPI_URL = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\r\nTOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\r\nurl = API_URL + '/api/2.0/preview/scim/v2/Users?attributes=userName,userId,roles'\r\nresponse = requests.get(url,headers={'Authorization': \"Bearer \" + TOKEN}, timeout=900)\r\nprint(\"took: {} seconds\".format(response.elapsed.total_seconds()))\r\nuser_list = list({'name': i['userName'], 'roles': i['roles'], 'id': i['id']} for i in response.json()['Resources'])</pre><p>Unable to get the users using SCIM API \u00a0in the notebook SCIM API timeout.</p><p>Please see the screenshot.</p><p><img src=\"https://static.helpjuice.com/helpjuice_production/uploads/upload/image/10723/direct/1649184549286-1649184549286.png\" class=\"fr-fic fr-dib\"></p><h1>Cause</h1><div data-aura-class=\"forcePageBlockSection forcePageBlockSectionView\" data-aura-rendered-by=\"440:1788;a\" id=\"isPasted\" style=\"box-sizing: border-box; margin-top: var(--lwc-spacingXSmall,0.5rem); margin-bottom: var(--lwc-spacingXSmall,0.5rem);\"><div aria-hidden=\"false\" data-aura-rendered-by=\"456:1788;a\" style=\"box-sizing: border-box; overflow: visible; visibility: visible; opacity: 1; height: auto; transition: none 0s ease 0s; padding-top: var(--lwc-spacingXSmall,0.5rem);\"><div data-aura-rendered-by=\"457:1788;a\" style=\"box-sizing: border-box;\"><div data-aura-class=\"forcePageBlockSectionRow\" data-aura-rendered-by=\"436:1788;a\" style=\"box-sizing: border-box; display: flex; margin-right: 0px; margin-left: 0px;\"><div data-aura-class=\"forcePageBlockItem forcePageBlockItemView\" data-aura-rendered-by=\"423:1788;a\" style=\"box-sizing: border-box; display: flex; flex: 1 1 0%; min-width: 0px; padding-right: var(--lwc-spacingSmall,0.75rem); padding-left: var(--lwc-spacingSmall,0.75rem); margin-top: ; margin-right: 0px; margin-bottom: ; margin-left: 0px;\"><div data-aura-rendered-by=\"424:1788;a\" style=\"box-sizing: border-box; position: relative; min-width: 0px; flex-basis: 100%; border-bottom: var(--lwc-borderWidthThin,1px) solid var(--lwc-colorBorder,rgb(229, 229, 229)); margin-bottom: 0px; flex-grow: 1; display: block; padding: 0 var(--lwc-spacingXxSmall,0.25rem); width: 942px;\"><div data-aura-rendered-by=\"428:1788;a\" style=\"box-sizing: border-box; clear: left; position: relative; display: flex; padding-top: var(--lwc-spacingXxxSmall,0.125rem); padding-bottom: var(--lwc-spacingXxxSmall,0.125rem); border-bottom: 0px; padding-left: 0px; width: 934px; flex-basis: 100%;\"><span data-aura-rendered-by=\"429:1788;a\" style=\"box-sizing: border-box; overflow-wrap: break-word; word-break: break-word; display: inline-block; font-size: var(--lwc-inputStaticFontSize,0.875rem); font-weight: var(--lwc-inputStaticFontWeight,400); color: var(--lwc-inputStaticColor,rgb(24, 24, 24)); width: calc(100% - var(--lwc-squareIconSmallBoundary,1.5rem)); flex-grow: 1; padding-top: 0px; padding-bottom: 0px; min-height: calc(var(--lwc-varFontSize7,1.25rem) + 1px);\"><div data-aura-class=\"uiOutputRichText forceOutputRichText forceKnowledgeOutputRichTextForKnowledge\" data-aura-rendered-by=\"420:1788;a\" dir=\"ltr\" style=\"box-sizing: border-box; line-height: var(--lwc-lineHeightText,1.5); overflow-wrap: break-word; hyphens: manual; overflow: auto;\"><p id=\"isPasted\">This issue happens when the user tries to fetch a large set of users/groups using SCIM API and Databricks API proxy times out.</p></div></span></div></div></div></div></div></div></div><div data-aura-class=\"forcePageBlockSection forcePageBlockSectionView\" data-aura-rendered-by=\"490:1788;a\" style=\"box-sizing: border-box; margin-top: var(--lwc-spacingXSmall,0.5rem); margin-bottom: var(--lwc-spacingXSmall,0.5rem);\"><div aria-hidden=\"false\" data-aura-rendered-by=\"506:1788;a\" style='box-sizing: border-box; overflow: visible; visibility: visible; opacity: 1; height: auto; transition: none 0s ease 0s; padding-top: var(--lwc-spacingXSmall,0.5rem); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 13px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><div data-aura-rendered-by=\"507:1788;a\" style=\"box-sizing: border-box;\"><br></div></div></div><h1>Solution</h1><p id=\"isPasted\">API Proxy timeouts requests with large payloads and SCIM API supports pagination. A better solution is to use the pagination with a batch size that doesn't take more than 5 min.</p><p>We can pass startIndex and count query parameters to the API call. \u00a0count=3 will fetch 3 results on every page.</p><p><strong><em>curl --request GET 'https://{{host}}/api/2.0/preview/scim/Users'? attributes=displayName&amp;startIndex=1&amp;count=3</em></strong></p><p>Alternatively, we can also use the below code within a notebook.</p><pre>import requests\r\nimport json\r\nAPI_URL = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\r\nTOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\r\nurl = API_URL + '/api/2.0/preview/scim/v2/Users?attributes=id'\r\nresponse = requests.get(url,headers={'Authorization': \"Bearer \" + TOKEN}, timeout=900)\r\nprint(\"took: {} seconds\".format(response.elapsed.total_seconds()))\r\ntotal_user_count = len(response.json()['Resources'])\r\nbatch_cnt=250\r\nif total_user_count &lt;= batch_cnt:\r\n\u00a0 batch_cnt = total_user_count\r\nuser_list = list()\r\nfor i in range(0, total_user_count, batch_cnt):\r\n\u00a0 \u00a0start_index = i + 1\r\n\u00a0 \u00a0count = batch_cnt\r\n\u00a0 \u00a0print(\"processing :\"+ str(start_index)+ \",\" + str(count))\r\n\u00a0 \u00a0url = API_URL + '/api/2.0/preview/scim/v2/Users?startIndex={}&amp;count={}'.format(start_index,count)\r\n\u00a0 \u00a0response = requests.get(url,headers={'Authorization': \"Bearer \" + TOKEN}, timeout=900)\r\n\u00a0 \u00a0user_list = user_list + list({'name': i['userName'], 'roles': i['roles'], 'id': i['id']} for i in response.json()['Resources'])\r\nprint(len(user_list))</pre><p><br></p>", "body_txt": "Problem When we ran the below codes in the notebook to get the users details, we are getting a SCIM API timeout issue. import requests import json API_URL = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None) TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None) url = API_URL + '/api/2.0/preview/scim/v2/Users response = requests.get(url,headers={'Authorization': \"Bearer \" + TOKEN}, timeout=900) print(\"took: {} seconds\".format(response.elapsed.total_seconds())) user_list = list({'name': i['userName'], 'roles': i['roles'], 'id': i['id']} for i in response.json()['Resources']) # Try to get the user details with attributes=userName,userId,roles # Try to get the user details with attributes=userName,userId,roles import requests import json API_URL = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None) TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None) url = API_URL + '/api/2.0/preview/scim/v2/Users?attributes=userName,userId,roles' response = requests.get(url,headers={'Authorization': \"Bearer \" + TOKEN}, timeout=900) print(\"took: {} seconds\".format(response.elapsed.total_seconds())) user_list = list({'name': i['userName'], 'roles': i['roles'], 'id': i['id']} for i in response.json()['Resources']) Unable to get the users using SCIM API \u00a0in the notebook SCIM API timeout. Please see the screenshot. Cause This issue happens when the user tries to fetch a large set of users/groups using SCIM API and Databricks API proxy times out. Solution API Proxy timeouts requests with large payloads and SCIM API supports pagination. A better solution is to use the pagination with a batch size that doesn't take more than 5 min. We can pass startIndex and count query parameters to the API call. \u00a0count=3 will fetch 3 results on every page. curl --request GET 'https://{{host}}/api/2.0/preview/scim/Users'? attributes=displayName&amp;startIndex=1&amp;count=3 Alternatively, we can also use the below code within a notebook. import requests import json API_URL = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None) TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None) url = API_URL + '/api/2.0/preview/scim/v2/Users?attributes=id' response = requests.get(url,headers={'Authorization': \"Bearer \" + TOKEN}, timeout=900) print(\"took: {} seconds\".format(response.elapsed.total_seconds())) total_user_count = len(response.json()['Resources']) batch_cnt=250 if total_user_count &lt;= batch_cnt: \u00a0 batch_cnt = total_user_count user_list = list() for i in range(0, total_user_count, batch_cnt): \u00a0 \u00a0start_index = i + 1 \u00a0 \u00a0count = batch_cnt \u00a0 \u00a0print(\"processing :\"+ str(start_index)+ \",\" + str(count)) \u00a0 \u00a0url = API_URL + '/api/2.0/preview/scim/v2/Users?startIndex={}&amp;count={}'.format(start_index,count) \u00a0 \u00a0response = requests.get(url,headers={'Authorization': \"Bearer \" + TOKEN}, timeout=900) \u00a0 \u00a0user_list = user_list + list({'name': i['userName'], 'roles': i['roles'], 'id': i['id']} for i in response.json()['Resources']) print(len(user_list))", "format": "html", "updated_at": "2022-04-06T08:43:16.147Z"}, "author": {"id": 791607, "email": "pratik.bhawsar@databricks.com", "name": "pratik.bhawsar ", "first_name": "pratik.bhawsar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-28T06:27:23.823Z", "updated_at": "2022-08-25T06:06:42.590Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 291926, "name": "Uncategorized (Internal)", "codename": "uncategorized-internal", "accessibility": 0, "description": "Articles that need to have a category assigned.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/uncategorized-internal/1333355-problem-cause-solution"}, {"id": 1333059, "name": "How-to Template", "views": 3, "accessibility": 0, "description": "Change job permission which is 2 weeks old", "codename": "change-job-permissions", "created_at": "2022-04-05T11:32:39.791Z", "updated_at": "2022-06-18T01:35:21.870Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\" data-encrypted-content=\"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.da0ca34f9d875ca37195e70b5921a16d\"></div><h1>How-to Introduction</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Customer has a requirement to change job permission which is older than 2 weeks and has the last successful last run. Customer also want to match a couple of job names which is their criteria</span><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Below snippet of python code that cover both use case</span><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>The code basically :</span><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>1.Call job list api and push those job runs to an array</span><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>2. Match the job name</span><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>3. If #2 succeeds, get the last successful job runs which are 2 weeks old</span><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>4. Call the job permission api based on the job array found above\u00a0</span><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>6. Change job permission</span><br data-aura-rendered-by=\"371:7343;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>5. Print necessary details.</span>Note</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><div class=\"alert-message\"><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><pre data-aura-rendered-by=\"423:7343;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">from __future__ import division\r\nimport requests\r\nimport json\r\nimport re\r\nimport calendar, time\r\nimport dateutil.relativedelta\r\nimport datetime\r\n\r\ndef requestcall():\r\n   url='<a href=\"https://%3Cshard/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://&lt;shard</a> name&gt;.<a href=\"http://cloud.databricks.com/api/2.0/jobs/list'\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloud.databricks.com/api/2.0/jobs/list'</a>;\r\n   myResponse = requests.get(url=url, headers={'Authorization': 'Bearer &lt;token&gt;'}, verify=True)\r\n   data = myResponse.json()\r\n   job_id_arr=[]\r\n   for item in data:\r\n    for jobid in data[item]:\r\n     txt=jobid['settings']['name']\r\n     x=re.match(\"^CDP_Development\",txt)\r\n     y=re.match(\"^CDP_Production\",txt)\r\n     if x or y:\r\n      # jobidforrun=jobid['job_id']\r\n      runurl='<a href=\"https://%3Cshardname%3E.cloud.databricks.com/api/2.0/jobs/runs/list?job_id=%27+str(jobid\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://&lt;shardname&gt;.cloud.databricks.com/api/2.0/jobs/runs/list?job_id='+str(jobid</a>['job_id'])+'&amp;active_only=false&amp;offset=0&amp;limit=1&amp;run_type=JOB_RUN'\r\n      runResponse = requests.get(url=runurl, headers={'Authorization': 'Bearer &lt;token&gt;'}, verify=True)\r\n      runData = runResponse.json()\r\n      converted_time = datetime.datetime.fromtimestamp(round(runData['runs'][0]['end_time'] / 1000))\r\n      current_time_utc = datetime.datetime.utcnow()\r\n      print((current_time_utc - converted_time))\r\n      dayspend=current_time_utc - converted_time\r\n      print(\"Job old :: \", dayspend)\r\n      dayspendinsec=(current_time_utc - converted_time).total_seconds()\r\n     #  print(dayspendinsec)\r\n     # print((current_time_utc - converted_d1).total_seconds() / 60)\r\n      if dayspendinsec &gt; 1209599:\r\n        job_id_arr.append(jobid['job_id'])\r\n   print(\"Matching job id found :: \", job_id_arr)\r\n   for j in job_id_arr :\r\n    jobid={\"job_id\": j}\r\n    print(json.dumps(jobid))    for j in jobid_arr1 :\r\n\u00a0 \u00a0def requestcall():\r\n\u00a0 \u00a0 \u00a0 payload = {\"access_control_list\": [{\"user_name\": \"&lt;email id user&gt;\",\"permission_level\": \"CAN_VIEW\"}]}\r\n\u00a0 \u00a0 \u00a0 url='<a href=\"https://cust-success.cloud.databricks.com/api/2.0/permissions/jobs/'+str(j\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://cust-success.cloud.databricks.com/api/2.0/permissions/jobs/'+str(j</a>)\r\n\u00a0 \u00a0 \u00a0 myResponse = requests.patch(url=url, headers={'Authorization': 'Bearer &lt;token&gt;'}, verify=True, \u00a0data=json.dumps(payload))\r\n\u00a0 \u00a0 \u00a0 print(myResponse.status_code)\r\n\u00a0 \u00a0 \u00a0 print(myResponse.content)\r\n\u00a0 \u00a0 \u00a0 \u00a0 # For successful API call, response code will be 200 (OK)\r\n\u00a0 \u00a0 \u00a0 if myResponse.ok:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # extracting data in json format\r\n\u00a0 \u00a0 \u00a0 \u00a0data = myResponse.json()\r\n\u00a0 \u00a0 \u00a0 \u00a0return data\r\n\u00a0 requestcall()</pre>", "body_txt": "How-to Introduction Customer has a requirement to change job permission which is older than 2 weeks and has the last successful last run. Customer also want to match a couple of job names which is their criteria Below snippet of python code that cover both use case The code basically : 1.Call job list api and push those job runs to an array 2. Match the job name 3. If #2 succeeds, get the last successful job runs which are 2 weeks old 4. Call the job permission api based on the job array found above\u00a0 6. Change job permission 5. Print necessary details.Note Insert your text here Instructions from __future__ import division import requests import json import re import calendar, time import dateutil.relativedelta import datetime def requestcall(): url='https://&lt;shard name&gt;.cloud.databricks.com/api/2.0/jobs/list'; myResponse = requests.get(url=url, headers={'Authorization': 'Bearer &lt;token&gt;'}, verify=True) data = myResponse.json() job_id_arr=[] for item in data: for jobid in data[item]: txt=jobid['settings']['name'] x=re.match(\"^CDP_Development\",txt) y=re.match(\"^CDP_Production\",txt) if x or y: # jobidforrun=jobid['job_id'] runurl='https://&lt;shardname&gt;.cloud.databricks.com/api/2.0/jobs/runs/list?job_id='+str(jobid['job_id'])+'&amp;active_only=false&amp;offset=0&amp;limit=1&amp;run_type=JOB_RUN' runResponse = requests.get(url=runurl, headers={'Authorization': 'Bearer &lt;token&gt;'}, verify=True) runData = runResponse.json() converted_time = datetime.datetime.fromtimestamp(round(runData['runs'][0]['end_time'] / 1000)) current_time_utc = datetime.datetime.utcnow() print((current_time_utc - converted_time)) dayspend=current_time_utc - converted_time print(\"Job old :: \", dayspend) dayspendinsec=(current_time_utc - converted_time).total_seconds() # print(dayspendinsec) # print((current_time_utc - converted_d1).total_seconds() / 60) if dayspendinsec &gt; 1209599: job_id_arr.append(jobid['job_id']) print(\"Matching job id found :: \", job_id_arr) for j in job_id_arr : jobid={\"job_id\": j} print(json.dumps(jobid)) for j in jobid_arr1 : \u00a0 \u00a0def requestcall(): \u00a0 \u00a0 \u00a0 payload = {\"access_control_list\": [{\"user_name\": \"&lt;email id user&gt;\",\"permission_level\": \"CAN_VIEW\"}]} \u00a0 \u00a0 \u00a0 url='https://cust-success.cloud.databricks.com/api/2.0/permissions/jobs/'+str(j) \u00a0 \u00a0 \u00a0 myResponse = requests.patch(url=url, headers={'Authorization': 'Bearer &lt;token&gt;'}, verify=True, \u00a0data=json.dumps(payload)) \u00a0 \u00a0 \u00a0 print(myResponse.status_code) \u00a0 \u00a0 \u00a0 print(myResponse.content) \u00a0 \u00a0 \u00a0 \u00a0 # For successful API call, response code will be 200 (OK) \u00a0 \u00a0 \u00a0 if myResponse.ok: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # extracting data in json format \u00a0 \u00a0 \u00a0 \u00a0data = myResponse.json() \u00a0 \u00a0 \u00a0 \u00a0return data \u00a0 requestcall()", "format": "html", "updated_at": "2022-05-10T21:32:37.109Z"}, "author": {"id": 818781, "email": "atanu.sarkar@databricks.com", "name": "Atanu.Sarkar ", "first_name": "Atanu.Sarkar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-03-05T13:43:53.249Z", "updated_at": "2023-03-24T21:50:55.464Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264387, "name": "Databricks administration (Internal)", "codename": "administration-internal", "accessibility": 0, "description": "These articles can help you administer your Databricks workspace, including user and group management, access control, and workspace storage.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 3192288, "name": "guide"}, {"id": 3192289, "name": "instructions"}], "url": "https://kb.databricks.com/administration-internal/change-job-permissions"}, {"id": 1333042, "name": "Elastic search connector fails with ClassCastException for geo field types.", "views": 7, "accessibility": 0, "description": "", "codename": "1333042-elastic-search-connector-fails-with-classcastexception-for-geo-field-types", "created_at": "2022-04-05T10:27:51.709Z", "updated_at": "2022-04-10T06:24:56.661Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p id=\"isPasted\">DBR Version: All</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: <a href=\"mailto:mohan.kumar@databricks.com\">mohan.kumar@databricks.com</a></p><p>Owning Team: India+Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000004uqy8/view\">https://databricks.lightning.force.com/lightning/r/Case/5003f000004uqy8/view</a></p><p>Last reviewed date: 8/24/2020 12:18 PM</p></div><h1 id=\"isPasted\">Problem</h1><p data-aura-rendered-by=\"371:766;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px; color: rgb(0, 0, 0);\">Elastic Spark connector job failing with below exception</span></p><p data-aura-rendered-by=\"371:766;a\"><span style=\"box-sizing: border-box; font-size: 14px; font-family: Arial, Helvetica, sans-serif; color: rgb(0, 0, 0);\">java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Double</span></p><p data-aura-rendered-by=\"371:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px; font-family: Arial, Helvetica, sans-serif; color: rgb(0, 0, 0);\">at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)</span></p><h1>Cause</h1><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px;\">The elastic documentation mentions that, elasticsearch-hadoop performs automatic schema detection for geo types and the mapping does not provide such information, elasticsearch-hadoop will sample the determined geo fields at startup and retrieve an arbitrary document that contains all the relevant fields; It will parse it and thus determine the necessary schema</span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"font-size: 14px;\">&nbsp;</span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px;\">Since Spark SQL is strongly-typed, each geo field needs to have the same format across all documents. Shy of that, the returned data will not fit the detected schema and thus lead to errors. Here is the doc link&nbsp;</span><span style=\"font-size: 14px;\"><a href=\"https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-size: 12px;\" target=\"_blank\">https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html</a></span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"font-size: 14px;\">&nbsp;</span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px;\">Verify by&nbsp;limiting the source data while writing to Delta table.</span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px;\">val df = reader.load(indexName)</span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px;\">.select($&quot;activity&quot;, $&quot;actors&quot;, $&quot;user_agent&quot;) //, $&quot;client&quot;</span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px;\">.withColumn(&quot;actor&quot;, explode($&quot;actors&quot;)).limit(2000)</span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"font-size: 14px;\">&nbsp;</span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px;\">elasticsearch-hadoop will sample the data asking elasticsearch-hadoop for one random document that is representative of the mapping, parse it and based on the values found, identify the format used and create the necessary schema. This happens automatically at start-up without any user interference. Here is the link&nbsp;</span><span style=\"font-size: 14px;\"><a href=\"https://www.elastic.co/guide/en/elasticsearch/hadoop/master/mapping.html#mapping-geo\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-size: 12px;\" target=\"_blank\">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/mapping.html#mapping-geo</a></span></p><p data-aura-rendered-by=\"421:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 14px;\">The issue caused by one of the documents value was integer in geo_point mapping from elastic search schema. The values of all the other documents are float with decimals except one doc with Integer which caused the exception in transforming the data.</span></p><h1>Solution</h1><p data-aura-rendered-by=\"471:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 12px;\">There was a difference in the schema of the file. Elastic connector generated the schema from the sampled data and any change in type change would cause this issue. Avoid the schema changing for the datatypes.&nbsp;</span></p><p data-aura-rendered-by=\"471:766;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style=\"box-sizing: border-box; font-size: 12px;\">It would help to understand How the data with Different type entered the system and avoid it from the source side.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">Provide custom schema to the elastic spark connector. </span></p>", "body_txt": "Restricted Content DBR Version: All\nCloud Version: AWS, Azure, GCP\nAuthor: mohan.kumar@databricks.com Owning Team: India+Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000004uqy8/view Last reviewed date: 8/24/2020 12:18 PM Problem Elastic Spark connector job failing with below exception java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Double at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114) Cause The elastic documentation mentions that, elasticsearch-hadoop performs automatic schema detection for geo types and the mapping does not provide such information, elasticsearch-hadoop will sample the determined geo fields at startup and retrieve an arbitrary document that contains all the relevant fields; It will parse it and thus determine the necessary schema \u00a0 Since Spark SQL is strongly-typed, each geo field needs to have the same format across all documents. Shy of that, the returned data will not fit the detected schema and thus lead to errors. Here is the doc link\u00a0 https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html \u00a0 Verify by\u00a0limiting the source data while writing to Delta table. val df = reader.load(indexName) .select($\"activity\", $\"actors\", $\"user_agent\") //, $\"client\" .withColumn(\"actor\", explode($\"actors\")).limit(2000) \u00a0 elasticsearch-hadoop will sample the data asking elasticsearch-hadoop for one random document that is representative of the mapping, parse it and based on the values found, identify the format used and create the necessary schema. This happens automatically at start-up without any user interference. Here is the link\u00a0 https://www.elastic.co/guide/en/elasticsearch/hadoop/master/mapping.html#mapping-geo The issue caused by one of the documents value was integer in geo_point mapping from elastic search schema. The values of all the other documents are float with decimals except one doc with Integer which caused the exception in transforming the data. Solution There was a difference in the schema of the file. Elastic connector generated the schema from the sampled data and any change in type change would cause this issue. Avoid the schema changing for the datatypes.\u00a0 It would help to understand How the data with Different type entered the system and avoid it from the source side. Provide custom schema to the elastic spark connector.", "format": "html", "updated_at": "2022-04-10T06:24:44.323Z"}, "author": {"id": 802559, "email": "mariaselvam.nishanth@databricks.com", "name": "mariaselvam.nishanth ", "first_name": "mariaselvam.nishanth", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-10T05:40:18.398Z", "updated_at": "2023-01-03T09:58:07.305Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264409, "name": "Scala with Apache Spark (Internal)", "codename": "scala-internal", "accessibility": 0, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/scala-internal/1333042-elastic-search-connector-fails-with-classcastexception-for-geo-field-types"}, {"id": 1332992, "name": "How to Publish Streaming Data from Databricks to Web Application", "views": null, "accessibility": 0, "description": "", "codename": "how-to-publish-streaming-data-from-databricks-to-web-application", "created_at": "2022-04-05T08:58:33.554Z", "updated_at": "2022-04-05T09:27:38.324Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: &nbsp;AWS, Azure, GCP</p><p>Author: Noopur</p><p>Owning Team: IST_Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000004wG7nAAE/view?0.source=alohaHeader\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5003f000004wG7nAAE/view?0.source=alohaHeader</a></p><p>Last reviewed date:&nbsp;<span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>8/28/2020</span></p></div><h1>How-to Introduction</h1><p><br id=\"isPasted\"><span style=\"color: rgb(22, 50, 92); font-family: Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(249, 249, 250); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">The requirement is to publish streaming data from Databricks to web application directly.</span></p><h1>Instructions</h1><p><span id=\"isPasted\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(22, 50, 92);\">Publishing streaming data from Databricks to webapp through web socket can be done using a microservice with a web API to gather messages from foreachBatch. But hand-rolling that reliably would be challenging.&nbsp;</span></span></span></span><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(22, 50, 92);\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\">As spark is a distributed execution engine, so there&#39;s no single point that sees all the data to push it through a web socket and hence writing to kafka is recommended approach.</span></span></span></span></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: \u00a0AWS, Azure, GCP\nAuthor: Noopur\nOwning Team: IST_Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000004wG7nAAE/view?0.source=alohaHeader Last reviewed date:\u00a08/28/2020 How-to Introduction The requirement is to publish streaming data from Databricks to web application directly. Instructions Publishing streaming data from Databricks to webapp through web socket can be done using a microservice with a web API to gather messages from foreachBatch. But hand-rolling that reliably would be challenging.\u00a0 As spark is a distributed execution engine, so there's no single point that sees all the data to push it through a web socket and hence writing to kafka is recommended approach.", "format": "html", "updated_at": "2022-04-05T09:17:17.800Z"}, "author": {"id": 488151, "email": "pradeepkumar.palaniswamy@databricks.com", "name": "pradeepkumar.palaniswamy ", "first_name": "pradeepkumar.palaniswamy", "last_name": "", "role_id": "draft_writer", "created_at": "2021-10-07T02:59:41.669Z", "updated_at": "2023-03-03T13:00:00.792Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264405, "name": "Streaming (Internal)", "codename": "streaming-internal", "accessibility": 0, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/streaming-internal/how-to-publish-streaming-data-from-databricks-to-web-application"}, {"id": 1332989, "name": "How-to Template", "views": null, "accessibility": 0, "description": "How to Publish Streaming Data from Databricks to Web Application", "codename": "how-to-publish-streaming-data-from-databricks-to-web-application", "created_at": "2022-04-05T08:50:57.104Z", "updated_at": "2022-06-07T00:49:57.621Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: <span class=\"atwho-query\">Noopur</span></p><p>Owning Team: IST_Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000004wG7nAAE/view?0.source=alohaHeader\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5003f000004wG7nAAE/view?0.source=alohaHeader</a></p><p>Last reviewed date:&nbsp;<span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>8/28/2020</span></p></div><h1>How-to Introduction</h1><p id=\"isPasted\"><br></p><p>The requirement is to publish streaming data from Databricks to web application directly.</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p><span data-aura-rendered-by=\"423:770;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(22, 50, 92);\">Publishing streaming data from Databricks to webapp through web socket can be done using a microservice with a web API to gather messages from foreachBatch. But hand-rolling that reliably would be challenging.&nbsp;</span></span></span></span><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box; color: rgb(22, 50, 92);\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\">As spark is a distributed execution engine, so there&#39;s no single point that sees all the data to push it through a web socket and hence writing to kafka is recommended approach.</span></span></span></span></span><br data-aura-rendered-by=\"423:770;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;</span></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: AWS, Azure, GCP\nAuthor: Noopur Owning Team: IST_Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000004wG7nAAE/view?0.source=alohaHeader Last reviewed date:\u00a08/28/2020 How-to Introduction The requirement is to publish streaming data from Databricks to web application directly. Note\nInsert your text here Instructions Publishing streaming data from Databricks to webapp through web socket can be done using a microservice with a web API to gather messages from foreachBatch. But hand-rolling that reliably would be challenging.\u00a0 As spark is a distributed execution engine, so there's no single point that sees all the data to push it through a web socket and hence writing to kafka is recommended approach. \u00a0", "format": "html", "updated_at": "2022-04-05T08:53:08.687Z"}, "author": {"id": 488151, "email": "pradeepkumar.palaniswamy@databricks.com", "name": "pradeepkumar.palaniswamy ", "first_name": "pradeepkumar.palaniswamy", "last_name": "", "role_id": "draft_writer", "created_at": "2021-10-07T02:59:41.669Z", "updated_at": "2023-03-03T13:00:00.792Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 291926, "name": "Uncategorized (Internal)", "codename": "uncategorized-internal", "accessibility": 0, "description": "Articles that need to have a category assigned.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/uncategorized-internal/how-to-publish-streaming-data-from-databricks-to-web-application"}, {"id": 1332948, "name": "Use from_avro function with Schema Registry in a jar", "views": 2, "accessibility": 0, "description": "", "codename": "1332948-use-from_avro-function-with-schema-registry-in-a-jar", "created_at": "2022-04-05T07:18:37.845Z", "updated_at": "2022-04-10T06:19:55.248Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">DBR Version: All</span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">Cloud Version: &nbsp;AWS, Azure, GCP</span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">Author:<a href=\"mailto:noopur.nigam@databricks.com\">noopur.nigam@databricks.com</a></span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">Owning Team: India+Spark</span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000001AdWKAA0/view?0.source=alohaHeader\">https://databricks.lightning.force.com/lightning/r/Case/5003f000001AdWKAA0/view?0.source=alohaHeader</a></span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 16px;\">Last reviewed date: 9/28/2020 1:34 AM</span></p></div><h1>Problem</h1><p><span id=\"isPasted\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif; font-size: 14px;\"><span style=\"box-sizing: border-box;\">The requirement is to use from_avro with schema registry URL with the same functionality as provided with Databricks notebook inside the jar.</span></span></span></span><span style=\"font-size: 14px;\"><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></span><span style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif; font-size: 14px;\"><span style=\"box-sizing: border-box;\">Using from_avro function with schema registry inside a jar with the open-source provided method would throw a compilation error if we try to pass more than two parameters as it takes only two arguments(the column and the schema ).</span></span></span></span><span style=\"font-size: 14px;\"><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></span><span style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif; font-size: 14px;\"><span style=\"box-sizing: border-box;\">However, Databricks notebook takes 3 - 4 parameters which makes from_avro easy to use by providing schema registry URL.</span></span></span></span></p><h1>Cause</h1><p><br></p><h1>Solution</h1><p><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box;\">To use the same functionality as provided by Databricks with a notebook inside the jar, we have stub jar approach.</span></span></span></span></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box;\">In this approach, we have created a dummy method with the required parameters inside a dummy class. To use this approach we need to perform the below steps in the consecutive order:</span></span></span></span></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box;\">1) Compile the code provided in file dbr-stub.zip and build the stub jar.</span></span></span></span></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box;\">2) Add this stub jar as a dependency in the actual project&#39;s pom.xml with scope as provided by using below dependency: db databricks-runtime-stub 1.0 provided</span></span></span></span></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box;\">3) Import the class containing from_avro method in the stub jar to access from_avro method with required parameters in the actual code where we want to use the (from_avro) function.</span></span></span></span></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box;\">Please find attached the code to create stub jar i.e. dbr-stub.zip, the code where we have shown how can we access the from_avro function inside the stub jar i.e. databricksproject.zip.</span></span></span></span></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"></span><span data-aura-rendered-by=\"471:1844;a\" id=\"isPasted\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 16px;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box;\">Please note that we can change the number of parameters passed to from_avro according to the notebook methods and requirement.&nbsp;</span></span></span></span></span><br data-aura-rendered-by=\"471:1844;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"471:1844;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span data-aura-rendered-by=\"471:1844;a\" style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; font-size: 12pt;'><span style=\"box-sizing: border-box; font-family: Calibri, sans-serif;\"><span style=\"box-sizing: border-box; background: rgb(249, 249, 250);\"><span style=\"box-sizing: border-box; font-family: Arial, sans-serif;\"><span style=\"box-sizing: border-box;\">I would like to give credit to <a href=\"mailto:subramanian.iyer@databricks.com\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\"><span style=\"box-sizing: border-box;\"><span style=\"box-sizing: border-box; text-decoration: none;\"><span style=\"box-sizing: border-box;\">subramanian.iyer@databricks.com</span></span></span></a> for creating this stub-jar approach to help our customers who are looking to implement the same functionality as provided by Databricks notebook from_avro function.</span></span></span></span></span></p>", "body_txt": "Restricted Content DBR Version: All Cloud Version: \u00a0AWS, Azure, GCP Author:noopur.nigam@databricks.com Owning Team: India+Spark Ticket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000001AdWKAA0/view?0.source=alohaHeader Last reviewed date: 9/28/2020 1:34 AM Problem The requirement is to use from_avro with schema registry URL with the same functionality as provided with Databricks notebook inside the jar. Using from_avro function with schema registry inside a jar with the open-source provided method would throw a compilation error if we try to pass more than two parameters as it takes only two arguments(the column and the schema ). However, Databricks notebook takes 3 - 4 parameters which makes from_avro easy to use by providing schema registry URL. Cause Solution To use the same functionality as provided by Databricks with a notebook inside the jar, we have stub jar approach. In this approach, we have created a dummy method with the required parameters inside a dummy class. To use this approach we need to perform the below steps in the consecutive order: 1) Compile the code provided in file dbr-stub.zip and build the stub jar. 2) Add this stub jar as a dependency in the actual project's pom.xml with scope as provided by using below dependency: db databricks-runtime-stub 1.0 provided 3) Import the class containing from_avro method in the stub jar to access from_avro method with required parameters in the actual code where we want to use the (from_avro) function. Please find attached the code to create stub jar i.e. dbr-stub.zip, the code where we have shown how can we access the from_avro function inside the stub jar i.e. databricksproject.zip. Please note that we can change the number of parameters passed to from_avro according to the notebook methods and requirement.\u00a0 I would like to give credit to subramanian.iyer@databricks.com for creating this stub-jar approach to help our customers who are looking to implement the same functionality as provided by Databricks notebook from_avro function.", "format": "html", "updated_at": "2022-04-10T06:19:40.078Z"}, "author": {"id": 802559, "email": "mariaselvam.nishanth@databricks.com", "name": "mariaselvam.nishanth ", "first_name": "mariaselvam.nishanth", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-10T05:40:18.398Z", "updated_at": "2023-01-03T09:58:07.305Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264410, "name": "SQL with Apache Spark (Internal)", "codename": "sql-internal", "accessibility": 0, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/sql-internal/1332948-use-from_avro-function-with-schema-registry-in-a-jar"}, {"id": 1332947, "name": "Fetch the data from specific point in Delta table using streaming", "views": 6, "accessibility": 0, "description": "", "codename": "1332947-fetch-the-data-from-specific-point-in-delta-table-using-streaming", "created_at": "2022-04-05T07:14:48.813Z", "updated_at": "2022-04-05T08:17:42.562Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: vikas.aravabhumi@databricks.com</p><p>Owning Team: India + Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f0000008Ov3AAE/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f0000008Ov3AAE/view</a></p><p>Last reviewed date: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>3/1/2021</span> by <a data-aura-class=\"forceOutputLookup\" data-aura-rendered-by=\"263:770;a\" data-navigable=\"true\" data-ownerid=\"252:770;a\" data-recordid=\"0054N000003p6pfQAA\" data-refid=\"recordId\" data-special-link=\"true\" href=\"https://databricks.lightning.force.com/lightning/r/0054N000003p6pfQAA/view\" rel=\"noreferrer\" style=\"box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; max-width: 100%; overflow: auto; text-overflow: initial; white-space: normal; display: inline; font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;\" target=\"_blank\" title=\"Pradeep Ravi\" id=\"isPasted\"></a>Pradeep Ravi</p></div><h1>How-to Introduction</h1><p data-aura-rendered-by=\"371:770;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>Currently, there are 3&nbsp;ways to fetch the data from a specific point in the Delta table using streaming.</p><p data-aura-rendered-by=\"371:770;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>1. If you have access to an incremented partition value, you can use partition filters to start processing from a specific point.<span style=\"box-sizing: border-box;\">&nbsp;<br style=\"box-sizing: border-box;\">2. Based on the Delta version.<br style=\"box-sizing: border-box;\">3.&nbsp;</span><span style=\"box-sizing: border-box;\">Based on the Timestamp version.</span></p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p data-aura-rendered-by=\"423:770;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>If&nbsp;the stream is fetching the data from the Delta table then if the stream is terminated due to some reason, in the restart, data will fetch from starting onwards. The stream&nbsp;will process from starting onwards, this will consume&nbsp;unnecessary&nbsp;resources and reprocess the same data which will create duplicates in the downstream sink. To avoid reprocessing all the data once again, below are the options.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">Option:1</b><br style=\"box-sizing: border-box;\">If the new data is loading to the Delta table based on a certain column value then we can mention the&nbsp;incremental partition in the filter condition. So, it will only fetch the data based on the filter condition.<br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">In the below table, eventid is a partitioned column. We can fetch the&nbsp;data from eventid &gt; 2.</p><pre data-aura-rendered-by=\"423:770;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">%sql\ndrop table lookup_test;\nCREATE TABLE lookup_test (eventId int,num int) USING delta PARTITIONED BY (eventId)\n\n%sql\ninsert into lookup_test values(1,2),(2,3),(3,4),(5,6)\n\ndisplay(spark.readStream.format(&quot;delta&quot;).load(&quot;/user/hive/warehouse/lookup_test/&quot;).where(&quot;<b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">eventId &gt; 2</b>&quot;))</pre><p data-aura-rendered-by=\"423:770;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">Option:2</b></p><p data-aura-rendered-by=\"423:770;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>This feature is available on Databricks Runtime 7.3 LTS and above.</p><p data-aura-rendered-by=\"423:770;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><code style=\"box-sizing: border-box; font-family: monospace, monospace; font-size: 1em;\">startingVersion</code>: The Delta Lake version to start from. All table changes starting from this version (inclusive) will be read by the streaming source. You can obtain the commit versions from the <code style=\"box-sizing: border-box; font-family: monospace, monospace; font-size: 1em;\">version</code> column of the <a href=\"https://docs.databricks.com/delta/delta-utility.html#delta-history\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">DESCRIBE HISTORY</a> command output.</p><p data-aura-rendered-by=\"423:770;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>In Databricks Runtime 7.4 and above, to return only the latest changes, specify <code style=\"box-sizing: border-box; font-family: monospace, monospace; font-size: 1em;\">latest</code>.</p><pre data-aura-rendered-by=\"423:770;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">spark.readStream.format(&quot;delta&quot;)\n  .option(&quot;startingVersion&quot;, &quot;5&quot;)\n  .load(&quot;/mnt/delta/user_events&quot;)</pre><p data-aura-rendered-by=\"423:770;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style=\"box-sizing: border-box;\"><b style=\"box-sizing: border-box; font-weight: var(--lwc-fontWeightBold,700);\">Option:3</b></p><p data-aura-rendered-by=\"423:770;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><code style=\"box-sizing: border-box; font-family: monospace, monospace; font-size: 1em;\">startingTimestamp</code>: The timestamp to start from. All table changes committed at or after the timestamp (inclusive) will be read by the streaming source. One of:</p><ul data-aura-rendered-by=\"423:770;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box;\">A timestamp string. For example, <code style=\"box-sizing: border-box; font-family: monospace, monospace; font-size: 1em;\">&quot;2019-01-01T00:00:00.000Z&quot;</code>.</li><li style=\"box-sizing: border-box;\">A date string. For example, <code style=\"box-sizing: border-box; font-family: monospace, monospace; font-size: 1em;\">&quot;2019-01-01&quot;</code>.</li></ul><p data-aura-rendered-by=\"423:770;a\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>You cannot set option 2 and option 3&nbsp;at the same time; you can use only one of them. They take effect only when starting a new streaming query. If a streaming query has started and the progress has been recorded in its checkpoint, these options are ignored.</p><pre data-aura-rendered-by=\"423:770;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">spark.readStream.format(&quot;delta&quot;)\n  .option(&quot;startingTimestamp&quot;, &quot;2018-10-18&quot;)\n  .load(&quot;/mnt/delta/user_events&quot;)</pre>", "body_txt": "Restricted Content DBR Version: All\nCloud Version: AWS, Azure, GCP\nAuthor: vikas.aravabhumi@databricks.com\nOwning Team: India + Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Knowledge__kav/ka03f0000008Ov3AAE/view Last reviewed date: 3/1/2021 by Pradeep Ravi How-to Introduction Currently, there are 3\u00a0ways to fetch the data from a specific point in the Delta table using streaming. 1. If you have access to an incremented partition value, you can use partition filters to start processing from a specific point.\u00a02. Based on the Delta version.3.\u00a0 Based on the Timestamp version. Note\nInsert your text here Instructions If\u00a0the stream is fetching the data from the Delta table then if the stream is terminated due to some reason, in the restart, data will fetch from starting onwards. The stream\u00a0will process from starting onwards, this will consume\u00a0unnecessary\u00a0resources and reprocess the same data which will create duplicates in the downstream sink. To avoid reprocessing all the data once again, below are the options. Option:1 If the new data is loading to the Delta table based on a certain column value then we can mention the\u00a0incremental partition in the filter condition. So, it will only fetch the data based on the filter condition. In the below table, eventid is a partitioned column. We can fetch the\u00a0data from eventid &gt; 2. %sql\ndrop table lookup_test;\nCREATE TABLE lookup_test (eventId int,num int) USING delta PARTITIONED BY (eventId) %sql\ninsert into lookup_test values(1,2),(2,3),(3,4),(5,6) display(spark.readStream.format(\"delta\").load(\"/user/hive/warehouse/lookup_test/\").where(\"eventId &gt; 2\")) Option:2 This feature is available on Databricks Runtime 7.3 LTS and above. startingVersion: The Delta Lake version to start from. All table changes starting from this version (inclusive) will be read by the streaming source. You can obtain the commit versions from the version column of the DESCRIBE HISTORY command output. In Databricks Runtime 7.4 and above, to return only the latest changes, specify latest. spark.readStream.format(\"delta\") .option(\"startingVersion\", \"5\") .load(\"/mnt/delta/user_events\") Option:3 startingTimestamp: The timestamp to start from. All table changes committed at or after the timestamp (inclusive) will be read by the streaming source. One of: A timestamp string. For example, \"2019-01-01T00:00:00.000Z\".\nA date string. For example, \"2019-01-01\". You cannot set option 2 and option 3\u00a0at the same time; you can use only one of them. They take effect only when starting a new streaming query. If a streaming query has started and the progress has been recorded in its checkpoint, these options are ignored. spark.readStream.format(\"delta\") .option(\"startingTimestamp\", \"2018-10-18\") .load(\"/mnt/delta/user_events\")", "format": "html", "updated_at": "2022-04-05T07:40:05.200Z"}, "author": {"id": 798433, "email": "pallavi.gowdar@databricks.com", "name": "pallavi.gowdar ", "first_name": "pallavi.gowdar", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-04T12:27:36.050Z", "updated_at": "2023-02-06T13:31:44.617Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264405, "name": "Streaming (Internal)", "codename": "streaming-internal", "accessibility": 0, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2627240, "name": "aws"}, {"id": 2627242, "name": "azure"}, {"id": 2627241, "name": "gcp"}], "url": "https://kb.databricks.com/streaming-internal/1332947-fetch-the-data-from-specific-point-in-delta-table-using-streaming"}, {"id": 1332946, "name": "Spark 3.0 performance degradation on json load", "views": 6, "accessibility": 0, "description": "", "codename": "1332946-spark-3-0-performance-degradation-on-json-load", "created_at": "2022-04-05T07:11:14.836Z", "updated_at": "2022-04-10T06:14:15.055Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: All</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author:<a href=\"mailto:mohan.kumar@databricks.com\">mohan.kumar@databricks.com</a></p><p>Owning Team: India+/Spark</p><p>Ticket URL:<a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000001BYI4AAO/view\">https://databricks.lightning.force.com/lightning/r/Case/5003f000001BYI4AAO/view</a></p><p>Last reviewed date:7/22/2020 2:44 AM</p></div><h1>Problem</h1><div data-aura-class=\"forcePageBlockSection forcePageBlockSectionView\" data-aura-rendered-by=\"390:770;a\" id=\"isPasted\" style=\"box-sizing: border-box; margin-top: var(--lwc-spacingXSmall,0.5rem); margin-bottom: var(--lwc-spacingXSmall,0.5rem);\"><div aria-hidden=\"false\" data-aura-rendered-by=\"406:770;a\" style=\"box-sizing: border-box; overflow: visible; visibility: visible; opacity: 1; height: auto; transition: none 0s ease 0s; padding-top: var(--lwc-spacingXSmall,0.5rem);\"><div data-aura-rendered-by=\"407:770;a\" style=\"box-sizing: border-box;\"><div data-aura-class=\"forcePageBlockSectionRow\" data-aura-rendered-by=\"386:770;a\" style=\"box-sizing: border-box; display: flex; margin-right: 0px; margin-left: 0px;\"><div data-aura-class=\"forcePageBlockItem forcePageBlockItemView\" data-aura-rendered-by=\"373:770;a\" style=\"box-sizing: border-box; display: flex; flex: 1 1 0%; min-width: 0px; padding-right: var(--lwc-spacingSmall,0.75rem); padding-left: var(--lwc-spacingSmall,0.75rem); margin-top: ; margin-right: 0px; margin-bottom: ; margin-left: 0px;\"><div data-aura-rendered-by=\"374:770;a\" style=\"box-sizing: border-box; position: relative; min-width: 0px; flex-basis: 100%; border-bottom: var(--lwc-borderWidthThin,1px) solid var(--lwc-colorBorder,rgb(229, 229, 229)); margin-bottom: 0px; flex-grow: 1; display: block; padding: 0 var(--lwc-spacingXxSmall,0.25rem); width: 1112.67px;\"><div data-aura-rendered-by=\"378:770;a\" style=\"box-sizing: border-box; clear: left; position: relative; display: flex; padding-top: var(--lwc-spacingXxxSmall,0.125rem); padding-bottom: var(--lwc-spacingXxxSmall,0.125rem); border-bottom: 0px; padding-left: 0px; width: 1104.67px; flex-basis: 100%;\"><span data-aura-rendered-by=\"379:770;a\" style=\"box-sizing: border-box; overflow-wrap: break-word; word-break: break-word; display: inline-block; font-size: var(--lwc-inputStaticFontSize,0.875rem); font-weight: var(--lwc-inputStaticFontWeight,400); color: var(--lwc-inputStaticColor,rgb(24, 24, 24)); width: calc(100% - var(--lwc-squareIconSmallBoundary,1.5rem)); flex-grow: 1; padding-top: 0px; padding-bottom: 0px; min-height: calc(var(--lwc-varFontSize7,1.25rem) + 1px);\"><div data-aura-class=\"uiOutputRichText forceOutputRichText forceKnowledgeOutputRichTextForKnowledge\" data-aura-rendered-by=\"370:770;a\" dir=\"ltr\" style=\"box-sizing: border-box; line-height: var(--lwc-lineHeightText,1.5); overflow-wrap: break-word; hyphens: manual; overflow: auto;\"><span style=\"font-family: Arial,Helvetica,sans-serif;\">The time is taken for a job in Spark 2.x and 3.x are drastically showing the difference.<br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\"><br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\">Spark 2.x taking 7s to compete the tasks<br></span><br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na109.content.force.com/servlet/rtaImage?eid=ka03f0000008OuZ&feoid=00N3f000000m2eW&refid=0EM3f000000VLzX\" alt=\"\" data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na109.content.force.com/servlet/rtaImage?eid=ka03f0000008OuZ&feoid=00N3f000000m2eW&refid=0EM3f000000VM0e\" alt=\"\" data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\"><br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\">Spark 3.0 takes 44s for complete the tasks<br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\"><br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na109.content.force.com/servlet/rtaImage?eid=ka03f0000008OuZ&feoid=00N3f000000m2eW&refid=0EM3f000000VM18\" alt=\"\" data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer;\" class=\"fr-fic fr-dii\"><br data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box;\"><img data-fr-image-pasted=\"true\" src=\"https://databricks--c.na109.content.force.com/servlet/rtaImage?eid=ka03f0000008OuZ&feoid=00N3f000000m2eW&refid=0EM3f000000VM0y\" alt=\"\" data-aura-rendered-by=\"371:770;a\" style=\"box-sizing: border-box; border: 0px; max-width: 100%; height: auto; cursor: pointer;\" class=\"fr-fic fr-dii\"></div></span><button data-aura-rendered-by=\"3150:0\" style=\"box-sizing: border-box; color: var(--lwc-colorTextButtonInverse,rgb(243, 243, 243)); font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: inherit; line-height: var(--lwc-lineHeightReset,1); font-family: inherit; margin: 0px; overflow: visible; text-transform: none; appearance: none; cursor: pointer; position: static; display: block; align-items: center; padding-top: var(--slds-c-button-spacing-block-start, var(--sds-c-button-spacing-block-start, 0)); padding-right: var(--slds-c-button-spacing-inline-end, var(--sds-c-button-spacing-inline-end, 0)); padding-bottom: var(--slds-c-button-spacing-block-end, var(--sds-c-button-spacing-block-end, 0)); padding-left: var(--slds-c-button-spacing-inline-start, var(--sds-c-button-spacing-inline-start, 0)); background-image: none; background-position: initial; background-size: initial; background-repeat: initial; background-attachment: initial; background-origin: initial; background-color: var(--slds-c-button-color-background, var(--sds-c-button-color-background, transparent)); background-clip: border-box; border-color: var(--slds-c-button-color-border, var(--sds-c-button-color-border, transparent)); border-style: solid; border-width: var(--slds-c-button-sizing-border, var(--sds-c-button-sizing-border, var(--lwc-borderWidthThin,1px))); border-radius: var(--slds-c-button-radius-border, var(--sds-c-button-radius-border, var(--lwc-buttonBorderRadius,0.25rem))); box-shadow: var(--slds-c-button-shadow, var(--sds-c-button-shadow)); text-decoration: none; white-space: normal; user-select: none; vertical-align: middle; justify-content: center; flex-shrink: 0; width: auto; height: auto; float: right; align-self: baseline;\" title=\"Edit Problem\" type=\"button\"><span data-aura-rendered-by=\"3153:0\" style=\"box-sizing: border-box; position: absolute !important; margin: -1px !important; border: 0px !important; padding: 0px !important; width: 1px !important; height: 1px !important; overflow: hidden !important; clip: rect(0px, 0px, 0px, 0px) !important; text-transform: none !important; white-space: nowrap !important;\">Edit Problem</span></button></div></div></div></div></div></div></div><div data-aura-class=\"forcePageBlockSection forcePageBlockSectionView\" data-aura-rendered-by=\"440:770;a\" style=\"box-sizing: border-box; margin-top: var(--lwc-spacingXSmall,0.5rem); margin-bottom: var(--lwc-spacingXSmall,0.5rem);\"><div aria-hidden=\"false\" data-aura-rendered-by=\"456:770;a\" style='box-sizing: border-box; overflow: visible; visibility: visible; opacity: 1; height: auto; transition: none 0s ease 0s; padding-top: var(--lwc-spacingXSmall,0.5rem); color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 13px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><div data-aura-rendered-by=\"457:770;a\" style=\"box-sizing: border-box;\"><br></div></div></div><h1>Cause</h1><ul id=\"isPasted\"><li style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">The issue that spark trying and failing to parse millions of things as timestamps, as part of schema inference, and almost all of that is wasted because they&#39;re not timestamps.</li><li style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">Even in Spark 2.4, the type inference takes much more time than the actual execution. It&#39;s always recommended to specify the schema manually to skip schema inference in production workloads. The configuration inferTimestamp by default set to true and this enables the inference and tasks runs with more time.</li></ul><h1>Solution</h1><p data-aura-rendered-by=\"471:770;a\" id=\"isPasted\" style='box-sizing: border-box; margin: 10px 0px 0px; padding: 0px; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(23, 43, 77); font-family: -apple-system, system-ui, \"Segoe UI\", Roboto, Oxygen, Ubuntu, \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif; background-color: rgb(244, 245, 247);'><span style=\"color: rgb(0, 0, 0); font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">This is mainly due to lots of time is used to infer `Timestamp` json datatype because of &nbsp;exception happens.</span></p><p data-aura-rendered-by=\"471:770;a\" style='box-sizing: border-box; margin: 10px 0px 0px; padding: 0px; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(23, 43, 77); font-family: -apple-system, system-ui, \"Segoe UI\", Roboto, Oxygen, Ubuntu, \"Fira Sans\", \"Droid Sans\", \"Helvetica Neue\", sans-serif; background-color: rgb(244, 245, 247);'><span style=\"color: rgb(0, 0, 0); font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">Please try use `.option(&quot;inferTimestamp&quot;, false)` as `spark.read.option(&quot;inferTimestamp&quot;, false).json(&quot;...&quot;)` or specify json schema. This issue is discussed in&nbsp;</span></p><p><span style=\"color: rgb(0, 0, 0);\"><a data-aura-rendered-by=\"471:770;a\" href=\"https://issues.apache.org/jira/browse/SPARK-32130\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">https://issues.apache.org/jira/browse/SPARK-32130</span></a></span></p>", "body_txt": "Restricted Content DBR Version: All\nCloud Version: AWS, Azure, GCP\nAuthor:mohan.kumar@databricks.com Owning Team: India+/Spark\nTicket URL:https://databricks.lightning.force.com/lightning/r/Case/5003f000001BYI4AAO/view Last reviewed date:7/22/2020 2:44 AM Problem The time is taken for a job in Spark 2.x and 3.x are drastically showing the difference. Spark 2.x taking 7s to compete the tasks Spark 3.0 takes 44s for complete the tasks Edit Problem Cause The issue that spark trying and failing to parse millions of things as timestamps, as part of schema inference, and almost all of that is wasted because they're not timestamps.\nEven in Spark 2.4, the type inference takes much more time than the actual execution. It's always recommended to specify the schema manually to skip schema inference in production workloads. The configuration inferTimestamp by default set to true and this enables the inference and tasks runs with more time. Solution This is mainly due to lots of time is used to infer `Timestamp` json datatype because of \u00a0exception happens. Please try use `.option(\"inferTimestamp\", false)` as `spark.read.option(\"inferTimestamp\", false).json(\"...\")` or specify json schema. This issue is discussed in\u00a0 https://issues.apache.org/jira/browse/SPARK-32130", "format": "html", "updated_at": "2022-04-10T06:14:08.246Z"}, "author": {"id": 802559, "email": "mariaselvam.nishanth@databricks.com", "name": "mariaselvam.nishanth ", "first_name": "mariaselvam.nishanth", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-10T05:40:18.398Z", "updated_at": "2023-01-03T09:58:07.305Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264397, "name": "Jobs (Internal)", "codename": "jobs-internal", "accessibility": 0, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/jobs-internal/1332946-spark-3-0-performance-degradation-on-json-load"}, {"id": 1332942, "name": "Spark throws _pickle.PicklingError: SPARK-5063 exception from python notebook", "views": 16, "accessibility": 0, "description": "", "codename": "1332942-spark-throws-_pickle-picklingerror-spark-5063-exception-from-python-notebook", "created_at": "2022-04-05T06:59:35.084Z", "updated_at": "2022-04-10T05:56:24.535Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p><span style=\"font-family: Arial,Helvetica,sans-serif;\">DBR Version: All</span></p><p><span style=\"font-family: Arial,Helvetica,sans-serif;\">Cloud Version: AWS, Azure, GCP</span></p><p><span style=\"font-family: Arial,Helvetica,sans-serif;\">Author:<a href=\"mailto:mohan.kumar@databricks.com\">mohan.kumar@databricks.com</a></span></p><p><span style=\"font-family: Arial,Helvetica,sans-serif;\">Owning Team: India+Spark</span></p><p><span style=\"font-family: Arial,Helvetica,sans-serif;\">Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f0000019mNPAAY/view\">https://databricks.lightning.force.com/lightning/r/Case/5003f0000019mNPAAY/view</a></span></p><p><span style=\"font-family: Arial,Helvetica,sans-serif;\">Last reviewed date: 3/10/2022 3:51 PM</span></p></div><h1>Problem</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;'><strong>Python Spark job fails to serialize object when trying to run python notebook.</strong></span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.</span></p><pre data-aura-rendered-by=\"371:770;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://serializers.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">serializers.py</a>&quot;, line 705, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://cloudpickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloudpickle.py</a>&quot;, line 863, in dumps\n    cp.dump(obj)\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://cloudpickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloudpickle.py</a>&quot;, line 260, in dump\n    return Pickler.dump(self, obj)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 437, in dump\n    self.save(obj)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 771, in save_tuple\n    save(element)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://cloudpickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloudpickle.py</a>&quot;, line 400, in save_function\n    self.save_function_tuple(obj)\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://cloudpickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloudpickle.py</a>&quot;, line 549, in save_function_tuple\n    save(state)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 882, in _batch_setitems\n    save(v)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 816, in save_list\n    self._batch_appends(obj)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 843, in _batch_appends\n    save(tmp[0])\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 633, in save_reduce\n    save(cls)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://cloudpickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloudpickle.py</a>&quot;, line 632, in save_global\n    return self.save_dynamic_class(obj)\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://cloudpickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloudpickle.py</a>&quot;, line 491, in save_dynamic_class\n    save(clsdict)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 882, in _batch_setitems\n    save(v)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://cloudpickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloudpickle.py</a>&quot;, line 400, in save_function\n    self.save_function_tuple(obj)\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://cloudpickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">cloudpickle.py</a>&quot;, line 549, in save_function_tuple\n    save(state)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 882, in _batch_setitems\n    save(v)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 887, in _batch_setitems\n    save(v)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 662, in save_reduce\n    save(state)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 882, in _batch_setitems\n    save(v)\n  File &quot;/usr/lib/python3.7/<a href=\"http://pickle.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">pickle.py</a>&quot;, line 524, in save\n    rv = reduce(self.proto)\n  File &quot;/databricks/spark/python/pyspark/<a href=\"http://context.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">context.py</a>&quot;, line 344, in __getnewargs__\n    &quot;It appears that you are attempting to reference SparkContext from a broadcast &quot;\nException: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063</pre><h1>Cause</h1><p id=\"isPasted\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;'>Spark transformation is called from a python class and spark tries to serialize the entire object to execute in worker nodes. As part of the serialization, the spark tries to serialize the spark context object and fails with the exception.</span></p><p><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;'>Example:</span></p><pre data-aura-rendered-by=\"421:770;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">from pyspark.sql.functions import udf, col, lit\n\nclass MyClass:\n  \n  def __init__(self):\n    &quot;&quot;&quot;This is the constructor by default of the class.&quot;&quot;&quot;\n    self.df = spark.read.load(&#39;dbfs:/mnt/bucket/data/&#39;).filter(&#39;type == &quot;H&quot;&#39;)\n    \n  def is_valid_email_format(self, email):\n    &quot;&quot;&quot;\n    // email validation logic \n    except ValueError as ex:\n        return False\n    except IndexError as ex:\n        return False\n    return True\n\n  def is_caps(self, email):\n      &quot;&quot;&quot;\n      // some check and return true or false\n\n  def udf_is_valid_email(self, email):\n      return udf(lambda e: self.is_valid_email_format(e) and not self.is_caps(e))(email)\n    \n  def contact_to_email(self, contact_sdf):\n    &quot;&quot;&quot;\n   This function transform contact to email sdf\n   @param contact_sdf: contact data as spark dataframe\n   @return: email sdf\n   &quot;&quot;&quot;\n\n    # add the is_valid column\n    sdf = contact_sdf.withColumn(&#39;is_valid&#39;, self.udf_is_valid_email(col(&#39;email&#39;)))\n    return sdf</pre><p id=\"isPasted\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">When the spark is invoked from the python class this issue occurs.</span></p><pre data-aura-rendered-by=\"421:770;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">my_class = MyClass()\nsdf = spark.read.load(&#39;dbfs:/mnt/somepath/data/dl/email&#39;)\nsdf_res = my_class.contact_to_email(sdf)</pre><ol id=\"isPasted\"><li style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">Check for spark context is used in the class or inside the method.</li><li style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">Verify the function custom function used in transformation and it triggered the exception</li></ol><h1>Solution</h1><p id=\"isPasted\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">It is not recommended to define a python object containing spark context and the same class functions are used in transformation. The issue comes from trying to serialize SparkContext and write functions in terms of transformations.</span></p><ul><li style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">Convert the custom conditions to the Spark inbuilt transformations functions and applying to the dataframe.</li><li style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">Specific transformations should be translated into the custom UDF functions and applied to the dataframe.</li></ul>", "body_txt": "Restricted Content DBR Version: All Cloud Version: AWS, Azure, GCP Author:mohan.kumar@databricks.com Owning Team: India+Spark Ticket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f0000019mNPAAY/view Last reviewed date: 3/10/2022 3:51 PM Problem Python Spark job fails to serialize object when trying to run python notebook. PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063. Traceback (most recent call last): File \"/databricks/spark/python/pyspark/serializers.py\", line 705, in dumps return cloudpickle.dumps(obj, 2) File \"/databricks/spark/python/pyspark/cloudpickle.py\", line 863, in dumps cp.dump(obj) File \"/databricks/spark/python/pyspark/cloudpickle.py\", line 260, in dump return Pickler.dump(self, obj) File \"/usr/lib/python3.7/pickle.py\", line 437, in dump self.save(obj) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/usr/lib/python3.7/pickle.py\", line 771, in save_tuple save(element) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/databricks/spark/python/pyspark/cloudpickle.py\", line 400, in save_function self.save_function_tuple(obj) File \"/databricks/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple save(state) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/usr/lib/python3.7/pickle.py\", line 856, in save_dict self._batch_setitems(obj.items()) File \"/usr/lib/python3.7/pickle.py\", line 882, in _batch_setitems save(v) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/usr/lib/python3.7/pickle.py\", line 816, in save_list self._batch_appends(obj) File \"/usr/lib/python3.7/pickle.py\", line 843, in _batch_appends save(tmp[0]) File \"/usr/lib/python3.7/pickle.py\", line 549, in save self.save_reduce(obj=obj, *rv) File \"/usr/lib/python3.7/pickle.py\", line 633, in save_reduce save(cls) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/databricks/spark/python/pyspark/cloudpickle.py\", line 632, in save_global return self.save_dynamic_class(obj) File \"/databricks/spark/python/pyspark/cloudpickle.py\", line 491, in save_dynamic_class save(clsdict) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/usr/lib/python3.7/pickle.py\", line 856, in save_dict self._batch_setitems(obj.items()) File \"/usr/lib/python3.7/pickle.py\", line 882, in _batch_setitems save(v) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/databricks/spark/python/pyspark/cloudpickle.py\", line 400, in save_function self.save_function_tuple(obj) File \"/databricks/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple save(state) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/usr/lib/python3.7/pickle.py\", line 856, in save_dict self._batch_setitems(obj.items()) File \"/usr/lib/python3.7/pickle.py\", line 882, in _batch_setitems save(v) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/usr/lib/python3.7/pickle.py\", line 856, in save_dict self._batch_setitems(obj.items()) File \"/usr/lib/python3.7/pickle.py\", line 887, in _batch_setitems save(v) File \"/usr/lib/python3.7/pickle.py\", line 549, in save self.save_reduce(obj=obj, *rv) File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce save(state) File \"/usr/lib/python3.7/pickle.py\", line 504, in save f(self, obj) # Call unbound method with explicit self File \"/usr/lib/python3.7/pickle.py\", line 856, in save_dict self._batch_setitems(obj.items()) File \"/usr/lib/python3.7/pickle.py\", line 882, in _batch_setitems save(v) File \"/usr/lib/python3.7/pickle.py\", line 524, in save rv = reduce(self.proto) File \"/databricks/spark/python/pyspark/context.py\", line 344, in __getnewargs__ \"It appears that you are attempting to reference SparkContext from a broadcast \"\nException: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063 Cause Spark transformation is called from a python class and spark tries to serialize the entire object to execute in worker nodes. As part of the serialization, the spark tries to serialize the spark context object and fails with the exception. Example: from pyspark.sql.functions import udf, col, lit class MyClass: def __init__(self): \"\"\"This is the constructor by default of the class.\"\"\" self.df = spark.read.load('dbfs:/mnt/bucket/data/').filter('type == \"H\"') def is_valid_email_format(self, email): \"\"\" // email validation logic except ValueError as ex: return False except IndexError as ex: return False return True def is_caps(self, email): \"\"\" // some check and return true or false def udf_is_valid_email(self, email): return udf(lambda e: self.is_valid_email_format(e) and not self.is_caps(e))(email) def contact_to_email(self, contact_sdf): \"\"\" This function transform contact to email sdf @param contact_sdf: contact data as spark dataframe @return: email sdf \"\"\" # add the is_valid column sdf = contact_sdf.withColumn('is_valid', self.udf_is_valid_email(col('email'))) return sdf When the spark is invoked from the python class this issue occurs. my_class = MyClass()\nsdf = spark.read.load('dbfs:/mnt/somepath/data/dl/email')\nsdf_res = my_class.contact_to_email(sdf) Check for spark context is used in the class or inside the method.\nVerify the function custom function used in transformation and it triggered the exception Solution It is not recommended to define a python object containing spark context and the same class functions are used in transformation. The issue comes from trying to serialize SparkContext and write functions in terms of transformations. Convert the custom conditions to the Spark inbuilt transformations functions and applying to the dataframe.\nSpecific transformations should be translated into the custom UDF functions and applied to the dataframe.", "format": "html", "updated_at": "2022-04-10T05:55:46.723Z"}, "author": {"id": 802559, "email": "mariaselvam.nishanth@databricks.com", "name": "mariaselvam.nishanth ", "first_name": "mariaselvam.nishanth", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-10T05:40:18.398Z", "updated_at": "2023-01-03T09:58:07.305Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264407, "name": "Python with Apache Spark (Internal)", "codename": "python-internal", "accessibility": 0, "description": "These articles can help you to use Python with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 3142737, "name": "pickle error"}, {"id": 3142736, "name": "spark exception"}], "url": "https://kb.databricks.com/python-internal/1332942-spark-throws-_pickle-picklingerror-spark-5063-exception-from-python-notebook"}, {"id": 1332934, "name": "Setting Scalac compiler flags for Notebook REPL", "views": 8, "accessibility": 0, "description": "", "codename": "1332934-setting-scalac-compiler-flags-for-notebook-repl", "created_at": "2022-04-05T06:48:09.870Z", "updated_at": "2022-04-10T06:08:06.082Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p><span style=\"font-family: Arial, Helvetica, sans-serif;\">DBR Version: All</span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif;\">Cloud Version: AWS, Azure, GCP</span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif;\">Author: <a data-aura-class=\"emailuiFormattedEmail\" data-aura-rendered-by=\"100:2397;a\" href=\"mailto:mohan.kumar@databricks.com\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLinkActive,rgb(1, 68, 134)); text-decoration: underline; transition: color 0.1s linear 0s; cursor: pointer; outline: 0px; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;'>mohan.kumar@databricks.com</a></span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif;\">Owning Team: India+Spark</span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif;\">Ticket URL: <a data-aura-class=\"uiOutputURL\" data-aura-rendered-by=\"173:1170;a\" data-interactive-lib-uid=\"3\" dir=\"ltr\" href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f0000019wLYAAY/view\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; overflow-wrap: break-word; word-break: break-word; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' title=\"\">https://databricks.lightning.force.com/lightning/r/Case/5003f0000019wLYAAY/view</a></span></p><p><span style=\"font-family: Arial, Helvetica, sans-serif;\">Last reviewed date:&nbsp;</span><span id=\"isPasted\" style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">2/24/2022 1:11 PM</span></p></div><h1>Problem</h1><p><span data-aura-rendered-by=\"371:1170;a\" id=\"isPasted\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">Is there any way to customize the Scalac compiler flags that Databricks uses when compiling Scala code in a notebook?&nbsp;</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">In particular, enabling the -Ypartial-unification flag would help&nbsp;dramatically.</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">Partial Unification</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">Consider the following code (which depends on cats 2.0.0</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">&lt;</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><a data-aura-rendered-by=\"371:1170;a\" href=\"https://typelevel.org/cats/#getting-started\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(0, 109, 204)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: \"Salesforce Sans\", Arial, sans-serif;' target=\"_blank\">https://typelevel.org/cats/#getting-started</a></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">&gt;):</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\"><strong>import cats.implicits._</strong></span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><strong><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></strong></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\"><strong>val x: Either[String, Int] = Right(1)val y: Either[String, Int] =</strong></span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><strong><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></strong></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\"><strong>Right(2)val z: Either[String, Int] = Right(3)</strong></span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><strong><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></strong></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\"><strong>(x, y, z).mapN(_ + _ + _)</strong></span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">In a Scala REPL with the -Ypartial-unification flag enabled, this returns res0:&nbsp;Either[String,Int] = Right(6).</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">However, in a Databricks notebook, this returns command-178635:1: error:</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">value mapN is not a member of (Either[String,Int], Either[String,Int],</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">Either[String,Int]) (x, y, z).mapN(_ + _ + _).</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">In order to work around the type inference issue in the last line of the&nbsp;code so that it works in a Databricks notebook, you have to write&nbsp;complex code:</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\"><strong>catsSyntaxTuple3Semigroupal[({type l[a] = Either[String, a]})#l, Int,</strong></span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><strong><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: &quot;Salesforce Sans&quot;, Arial, sans-serif; background-color: rgb(255, 255, 255);\"></strong></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\"><strong>Int, Int]((x, y, z)).mapN(_ + _ + _)</strong></span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"></span><span data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: Arial, Helvetica, sans-serif; float: none; display: inline !important;\">there are other flags that change to improve by setting -language:higherKinds.</span><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"371:1170;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span data-aura-rendered-by=\"371:1170;a\" style='box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(8, 7, 7); font-family: \"Salesforce Sans\", Arial, sans-serif; float: none; display: inline !important;'>This code like the above is more complex or we need to write a library to do the data exploration and upload the jar to the cluster. But the jar upload approach is really bad for data exploration because you can&rsquo;t modify the patterns that you are searching for without publishing a new jar, attaching it to your cluster, and restarting the cluster.</span></p><h1>Cause</h1><p><span id=\"isPasted\" style=\"color: rgb(0, 0, 0); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">The scalac compiler options are embedded inside the code, and an optional flag to set externally is not supported. In the case of an on-prem setup, this would be the same behavior until the runtime code is compiled with options and deployed.</span></p><h1>Solution</h1><p><span id=\"isPasted\" style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">In Databricks runtime this is hardcoded and it cant be parameterized. This limitation currently exists in the DBR and we have created a&nbsp;</span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><a data-aura-rendered-by=\"471:1170;a\" href=\"https://databricks.aha.io/ideas/DB-I-2472\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(249, 249, 250); color: rgb(0, 109, 204); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: \"Salesforce Sans\", Arial, sans-serif; font-size: 12px; text-align: left;' target=\"_blank\">https://databricks.aha.io/ideas/DB-I-2472</a></span><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">&nbsp;for future PM tracking.</span></p><pre data-aura-rendered-by=\"471:1170;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\">val interpArguments =\n          List(&quot;-Yrepl-class-based&quot;, &quot;-Yrepl-outdir&quot;, s&quot;${classServerOutputDir}&quot;, &quot;-target:jvm-1.8&quot;)\n        settings.processArguments(interpArguments, true)</span></pre><p><span style=\"color: rgb(24, 24, 24); font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">We have a few workarounds to achieve the behavior.</span></p><ul data-aura-rendered-by=\"471:1170;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box; font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><span style=\"box-sizing: border-box; color: rgb(22, 50, 92); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">Dbconnect allows setting compiler flags&nbsp;</span></li></ul><div data-aura-rendered-by=\"471:1170;a\" style='box-sizing: border-box; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(51, 51, 51); font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; background-color: rgb(255, 255, 255);'><div style=\"box-sizing: border-box;\"><pre style='box-sizing: border-box; overflow: auto; font-family: Menlo, Monaco, Consolas, \"Courier New\", monospace; font-size: 11.2px; display: block; padding: 6px 10px; margin: 11pt 0px; line-height: 1.42857; color: rgb(51, 51, 51); word-break: break-all; overflow-wrap: break-word; background-color: rgb(248, 248, 248); border: 1px solid rgb(204, 204, 204); border-radius: 5pt;'><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px;\"><code style='box-sizing: border-box; font-family: Menlo, Monaco, Consolas, \"Courier New\", monospace; font-size: inherit; padding: 0px; color: inherit; background: transparent; border-radius: 0px; white-space: pre;'><span style=\"box-sizing: border-box; color: rgb(51, 51, 51);\">scalacOptions</span> <span style=\"box-sizing: border-box; font-weight: bold;\">++</span> <span style=\"box-sizing: border-box; color: rgb(68, 85, 136); font-weight: bold;\">Seq</span><span style=\"box-sizing: border-box; font-weight: bold;\">(</span>\n  <span style=\"box-sizing: border-box; color: rgb(221, 17, 68);\">&quot;-Yinduction-heuristics&quot;</span><span style=\"box-sizing: border-box; font-weight: bold;\">,</span>       <span style=\"box-sizing: border-box; color: rgb(153, 153, 136); font-style: italic;\">// speeds up the compilation of inductive implicit resolution\n</span>  <span style=\"box-sizing: border-box; color: rgb(221, 17, 68);\">&quot;-Ykind-polymorphism&quot;</span><span style=\"box-sizing: border-box; font-weight: bold;\">,</span>          <span style=\"box-sizing: border-box; color: rgb(153, 153, 136); font-style: italic;\">// type and method definitions with type parameters of arbitrary kinds\n</span>  <span style=\"box-sizing: border-box; color: rgb(221, 17, 68);\">&quot;-Yliteral-types&quot;</span><span style=\"box-sizing: border-box; font-weight: bold;\">,</span>              <span style=\"box-sizing: border-box; color: rgb(153, 153, 136); font-style: italic;\">// literals can appear in type position\n</span>  <span style=\"box-sizing: border-box; color: rgb(221, 17, 68);\">&quot;-Xstrict-patmat-analysis&quot;</span><span style=\"box-sizing: border-box; font-weight: bold;\">,</span>     <span style=\"box-sizing: border-box; color: rgb(153, 153, 136); font-style: italic;\">// more accurate reporting of failures of match exhaustivity\n</span>  <span style=\"box-sizing: border-box; color: rgb(221, 17, 68);\">&quot;-Xlint:strict-unsealed-patmat&quot;</span> <span style=\"box-sizing: border-box; color: rgb(153, 153, 136); font-style: italic;\">// warn on inexhaustive matches against unsealed traits\n</span><span style=\"box-sizing: border-box; font-weight: bold;\">)</span></code></span></pre></div></div><ul data-aura-rendered-by=\"471:1170;a\" style='box-sizing: border-box; margin-top: 0px; margin-right: 0px; margin-bottom: var(--lwc-spacingSmall,0.75rem); margin-left: var(--lwc-spacingLarge,1.5rem); padding: 0px; list-style: disc; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><li style=\"box-sizing: border-box; font-family: Arial, Helvetica, sans-serif; font-size: 14px; color: rgb(0, 0, 0);\"><span style=\"box-sizing: border-box;\">Compiling the code locally and running the job in databricks cluster.</span></li><li style=\"box-sizing: border-box; font-family: Arial, Helvetica, sans-serif; font-size: 14px; color: rgb(0, 0, 0);\"><span style=\"box-sizing: border-box; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">Set notebook scoped libraries (not available for scala).</span></li></ul>", "body_txt": "Restricted Content DBR Version: All Cloud Version: AWS, Azure, GCP Author: mohan.kumar@databricks.com Owning Team: India+Spark Ticket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f0000019wLYAAY/view Last reviewed date:\u00a0 2/24/2022 1:11 PM Problem Is there any way to customize the Scalac compiler flags that Databricks uses when compiling Scala code in a notebook?\u00a0 In particular, enabling the -Ypartial-unification flag would help\u00a0dramatically. Partial Unification Consider the following code (which depends on cats 2.0.0 &lt; https://typelevel.org/cats/#getting-started &gt;): import cats.implicits._ val x: Either[String, Int] = Right(1)val y: Either[String, Int] = Right(2)val z: Either[String, Int] = Right(3) (x, y, z).mapN(_ + _ + _) In a Scala REPL with the -Ypartial-unification flag enabled, this returns res0:\u00a0Either[String,Int] = Right(6). However, in a Databricks notebook, this returns command-178635:1: error: value mapN is not a member of (Either[String,Int], Either[String,Int], Either[String,Int]) (x, y, z).mapN(_ + _ + _). In order to work around the type inference issue in the last line of the\u00a0code so that it works in a Databricks notebook, you have to write\u00a0complex code: catsSyntaxTuple3Semigroupal[({type l[a] = Either[String, a]})#l, Int, Int, Int]((x, y, z)).mapN(_ + _ + _) there are other flags that change to improve by setting -language:higherKinds. This code like the above is more complex or we need to write a library to do the data exploration and upload the jar to the cluster. But the jar upload approach is really bad for data exploration because you can\u2019t modify the patterns that you are searching for without publishing a new jar, attaching it to your cluster, and restarting the cluster. Cause The scalac compiler options are embedded inside the code, and an optional flag to set externally is not supported. In the case of an on-prem setup, this would be the same behavior until the runtime code is compiled with options and deployed. Solution In Databricks runtime this is hardcoded and it cant be parameterized. This limitation currently exists in the DBR and we have created a\u00a0 https://databricks.aha.io/ideas/DB-I-2472 \u00a0for future PM tracking. val interpArguments = List(\"-Yrepl-class-based\", \"-Yrepl-outdir\", s\"${classServerOutputDir}\", \"-target:jvm-1.8\") settings.processArguments(interpArguments, true) We have a few workarounds to achieve the behavior. Dbconnect allows setting compiler flags\u00a0 scalacOptions ++ Seq ( \"-Yinduction-heuristics\" , // speeds up the compilation of inductive implicit resolution \"-Ykind-polymorphism\" , // type and method definitions with type parameters of arbitrary kinds \"-Yliteral-types\" , // literals can appear in type position \"-Xstrict-patmat-analysis\" , // more accurate reporting of failures of match exhaustivity \"-Xlint:strict-unsealed-patmat\" // warn on inexhaustive matches against unsealed traits ) Compiling the code locally and running the job in databricks cluster. Set notebook scoped libraries (not available for scala).", "format": "html", "updated_at": "2022-04-10T06:07:31.836Z"}, "author": {"id": 802559, "email": "mariaselvam.nishanth@databricks.com", "name": "mariaselvam.nishanth ", "first_name": "mariaselvam.nishanth", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-10T05:40:18.398Z", "updated_at": "2023-01-03T09:58:07.305Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264409, "name": "Scala with Apache Spark (Internal)", "codename": "scala-internal", "accessibility": 0, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/scala-internal/1332934-setting-scalac-compiler-flags-for-notebook-repl"}, {"id": 1332902, "name": "Databricks-connect query fails for spark-alchemy jar function with \"Undefined function\"", "views": 13, "accessibility": 0, "description": "", "codename": "databricks-connect-query-fails-for-spark-alchemy-jar-function-with-undefined-function", "created_at": "2022-04-05T05:58:22.184Z", "updated_at": "2023-03-03T13:15:32.109Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: <span data-sheets-userformat='{\"2\":4801,\"3\":{\"1\":0},\"9\":0,\"10\":2,\"12\":0,\"15\":\"Arial\"}' data-sheets-value='{\"1\":2,\"2\":\"DBR 7.3 LTS\"}' id=\"isPasted\" style=\"font-size:10pt;font-family:Arial;font-style:normal;\">DBR 7.3 LTS</span></p><p>Cloud Version: Multi-Cloud</p><p>Author: <a data-aura-class=\"emailuiFormattedEmail\" data-aura-rendered-by=\"100:2413;a\" href=\"mailto:mohan.kumar@databricks.com\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLinkActive,rgb(1, 68, 134)); text-decoration: underline; transition: color 0.1s linear 0s; cursor: pointer; outline: 0px; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;'>mohan.kumar@databricks.com</a></p><p>Owning Team: Spark</p><p>Ticket URL: <a data-aura-class=\"uiOutputURL\" data-aura-rendered-by=\"173:768;a\" data-interactive-lib-uid=\"3\" dir=\"ltr\" href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000004vn4zAAA/view\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLinkActive,rgb(1, 68, 134)); text-decoration: underline; transition: color 0.1s linear 0s; cursor: pointer; outline: 0px; overflow-wrap: break-word; word-break: break-word; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' title=\"\">https://databricks.lightning.force.com/lightning/r/Case/5003f000004vn4zAAA/view</a></p><p>Last reviewed date: <span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>2/28/2022 8:33 PM</span></p></div><h1>Problem</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>databricks-connect with spark-alchemy jar export functions not behaving as expected (</span><a data-aura-rendered-by=\"371:768;a\" href=\"https://github.com/swoop-inc/spark-alchemy/wiki/Spark-Native-Functions\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://github.com/swoop-inc/spark-alchemy/wiki/Spark-Native-Functions</a><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>). Running the query on Databricks via databricks-connect but the process that he is using isn&#39;t working as expected or as it does in a notebook.</span></p><pre data-aura-rendered-by=\"371:768;a\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">&gt;&gt;&gt; from pyspark.sql.functions import expr\n&gt;&gt;&gt; from pyspark.sql.functions import countDistinct\n&gt;&gt;&gt; sc._jvm.com.swoop.alchemy.spark.expressions.hll.HLLFunctionRegistration.registerFunctions(spark._jsparkSession)\n&gt;&gt;&gt; diamonds = spark.read.csv(&quot;/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&quot;, header=&quot;true&quot;, inferSchema=&quot;true&quot;)\n&gt;&gt;&gt; colors = diamonds.groupBy(&quot;color&quot;).agg(countDistinct(&quot;_c0&quot;), expr(&quot;hll_cardinality(hll_init_agg(_c0, .01)) as diamonds&quot;))\n&gt;&gt;&gt; colors.show()\nView job details at <a href=\"https://disneystreaming.cloud.databricks.com/?o=0#/setting/clusters/0813-220858-rage681/sparkUi\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://disneystreaming.cloud.databricks.com/?o=0#/setting/clusters/0813-220858-rage681/sparkUi</a>\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/Users/dvizzini/.pyenv/versions/3.7.8/lib/python3.7/site-packages/pyspark/sql/<a href=\"http://dataframe.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">dataframe.py</a>&quot;, line 441, in show\n    print(self._jdf.showString(n, 20, vertical))\n  File &quot;/Users/dvizzini/.pyenv/versions/3.7.8/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py&quot;, line 1305, in __call__\n  File &quot;/Users/dvizzini/.pyenv/versions/3.7.8/lib/python3.7/site-packages/pyspark/sql/<a href=\"http://utils.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">utils.py</a>&quot;, line 134, in deco\n    raise_from(converted)\n  File &quot;&lt;string&gt;&quot;, line 3, in raise_from\npyspark.sql.utils.AnalysisException: Undefined function: &#39;hll_cardinality&#39;. This function is neither a registered temporary function nor a permanent function registered in the database &#39;default&#39;.; line 1 pos 0</pre><h1>Cause</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>The locally registered com.swoop:spark-alchemy_2.12:1.0.0 jar UDF functions don&#39;t work as It cant serialize the spark context.&nbsp;</span></p><pre data-aura-rendered-by=\"421:768;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">the function registration happens to spark context, this needs to be done in the native Spark\nAs per the current design, the local spark acts as the driver and generated the logical plan, and the cluster acts as the execution engine. The locally registered functions don&#39;t propagate to the cluster.</pre><h1>Solution</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>As the solution this functions needs to be imported into the cluster manually before running the query in Notebook.&nbsp;</span><br data-aura-rendered-by=\"471:768;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><br data-aura-rendered-by=\"471:768;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>As the future resolution there is a PR&nbsp;</span><a data-aura-rendered-by=\"471:768;a\" href=\"https://databricks.atlassian.net/browse/SC-44928\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">https://databricks.atlassian.net/browse/SC-44928</a><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&nbsp;and targeted for dbr-8.0.0. This provides a functionality to Trigger and registers a local synced function to cluster.</span></p>", "body_txt": "Restricted Content DBR Version: DBR 7.3 LTS Cloud Version: Multi-Cloud\nAuthor: mohan.kumar@databricks.com Owning Team: Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000004vn4zAAA/view Last reviewed date: 2/28/2022 8:33 PM Problem databricks-connect with spark-alchemy jar export functions not behaving as expected ( https://github.com/swoop-inc/spark-alchemy/wiki/Spark-Native-Functions ). Running the query on Databricks via databricks-connect but the process that he is using isn't working as expected or as it does in a notebook. &gt;&gt;&gt; from pyspark.sql.functions import expr\n&gt;&gt;&gt; from pyspark.sql.functions import countDistinct\n&gt;&gt;&gt; sc._jvm.com.swoop.alchemy.spark.expressions.hll.HLLFunctionRegistration.registerFunctions(spark._jsparkSession)\n&gt;&gt;&gt; diamonds = spark.read.csv(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header=\"true\", inferSchema=\"true\")\n&gt;&gt;&gt; colors = diamonds.groupBy(\"color\").agg(countDistinct(\"_c0\"), expr(\"hll_cardinality(hll_init_agg(_c0, .01)) as diamonds\"))\n&gt;&gt;&gt; colors.show()\nView job details at https://disneystreaming.cloud.databricks.com/?o=0#/setting/clusters/0813-220858-rage681/sparkUi\nTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"/Users/dvizzini/.pyenv/versions/3.7.8/lib/python3.7/site-packages/pyspark/sql/dataframe.py\", line 441, in show print(self._jdf.showString(n, 20, vertical)) File \"/Users/dvizzini/.pyenv/versions/3.7.8/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__ File \"/Users/dvizzini/.pyenv/versions/3.7.8/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 134, in deco raise_from(converted) File \"&lt;string&gt;\", line 3, in raise_from\npyspark.sql.utils.AnalysisException: Undefined function: 'hll_cardinality'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 0 Cause The locally registered com.swoop:spark-alchemy_2.12:1.0.0 jar UDF functions don't work as It cant serialize the spark context.\u00a0 the function registration happens to spark context, this needs to be done in the native Spark\nAs per the current design, the local spark acts as the driver and generated the logical plan, and the cluster acts as the execution engine. The locally registered functions don't propagate to the cluster. Solution As the solution this functions needs to be imported into the cluster manually before running the query in Notebook.\u00a0 As the future resolution there is a PR\u00a0 https://databricks.atlassian.net/browse/SC-44928 \u00a0and targeted for dbr-8.0.0. This provides a functionality to Trigger and registers a local synced function to cluster.", "format": "html", "updated_at": "2022-04-05T06:05:54.396Z"}, "author": {"id": 802559, "email": "mariaselvam.nishanth@databricks.com", "name": "mariaselvam.nishanth ", "first_name": "mariaselvam.nishanth", "last_name": "", "role_id": "draft_writer", "created_at": "2022-02-10T05:40:18.398Z", "updated_at": "2023-01-03T09:58:07.305Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264409, "name": "Scala with Apache Spark (Internal)", "codename": "scala-internal", "accessibility": 0, "description": "These articles can help you to use Scala with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2627195, "name": "databricks-connect"}, {"id": 3087765, "name": "function error"}], "url": "https://kb.databricks.com/scala-internal/databricks-connect-query-fails-for-spark-alchemy-jar-function-with-undefined-function"}, {"id": 1332900, "name": "How to pass & read parameters in python-task using Jobs API", "views": null, "accessibility": 0, "description": "", "codename": "how-to-pass-read-parameters-in-python-task-using-jobs-api", "created_at": "2022-04-05T05:35:43.963Z", "updated_at": "2022-04-05T05:43:38.374Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author:&nbsp;<a data-aura-class=\"emailuiFormattedEmail\" data-aura-rendered-by=\"100:770;a\" href=\"mailto:ram.sankarasubramanian@databricks.com\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;'>ram.sankarasubramanian@databricks.com</a></p><p>Owning Team: India + Spark</p><p>Ticket URL: <a data-aura-class=\"uiOutputURL\" data-aura-rendered-by=\"173:771;a\" data-interactive-lib-uid=\"3\" dir=\"ltr\" href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000006LhZ2AAK/view?0.source=alohaHeader\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; overflow-wrap: break-word; word-break: break-word; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' title=\"\">https://databricks.lightning.force.com/lightning/r/Case/5003f000006LhZ2AAK/view?0.source=alohaHeader</a></p><p>Last reviewed date: 10-30-2020 by <a data-aura-class=\"forceOutputLookup\" data-aura-rendered-by=\"38:771;a\" data-navigable=\"true\" data-ownerid=\"27:771;a\" data-recordid=\"0054N000003pA86QAE\" data-refid=\"recordId\" data-special-link=\"true\" href=\"https://databricks.lightning.force.com/lightning/r/0054N000003pA86QAE/view\" id=\"isPasted\" rel=\"noreferrer\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; max-width: 100%; overflow: auto; text-overflow: initial; white-space: normal; display: inline; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\" title=\"Ram Sankarasubramanian\">Ram Sankarasubramanian</a></p></div><h1><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: \"Times New Roman\", Times, serif, -webkit-standard; font-size: 30px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;'><strong>This article explains how parameters can be passed &amp; read in python-task using Jobs API</strong></span></h1><p>&lt;Enter problem text&gt;</p><div class=\"hj-alert-block hj-info-block\" data-controller=\"alert-block\"><a class=\"delete-alert-block fa-trash-alt\" contenteditable=\"false\" data-action=\"alert-block#delete\" href=\"#\">Delete</a><div class=\"alert-message\"><h3 class=\"hj-alert-heading\">Note</h3><p class=\"hj-alert-text\">Insert your text here</p></div></div><h1>Instructions</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>1. Create a python file with the below code to access the python arguments. Save it with the name &quot;</span><a data-aura-rendered-by=\"423:771;a\" href=\"http://sample.py/\" rel=\"noopener\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' target=\"_blank\">sample.py</a><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>&quot; and upload it to DBFS.</span></p><pre data-aura-rendered-by=\"423:771;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">import sys\n\nprint(sys.argv)\nprint(sys.argv[1])\nprint(sys.argv[2])</pre><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>2. The below invokes a new job in job cluster. This can be executed from a notebook in Databricks workspace. Parameters are passed as below (can be seen in the last line of the api request).</span></p><pre data-aura-rendered-by=\"423:771;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">&quot;parameters&quot;: [&quot;sample&quot;,&quot;arg1&quot;,&quot;arg2&quot;,&quot;arg3&quot;]</pre><p><br></p><pre data-aura-rendered-by=\"423:771;a\" id=\"isPasted\" style=\"box-sizing: border-box; overflow: auto; font-family: monospace, monospace; font-size: 14px; background: rgb(239, 239, 239); display: block; padding: var(--lwc-varSpacingXSmall,0.5rem); border: 1px solid rgb(204, 204, 204); color: rgb(24, 24, 24); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">%sh\ncurl --location --request POST &#39;<a href=\"https://cust-success.cloud.databricks.com/api/2.0/jobs/runs/submit'\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://cust-success.cloud.databricks.com/api/2.0/jobs/runs/submit&#39;</a>; \\\n--header &#39;Authorization: Bearer &lt;workspace_token&gt;&#39; \\\n--header &#39;Content-Type: application/json&#39; \\\n--data-raw &#39;{\n\u00a0 &quot;run_name&quot;: &quot;my spark task&quot;,\n\u00a0 &quot;new_cluster&quot;: {\n\u00a0 \u00a0 &quot;spark_version&quot;: &quot;7.3.x-scala2.12&quot;,\n\u00a0 \u00a0 &quot;node_type_id&quot;: &quot;i3.xlarge&quot;,\n\u00a0 \u00a0 &quot;aws_attributes&quot;: {\n\u00a0 \u00a0 \u00a0 &quot;availability&quot;: &quot;ON_DEMAND&quot;\n\u00a0 \u00a0 },\n\u00a0 \u00a0 &quot;num_workers&quot;: 1\n\u00a0 },\n\u00a0 &quot;spark_conf&quot;: {\n\u00a0 \u00a0 \u00a0 \u00a0 &quot;spark.sql.broadcastTimeout&quot;: &quot;600&quot;\n\u00a0 \u00a0 \u00a0 },\n\u00a0 &quot;spark_python_task&quot;: {\n\u00a0 \u00a0 \u00a0 &quot;python_file&quot;: &quot;dbfs:/ram/job_api/py_task/<a href=\"http://sample.py/\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">sample.py</a>&quot;,\n\u00a0 \u00a0 \u00a0 &quot;parameters&quot;: [&quot;sample&quot;,&quot;arg1&quot;,&quot;arg2&quot;,&quot;arg3&quot;]\n\u00a0 \u00a0 }\n}&#39;</pre><p><br></p><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>3. Executing the above, the job gets created in a job cluster and reads &amp; prints the first two arguments passed - &nbsp;sample, arg1 (out of the 4 arguments passed)</span></p><p><br></p><p><br></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: AWS, Azure, GCP\nAuthor:\u00a0ram.sankarasubramanian@databricks.com Owning Team: India + Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000006LhZ2AAK/view?0.source=alohaHeader Last reviewed date: 10-30-2020 by Ram Sankarasubramanian This article explains how parameters can be passed &amp; read in python-task using Jobs API &lt;Enter problem text&gt; Note\nInsert your text here Instructions 1. Create a python file with the below code to access the python arguments. Save it with the name \" sample.py \" and upload it to DBFS. import sys print(sys.argv)\nprint(sys.argv[1])\nprint(sys.argv[2]) 2. The below invokes a new job in job cluster. This can be executed from a notebook in Databricks workspace. Parameters are passed as below (can be seen in the last line of the api request). \"parameters\": [\"sample\",\"arg1\",\"arg2\",\"arg3\"] %sh\ncurl --location --request POST 'https://cust-success.cloud.databricks.com/api/2.0/jobs/runs/submit'; \\\n--header 'Authorization: Bearer &lt;workspace_token&gt;' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n\u00a0 \"run_name\": \"my spark task\",\n\u00a0 \"new_cluster\": {\n\u00a0 \u00a0 \"spark_version\": \"7.3.x-scala2.12\",\n\u00a0 \u00a0 \"node_type_id\": \"i3.xlarge\",\n\u00a0 \u00a0 \"aws_attributes\": {\n\u00a0 \u00a0 \u00a0 \"availability\": \"ON_DEMAND\"\n\u00a0 \u00a0 },\n\u00a0 \u00a0 \"num_workers\": 1\n\u00a0 },\n\u00a0 \"spark_conf\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \"spark.sql.broadcastTimeout\": \"600\"\n\u00a0 \u00a0 \u00a0 },\n\u00a0 \"spark_python_task\": {\n\u00a0 \u00a0 \u00a0 \"python_file\": \"dbfs:/ram/job_api/py_task/sample.py\",\n\u00a0 \u00a0 \u00a0 \"parameters\": [\"sample\",\"arg1\",\"arg2\",\"arg3\"]\n\u00a0 \u00a0 }\n}' 3. Executing the above, the job gets created in a job cluster and reads &amp; prints the first two arguments passed - \u00a0sample, arg1 (out of the 4 arguments passed)", "format": "html", "updated_at": "2022-04-05T05:42:08.305Z"}, "author": {"id": 791114, "email": "ujjawal.kashyap@databricks.com", "name": "ujjawal.kashyap ", "first_name": "ujjawal.kashyap", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T18:22:26.750Z", "updated_at": "2022-11-27T13:49:23.010Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264397, "name": "Jobs (Internal)", "codename": "jobs-internal", "accessibility": 0, "description": "These articles can help you with your Databricks jobs.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2627175, "name": "jobs"}, {"id": 2627176, "name": "python with spark"}], "url": "https://kb.databricks.com/jobs-internal/how-to-pass-read-parameters-in-python-task-using-jobs-api"}, {"id": 1332898, "name": "In DB SQL, text tables created with Hive SerDe is not supported", "views": null, "accessibility": 0, "description": "", "codename": "in-db-sql-text-tables-created-with-hive-serde-is-not-supported", "created_at": "2022-04-05T05:19:22.903Z", "updated_at": "2022-06-23T18:03:52.388Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: <a data-aura-class=\"emailuiFormattedEmail\" data-aura-rendered-by=\"100:770;a\" href=\"mailto:ram.sankarasubramanian@databricks.com\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;'>ram.sankarasubramanian@databricks.com</a></p><p>Owning Team: India + Spark</p><p>Ticket URL: <a data-aura-class=\"uiOutputURL\" data-aura-rendered-by=\"173:772;a\" data-interactive-lib-uid=\"3\" dir=\"ltr\" href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000004xFWoAAM/view?0.source=alohaHeader\" id=\"isPasted\" style='box-sizing: border-box; background-color: rgb(255, 255, 255); color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer; overflow-wrap: break-word; word-break: break-word; font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;' title=\"\">https://databricks.lightning.force.com/lightning/r/Case/5003f000004xFWoAAM/view?0.source=alohaHeader</a></p><p>Last reviewed date: 2-21-2022 by &lt;Name of Tech Reviewer&gt;</p></div><h1>Problem</h1><p data-aura-rendered-by=\"371:772;a\" dir=\"ltr\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>In SQL endpoints, tables that were already created with Hive SerDe do not get executed and fail with the below exception.</p><p data-aura-rendered-by=\"371:772;a\" dir=\"ltr\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='box-sizing: border-box; font-family: \"Courier New\", Courier, monospace;'>Error running query: [42000] [Simba][Hardy] (80) Syntax or semantic analysis error thrown in server while executing query. Error message from server: Error running query: org.apache.spark.sql.AnalysisException: Cannot use this data source. Only csv,json,avro,delta,parquet,orc data sources are supported on SQL Gateway.;; GlobalLimit 1 +- LocalLimit 1 +- Project [date_format(created_utc#22621, YYYY-w (80) (SQLExecDirectW)</span></p><h1>Cause</h1><p data-aura-rendered-by=\"421:772;a\" dir=\"ltr\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>The issue happens because, in SQL endpoints, TEXT data sources are not supported currently. Only CSV, JSON, Avro,delta,parquet,orc data sources are supported in SQL endpoints. Also, the tables created with Hive SerDe get created implicitly with spark format in clusters with Spark 3.0.0. For example, a table created with the below Hive SerDe DDL gets converted in Spark 3.0.0 clusters as below.</p><p data-aura-rendered-by=\"421:772;a\" dir=\"ltr\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><u style=\"box-sizing: border-box;\">Hive SerDe DDL:</u><br style=\"box-sizing: border-box;\"><span style='box-sizing: border-box; font-family: \"Courier New\", Courier, monospace;'>CREATE EXTERNAL TABLE test_table_name<br style=\"box-sizing: border-box;\">(row_number string,<br style=\"box-sizing: border-box;\">token string)<br style=\"box-sizing: border-box;\">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39;<br style=\"box-sizing: border-box;\">collection items terminated by &#39;\\n&#39;<br style=\"box-sizing: border-box;\">LOCATION &#39;dbfs:/user/hive/warehouse/test_table_name/&#39;;</span><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\"><u style=\"box-sizing: border-box;\">DDL in Spark 3.0.0 clusters:</u><br style=\"box-sizing: border-box;\"><span style='box-sizing: border-box; font-family: \"Courier New\", Courier, monospace;'>CREATE TABLE `default`.`test_table_name` ( `row_number` STRING, `token` STRING) USING text LOCATION &#39;dbfs:/user/hive/warehouse/test_table_name&#39; TBLPROPERTIES ( &#39;transient_lastDdlTime&#39; = &#39;1600268441&#39;)</span><br style=\"box-sizing: border-box;\">&nbsp;</p><p data-aura-rendered-by=\"421:772;a\" dir=\"ltr\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>Reference: <a href=\"https://spark.apache.org/docs/latest/sql-migration-guide.html\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://spark.apache.org/docs/latest/sql-migration-guide.html</a></p><p data-aura-rendered-by=\"421:772;a\" dir=\"ltr\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>&quot;In Spark 3.0, SHOW CREATE TABLE always returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use SHOW CREATE TABLE AS SERDE command instead.&quot;</p><h1>Solution</h1><p data-aura-rendered-by=\"471:772;a\" dir=\"ltr\" id=\"isPasted\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'>The solution for this issue is to drop the table and create the table again with Spark DDL using one of the sources which are supported for SQL endpoints (Only CSV, JSON, Avro, delta, parquet, orc data sources are supported in virtual clusters).&nbsp;Once the table is created with the data source supported by virtual clusters, the table will be able to be queried in virtual clusters.</p><p data-aura-rendered-by=\"471:772;a\" dir=\"ltr\" style='box-sizing: border-box; margin: 0px; padding: 0px; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='box-sizing: border-box; font-family: \"Courier New\", Courier, monospace;'>CREATE TABLE `default`.`test_table_name` ( `row_number` STRING, `token` STRING) USING CSV LOCATION &#39;dbfs:/user/hive/warehouse/test_table_name&#39;</span><br style=\"box-sizing: border-box;\"><br style=\"box-sizing: border-box;\">Refer: <a href=\"https://databricks.slack.com/archives/CTV173T6G/p1643739767184509\" rel=\"noopener\" style=\"box-sizing: border-box; background-color: transparent; color: var(--lwc-brandTextLink,rgb(1, 118, 211)); text-decoration: none; transition: color 0.1s linear 0s; cursor: pointer;\" target=\"_blank\">https://databricks.slack.com/archives/CTV173T6G/p1643739767184509</a></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: AWS, Azure, GCP\nAuthor: ram.sankarasubramanian@databricks.com Owning Team: India + Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000004xFWoAAM/view?0.source=alohaHeader Last reviewed date: 2-21-2022 by &lt;Name of Tech Reviewer&gt; Problem In SQL endpoints, tables that were already created with Hive SerDe do not get executed and fail with the below exception. Error running query: [42000] [Simba][Hardy] (80) Syntax or semantic analysis error thrown in server while executing query. Error message from server: Error running query: org.apache.spark.sql.AnalysisException: Cannot use this data source. Only csv,json,avro,delta,parquet,orc data sources are supported on SQL Gateway.;; GlobalLimit 1 +- LocalLimit 1 +- Project [date_format(created_utc#22621, YYYY-w (80) (SQLExecDirectW) Cause The issue happens because, in SQL endpoints, TEXT data sources are not supported currently. Only CSV, JSON, Avro,delta,parquet,orc data sources are supported in SQL endpoints. Also, the tables created with Hive SerDe get created implicitly with spark format in clusters with Spark 3.0.0. For example, a table created with the below Hive SerDe DDL gets converted in Spark 3.0.0 clusters as below. Hive SerDe DDL: CREATE EXTERNAL TABLE test_table_name(row_number string,token string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','collection items terminated by '\\n'LOCATION 'dbfs:/user/hive/warehouse/test_table_name/'; DDL in Spark 3.0.0 clusters: CREATE TABLE `default`.`test_table_name` ( `row_number` STRING, `token` STRING) USING text LOCATION 'dbfs:/user/hive/warehouse/test_table_name' TBLPROPERTIES ( 'transient_lastDdlTime' = '1600268441') \u00a0 Reference: https://spark.apache.org/docs/latest/sql-migration-guide.html \"In Spark 3.0, SHOW CREATE TABLE always returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use SHOW CREATE TABLE AS SERDE command instead.\" Solution The solution for this issue is to drop the table and create the table again with Spark DDL using one of the sources which are supported for SQL endpoints (Only CSV, JSON, Avro, delta, parquet, orc data sources are supported in virtual clusters).\u00a0Once the table is created with the data source supported by virtual clusters, the table will be able to be queried in virtual clusters. CREATE TABLE `default`.`test_table_name` ( `row_number` STRING, `token` STRING) USING CSV LOCATION 'dbfs:/user/hive/warehouse/test_table_name' Refer: https://databricks.slack.com/archives/CTV173T6G/p1643739767184509", "format": "html", "updated_at": "2022-04-05T05:28:53.597Z"}, "author": {"id": 791114, "email": "ujjawal.kashyap@databricks.com", "name": "ujjawal.kashyap ", "first_name": "ujjawal.kashyap", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T18:22:26.750Z", "updated_at": "2022-11-27T13:49:23.010Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264410, "name": "SQL with Apache Spark (Internal)", "codename": "sql-internal", "accessibility": 0, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2627174, "name": "dbsql"}, {"id": 2627173, "name": "sql"}], "url": "https://kb.databricks.com/sql-internal/in-db-sql-text-tables-created-with-hive-serde-is-not-supported"}, {"id": 1332897, "name": "In DB SQL, text tables created with Hive SerDe is not supported", "views": null, "accessibility": 0, "description": "", "codename": "in-db-sql-text-tables-created-with-hive-serde-is-not-supported", "created_at": "2022-04-05T05:13:47.618Z", "updated_at": "2022-09-22T16:30:35.562Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\">\n  <div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\">\n    <div>\n      <i class=\"fal fa-user-lock\"></i> Restricted Content\n    </div>\n    <a class=\"open-access\" data-action=\"click-&gt;internal-block#toggleSettings\" href=\"#\">\n    <i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i>\n  </div>\n  \n    <p id=\"isPasted\" class=\"internal-block-body\">DBR Version: &lt;list all applicable DBR versions&gt;</p>\n<p class=\"internal-block-body\"><i class=\"helpjuice-thread\" data-id=\"1803717967-kiim4\">Category:</i>\u00a0 Spark with SQL</p>\n<p class=\"internal-block-body\">Cloud Version: AWS, Azure, GCP</p>\n<p class=\"internal-block-body\">Author:\u00a0</p>\n<p class=\"internal-block-body\">Owning Team: India Spark</p>\n<p class=\"internal-block-body\">Ticket URL:\u00a0</p>\n<p class=\"internal-block-body\">Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by Vikas Aravabhumi</p>\n<p class=\"internal-block-body\">Review status: In-Progress</p>\n  \n</div>\n\n<h1 data-toc=\"true\" id=\"problem-0\">Problem</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>When streaming to a Delta table, both repartitioning on the partition column and optimized write can help to avoid small files.</span><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Which is recommended between Delta Optimized Write vs Repartitioning?</span></p><h1 data-toc=\"true\" id=\"cause-1\">Cause</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Repartitioning vs Optimized Write both can help to avoid small files, but there is no clear documented recommendation for this. This article provides a recommendation between Repartitioning vs Optimized Write.</span></p><h1 data-toc=\"true\" id=\"solution-2\">Solution</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Yes, Optimized write is recommended over repartitioning for the below reasons.</span><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>* The key part of Optimized Writes is that it is an adaptive shuffle. If you have a streaming ingest use case and input data rates change over time, the adaptive shuffle will adjust itself accordingly to the incoming data rates across micro-batches. If you have code snippets where you coalesce(n) or repartition(n) just before you write out your stream, you can remove those lines.</span><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>* Databricks dynamically optimizes Spark partition sizes based on the actual data and attempts to write out 128 MB files for each table partition. This is an approximate size and can vary depending on dataset characteristics.</span><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>* Repartitioning on a partition column can result in partitions with varying sizes when there is data skew, this will result in not so optimized file sizes.</span><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><br style='box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;'><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>The bottom line is that Optimize write is no different than Repartitioning, To simple put Optimized write is a repartition where we pick the number of partitions in an adaptive and optimal way on the fly based on data.</span></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt; Category:\u00a0 Spark with SQL\nCloud Version: AWS, Azure, GCP\nAuthor:\u00a0\nOwning Team: India Spark\nTicket URL:\u00a0\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by Vikas Aravabhumi\nReview status: In-Progress Problem When streaming to a Delta table, both repartitioning on the partition column and optimized write can help to avoid small files. Which is recommended between Delta Optimized Write vs Repartitioning? Cause Repartitioning vs Optimized Write both can help to avoid small files, but there is no clear documented recommendation for this. This article provides a recommendation between Repartitioning vs Optimized Write. Solution Yes, Optimized write is recommended over repartitioning for the below reasons. * The key part of Optimized Writes is that it is an adaptive shuffle. If you have a streaming ingest use case and input data rates change over time, the adaptive shuffle will adjust itself accordingly to the incoming data rates across micro-batches. If you have code snippets where you coalesce(n) or repartition(n) just before you write out your stream, you can remove those lines. * Databricks dynamically optimizes Spark partition sizes based on the actual data and attempts to write out 128 MB files for each table partition. This is an approximate size and can vary depending on dataset characteristics. * Repartitioning on a partition column can result in partitions with varying sizes when there is data skew, this will result in not so optimized file sizes. The bottom line is that Optimize write is no different than Repartitioning, To simple put Optimized write is a repartition where we pick the number of partitions in an adaptive and optimal way on the fly based on data.", "format": "html", "updated_at": "2022-09-22T16:30:35.553Z"}, "author": {"id": 791114, "email": "ujjawal.kashyap@databricks.com", "name": "ujjawal.kashyap ", "first_name": "ujjawal.kashyap", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T18:22:26.750Z", "updated_at": "2022-11-27T13:49:23.010Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 291926, "name": "Uncategorized (Internal)", "codename": "uncategorized-internal", "accessibility": 0, "description": "Articles that need to have a category assigned.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/uncategorized-internal/in-db-sql-text-tables-created-with-hive-serde-is-not-supported"}, {"id": 1332895, "name": "Merge requires consistent schema between source and target except the new column", "views": null, "accessibility": 0, "description": "", "codename": "1332895-untitled-article", "created_at": "2022-04-05T05:13:17.906Z", "updated_at": "2022-04-07T13:24:16.883Z", "next_expiration_on": null, "published": false, "answer": {"body": "", "body_txt": "", "format": "html", "updated_at": "2022-04-05T05:13:17.916Z"}, "author": {"id": 791114, "email": "ujjawal.kashyap@databricks.com", "name": "ujjawal.kashyap ", "first_name": "ujjawal.kashyap", "last_name": "", "role_id": "draft_writer", "created_at": "2022-01-27T18:22:26.750Z", "updated_at": "2022-11-27T13:49:23.010Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264396, "name": "Delta Lake (Internal)", "codename": "delta-internal", "accessibility": 0, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [], "url": "https://kb.databricks.com/delta-internal/1332895-untitled-article"}, {"id": 1332871, "name": "Snowflake library issue on DBR 7.3", "views": null, "accessibility": 0, "description": "", "codename": "snowflake-library-issue-on-dbr-73", "created_at": "2022-04-05T01:10:01.367Z", "updated_at": "2022-04-05T01:13:22.246Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: AWS</p><p>Author: jose.gonzalezmunoz@databricks.com</p><p>Owning Team: US + Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f000006LxX7AAK/view?0.source=alohaHeader\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5003f000006LxX7AAK/view</a></p><p>Last reviewed date: 10/30/2020 by &lt;Name of Tech Reviewer&gt;</p></div><h1>Problem</h1><p id=\"isPasted\">Customer migrate from DBR 6.6 to DBR 7.3.</p><p>the job was giving the following error:</p><p>org.apache.spark.sql.execution.OutOfMemorySparkException: Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=4294967296.</p><p>Customer was reading data from Snowflake</p><h1>Cause</h1><p id=\"isPasted\">The customer was using an old Snowflake library.</p><p>The library was for Spark 2.4 which is DBR 6.x+ and DBR 5.5</p><p>The new library is compatible with Spark 3, which is DBR 7.x+</p><p>old library version: net.snowflake:spark-snowflake_2.11:2.4.8</p><p>new library version: net.snowflake:spark-snowflake_2.12:2.8.2-spark_3.0</p><h1>Solution</h1><p>There is no need to install the Snowflake library using Maven. This library is already installed at the cluster level. Here is the reference https://docs.databricks.com/release-notes/runtime/7.3.html#installed-java-and-scala-libraries-scala-212-cluster-version</p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: AWS\nAuthor: jose.gonzalezmunoz@databricks.com\nOwning Team: US + Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f000006LxX7AAK/view Last reviewed date: 10/30/2020 by &lt;Name of Tech Reviewer&gt; Problem Customer migrate from DBR 6.6 to DBR 7.3. the job was giving the following error: org.apache.spark.sql.execution.OutOfMemorySparkException: Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=4294967296. Customer was reading data from Snowflake Cause The customer was using an old Snowflake library. The library was for Spark 2.4 which is DBR 6.x+ and DBR 5.5 The new library is compatible with Spark 3, which is DBR 7.x+ old library version: net.snowflake:spark-snowflake_2.11:2.4.8 new library version: net.snowflake:spark-snowflake_2.12:2.8.2-spark_3.0 Solution There is no need to install the Snowflake library using Maven. This library is already installed at the cluster level. Here is the reference https://docs.databricks.com/release-notes/runtime/7.3.html#installed-java-and-scala-libraries-scala-212-cluster-version", "format": "html", "updated_at": "2022-04-05T01:13:06.384Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264392, "name": "Data sources (Internal)", "codename": "data-sources-internal", "accessibility": 0, "description": "These articles can help you manage your data source integrations.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2625537, "name": "aws"}, {"id": 3269590, "name": "snowflake library"}], "url": "https://kb.databricks.com/data-sources-internal/snowflake-library-issue-on-dbr-73"}, {"id": 1332869, "name": "AWS S3 limit threshold config", "views": null, "accessibility": 0, "description": "", "codename": "aws-s3-limit-threshold-config", "created_at": "2022-04-05T01:04:20.739Z", "updated_at": "2022-04-05T01:07:07.303Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: AWS</p><p>Author: jose.gonzalezmunoz@databricks.com</p><p>Owning Team: US + Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f0000017StdAAE/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5003f0000017StdAAE/view</a></p><p>Last reviewed date: 10/19/2020 by &lt;Name of Tech Reviewer&gt;</p></div><h1>Problem</h1><p id=\"isPasted\">A Databricks notebook returns the following error when writing data to S3:</p><p>Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: We encountered an internal error. Please try again.; request: GET ....; Request ID: xxxx, Extended Request ID: xxxx, Cloud Provider: AWS, Instance ID: xxxx (Service: Amazon S3; Status Code: 500; Error Code: InternalError)</p><h1>Cause</h1><p>This error happens when you are writing data back to S3 and you are using a large cluster or are making too many request to S3. You might be hitting your max limit of requests on S3.</p><h1>Solution</h1><p id=\"isPasted\">I would recommend to add these setting to your cluster to try to avoid this issue in the future.</p><p>Add the following Spark settings to your cluster:</p><p>[How big (in bytes) to split upload or copy operations up into. Default is 100 MB]</p><p>- &quot;spark.hadoop.fs.s3a.multipart.size 504857600&quot;</p><p>[How big (in bytes) to split upload or copy operations up into]</p><p>- &quot;spark.hadoop.fs.s3a.multipart.threshold 2097152000&quot;</p><p>[Controls the maximum number of simultaneous connections to S3. Default 250]</p><p>- &quot;spark.hadoop.fs.s3a.connection.maximum 500&quot;</p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: AWS\nAuthor: jose.gonzalezmunoz@databricks.com\nOwning Team: US + Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f0000017StdAAE/view Last reviewed date: 10/19/2020 by &lt;Name of Tech Reviewer&gt; Problem A Databricks notebook returns the following error when writing data to S3: Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: We encountered an internal error. Please try again.; request: GET ....; Request ID: xxxx, Extended Request ID: xxxx, Cloud Provider: AWS, Instance ID: xxxx (Service: Amazon S3; Status Code: 500; Error Code: InternalError) Cause This error happens when you are writing data back to S3 and you are using a large cluster or are making too many request to S3. You might be hitting your max limit of requests on S3. Solution I would recommend to add these setting to your cluster to try to avoid this issue in the future. Add the following Spark settings to your cluster: [How big (in bytes) to split upload or copy operations up into. Default is 100 MB] - \"spark.hadoop.fs.s3a.multipart.size 504857600\" [How big (in bytes) to split upload or copy operations up into] - \"spark.hadoop.fs.s3a.multipart.threshold 2097152000\" [Controls the maximum number of simultaneous connections to S3. Default 250] - \"spark.hadoop.fs.s3a.connection.maximum 500\"", "format": "html", "updated_at": "2022-04-05T01:06:49.231Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264393, "name": "Databricks File System (Internal)", "codename": "dbfs-internal", "accessibility": 0, "description": "These articles can help you with the Databricks File System (DBFS).", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2625536, "name": "aws"}], "url": "https://kb.databricks.com/dbfs-internal/aws-s3-limit-threshold-config"}, {"id": 1332868, "name": "Out of memory error when Optimizing and Zordering", "views": null, "accessibility": 0, "description": "", "codename": "out-of-memory-error-when-optimizing-and-zordering", "created_at": "2022-04-05T00:59:15.289Z", "updated_at": "2022-04-05T01:02:37.793Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: AWS, Azure, GCP</p><p>Author: jose.gonzalezmunoz@databricks.com</p><p>Owning Team: US + Spark</p><p>Ticket URL: <a href=\"https://databricks.lightning.force.com/lightning/r/Case/5003f0000017WO7AAM/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5003f0000017WO7AAM/view</a></p><p>Last reviewed date: 10/19/2020 by &lt;Name of Tech Reviewer&gt;</p></div><h1>Problem</h1><p id=\"isPasted\">A Databricks notebook returns the following error when executing optimize and/or zordering:</p><p>io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 16777216 byte(s) of direct memory (used: 8271167488, max: 8271167488)</p><p>at io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:725)</p><p>at io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:680)</p><h1>Cause</h1><p>This issue happens when you try to execute an optimize and/or zordering job using a small cluster with not enough memory resources.</p><h1>Solution</h1><p id=\"isPasted\">Upgrade to the latest DBR version and bump-up your cluster&#39;s memory.</p><p>Generally the price/performance ratio is better with faster machines. You could get the same work done with half the number of nodes, but twice the specs, for cheaper. Use a cluster with a larger worker&#39;s memory will solve this error.</p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: AWS, Azure, GCP\nAuthor: jose.gonzalezmunoz@databricks.com\nOwning Team: US + Spark\nTicket URL: https://databricks.lightning.force.com/lightning/r/Case/5003f0000017WO7AAM/view Last reviewed date: 10/19/2020 by &lt;Name of Tech Reviewer&gt; Problem A Databricks notebook returns the following error when executing optimize and/or zordering: io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 16777216 byte(s) of direct memory (used: 8271167488, max: 8271167488) at io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:725) at io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:680) Cause This issue happens when you try to execute an optimize and/or zordering job using a small cluster with not enough memory resources. Solution Upgrade to the latest DBR version and bump-up your cluster's memory. Generally the price/performance ratio is better with faster machines. You could get the same work done with half the number of nodes, but twice the specs, for cheaper. Use a cluster with a larger worker's memory will solve this error.", "format": "html", "updated_at": "2022-04-05T01:01:51.744Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264396, "name": "Delta Lake (Internal)", "codename": "delta-internal", "accessibility": 0, "description": "These articles can help you with Delta Lake.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2625533, "name": "aws"}, {"id": 2625534, "name": "azure"}, {"id": 2625535, "name": "gcp"}], "url": "https://kb.databricks.com/delta-internal/out-of-memory-error-when-optimizing-and-zordering"}, {"id": 1332867, "name": "Exception thrown in Future.get", "views": null, "accessibility": 0, "description": "", "codename": "exception-thrown-in-futureget", "created_at": "2022-04-05T00:52:19.770Z", "updated_at": "2022-04-05T00:57:21.407Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;</p><p>Author: &lt;Databricks email of author&gt;</p><p>Owning Team: &lt;Region + Platform/Spark&gt;</p><p>Ticket URL: &lt;Link to original Salesforce or Jira ticket&gt;</p><p>Last reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt;</p></div><h1>Problem</h1><p id=\"isPasted\">A Databricks notebook returns the following error when doing a join or reading a temp table:</p><p>com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.SparkException: Exception thrown in Future.get:</p><p>at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:195)</p><p>at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:391)</p><p>at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:167)</p><p>at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:155)</p><p>at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:187)</p><p>at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</p><p>at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:183)</p><h1>Cause</h1><p>When you are tying to query a temp table or trying to join temp tables, Spark will go back and re-create the temp table again and then apply the join. If there is a timeout when trying to recreate the temp tables, then you will get the error message.</p><h1>Solution</h1><p><span id=\"isPasted\" style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Create a permanent, physical table then apply the join. By creating a permanent table, you will create a Spark action and will read the whole data and write it to a location.</span><br data-aura-rendered-by=\"471:770;a\" style=\"box-sizing: border-box; color: rgb(24, 24, 24); font-family: -apple-system, &quot;system-ui&quot;, &quot;Segoe UI&quot;, Roboto, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span style='color: rgb(24, 24, 24); font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;'>Another workaround, should be to disable the broadcast join to avoid getting the timeout exception.</span></p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: &lt;list applicable clouds (ex. AWS, Azure, GCP)&gt;\nAuthor: &lt;Databricks email of author&gt;\nOwning Team: &lt;Region + Platform/Spark&gt;\nTicket URL: &lt;Link to original Salesforce or Jira ticket&gt;\nLast reviewed date: &lt;Date the KB was last reviewed for accuracy&gt; by &lt;Name of Tech Reviewer&gt; Problem A Databricks notebook returns the following error when doing a join or reading a temp table: com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.SparkException: Exception thrown in Future.get: at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:195) at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:391) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:167) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:155) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:187) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:183) Cause When you are tying to query a temp table or trying to join temp tables, Spark will go back and re-create the temp table again and then apply the join. If there is a timeout when trying to recreate the temp tables, then you will get the error message. Solution Create a permanent, physical table then apply the join. By creating a permanent table, you will create a Spark action and will read the whole data and write it to a location. Another workaround, should be to disable the broadcast join to avoid getting the timeout exception.", "format": "html", "updated_at": "2022-04-05T00:53:31.291Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264410, "name": "SQL with Apache Spark (Internal)", "codename": "sql-internal", "accessibility": 0, "description": "These articles can help you to use SQL with Apache Spark.", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2625525, "name": "aws"}, {"id": 2625526, "name": "azure"}, {"id": 2625527, "name": "gcp"}], "url": "https://kb.databricks.com/sql-internal/exception-thrown-in-futureget"}, {"id": 1332855, "name": "Data Lag in Spark Streaming-SQS Pipeline", "views": 1, "accessibility": 0, "description": "", "codename": "1332855-problem-cause-solution", "created_at": "2022-04-05T00:30:55.718Z", "updated_at": "2022-04-05T00:48:07.702Z", "next_expiration_on": null, "published": false, "answer": {"body": "<div class=\"hj-internal-block\" data-controller=\"internal-block\" data-internal-block-id=\"f31b714e372f8\" data-permitted-groups=\"\" data-permitted-users=\"\"><div class=\"header\" contenteditable=\"false\" title=\"You are viewing this restricted content as your user/group has been included\"><div><i class=\"fal fa-user-lock\"></i> Restricted Content</div><a class=\"open-access\" data-action=\"click->internal-block#toggleSettings\" href=\"#\"><i class=\"fas fa-cog\"></i></a> <i class=\"far fa-trash-alt\"></i></div><p>DBR Version: &lt;list all applicable DBR versions&gt;</p><p>Cloud Version: AWS</p><p>Author: &lt;Databricks email of author&gt;</p><p>Owning Team: &lt;Region + Platform/Spark&gt;</p><p>Ticket URL:&nbsp;<a href=\"https://databricks.lightning.force.com/lightning/r/Case/5004N00000luV3UQAU/view\" id=\"isPasted\">https://databricks.lightning.force.com/lightning/r/Case/5004N00000luV3UQAU/view</a></p><p>Last reviewed date: 6/5/2020 2:44 PM by &lt;Name of Tech Reviewer&gt;</p></div><h1>Problem</h1><p id=\"isPasted\">A streaming job returns the following error:</p><p>java.lang.OutOfMemoryError: GC overhead limit exceeded</p><p>at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:174)</p><p>at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45)</p><h1>Cause</h1><p id=\"isPasted\">This error happens when you are running a streaming job that is not able to process all the incoming data. SQS will send the data at a constant rate, but you might now have enough computing power to process all the input records.</p><p>It will create extra events for the next micro batch execution. It will reach a point when you will go out of memory.</p><p>It seem like &quot;getOffset&quot; from the microbatch metrics keeps increasing. This measures the time used to retrieve offsets for new data to process for each of defined sources.</p><p>It seems like the old events being fetched takes extra time because the deletion rate is not fast enough to keep up with new files coming in.</p><h1>Solution</h1><p id=\"isPasted\">We have a flag in the SQS connector that defines the number of deletions that should be running in parallel when reading events from SQS.&nbsp;</p><p>DELETION_PARALLELISM_DEFAULT is the flag and the default value is 8. We need to increase this value to be able to avoid lag. We will change it from 8 to 128.</p><p>Add the following setting to your SQS read stream.</p><p>.option(&quot;deletionParallelism&quot;, &quot;128&quot;)</p>", "body_txt": "Restricted Content DBR Version: &lt;list all applicable DBR versions&gt;\nCloud Version: AWS\nAuthor: &lt;Databricks email of author&gt;\nOwning Team: &lt;Region + Platform/Spark&gt;\nTicket URL:\u00a0https://databricks.lightning.force.com/lightning/r/Case/5004N00000luV3UQAU/view Last reviewed date: 6/5/2020 2:44 PM by &lt;Name of Tech Reviewer&gt; Problem A streaming job returns the following error: java.lang.OutOfMemoryError: GC overhead limit exceeded at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:174) at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45) Cause This error happens when you are running a streaming job that is not able to process all the incoming data. SQS will send the data at a constant rate, but you might now have enough computing power to process all the input records. It will create extra events for the next micro batch execution. It will reach a point when you will go out of memory. It seem like \"getOffset\" from the microbatch metrics keeps increasing. This measures the time used to retrieve offsets for new data to process for each of defined sources. It seems like the old events being fetched takes extra time because the deletion rate is not fast enough to keep up with new files coming in. Solution We have a flag in the SQS connector that defines the number of deletions that should be running in parallel when reading events from SQS.\u00a0 DELETION_PARALLELISM_DEFAULT is the flag and the default value is 8. We need to increase this value to be able to avoid lag. We will change it from 8 to 128. Add the following setting to your SQS read stream. .option(\"deletionParallelism\", \"128\")", "format": "html", "updated_at": "2022-04-05T00:41:08.077Z"}, "author": {"id": 790019, "email": "jose.gonzalezmunoz@databricks.com", "name": "Jose Gonzalez", "first_name": "Jose", "last_name": "Gonzalez", "role_id": "draft_writer", "created_at": "2022-01-26T16:53:10.488Z", "updated_at": "2023-04-13T21:47:45.638Z", "groups": [{"id": 7953, "name": "costCenter.701-Support"}]}, "category": {"id": 264405, "name": "Streaming (Internal)", "codename": "streaming-internal", "accessibility": 0, "description": "These articles can help you with Structured Streaming and Spark Streaming (the legacy Apache Spark streaming feature).", "icon": "", "image_url": null}, "hierarchy": [{"id": 264411, "name": "KB (Internal)", "codename": "kb-internal", "accessibility": 0, "description": "These KB entries are only viewable by Databricks employees.", "icon": "", "image_url": null}], "keywords": [{"id": 2625521, "name": "aws"}], "url": "https://kb.databricks.com/streaming-internal/1332855-problem-cause-solution"}]